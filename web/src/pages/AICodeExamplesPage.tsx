import React, { useState } from 'react';
import { useNavigate } from 'react-router-dom';
import {
  Home,
  ChevronRight,
  Code2,
  Copy,
  Check,
  Terminal,
  MessageSquare,
  Database,
  Zap,
  Play,
  FileText,
  Bot,
  Sparkles,
  Rocket,
  Briefcase
} from 'lucide-react';
import { useLanguageStore } from '../stores/languageStore';
import { ColabButton, StackBlitzButton } from '../components/CodeRunner';

type Category = 'langchain' | 'ollama' | 'openai' | 'claude' | 'agent' | 'rag' | 'prompts' | 'cases' | 'deploy';

interface CodeExample {
  id: string;
  title: { zh: string; ja: string };
  description: { zh: string; ja: string };
  code: string;
  language: string;
  difficulty: 'beginner' | 'intermediate' | 'advanced';
  tags: string[];
}

const codeExamples: Record<Category, {
  name: { zh: string; ja: string };
  description: { zh: string; ja: string };
  icon: React.ElementType;
  gradient: string;
  examples: CodeExample[];
}> = {
  langchain: {
    name: { zh: 'LangChain åŸºç¡€', ja: 'LangChain åŸºç¤' },
    description: { zh: 'LangChain æ ¸å¿ƒæ¦‚å¿µä¸ä½¿ç”¨æ–¹æ³•', ja: 'LangChain ã®ã‚³ã‚¢æ¦‚å¿µã¨ä½¿ã„æ–¹' },
    icon: Zap,
    gradient: 'from-green-500 to-emerald-600',
    examples: [
      {
        id: 'lc-1',
        title: { zh: 'ç®€å•å¯¹è¯é“¾', ja: 'ã‚·ãƒ³ãƒ—ãƒ«ãªä¼šè©±ãƒã‚§ãƒ¼ãƒ³' },
        description: { zh: 'ä½¿ç”¨ LangChain åˆ›å»ºåŸºæœ¬çš„å¯¹è¯é“¾', ja: 'LangChain ã§åŸºæœ¬çš„ãªä¼šè©±ãƒã‚§ãƒ¼ãƒ³ã‚’ä½œæˆ' },
        code: `# LangChain ç®€å•å¯¹è¯ç¤ºä¾‹
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# åˆå§‹åŒ–æ¨¡å‹
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.7)

# åˆ›å»ºæç¤ºæ¨¡æ¿
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ï¼Œç”¨ç®€æ´çš„è¯­è¨€å›ç­”é—®é¢˜ã€‚"),
    ("human", "{input}")
])

# åˆ›å»ºé“¾
chain = prompt | llm | StrOutputParser()

# è¿è¡Œ
response = chain.invoke({"input": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"})
print(response)`,
        language: 'python',
        difficulty: 'beginner',
        tags: ['LangChain', 'Chat', 'Chain']
      },
      {
        id: 'lc-2',
        title: { zh: 'å¸¦è®°å¿†çš„å¯¹è¯', ja: 'ãƒ¡ãƒ¢ãƒªä»˜ãä¼šè©±' },
        description: { zh: 'ä½¿ç”¨ ConversationBufferMemory ä¿æŒä¸Šä¸‹æ–‡', ja: 'ConversationBufferMemory ã§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿æŒ' },
        code: `# LangChain å¸¦è®°å¿†å¯¹è¯ç¤ºä¾‹
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

# åˆå§‹åŒ–æ¨¡å‹å’Œè®°å¿†
llm = ChatOpenAI(model="gpt-4o-mini")
memory = ConversationBufferMemory()

# åˆ›å»ºå¯¹è¯é“¾
conversation = ConversationChain(
    llm=llm,
    memory=memory,
    verbose=True  # æ˜¾ç¤ºä¸­é—´è¿‡ç¨‹
)

# å¤šè½®å¯¹è¯
print(conversation.predict(input="æˆ‘å«å°æ˜"))
print(conversation.predict(input="æˆ‘åˆšæ‰è¯´æˆ‘å«ä»€ä¹ˆï¼Ÿ"))
# è¾“å‡ºï¼šä½ åˆšæ‰è¯´ä½ å«å°æ˜ã€‚

# æŸ¥çœ‹è®°å¿†å†…å®¹
print(memory.buffer)`,
        language: 'python',
        difficulty: 'intermediate',
        tags: ['LangChain', 'Memory', 'Conversation']
      },
      {
        id: 'lc-3',
        title: { zh: 'LCEL è¡¨è¾¾å¼', ja: 'LCEL å¼' },
        description: { zh: 'LangChain Expression Language é“¾å¼è°ƒç”¨', ja: 'LangChain Expression Language ã®ãƒã‚§ãƒ¼ãƒ³å‘¼ã³å‡ºã—' },
        code: `# LCEL (LangChain Expression Language) ç¤ºä¾‹
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

llm = ChatOpenAI(model="gpt-4o-mini")

# ä½¿ç”¨ LCEL åˆ›å»ºå¤æ‚é“¾
template = """
æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚

ä¸Šä¸‹æ–‡ï¼š{context}

é—®é¢˜ï¼š{question}

å›ç­”ï¼š"""

prompt = ChatPromptTemplate.from_template(template)

# LCEL é“¾å¼è¯­æ³•
chain = (
    {"context": RunnablePassthrough(), "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# æ‰¹é‡å¤„ç†
questions = ["ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ", "ç¥ç»ç½‘ç»œå¦‚ä½•å·¥ä½œï¼Ÿ"]
results = chain.batch([
    {"context": "AIåŸºç¡€çŸ¥è¯†", "question": q}
    for q in questions
])`,
        language: 'python',
        difficulty: 'intermediate',
        tags: ['LangChain', 'LCEL', 'Batch']
      }
    ]
  },
  ollama: {
    name: { zh: 'Ollama æœ¬åœ°éƒ¨ç½²', ja: 'Ollama ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ—ãƒ­ã‚¤' },
    description: { zh: 'åœ¨æœ¬åœ°è¿è¡Œå¼€æºå¤§è¯­è¨€æ¨¡å‹', ja: 'ãƒ­ãƒ¼ã‚«ãƒ«ã§ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹LLMã‚’å®Ÿè¡Œ' },
    icon: Terminal,
    gradient: 'from-slate-600 to-zinc-700',
    examples: [
      {
        id: 'ol-1',
        title: { zh: 'å®‰è£…å’Œè¿è¡Œ', ja: 'ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨å®Ÿè¡Œ' },
        description: { zh: 'Ollama åŸºç¡€å®‰è£…å’Œæ¨¡å‹ä¸‹è½½', ja: 'Ollama ã®åŸºæœ¬ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰' },
        code: `# Ollama å®‰è£…å’Œä½¿ç”¨

# 1. å®‰è£… Ollama (macOS/Linux)
curl -fsSL https://ollama.com/install.sh | sh

# 2. å¯åŠ¨æœåŠ¡
ollama serve

# 3. ä¸‹è½½æ¨¡å‹ï¼ˆå¦å¼€ç»ˆç«¯ï¼‰
ollama pull llama3.2        # ä¸‹è½½ Llama 3.2 (3B)
ollama pull qwen2.5:7b      # ä¸‹è½½ Qwen 2.5 (7B)
ollama pull deepseek-r1:8b  # ä¸‹è½½ DeepSeek-R1 (8B)

# 4. å‘½ä»¤è¡Œå¯¹è¯
ollama run llama3.2

# 5. æŸ¥çœ‹å·²ä¸‹è½½æ¨¡å‹
ollama list

# 6. æŸ¥çœ‹æ¨¡å‹ä¿¡æ¯
ollama show llama3.2`,
        language: 'bash',
        difficulty: 'beginner',
        tags: ['Ollama', 'Installation', 'CLI']
      },
      {
        id: 'ol-2',
        title: { zh: 'Python API è°ƒç”¨', ja: 'Python API å‘¼ã³å‡ºã—' },
        description: { zh: 'ä½¿ç”¨ Python è°ƒç”¨ Ollama API', ja: 'Python ã§ Ollama API ã‚’å‘¼ã³å‡ºã™' },
        code: `# Ollama Python API ç¤ºä¾‹
import requests
import json

# æ–¹æ³•1ï¼šä½¿ç”¨ requests ç›´æ¥è°ƒç”¨ API
def chat_with_ollama(message: str, model: str = "llama3.2"):
    response = requests.post(
        "http://localhost:11434/api/chat",
        json={
            "model": model,
            "messages": [{"role": "user", "content": message}],
            "stream": False
        }
    )
    return response.json()["message"]["content"]

# è°ƒç”¨
result = chat_with_ollama("ç”¨ä¸€å¥è¯è§£é‡Šä»€ä¹ˆæ˜¯AI")
print(result)

# æ–¹æ³•2ï¼šä½¿ç”¨å®˜æ–¹ ollama åº“
# pip install ollama
import ollama

response = ollama.chat(
    model='llama3.2',
    messages=[
        {'role': 'user', 'content': 'å†™ä¸€é¦–å…³äºç¼–ç¨‹çš„çŸ­è¯—'}
    ]
)
print(response['message']['content'])

# æµå¼è¾“å‡º
for chunk in ollama.chat(
    model='llama3.2',
    messages=[{'role': 'user', 'content': 'è®²ä¸ªç¬‘è¯'}],
    stream=True
):
    print(chunk['message']['content'], end='', flush=True)`,
        language: 'python',
        difficulty: 'beginner',
        tags: ['Ollama', 'Python', 'API']
      },
      {
        id: 'ol-3',
        title: { zh: 'LangChain + Ollama', ja: 'LangChain + Ollama' },
        description: { zh: 'ç”¨ LangChain è¿æ¥æœ¬åœ° Ollama æ¨¡å‹', ja: 'LangChain ã§ãƒ­ãƒ¼ã‚«ãƒ« Ollama ãƒ¢ãƒ‡ãƒ«ã«æ¥ç¶š' },
        code: `# LangChain + Ollama é›†æˆ
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# è¿æ¥æœ¬åœ° Ollama
llm = ChatOllama(
    model="llama3.2",
    temperature=0.7,
    base_url="http://localhost:11434"  # é»˜è®¤åœ°å€
)

# åˆ›å»ºç¿»è¯‘é“¾
prompt = ChatPromptTemplate.from_template(
    "å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆ{target_lang}ï¼š\\n\\n{text}"
)

chain = prompt | llm | StrOutputParser()

# ä½¿ç”¨
result = chain.invoke({
    "target_lang": "æ—¥è¯­",
    "text": "äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œ"
})
print(result)

# æ‰¹é‡ç¿»è¯‘
texts = ["ä½ å¥½", "è°¢è°¢", "å†è§"]
results = chain.batch([
    {"target_lang": "è‹±è¯­", "text": t} for t in texts
])`,
        language: 'python',
        difficulty: 'intermediate',
        tags: ['LangChain', 'Ollama', 'Integration']
      }
    ]
  },
  openai: {
    name: { zh: 'OpenAI API', ja: 'OpenAI API' },
    description: { zh: 'OpenAI GPT ç³»åˆ— API ä½¿ç”¨', ja: 'OpenAI GPT ã‚·ãƒªãƒ¼ã‚º API ã®ä½¿ç”¨' },
    icon: MessageSquare,
    gradient: 'from-teal-500 to-cyan-600',
    examples: [
      {
        id: 'oa-1',
        title: { zh: 'åŸºç¡€å¯¹è¯', ja: 'åŸºæœ¬å¯¾è©±' },
        description: { zh: 'OpenAI Chat Completions API åŸºç¡€ç”¨æ³•', ja: 'OpenAI Chat Completions API ã®åŸºæœ¬çš„ãªä½¿ã„æ–¹' },
        code: `# OpenAI åŸºç¡€å¯¹è¯
from openai import OpenAI

# åˆå§‹åŒ–å®¢æˆ·ç«¯
client = OpenAI(api_key="your-api-key")

# ç®€å•å¯¹è¯
response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¼–ç¨‹å¯¼å¸ˆã€‚"},
        {"role": "user", "content": "è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯é€’å½’ï¼Ÿ"}
    ],
    temperature=0.7,
    max_tokens=500
)

print(response.choices[0].message.content)

# æµå¼è¾“å‡º
stream = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "å†™ä¸€ä¸ªå¿«é€Ÿæ’åºçš„ä»£ç "}],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)`,
        language: 'python',
        difficulty: 'beginner',
        tags: ['OpenAI', 'Chat', 'Streaming']
      },
      {
        id: 'oa-2',
        title: { zh: 'Function Calling', ja: 'Function Calling' },
        description: { zh: 'è®© GPT è°ƒç”¨è‡ªå®šä¹‰å‡½æ•°', ja: 'GPT ã«ã‚«ã‚¹ã‚¿ãƒ é–¢æ•°ã‚’å‘¼ã³å‡ºã•ã›ã‚‹' },
        code: `# OpenAI Function Calling ç¤ºä¾‹
from openai import OpenAI
import json

client = OpenAI()

# å®šä¹‰å¯è°ƒç”¨çš„å‡½æ•°
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”ä¿¡æ¯",
            "parameters": {
                "type": "object",
                "properties": {
                    "city": {
                        "type": "string",
                        "description": "åŸå¸‚åç§°ï¼Œå¦‚ï¼šåŒ—äº¬ã€ä¸Šæµ·"
                    },
                    "unit": {
                        "type": "string",
                        "enum": ["celsius", "fahrenheit"],
                        "description": "æ¸©åº¦å•ä½"
                    }
                },
                "required": ["city"]
            }
        }
    }
]

# è°ƒç”¨
response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "åŒ—äº¬ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"}],
    tools=tools,
    tool_choice="auto"
)

# è§£æå‡½æ•°è°ƒç”¨
tool_call = response.choices[0].message.tool_calls[0]
function_name = tool_call.function.name
arguments = json.loads(tool_call.function.arguments)
print(f"è°ƒç”¨å‡½æ•°ï¼š{function_name}")
print(f"å‚æ•°ï¼š{arguments}")  # {"city": "åŒ—äº¬"}`,
        language: 'python',
        difficulty: 'intermediate',
        tags: ['OpenAI', 'Functions', 'Tools']
      },
      {
        id: 'oa-3',
        title: { zh: 'å›¾åƒç†è§£', ja: 'ç”»åƒç†è§£' },
        description: { zh: 'GPT-4o è§†è§‰èƒ½åŠ›', ja: 'GPT-4o ã®ãƒ“ã‚¸ãƒ§ãƒ³æ©Ÿèƒ½' },
        code: `# GPT-4o å›¾åƒç†è§£ç¤ºä¾‹
from openai import OpenAI
import base64

client = OpenAI()

# æ–¹æ³•1ï¼šä½¿ç”¨å›¾ç‰‡ URL
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ"},
                {
                    "type": "image_url",
                    "image_url": {
                        "url": "https://example.com/image.jpg"
                    }
                }
            ]
        }
    ],
    max_tokens=300
)

print(response.choices[0].message.content)

# æ–¹æ³•2ï¼šä½¿ç”¨ base64 ç¼–ç çš„æœ¬åœ°å›¾ç‰‡
def encode_image(image_path):
    with open(image_path, "rb") as f:
        return base64.b64encode(f.read()).decode("utf-8")

base64_image = encode_image("my_image.jpg")

response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹"},
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/jpeg;base64,{base64_image}"
                    }
                }
            ]
        }
    ]
)`,
        language: 'python',
        difficulty: 'intermediate',
        tags: ['OpenAI', 'Vision', 'GPT-4o']
      }
    ]
  },
  rag: {
    name: { zh: 'RAG æ£€ç´¢å¢å¼º', ja: 'RAG æ¤œç´¢æ‹¡å¼µ' },
    description: { zh: 'æ„å»ºçŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ', ja: 'çŸ¥è­˜ãƒ™ãƒ¼ã‚¹Q&Aã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰' },
    icon: Database,
    gradient: 'from-purple-500 to-violet-600',
    examples: [
      {
        id: 'rag-1',
        title: { zh: 'ç®€å• RAG å®ç°', ja: 'ã‚·ãƒ³ãƒ—ãƒ«ãª RAG å®Ÿè£…' },
        description: { zh: 'ä½¿ç”¨ ChromaDB æ„å»ºåŸºç¡€ RAG', ja: 'ChromaDB ã§åŸºæœ¬çš„ãª RAG ã‚’æ§‹ç¯‰' },
        code: `# ç®€å• RAG å®ç°
# pip install chromadb langchain-chroma langchain-openai

from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_text_splitters import RecursiveCharacterTextSplitter

# 1. å‡†å¤‡æ–‡æ¡£
documents = [
    "LangChain æ˜¯ä¸€ä¸ªç”¨äºå¼€å‘ LLM åº”ç”¨çš„æ¡†æ¶ã€‚",
    "RAG å…¨ç§°æ˜¯ Retrieval-Augmented Generationï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆã€‚",
    "å‘é‡æ•°æ®åº“å¯ä»¥å­˜å‚¨æ–‡æœ¬çš„åµŒå…¥å‘é‡ï¼Œæ”¯æŒè¯­ä¹‰æœç´¢ã€‚",
    "ChromaDB æ˜¯ä¸€ä¸ªè½»é‡çº§çš„å‘é‡æ•°æ®åº“ï¼Œé€‚åˆæœ¬åœ°å¼€å‘ã€‚"
]

# 2. æ–‡æœ¬åˆ†å‰²ï¼ˆå¦‚æœæ–‡æ¡£è¾ƒé•¿ï¼‰
splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)
splits = splitter.create_documents(documents)

# 3. åˆ›å»ºå‘é‡å­˜å‚¨
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(splits, embeddings)

# 4. åˆ›å»ºæ£€ç´¢å™¨
retriever = vectorstore.as_retriever(search_kwargs={"k": 2})

# 5. æ„å»º RAG é“¾
llm = ChatOpenAI(model="gpt-4o-mini")
prompt = ChatPromptTemplate.from_template("""
åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœæ— æ³•ä»ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ°ç­”æ¡ˆï¼Œè¯·è¯´"æˆ‘ä¸çŸ¥é“"ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}
""")

def format_docs(docs):
    return "\\n\\n".join(doc.page_content for doc in docs)

# 6. é—®ç­”
question = "ä»€ä¹ˆæ˜¯ RAGï¼Ÿ"
docs = retriever.invoke(question)
context = format_docs(docs)
response = llm.invoke(prompt.format(context=context, question=question))
print(response.content)`,
        language: 'python',
        difficulty: 'intermediate',
        tags: ['RAG', 'ChromaDB', 'Retrieval']
      },
      {
        id: 'rag-2',
        title: { zh: 'å¤„ç† PDF æ–‡æ¡£', ja: 'PDF ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†' },
        description: { zh: 'ä» PDF æ„å»ºçŸ¥è¯†åº“', ja: 'PDF ã‹ã‚‰çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰' },
        code: `# PDF æ–‡æ¡£ RAG
# pip install pypdf langchain-community

from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma

# 1. åŠ è½½ PDF
loader = PyPDFLoader("document.pdf")
pages = loader.load()  # æ¯é¡µä¸€ä¸ªæ–‡æ¡£

print(f"å…± {len(pages)} é¡µ")
print(f"ç¬¬ä¸€é¡µå†…å®¹é¢„è§ˆï¼š{pages[0].page_content[:200]}...")

# 2. åˆ†å‰²æ–‡æ¡£
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    length_function=len,
    separators=["\\n\\n", "\\n", "ã€‚", "ï¼Œ", " ", ""]
)
chunks = splitter.split_documents(pages)
print(f"åˆ†å‰²æˆ {len(chunks)} ä¸ªå—")

# 3. åˆ›å»ºå‘é‡å­˜å‚¨ï¼ˆæŒä¹…åŒ–ï¼‰
vectorstore = Chroma.from_documents(
    chunks,
    OpenAIEmbeddings(),
    persist_directory="./chroma_db"  # æŒä¹…åŒ–ç›®å½•
)

# 4. åç»­ä½¿ç”¨ï¼ˆåŠ è½½å·²æœ‰æ•°æ®åº“ï¼‰
vectorstore = Chroma(
    persist_directory="./chroma_db",
    embedding_function=OpenAIEmbeddings()
)

# 5. ç›¸ä¼¼åº¦æœç´¢
results = vectorstore.similarity_search("æŸ¥è¯¢å†…å®¹", k=3)
for doc in results:
    print(f"æ¥æºï¼š{doc.metadata.get('source', 'unknown')}")
    print(f"å†…å®¹ï¼š{doc.page_content[:100]}...")`,
        language: 'python',
        difficulty: 'intermediate',
        tags: ['RAG', 'PDF', 'Persistence']
      },
      {
        id: 'rag-3',
        title: { zh: 'Hybrid Search æ··åˆæ£€ç´¢', ja: 'Hybrid Search ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢' },
        description: { zh: 'ç»“åˆå…³é”®è¯æœç´¢å’Œè¯­ä¹‰æœç´¢', ja: 'ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã¨æ„å‘³æ¤œç´¢ã®çµ„ã¿åˆã‚ã›' },
        code: `# æ··åˆæ£€ç´¢ç¤ºä¾‹
# pip install rank-bm25

from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
from rank_bm25 import BM25Okapi
import numpy as np

class HybridRetriever:
    """ç»“åˆ BM25 å…³é”®è¯æœç´¢å’Œå‘é‡è¯­ä¹‰æœç´¢"""

    def __init__(self, documents: list[str]):
        self.documents = documents

        # BM25 åˆå§‹åŒ–
        tokenized = [doc.split() for doc in documents]
        self.bm25 = BM25Okapi(tokenized)

        # å‘é‡å­˜å‚¨åˆå§‹åŒ–
        self.vectorstore = Chroma.from_texts(
            documents,
            OpenAIEmbeddings()
        )

    def search(self, query: str, k: int = 3, alpha: float = 0.5):
        """
        æ··åˆæ£€ç´¢
        alpha: å‘é‡æœç´¢æƒé‡ (0-1)ï¼Œ1-alpha æ˜¯ BM25 æƒé‡
        """
        # BM25 æœç´¢
        bm25_scores = self.bm25.get_scores(query.split())
        bm25_scores = bm25_scores / (bm25_scores.max() + 1e-6)  # å½’ä¸€åŒ–

        # å‘é‡æœç´¢
        vector_results = self.vectorstore.similarity_search_with_score(query, k=len(self.documents))
        vector_scores = {doc.page_content: 1 - score for doc, score in vector_results}
        vector_scores = np.array([vector_scores.get(doc, 0) for doc in self.documents])
        vector_scores = vector_scores / (vector_scores.max() + 1e-6)

        # èåˆåˆ†æ•°
        hybrid_scores = alpha * vector_scores + (1 - alpha) * bm25_scores

        # è¿”å› top-k
        top_indices = np.argsort(hybrid_scores)[::-1][:k]
        return [(self.documents[i], hybrid_scores[i]) for i in top_indices]

# ä½¿ç”¨
docs = ["LangChain å¼€å‘æ¡†æ¶", "RAG æ£€ç´¢å¢å¼ºç”Ÿæˆ", "å‘é‡æ•°æ®åº“æŠ€æœ¯"]
retriever = HybridRetriever(docs)
results = retriever.search("LangChain RAG", k=2, alpha=0.7)`,
        language: 'python',
        difficulty: 'advanced',
        tags: ['RAG', 'BM25', 'Hybrid Search']
      }
    ]
  },
  prompts: {
    name: { zh: 'æç¤ºè¯å·¥ç¨‹', ja: 'ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°' },
    description: { zh: 'é«˜æ•ˆæç¤ºè¯è®¾è®¡æ¨¡å¼', ja: 'åŠ¹æœçš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³' },
    icon: FileText,
    gradient: 'from-amber-500 to-orange-600',
    examples: [
      {
        id: 'pr-1',
        title: { zh: 'Few-Shot ç¤ºä¾‹', ja: 'Few-Shot ä¾‹' },
        description: { zh: 'é€šè¿‡ç¤ºä¾‹å¼•å¯¼æ¨¡å‹è¾“å‡º', ja: 'ä¾‹ã‚’é€šã˜ã¦ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã‚’èª˜å°' },
        code: `# Few-Shot Prompting ç¤ºä¾‹

few_shot_prompt = """
ä½ æ˜¯ä¸€ä¸ªæƒ…æ„Ÿåˆ†æä¸“å®¶ã€‚åˆ†ææ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘ã€‚

ç¤ºä¾‹ 1ï¼š
æ–‡æœ¬ï¼šè¿™å®¶é¤å…çš„èœçœŸå¥½åƒï¼ŒæœåŠ¡ä¹Ÿå¾ˆå‘¨åˆ°ï¼
æƒ…æ„Ÿï¼šæ­£é¢
ç½®ä¿¡åº¦ï¼š0.95

ç¤ºä¾‹ 2ï¼š
æ–‡æœ¬ï¼šç­‰äº†ä¸€ä¸ªå°æ—¶æ‰ä¸Šèœï¼Œå¤ªæ…¢äº†ã€‚
æƒ…æ„Ÿï¼šè´Ÿé¢
ç½®ä¿¡åº¦ï¼š0.85

ç¤ºä¾‹ 3ï¼š
æ–‡æœ¬ï¼šå‘³é“ä¸€èˆ¬ï¼Œä»·æ ¼è¿˜è¡Œã€‚
æƒ…æ„Ÿï¼šä¸­æ€§
ç½®ä¿¡åº¦ï¼š0.70

ç°åœ¨åˆ†æä»¥ä¸‹æ–‡æœ¬ï¼š
æ–‡æœ¬ï¼š{input_text}
"""

# ä½¿ç”¨
from openai import OpenAI
client = OpenAI()

def analyze_sentiment(text: str):
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "user", "content": few_shot_prompt.format(input_text=text)}
        ],
        temperature=0  # å‡å°‘éšæœºæ€§
    )
    return response.choices[0].message.content

result = analyze_sentiment("äº§å“è´¨é‡ä¸é”™ï¼Œä½†ç‰©æµå¤ªæ…¢äº†")
print(result)`,
        language: 'python',
        difficulty: 'beginner',
        tags: ['Prompts', 'Few-Shot', 'Sentiment']
      },
      {
        id: 'pr-2',
        title: { zh: 'Chain of Thought', ja: 'Chain of Thought' },
        description: { zh: 'å¼•å¯¼æ¨¡å‹é€æ­¥æ¨ç†', ja: 'ãƒ¢ãƒ‡ãƒ«ã«æ®µéšçš„ãªæ¨è«–ã‚’ä¿ƒã™' },
        code: `# Chain of Thought (CoT) æç¤ºè¯

cot_prompt = """
è¯·ä¸€æ­¥ä¸€æ­¥æ€è€ƒå¹¶è§£å†³ä»¥ä¸‹é—®é¢˜ã€‚

é—®é¢˜ï¼šä¸€ä¸ªç­çº§æœ‰ 32 åå­¦ç”Ÿï¼Œå…¶ä¸­ 60% æ˜¯å¥³ç”Ÿã€‚å¦‚æœ 1/4 çš„å¥³ç”Ÿå‚åŠ äº†ç¯®çƒé˜Ÿï¼Œ
é‚£ä¹ˆæœ‰å¤šå°‘å¥³ç”Ÿå‚åŠ äº†ç¯®çƒé˜Ÿï¼Ÿ

è®©æˆ‘ä»¬é€æ­¥åˆ†æï¼š
1. é¦–å…ˆï¼Œè®¡ç®—å¥³ç”Ÿæ€»æ•°
2. ç„¶åï¼Œè®¡ç®—å‚åŠ ç¯®çƒé˜Ÿçš„å¥³ç”Ÿæ•°é‡
3. æœ€åï¼Œç»™å‡ºç­”æ¡ˆ

æ€è€ƒè¿‡ç¨‹ï¼š
"""

# ä½¿ç”¨ LangChain
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# é€šç”¨ CoT æ¨¡æ¿
cot_template = ChatPromptTemplate.from_messages([
    ("system", """ä½ æ˜¯ä¸€ä¸ªæ“…é•¿é€»è¾‘æ¨ç†çš„åŠ©æ‰‹ã€‚
å›ç­”é—®é¢˜æ—¶è¯·éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼š
1. ç†è§£é—®é¢˜ï¼Œæå–å…³é”®ä¿¡æ¯
2. åˆ†è§£é—®é¢˜ä¸ºå­æ­¥éª¤
3. é€æ­¥è®¡ç®—/æ¨ç†
4. éªŒè¯ç­”æ¡ˆ
5. ç»™å‡ºæœ€ç»ˆç»“è®º"""),
    ("human", "{question}")
])

chain = cot_template | llm

# ä½¿ç”¨
response = chain.invoke({
    "question": "å¦‚æœä¸€ä¸ªæ•°çš„ 3 å€åŠ ä¸Š 15 ç­‰äºè¿™ä¸ªæ•°çš„ 5 å€å‡å» 9ï¼Œè¿™ä¸ªæ•°æ˜¯å¤šå°‘ï¼Ÿ"
})
print(response.content)`,
        language: 'python',
        difficulty: 'intermediate',
        tags: ['Prompts', 'CoT', 'Reasoning']
      },
      {
        id: 'pr-3',
        title: { zh: 'Structured Output', ja: 'æ§‹é€ åŒ–å‡ºåŠ›' },
        description: { zh: 'è®©æ¨¡å‹è¾“å‡ºç»“æ„åŒ– JSON', ja: 'ãƒ¢ãƒ‡ãƒ«ã«æ§‹é€ åŒ– JSON ã‚’å‡ºåŠ›ã•ã›ã‚‹' },
        code: `# ç»“æ„åŒ–è¾“å‡ºç¤ºä¾‹
from openai import OpenAI
from pydantic import BaseModel
from typing import Literal

client = OpenAI()

# ä½¿ç”¨ Pydantic å®šä¹‰è¾“å‡ºç»“æ„
class ProductReview(BaseModel):
    sentiment: Literal["positive", "negative", "neutral"]
    confidence: float
    key_points: list[str]
    suggested_improvements: list[str]

# æ–¹æ³•1ï¼šä½¿ç”¨ response_format
response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {"role": "system", "content": "åˆ†æäº§å“è¯„è®ºï¼Œæå–ç»“æ„åŒ–ä¿¡æ¯ã€‚"},
        {"role": "user", "content": "è¿™ä¸ªè€³æœºéŸ³è´¨å¾ˆå¥½ï¼Œä½†ç»­èˆªå¤ªçŸ­äº†ï¼Œåªèƒ½ç”¨4å°æ—¶ã€‚"}
    ],
    response_format={"type": "json_object"}
)

import json
result = json.loads(response.choices[0].message.content)
print(result)

# æ–¹æ³•2ï¼šä½¿ç”¨ Instructor åº“ï¼ˆæ¨èï¼‰
# pip install instructor
import instructor

client = instructor.from_openai(OpenAI())

review = client.chat.completions.create(
    model="gpt-4o-mini",
    response_model=ProductReview,  # ä½¿ç”¨ Pydantic æ¨¡å‹
    messages=[
        {"role": "user", "content": "è¿™ä¸ªè€³æœºéŸ³è´¨å¾ˆå¥½ï¼Œä½†ç»­èˆªå¤ªçŸ­äº†ï¼Œåªèƒ½ç”¨4å°æ—¶ã€‚"}
    ]
)

print(f"æƒ…æ„Ÿï¼š{review.sentiment}")
print(f"è¦ç‚¹ï¼š{review.key_points}")`,
        language: 'python',
        difficulty: 'intermediate',
        tags: ['Prompts', 'JSON', 'Structured']
      }
    ]
  },
  claude: {
    name: { zh: 'Claude API', ja: 'Claude API' },
    description: { zh: 'Anthropic Claude API ä½¿ç”¨', ja: 'Anthropic Claude API ã®ä½¿ç”¨' },
    icon: Sparkles,
    gradient: 'from-orange-500 to-amber-600',
    examples: [
      {
        id: 'cl-1',
        title: { zh: 'Claude åŸºç¡€å¯¹è¯', ja: 'Claude åŸºæœ¬å¯¾è©±' },
        description: { zh: 'Claude API åŸºç¡€è°ƒç”¨', ja: 'Claude API ã®åŸºæœ¬å‘¼ã³å‡ºã—' },
        code: `# Claude API åŸºç¡€
# pip install anthropic

from anthropic import Anthropic

client = Anthropic(api_key="your-api-key")

# åŸºç¡€å¯¹è¯
message = client.messages.create(
    model="claude-sonnet-4-20250514",
    max_tokens=1024,
    messages=[
        {"role": "user", "content": "è§£é‡Šä»€ä¹ˆæ˜¯é‡å­è®¡ç®—"}
    ]
)
print(message.content[0].text)

# å¸¦ç³»ç»Ÿæç¤º
message = client.messages.create(
    model="claude-sonnet-4-20250514",
    max_tokens=1024,
    system="ä½ æ˜¯Pythonç¼–ç¨‹å¯¼å¸ˆ",
    messages=[{"role": "user", "content": "å¦‚ä½•è¯»å–CSVï¼Ÿ"}]
)

# å¤šè½®å¯¹è¯
messages = [
    {"role": "user", "content": "æˆ‘å«å°æ˜"},
    {"role": "assistant", "content": "ä½ å¥½å°æ˜ï¼"},
    {"role": "user", "content": "æˆ‘å«ä»€ä¹ˆï¼Ÿ"}
]
response = client.messages.create(
    model="claude-sonnet-4-20250514",
    max_tokens=1024,
    messages=messages
)`,
        language: 'python',
        difficulty: 'beginner',
        tags: ['Claude', 'Anthropic', 'Chat']
      },
      {
        id: 'cl-2',
        title: { zh: 'Claude æµå¼è¾“å‡º', ja: 'Claude ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°' },
        description: { zh: 'å®æ—¶æµå¼å“åº”', ja: 'ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°' },
        code: `# Claude æµå¼è¾“å‡º
from anthropic import Anthropic

client = Anthropic()

with client.messages.stream(
    model="claude-sonnet-4-20250514",
    max_tokens=1024,
    messages=[{"role": "user", "content": "å†™ä¸€é¦–è¯—"}]
) as stream:
    for text in stream.text_stream:
        print(text, end="", flush=True)

# è·å–ç»Ÿè®¡ä¿¡æ¯
with client.messages.stream(
    model="claude-sonnet-4-20250514",
    max_tokens=1024,
    messages=[{"role": "user", "content": "è§£é‡Šé€’å½’"}]
) as stream:
    for text in stream.text_stream:
        print(text, end="", flush=True)

    final = stream.get_final_message()
    print(f"\\nè¾“å…¥: {final.usage.input_tokens}")
    print(f"è¾“å‡º: {final.usage.output_tokens}")`,
        language: 'python',
        difficulty: 'intermediate',
        tags: ['Claude', 'Streaming']
      },
      {
        id: 'cl-3',
        title: { zh: 'Claude Tool Use', ja: 'Claude Tool Use' },
        description: { zh: 'Claude å·¥å…·è°ƒç”¨', ja: 'Claude ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—' },
        code: `# Claude Tool Use
from anthropic import Anthropic
import json

client = Anthropic()

tools = [{
    "name": "get_weather",
    "description": "è·å–å¤©æ°”",
    "input_schema": {
        "type": "object",
        "properties": {
            "city": {"type": "string", "description": "åŸå¸‚"}
        },
        "required": ["city"]
    }
}]

def get_weather(city):
    return json.dumps({"city": city, "temp": 22, "weather": "æ™´"})

response = client.messages.create(
    model="claude-sonnet-4-20250514",
    max_tokens=1024,
    tools=tools,
    messages=[{"role": "user", "content": "åŒ—äº¬å¤©æ°”ï¼Ÿ"}]
)

if response.stop_reason == "tool_use":
    tool = next(b for b in response.content if b.type == "tool_use")
    result = get_weather(**tool.input)

    final = client.messages.create(
        model="claude-sonnet-4-20250514",
        max_tokens=1024,
        tools=tools,
        messages=[
            {"role": "user", "content": "åŒ—äº¬å¤©æ°”ï¼Ÿ"},
            {"role": "assistant", "content": response.content},
            {"role": "user", "content": [
                {"type": "tool_result", "tool_use_id": tool.id, "content": result}
            ]}
        ]
    )
    print(final.content[0].text)`,
        language: 'python',
        difficulty: 'advanced',
        tags: ['Claude', 'Tools']
      },
      {
        id: 'cl-4',
        title: { zh: 'Claude å›¾åƒç†è§£', ja: 'Claude ç”»åƒç†è§£' },
        description: { zh: 'Claude è§†è§‰èƒ½åŠ›', ja: 'Claude ãƒ“ã‚¸ãƒ§ãƒ³æ©Ÿèƒ½' },
        code: `# Claude å›¾åƒç†è§£
from anthropic import Anthropic
import base64

client = Anthropic()

# URLå›¾ç‰‡
message = client.messages.create(
    model="claude-sonnet-4-20250514",
    max_tokens=1024,
    messages=[{
        "role": "user",
        "content": [
            {"type": "image", "source": {"type": "url", "url": "https://example.com/image.jpg"}},
            {"type": "text", "text": "æè¿°è¿™å¼ å›¾ç‰‡"}
        ]
    }]
)

# Base64å›¾ç‰‡
def encode_image(path):
    with open(path, "rb") as f:
        return base64.b64encode(f.read()).decode()

image_data = encode_image("photo.jpg")
message = client.messages.create(
    model="claude-sonnet-4-20250514",
    max_tokens=1024,
    messages=[{
        "role": "user",
        "content": [
            {"type": "image", "source": {"type": "base64", "media_type": "image/jpeg", "data": image_data}},
            {"type": "text", "text": "è¿™æ˜¯ä»€ä¹ˆï¼Ÿ"}
        ]
    }]
)`,
        language: 'python',
        difficulty: 'intermediate',
        tags: ['Claude', 'Vision', 'Multimodal']
      }
    ]
  },
  agent: {
    name: { zh: 'Agent å¼€å‘', ja: 'Agent é–‹ç™º' },
    description: { zh: 'æ„å»º AI Agent', ja: 'AI Agent ã®æ§‹ç¯‰' },
    icon: Bot,
    gradient: 'from-indigo-500 to-purple-600',
    examples: [
      {
        id: 'ag-1',
        title: { zh: 'ReAct Agent', ja: 'ReAct Agent' },
        description: { zh: 'æ€è€ƒ-è¡ŒåŠ¨å¾ªç¯', ja: 'æ€è€ƒ-è¡Œå‹•ãƒ«ãƒ¼ãƒ—' },
        code: `# ReAct Agent å®ç°
from anthropic import Anthropic

client = Anthropic()

# å·¥å…·å®šä¹‰
def calculator(expr):
    try: return str(eval(expr))
    except: return "é”™è¯¯"

def get_time():
    from datetime import datetime
    return datetime.now().strftime("%Y-%m-%d %H:%M")

tools = {
    "calculator": ("è®¡ç®—è¡¨è¾¾å¼", calculator),
    "get_time": ("è·å–æ—¶é—´", get_time),
}

def run_agent(query, max_iter=5):
    tool_desc = "\\n".join([f"- {k}: {v[0]}" for k, v in tools.items()])
    system = f"""å¯ç”¨å·¥å…·ï¼š
{tool_desc}

æ ¼å¼ï¼š
æ€è€ƒï¼š[åˆ†æ]
è¡ŒåŠ¨ï¼š[tool_name]
è¾“å…¥ï¼š[input]"""

    messages = [{"role": "user", "content": query}]

    for _ in range(max_iter):
        resp = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=1024,
            system=system,
            messages=messages
        )
        text = resp.content[0].text
        print(text)

        if "è¡ŒåŠ¨ï¼š" in text:
            lines = text.split("\\n")
            action = next((l.split("ï¼š")[1].strip() for l in lines if "è¡ŒåŠ¨ï¼š" in l), None)
            inp = next((l.split("ï¼š")[1].strip() for l in lines if "è¾“å…¥ï¼š" in l), "")

            if action in tools:
                result = tools[action][1](inp) if inp else tools[action][1]()
                print(f"è§‚å¯Ÿï¼š{result}")
                messages.append({"role": "assistant", "content": text})
                messages.append({"role": "user", "content": f"è§‚å¯Ÿï¼š{result}"})
                continue
        return text

run_agent("ç°åœ¨å‡ ç‚¹ï¼Ÿç®—123*456")`,
        language: 'python',
        difficulty: 'advanced',
        tags: ['Agent', 'ReAct']
      },
      {
        id: 'ag-2',
        title: { zh: 'LangChain Agent', ja: 'LangChain Agent' },
        description: { zh: 'LangChain å·¥å…· Agent', ja: 'LangChain ãƒ„ãƒ¼ãƒ« Agent' },
        code: `# LangChain Agent
from langchain_anthropic import ChatAnthropic
from langchain.agents import tool, AgentExecutor, create_tool_calling_agent
from langchain_core.prompts import ChatPromptTemplate

llm = ChatAnthropic(model="claude-sonnet-4-20250514")

@tool
def calculator(expression: str) -> str:
    """è®¡ç®—æ•°å­¦è¡¨è¾¾å¼ï¼Œå¦‚ 2+2"""
    return str(eval(expression))

@tool
def get_datetime() -> str:
    """è·å–å½“å‰æ—¥æœŸæ—¶é—´"""
    from datetime import datetime
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

tools = [calculator, get_datetime]

prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯AIåŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨å·¥å…·"),
    ("human", "{input}"),
    ("placeholder", "{agent_scratchpad}")
])

agent = create_tool_calling_agent(llm, tools, prompt)
executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

result = executor.invoke({"input": "ç°åœ¨å‡ ç‚¹ï¼Ÿç®—2024-2000"})
print(result['output'])`,
        language: 'python',
        difficulty: 'intermediate',
        tags: ['LangChain', 'Agent']
      },
      {
        id: 'ag-3',
        title: { zh: 'Agentic Loop', ja: 'Agentic Loop' },
        description: { zh: 'ç”Ÿäº§çº§ Agent å¾ªç¯', ja: 'æœ¬ç•ªãƒ¬ãƒ™ãƒ« Agent ãƒ«ãƒ¼ãƒ—' },
        code: `# ç”Ÿäº§çº§ Agentic Loop
from anthropic import Anthropic
from dataclasses import dataclass
import os

@dataclass
class ToolResult:
    success: bool
    output: str
    error: str = None

class AgenticLoop:
    def __init__(self):
        self.client = Anthropic()
        self.tools = {}
        self.schemas = []
        self.history = []

    def register(self, name, desc, params, handler):
        self.tools[name] = handler
        self.schemas.append({
            "name": name,
            "description": desc,
            "input_schema": {"type": "object", "properties": params, "required": list(params.keys())}
        })

    def execute(self, name, input_data):
        try:
            return ToolResult(True, str(self.tools[name](**input_data)))
        except Exception as e:
            return ToolResult(False, "", str(e))

    def run(self, query, max_turns=10):
        self.history.append({"role": "user", "content": query})

        for _ in range(max_turns):
            resp = self.client.messages.create(
                model="claude-sonnet-4-20250514",
                max_tokens=4096,
                tools=self.schemas or None,
                messages=self.history
            )
            self.history.append({"role": "assistant", "content": resp.content})

            if resp.stop_reason == "end_turn":
                return "\\n".join([b.text for b in resp.content if hasattr(b, 'text')])

            if resp.stop_reason == "tool_use":
                results = []
                for b in resp.content:
                    if b.type == "tool_use":
                        r = self.execute(b.name, b.input)
                        results.append({"type": "tool_result", "tool_use_id": b.id, "content": r.output or r.error})
                self.history.append({"role": "user", "content": results})

        return "è¾¾åˆ°æœ€å¤§è½®æ¬¡"

# ä½¿ç”¨
agent = AgenticLoop()
agent.register("list_files", "åˆ—å‡ºç›®å½•æ–‡ä»¶", {"dir": {"type": "string"}}, lambda dir: str(os.listdir(dir)))
agent.register("read_file", "è¯»å–æ–‡ä»¶", {"path": {"type": "string"}}, lambda path: open(path).read()[:500])

result = agent.run("åˆ—å‡ºå½“å‰ç›®å½•çš„æ–‡ä»¶")
print(result)`,
        language: 'python',
        difficulty: 'advanced',
        tags: ['Agent', 'Production']
      }
    ]
  },
  cases: {
    name: { zh: 'å®æˆ˜æ¡ˆä¾‹', ja: 'å®Ÿè·µäº‹ä¾‹' },
    description: { zh: 'å®Œæ•´å¯è¿è¡Œé¡¹ç›®', ja: 'å®Œå…¨å®Ÿè¡Œå¯èƒ½ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ' },
    icon: Briefcase,
    gradient: 'from-rose-500 to-pink-600',
    examples: [
      {
        id: 'case-1',
        title: { zh: 'æ™ºèƒ½å®¢æœæœºå™¨äºº', ja: 'ã‚¹ãƒãƒ¼ãƒˆã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒ¼ãƒ“ã‚¹' },
        description: { zh: 'RAG çŸ¥è¯†åº“å®¢æœ (FastAPI)', ja: 'RAG ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ (FastAPI)' },
        code: `# æ™ºèƒ½å®¢æœ - FastAPI + RAG
# pip install fastapi uvicorn langchain langchain-anthropic langchain-chroma sentence-transformers

from fastapi import FastAPI
from pydantic import BaseModel
from langchain_chroma import Chroma
from langchain_anthropic import ChatAnthropic
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate

app = FastAPI(title="æ™ºèƒ½å®¢æœAPI")

# çŸ¥è¯†åº“
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
vectorstore = Chroma(embedding_function=embeddings)

# åˆå§‹åŒ–çŸ¥è¯†
knowledge = [
    "é€€æ¬¾æ”¿ç­–ï¼šè´­ä¹°å7å¤©å†…å¯æ— ç†ç”±é€€æ¬¾ï¼Œè¯·è”ç³»å®¢æœæä¾›è®¢å•å·",
    "é…é€æ—¶é—´ï¼šæ™®é€šå¿«é€’3-5ä¸ªå·¥ä½œæ—¥ï¼Œé¡ºä¸°æ¬¡æ—¥è¾¾",
    "VIPä¼šå‘˜ï¼šäº«å—9æŠ˜ä¼˜æƒ ï¼Œå…è´¹é…é€ï¼Œä¼˜å…ˆå®¢æœ",
    "æ”¯ä»˜æ–¹å¼ï¼šæ”¯æŒæ”¯ä»˜å®ã€å¾®ä¿¡ã€é“¶è¡Œå¡",
    "è¥ä¸šæ—¶é—´ï¼šå‘¨ä¸€è‡³å‘¨äº” 9:00-18:00",
]
vectorstore.add_texts(knowledge)

llm = ChatAnthropic(model="claude-sonnet-4-20250514")
prompt = ChatPromptTemplate.from_template("""
ä½ æ˜¯ä¸“ä¸šå®¢æœã€‚åŸºäºçŸ¥è¯†åº“å›ç­”ï¼Œä¸çŸ¥é“å°±è¯´ä¸çŸ¥é“ã€‚

çŸ¥è¯†åº“ï¼š
{context}

é—®é¢˜ï¼š{question}

å‹å¥½ä¸“ä¸šåœ°å›ç­”ï¼š""")

class ChatRequest(BaseModel):
    message: str

@app.post("/chat")
async def chat(req: ChatRequest):
    docs = vectorstore.similarity_search(req.message, k=2)
    context = "\\n".join([d.page_content for d in docs])

    chain = prompt | llm
    response = chain.invoke({"context": context, "question": req.message})
    return {"reply": response.content}

@app.post("/knowledge/add")
async def add(texts: list[str]):
    vectorstore.add_texts(texts)
    return {"status": "ok", "count": len(texts)}

# uvicorn app:app --reload --port 8000
# curl -X POST localhost:8000/chat -H "Content-Type: application/json" -d '{"message":"é€€æ¬¾æ”¿ç­–ï¼Ÿ"}'`,
        language: 'python',
        difficulty: 'intermediate',
        tags: ['RAG', 'FastAPI', 'å®Œæ•´é¡¹ç›®']
      },
      {
        id: 'case-2',
        title: { zh: 'PDF é—®ç­”åŠ©æ‰‹', ja: 'PDF Q&A ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ' },
        description: { zh: 'ä¸Šä¼  PDF æ™ºèƒ½é—®ç­” (Gradio)', ja: 'PDF ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆ Q&A (Gradio)' },
        code: `# PDF é—®ç­” - Gradio ç•Œé¢
# pip install gradio pypdf langchain langchain-anthropic langchain-chroma sentence-transformers

import gradio as gr
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_anthropic import ChatAnthropic
from langchain_community.embeddings import HuggingFaceEmbeddings
import tempfile

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
llm = ChatAnthropic(model="claude-sonnet-4-20250514")
vectorstore = None

def load_pdf(file):
    global vectorstore
    if not file: return "è¯·ä¸Šä¼ PDF"

    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
        tmp.write(file)
        loader = PyPDFLoader(tmp.name)
        pages = loader.load()

    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    chunks = splitter.split_documents(pages)
    vectorstore = Chroma.from_documents(chunks, embeddings)
    return f"âœ… åŠ è½½æˆåŠŸï¼{len(pages)}é¡µï¼Œ{len(chunks)}å—"

def ask(question):
    if not vectorstore: return "è¯·å…ˆä¸Šä¼ PDF"
    if not question.strip(): return "è¯·è¾“å…¥é—®é¢˜"

    docs = vectorstore.similarity_search(question, k=3)
    context = "\\n---\\n".join([d.page_content for d in docs])

    response = llm.invoke(f"åŸºäºä»¥ä¸‹å†…å®¹å›ç­”ï¼š\\n{context}\\n\\né—®é¢˜ï¼š{question}")
    return response.content

with gr.Blocks(title="PDFé—®ç­”") as demo:
    gr.Markdown("# ğŸ“„ PDF æ™ºèƒ½é—®ç­”")

    with gr.Row():
        with gr.Column(scale=1):
            pdf = gr.File(label="ä¸Šä¼ PDF", file_types=[".pdf"], type="binary")
            btn = gr.Button("ğŸ“¥ åŠ è½½", variant="primary")
            status = gr.Textbox(label="çŠ¶æ€")

        with gr.Column(scale=2):
            question = gr.Textbox(label="é—®é¢˜", lines=2)
            ask_btn = gr.Button("ğŸ” æé—®", variant="primary")
            answer = gr.Textbox(label="å›ç­”", lines=8)

    btn.click(load_pdf, pdf, status)
    ask_btn.click(ask, question, answer)
    question.submit(ask, question, answer)

demo.launch(server_port=7860)`,
        language: 'python',
        difficulty: 'intermediate',
        tags: ['Gradio', 'PDF', 'RAG', 'å®Œæ•´é¡¹ç›®']
      },
      {
        id: 'case-3',
        title: { zh: 'ä»£ç å®¡æŸ¥åŠ©æ‰‹', ja: 'ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ' },
        description: { zh: 'AI è‡ªåŠ¨ä»£ç å®¡æŸ¥', ja: 'AI è‡ªå‹•ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼' },
        code: `# ä»£ç å®¡æŸ¥åŠ©æ‰‹
from anthropic import Anthropic

client = Anthropic()

SYSTEM = """ä½ æ˜¯èµ„æ·±ä»£ç å®¡æŸ¥ä¸“å®¶ã€‚å®¡æŸ¥ä»¥ä¸‹æ–¹é¢ï¼š

1. **ä»£ç è´¨é‡**: å¯è¯»æ€§ã€å‘½åè§„èŒƒ
2. **æ½œåœ¨Bug**: è¾¹ç•Œæ¡ä»¶ã€ç©ºå€¼å¤„ç†
3. **å®‰å…¨éšæ‚£**: SQLæ³¨å…¥ã€XSSç­‰
4. **æœ€ä½³å®è·µ**: æ˜¯å¦ç¬¦åˆè§„èŒƒ

è¾“å‡ºæ ¼å¼ï¼š
## æ€»ä½“è¯„ä»·
[ç®€è¯„]

## ğŸ”´ å¿…é¡»ä¿®æ”¹
- [ä¸¥é‡é—®é¢˜]

## ğŸŸ¡ å»ºè®®ä¿®æ”¹
- [æ”¹è¿›å»ºè®®]

## ğŸŸ¢ ä»£ç äº®ç‚¹
- [å¥½çš„åœ°æ–¹]

## ä¿®æ”¹ç¤ºä¾‹
\\\`\\\`\\\`
[ä¿®æ”¹åçš„ä»£ç ]
\\\`\\\`\\\`"""

def review_code(code: str, lang: str = "python") -> str:
    response = client.messages.create(
        model="claude-sonnet-4-20250514",
        max_tokens=2048,
        system=SYSTEM,
        messages=[{"role": "user", "content": f"å®¡æŸ¥{lang}ä»£ç ï¼š\\n\\\`\\\`\\\`{lang}\\n{code}\\n\\\`\\\`\\\`"}]
    )
    return response.content[0].text

# æµ‹è¯•
code = '''
def get_user(id):
    query = f"SELECT * FROM users WHERE id = {id}"
    result = db.execute(query)
    return result[0]
'''

review = review_code(code)
print(review)

# è¾“å‡ºç¤ºä¾‹ï¼š
# ## æ€»ä½“è¯„ä»·
# ä»£ç å­˜åœ¨ä¸¥é‡å®‰å…¨éšæ‚£
#
# ## ğŸ”´ å¿…é¡»ä¿®æ”¹
# - SQLæ³¨å…¥é£é™©ï¼šç›´æ¥æ‹¼æ¥ç”¨æˆ·è¾“å…¥
# - IndexErrorï¼šç©ºç»“æœæ—¶å´©æºƒ
#
# ## ä¿®æ”¹ç¤ºä¾‹
# def get_user(user_id: int) -> dict | None:
#     query = "SELECT * FROM users WHERE id = ?"
#     result = db.execute(query, (user_id,))
#     return result[0] if result else None`,
        language: 'python',
        difficulty: 'beginner',
        tags: ['Code Review', 'Security', 'å®Œæ•´é¡¹ç›®']
      },
      {
        id: 'case-4',
        title: { zh: 'ä¼šè®®çºªè¦ç”Ÿæˆå™¨', ja: 'è­°äº‹éŒ²ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼' },
        description: { zh: 'ä»æ–‡å­—ç”Ÿæˆä¼šè®®çºªè¦', ja: 'ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰è­°äº‹éŒ²ã‚’ç”Ÿæˆ' },
        code: `# ä¼šè®®çºªè¦ç”Ÿæˆå™¨ - Gradio
# pip install gradio anthropic

import gradio as gr
from anthropic import Anthropic

client = Anthropic()

SYSTEM = """ä½ æ˜¯ä¸“ä¸šä¼šè®®è®°å½•å‘˜ã€‚æ ¹æ®å†…å®¹ç”Ÿæˆç»“æ„åŒ–çºªè¦ï¼š

# ğŸ“‹ ä¼šè®®çºªè¦

## åŸºæœ¬ä¿¡æ¯
- **ä¼šè®®ä¸»é¢˜**: [æ¨æ–­]
- **å‚ä¼šäººå‘˜**: [æå–]

## ğŸ“Œ æ ¸å¿ƒè®®é¢˜
1. [è®®é¢˜]

## ğŸ’¡ å…³é”®å†³ç­–
- [å†³ç­–]

## âœ… è¡ŒåŠ¨é¡¹ç›®
| è´Ÿè´£äºº | ä»»åŠ¡ | æˆªæ­¢ |
|--------|------|------|

## ğŸ“ è®¨è®ºè¦ç‚¹
### è®®é¢˜1
- å†…å®¹

## ğŸ”œ ä¸‹ä¸€æ­¥
- [è®¡åˆ’]"""

def generate(text):
    if not text.strip(): return "è¯·è¾“å…¥ä¼šè®®å†…å®¹"

    response = client.messages.create(
        model="claude-sonnet-4-20250514",
        max_tokens=4096,
        system=SYSTEM,
        messages=[{"role": "user", "content": f"ç”Ÿæˆä¼šè®®çºªè¦ï¼š\\n\\n{text}"}]
    )
    return response.content[0].text

with gr.Blocks(title="ä¼šè®®çºªè¦") as demo:
    gr.Markdown("# ğŸ“ AI ä¼šè®®çºªè¦ç”Ÿæˆå™¨")

    text_input = gr.Textbox(label="ä¼šè®®å†…å®¹", placeholder="ç²˜è´´ä¼šè®®è®°å½•...", lines=10)
    btn = gr.Button("ç”Ÿæˆçºªè¦", variant="primary")
    output = gr.Markdown(label="ä¼šè®®çºªè¦")

    btn.click(generate, text_input, output)

demo.launch()`,
        language: 'python',
        difficulty: 'beginner',
        tags: ['Gradio', 'Meeting', 'å®Œæ•´é¡¹ç›®']
      }
    ]
  },
  deploy: {
    name: { zh: 'éƒ¨ç½²ä¸Šçº¿', ja: 'ãƒ‡ãƒ—ãƒ­ã‚¤' },
    description: { zh: 'AI åº”ç”¨éƒ¨ç½²', ja: 'AI ã‚¢ãƒ—ãƒªãƒ‡ãƒ—ãƒ­ã‚¤' },
    icon: Rocket,
    gradient: 'from-cyan-500 to-blue-600',
    examples: [
      {
        id: 'dep-1',
        title: { zh: 'Docker éƒ¨ç½²', ja: 'Docker ãƒ‡ãƒ—ãƒ­ã‚¤' },
        description: { zh: 'AI åº”ç”¨å®¹å™¨åŒ–', ja: 'AI ã‚¢ãƒ—ãƒªã®ã‚³ãƒ³ãƒ†ãƒŠåŒ–' },
        code: `# === Dockerfile ===
FROM python:3.11-slim
WORKDIR /app

RUN apt-get update && apt-get install -y build-essential && rm -rf /var/lib/apt/lists/*

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .
EXPOSE 8000
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]

# === docker-compose.yml ===
version: '3.8'
services:
  ai-app:
    build: .
    ports:
      - "8000:8000"
    environment:
      - ANTHROPIC_API_KEY=\${ANTHROPIC_API_KEY}
    volumes:
      - ./data:/app/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s

  chroma:
    image: chromadb/chroma:latest
    ports:
      - "8001:8000"
    volumes:
      - chroma_data:/chroma/chroma

volumes:
  chroma_data:

# === .env ===
ANTHROPIC_API_KEY=your-key

# === éƒ¨ç½²å‘½ä»¤ ===
docker build -t ai-app .
docker-compose up -d
docker-compose logs -f
docker-compose down`,
        language: 'bash',
        difficulty: 'intermediate',
        tags: ['Docker', 'Production']
      },
      {
        id: 'dep-2',
        title: { zh: 'Hugging Face Spaces', ja: 'HF Spaces' },
        description: { zh: 'å…è´¹éƒ¨ç½² Gradio', ja: 'Gradio ç„¡æ–™ãƒ‡ãƒ—ãƒ­ã‚¤' },
        code: `# Hugging Face Spaces å…è´¹éƒ¨ç½²

# === é¡¹ç›®ç»“æ„ ===
# my-app/
# â”œâ”€â”€ app.py
# â”œâ”€â”€ requirements.txt
# â””â”€â”€ README.md

# === app.py ===
import gradio as gr
from anthropic import Anthropic
import os

client = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))

def chat(message, history):
    messages = []
    for h, a in history:
        messages.extend([
            {"role": "user", "content": h},
            {"role": "assistant", "content": a}
        ])
    messages.append({"role": "user", "content": message})

    response = client.messages.create(
        model="claude-sonnet-4-20250514",
        max_tokens=1024,
        messages=messages
    )
    return response.content[0].text

demo = gr.ChatInterface(
    fn=chat,
    title="ğŸ¤– AI èŠå¤©åŠ©æ‰‹",
    examples=["ä½ å¥½", "è§£é‡Šæœºå™¨å­¦ä¹ ", "å†™é¦–è¯—"]
)
demo.launch()

# === requirements.txt ===
gradio>=4.0.0
anthropic>=0.18.0

# === README.md (Spaceé…ç½®) ===
---
title: AI Chat
emoji: ğŸ¤–
colorFrom: blue
colorTo: purple
sdk: gradio
sdk_version: 4.15.0
app_file: app.py
---

# === éƒ¨ç½²æ­¥éª¤ ===
# 1. ç™»å½• huggingface.co
# 2. New Space -> é€‰æ‹© Gradio
# 3. ä¸Šä¼ æ–‡ä»¶æˆ–è¿æ¥ GitHub
# 4. Settings > Secrets æ·»åŠ  ANTHROPIC_API_KEY
# 5. å®Œæˆï¼è®¿é—® https://huggingface.co/spaces/ä½ çš„ç”¨æˆ·å/ç©ºé—´å`,
        language: 'python',
        difficulty: 'beginner',
        tags: ['Hugging Face', 'Free', 'Gradio']
      },
      {
        id: 'dep-3',
        title: { zh: 'Vercel + Next.js', ja: 'Vercel + Next.js' },
        description: { zh: 'å…¨æ ˆ AI åº”ç”¨', ja: 'ãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯ AI ã‚¢ãƒ—ãƒª' },
        code: `// Next.js AI åº”ç”¨éƒ¨ç½²åˆ° Vercel

// === app/api/chat/route.ts ===
import Anthropic from '@anthropic-ai/sdk';
import { NextRequest, NextResponse } from 'next/server';

const client = new Anthropic();

export async function POST(req: NextRequest) {
  const { messages } = await req.json();

  const response = await client.messages.create({
    model: 'claude-sonnet-4-20250514',
    max_tokens: 1024,
    messages: messages,
  });

  return NextResponse.json({
    content: response.content[0].text,
  });
}

// === app/page.tsx ===
'use client';
import { useState } from 'react';

export default function Home() {
  const [messages, setMessages] = useState([]);
  const [input, setInput] = useState('');
  const [loading, setLoading] = useState(false);

  const send = async () => {
    if (!input.trim()) return;

    const newMessages = [...messages, { role: 'user', content: input }];
    setMessages(newMessages);
    setInput('');
    setLoading(true);

    const res = await fetch('/api/chat', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ messages: newMessages }),
    });
    const data = await res.json();
    setMessages([...newMessages, { role: 'assistant', content: data.content }]);
    setLoading(false);
  };

  return (
    <main className="p-8 max-w-2xl mx-auto">
      <h1 className="text-2xl font-bold mb-4">ğŸ¤– AI Chat</h1>
      <div className="border rounded p-4 h-96 overflow-y-auto mb-4">
        {messages.map((m, i) => (
          <div key={i} className={\`mb-2 \${m.role === 'user' ? 'text-right' : ''}\`}>
            <span className={\`inline-block p-2 rounded \${m.role === 'user' ? 'bg-blue-500 text-white' : 'bg-gray-200'}\`}>
              {m.content}
            </span>
          </div>
        ))}
        {loading && <div>æ€è€ƒä¸­...</div>}
      </div>
      <div className="flex gap-2">
        <input value={input} onChange={e => setInput(e.target.value)} onKeyPress={e => e.key === 'Enter' && send()} className="flex-1 border p-2 rounded" />
        <button onClick={send} className="bg-blue-500 text-white px-4 rounded">å‘é€</button>
      </div>
    </main>
  );
}

// === éƒ¨ç½² ===
// 1. æ¨é€åˆ° GitHub
// 2. ç™»å½• vercel.com
// 3. Import ä»“åº“
// 4. æ·»åŠ ç¯å¢ƒå˜é‡ ANTHROPIC_API_KEY
// 5. Deploy!`,
        language: 'typescript',
        difficulty: 'intermediate',
        tags: ['Vercel', 'Next.js', 'Full Stack']
      }
    ]
  }
};

const CopyButton: React.FC<{ text: string }> = ({ text }) => {
  const [copied, setCopied] = useState(false);

  const handleCopy = async () => {
    await navigator.clipboard.writeText(text);
    setCopied(true);
    setTimeout(() => setCopied(false), 2000);
  };

  return (
    <button
      onClick={handleCopy}
      className="p-2 hover:bg-white/10 rounded-lg transition-colors"
      title="Copy code"
    >
      {copied ? (
        <Check size={16} className="text-green-400" />
      ) : (
        <Copy size={16} className="text-gray-400 hover:text-white" />
      )}
    </button>
  );
};

const CodeBlock: React.FC<{ example: CodeExample; language: string }> = ({ example, language: lang }) => {
  const difficultyColors = {
    beginner: 'bg-green-100 text-green-700',
    intermediate: 'bg-yellow-100 text-yellow-700',
    advanced: 'bg-red-100 text-red-700'
  };

  const difficultyLabels = {
    beginner: { zh: 'å…¥é—¨', ja: 'å…¥é–€' },
    intermediate: { zh: 'ä¸­çº§', ja: 'ä¸­ç´š' },
    advanced: { zh: 'é«˜çº§', ja: 'ä¸Šç´š' }
  };

  return (
    <div className="bg-white rounded-lg border border-slate-200 shadow-sm overflow-hidden">
      <div className="p-4 border-b border-slate-100">
        <div className="flex items-center justify-between mb-2">
          <h3 className="font-semibold text-slate-800">
            {example.title[lang === 'ja' ? 'ja' : 'zh']}
          </h3>
          <span className={`px-2 py-0.5 text-xs font-medium rounded ${difficultyColors[example.difficulty]}`}>
            {difficultyLabels[example.difficulty][lang === 'ja' ? 'ja' : 'zh']}
          </span>
        </div>
        <p className="text-sm text-slate-500">
          {example.description[lang === 'ja' ? 'ja' : 'zh']}
        </p>
        <div className="flex flex-wrap gap-1 mt-2">
          {example.tags.map((tag, i) => (
            <span key={i} className="px-2 py-0.5 bg-slate-100 text-slate-600 text-xs rounded">
              {tag}
            </span>
          ))}
        </div>
      </div>
      <div className="relative">
        <div className="absolute top-2 right-2 flex items-center gap-2 z-10">
          <span className="px-2 py-0.5 bg-slate-700 text-slate-300 text-xs rounded">
            {example.language}
          </span>
          {/* Run buttons based on language */}
          {example.language === 'python' && (
            <ColabButton
              code={example.code}
              title={example.title[lang === 'ja' ? 'ja' : 'zh']}
            />
          )}
          {(example.language === 'javascript' || example.language === 'typescript') && (
            <StackBlitzButton
              code={example.code}
              language={example.language as 'javascript' | 'typescript'}
              title={example.title[lang === 'ja' ? 'ja' : 'zh']}
            />
          )}
          <CopyButton text={example.code} />
        </div>
        <pre className="bg-slate-900 text-slate-100 p-4 overflow-x-auto text-sm">
          <code>{example.code}</code>
        </pre>
      </div>
    </div>
  );
};

export default function AICodeExamplesPage() {
  const navigate = useNavigate();
  const language = useLanguageStore(state => state.language);
  const [selectedCategory, setSelectedCategory] = useState<Category>('langchain');

  const category = codeExamples[selectedCategory];

  return (
    <div className="min-h-screen bg-slate-50">
      {/* Header */}
      <header className="bg-slate-800 text-white sticky top-0 z-50">
        <div className="px-6 lg:px-10 py-4">
          <div className="flex items-center justify-between">
            <div className="flex items-center gap-4">
              <button
                onClick={() => navigate('/')}
                className="flex items-center gap-2 text-slate-300 hover:text-white transition-colors"
              >
                <Home size={20} />
                <span className="hidden sm:inline">{language === 'ja' ? 'ãƒ›ãƒ¼ãƒ ' : 'é¦–é¡µ'}</span>
              </button>
              <ChevronRight size={16} className="text-slate-500" />
              <h1 className="text-lg font-semibold">
                {language === 'ja' ? 'AI ã‚³ãƒ¼ãƒ‰å®Ÿè·µä¾‹' : 'AI ä»£ç å®æˆ˜'}
              </h1>
            </div>
            <div className="flex items-center gap-2">
              <Code2 size={20} className="text-slate-400" />
              <span className="font-semibold">StudyForge</span>
            </div>
          </div>
        </div>
      </header>

      {/* Category Tabs */}
      <div className="bg-white border-b border-slate-200">
        <div className="px-6 lg:px-10">
          <div className="flex overflow-x-auto gap-1 py-2">
            {(Object.keys(codeExamples) as Category[]).map((cat) => {
              const Icon = codeExamples[cat].icon;
              const isActive = selectedCategory === cat;
              return (
                <button
                  key={cat}
                  onClick={() => setSelectedCategory(cat)}
                  className={`flex items-center gap-2 px-4 py-2 rounded-lg whitespace-nowrap transition-all ${
                    isActive
                      ? 'bg-cyan-100 text-cyan-700 font-medium'
                      : 'text-slate-600 hover:bg-slate-100'
                  }`}
                >
                  <Icon size={18} />
                  <span>{codeExamples[cat].name[language === 'ja' ? 'ja' : 'zh']}</span>
                </button>
              );
            })}
          </div>
        </div>
      </div>

      {/* Main Content */}
      <main className="px-6 lg:px-10 py-8">
        {/* Category Header */}
        <div className="mb-6">
          <div className="flex items-center gap-3 mb-2">
            <div className={`p-2 rounded-lg bg-gradient-to-br ${category.gradient}`}>
              <category.icon size={24} className="text-white" />
            </div>
            <div>
              <h2 className="text-xl font-bold text-slate-800">
                {category.name[language === 'ja' ? 'ja' : 'zh']}
              </h2>
              <p className="text-sm text-slate-500">
                {category.description[language === 'ja' ? 'ja' : 'zh']}
              </p>
            </div>
          </div>
        </div>

        {/* Code Examples */}
        <div className="grid gap-6">
          {category.examples.map((example) => (
            <CodeBlock key={example.id} example={example} language={language} />
          ))}
        </div>

        {/* Quick Navigation */}
        <div className="mt-12 p-6 bg-gradient-to-br from-slate-100 to-slate-50 rounded-lg">
          <h3 className="text-lg font-bold text-slate-800 mb-4 flex items-center gap-2">
            <Play size={20} className="text-cyan-600" />
            {language === 'ja' ? 'ä»–ã®ã‚«ãƒ†ã‚´ãƒªã‚’è¦‹ã‚‹' : 'æŸ¥çœ‹å…¶ä»–åˆ†ç±»'}
          </h3>
          <div className="grid grid-cols-2 md:grid-cols-5 gap-3">
            {(Object.keys(codeExamples) as Category[]).map((cat) => {
              if (cat === selectedCategory) return null;
              const Icon = codeExamples[cat].icon;
              return (
                <button
                  key={cat}
                  onClick={() => setSelectedCategory(cat)}
                  className="flex items-center gap-2 p-3 bg-white rounded-lg border border-slate-200 hover:border-slate-300 hover:shadow-md transition-all text-left"
                >
                  <Icon size={18} className="text-slate-500" />
                  <span className="text-sm font-medium text-slate-700">
                    {codeExamples[cat].name[language === 'ja' ? 'ja' : 'zh']}
                  </span>
                </button>
              );
            })}
          </div>
        </div>
      </main>

      {/* Footer */}
      <footer className="bg-slate-800 text-white py-4 mt-auto">
        <div className="px-6 lg:px-10 text-center">
          <p className="text-slate-300 text-sm">
            <span className="font-semibold text-white">StudyForge</span>
            <span className="mx-2">Â·</span>
            {language === 'ja' ? 'AIå­¦ç¿’ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ' : 'AI å­¦ä¹ å¹³å°'}
          </p>
        </div>
      </footer>
    </div>
  );
}

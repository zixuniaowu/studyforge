// AI 进阶实战电子书内容
// 设计理念：面向有基础的用户，深入实践技巧

import { Book } from './aiBookContent';

// ============================================
// AI 进阶实战 - 实用技巧篇
// ============================================
export const aiAdvancedBook: Book = {
  id: 'ai-advanced',
  title: {
    zh: 'AI 进阶实战',
    ja: 'AI 実践上級編'
  },
  subtitle: {
    zh: '掌握 AI 的高级使用技巧',
    ja: 'AIの上級テクニックをマスターする'
  },
  author: 'StudyForge',
  chapters: [
    // ============================================
    // 序章：AI 动态时间轴
    // ============================================
    {
      id: 'chapter-0',
      number: 0,
      title: { zh: 'AI 动态时间轴', ja: 'AI動向タイムライン' },
      subtitle: { zh: '追踪最新 AI 技术发展', ja: '最新AI技術の動向を追跡' },
      sections: [
        {
          id: 'ch0-timeline',
          title: { zh: '2025 年 AI 大事记', ja: '2025年AI大事記' },
          content: {
            zh: `
## 🚀 AI 技术动态时间轴

追踪最新 AI 发展动态，点击链接深入了解相关知识点。

---

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        2025 年 AI 技术发展时间轴                          │
└─────────────────────────────────────────────────────────────────────────┘

  2025.01                                                          2025.07
     │                                                                │
     ▼                                                                ▼
─────●────────●────────●────────●────────●────────●────────●────────●─────
     │        │        │        │        │        │        │        │
   Claude   GPT-5    Claude   Codex    Sub-     Skills   Chrome   最新
   Opus4.5  发布     Code     Agent    agents   标准化   集成     动态
   发布              MCP      Skills   功能     通用
\`\`\`

---

## 📅 详细时间轴

### 🔵 2025年1月 - Claude Opus 4.5 发布

Anthropic 发布 Claude Opus 4.5，成为当时最强大的 AI 模型。

| 特性 | 说明 |
|------|------|
| 知识截止 | 2025年1月 |
| 长文本能力 | 可输出 1.2 万+ 字符 |
| 创作能力 | 文学创作、设计感更强 |

📖 **相关知识点**：→ 本书 [3.10 AI 模型对比与选型]

---

### 🔵 2025年2月 - GPT-5 系列发布

OpenAI 陆续发布 GPT-5.0、5.1、5.2，持续迭代改进。

**实测对比要点**：
- 长文本生成：Claude 约 12000 字符，GPT-5.1 约 6900 字符
- 数学编程：GPT-5.1 更强
- 浏览器自动化：GPT-5.1 略胜

📖 **深入学习**：→ 本书 [3.10 Claude vs GPT 实测对比]

---

### 🟢 2025年3月 - Claude Code 正式发布

基于 MCP 协议的 AI 编程助手 Claude Code 发布，改变开发方式。

**核心特性**：
- ✅ 直接读写代码文件
- ✅ 运行终端命令
- ✅ MCP 协议扩展能力

**Boris 核心经验**：Opus 4.5 + Plan 模式、多实例并行、Slash 命令自动化

📖 **深入学习**：→ 本书 [3.3 MCP：AI 的"万能接口"] 和 [序章：大佬经验分享]

---

### 🟢 2025年4月 - Sub-agents 功能上线

Claude Code 支持 Sub-agents，实现专家团队协作模式。

\`\`\`
Sub-agents 架构示意
───────────────────
        主 Agent
            │
    ┌───────┼───────┐
    ▼       ▼       ▼
  代码    测试    文档
  审查    专家    生成
\`\`\`

**核心价值**：上下文隔离避免污染、专业化分工成功率更高、可复用团队共享

📖 **深入学习**：→ 本书 [3.3 Sub-agents 专家团队协作]

---

### 🟡 2025年5月 - Agent Skills 成为行业标准

OpenAI Codex 采用 Anthropic 的 Skills 规范，实现跨平台通用。

| 平台 | Skills 支持 |
|------|------------|
| Claude Code | ✅ 原生支持 |
| GPT Codex | ✅ 新增支持 |
| Cursor | ✅ 兼容 |

> 💡 你写的 Skills 可以在 Claude 和 Codex 之间通用！

**重大意义**：一次编写到处运行，团队知识资产化，AI 能力可定制

📖 **深入学习**：→ 本书 [3.9 OpenAI Codex 使用指南]

---

### 🟡 2025年6月 - Claude Code Chrome 集成

Claude Code 原生支持 Chrome 浏览器，实现端到端自动化测试。

**应用场景**：
- 前端 UI 调试
- 浏览器自动化测试
- 保留登录状态和扩展插件

**验证循环技巧**：让 Claude 通过 Chrome 自动验证 UI，直到功能和体验满意，质量可提升 2-3 倍

📖 **深入学习**：→ 本书 [3.3 Boris 的验证反馈循环]

---

### 🔴 2025年7月 - 最新动态

**最新工具一览：**

| 工具 | 功能 | 亮点 |
|------|------|------|
| Ralph Wiggum | AI 自动迭代修复 | Bug 到完美应用只需一条命令 |
| Claudia | Claude Code GUI | 告别命令行，可视化操作 |
| SuperClaude | 能力增强 300% | 19 个命令 + 9 大专家角色 |
| Claude Code PM | 并行开发 | GitHub Issues 秒变独立分支 |
| Kilo Code | 融合 Cline + Roo | 5 种智能模式切换 |

**发展趋势总结**：
- 📊 Spec-Driven 开发逐渐取代 Vibe Coding（先写规格再编码）
- 🔧 上下文工程比提示工程效果好 10 倍（给足背景信息）
- 🤝 多 AI 协作成为常态（Claude + GPT + Gemini 各取所长）

📖 **深入学习**：→ 本书 [序章：大佬经验分享] 有 SuperClaude 完整使用指南

---

## 🗺️ 本书导航

> 💡 **使用左侧目录可快速跳转到任意章节**

| 章节 | 内容 |
|------|------|
| 第0章 | AI 动态时间轴 - 追踪2025年AI发展 |
| 第2章 | 提示词工程进阶 - 与AI对话的艺术 |
| 第3章 | AI Agents 智能体 - MCP、Claude Code等 |
| 第4章 | RAG 检索增强生成 - 让AI获取新知识 |
| 第1章 | 大模型技术深度解析 - Transformer与微调 |

---

> 📌 **提示**：本时间轴会持续更新，记录 AI 领域重要发展节点。
            `,
            ja: `
## 🚀 AI技術動向タイムライン

最新のAI発展動向を追跡し、リンクをクリックして関連知識を深く理解しましょう。

---

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        2025年 AI技術発展タイムライン                       │
└─────────────────────────────────────────────────────────────────────────┘

  2025.01                                                          2025.07
     │                                                                │
─────●────────●────────●────────●────────●────────●────────●────────●─────
   Claude   GPT-5    Claude   Codex    Sub-     Skills   Chrome   最新
   Opus4.5  発表     Code     Agent    agents   標準化   統合     動向
\`\`\`

---

## 📅 詳細タイムライン

### 🔵 2025年1月 - Claude Opus 4.5 リリース

AnthropicがClaude Opus 4.5をリリース。当時最強のAIモデルに。

📖 **関連知識**：→ 本書 [3.5 AIモデル比較と選定]

---

### 🔵 2025年2月 - GPT-5シリーズ発表

OpenAIがGPT-5.0、5.1、5.2を順次リリース。

**実測比較ポイント**：長文生成はClaude優位（約12000字 vs 6900字）、数学・プログラミングはGPT-5.1が強い

📖 **詳細**：→ 本書 [3.10 Claude vs GPT 実測比較]

---

### 🟢 2025年3月 - Claude Code 正式リリース

MCPベースのAIプログラミングアシスタントがリリース。

**Borisの核心経験**：Opus 4.5 + Planモード、マルチインスタンス並列、Slashコマンド自動化

📖 **詳細**：→ 本書 [3.3 MCP] と [序章：エキスパートの知見]

---

### 🟢 2025年4月 - Sub-agents 機能リリース

Claude CodeがSub-agentsをサポート、専門家チーム協力を実現。

**コアバリュー**：コンテキスト分離で汚染防止、専門化分業で成功率向上、再利用可能

📖 **詳細**：→ 本書 [3.3 Sub-agents 専門家チーム協力]

---

### 🟡 2025年5月 - Agent Skills が業界標準に

OpenAI CodexがAnthropicのSkills仕様を採用。

**重要な意義**：一度作成すればどこでも使える、チーム知識の資産化、AIカスタマイズ可能

📖 **詳細**：→ 本書 [3.9 OpenAI Codex 使用ガイド]

---

### 🟡 2025年6月 - Claude Code Chrome 統合

Claude CodeがChromeブラウザをネイティブサポート。

**検証ループ技術**：ClaudeがChromeでUI自動検証、品質2〜3倍向上

---

### 🔴 2025年7月 - 最新動向

| ツール | 機能 | ハイライト |
|--------|------|-----------|
| Ralph Wiggum | AI自動反復修正 | バグから完璧なアプリへ一コマンド |
| Claudia | Claude Code GUI | コマンドライン不要、視覚化操作 |
| SuperClaude | 能力強化300% | 19コマンド + 9エキスパートロール |
| Kilo Code | Cline + Roo融合 | 5つのスマートモード |

**トレンド**：
- Spec-Driven開発がVibeコーディングに代わる
- コンテキストエンジニアリングはプロンプトの10倍効果的
- マルチAI協力が常態化（Claude + GPT + Gemini）

📖 **詳細**：→ 本書 [序章：エキスパートの知見] に SuperClaude 完全ガイド

---

## 🗺️ クイックナビゲーション

| 知りたいこと | ジャンプ先 |
|-------------|----------|
| モデル選び方 | → 第3章第10節 [AIモデル比較] |
| MCPとは？ | → 第3章第3節 [MCP] |
| プロンプト技術 | → 第2章 [プロンプトエンジニアリング] |
| RAG最適化 | → 第4章第3節 [RAG最適化] |
            `
          }
        },
        {
          id: 'ch0-experts',
          title: { zh: '大佬经验分享', ja: 'エキスパートの知見' },
          content: {
            zh: `
## 🎯 大佬经验分享

来自 AI 领域顶尖开发者的实战经验，帮你少走弯路。

---

## 👤 Boris Cherny - Claude Code 创始人

> "开箱即用才是最强工作流，复利工程思维让效率翻倍！"

Boris 是 Anthropic 的工程师，Claude Code 项目的核心创建者。他在社交媒体上分享了自己的工作流程，获得了数万点赞和转发。以下是他的核心经验总结。

### 核心理念：简单但极致

Boris 的哲学是"**让 AI 成为工作流的自然延伸**"，而不是过度定制。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                     Boris 的工作流核心                                   │
└─────────────────────────────────────────────────────────────────────────┘

  ┌─────────────┐    ┌─────────────┐    ┌─────────────┐
  │  开箱即用   │ ──▶│  复利思维   │ ──▶│  极致效率   │
  │  不过度配置 │    │  经验复用   │    │  一次搞定   │
  └─────────────┘    └─────────────┘    └─────────────┘
\`\`\`

---

### 🔧 Boris 的具体配置

#### 1. 大模型优先策略

| 选择 | 原因 |
|------|------|
| Opus 4.5 + thinking 模式 | "一次搞定"成功率更高 |
| 不用小模型 | 减少反复纠正的轮次 |

> 💡 **复利思维**：虽然大模型更慢，但减少了 3-5 次修改循环，总时间反而更短。

#### 2. Plan 模式驱动

\`\`\`
Boris 的工作流程
────────────────
1. Shift+Tab 两次 → 进入 Plan 模式
2. 和 Claude 把整个 PR 计划聊透
3. 满意后 → 切到 auto-accept 模式
4. 让 Claude 一次性完成所有工作

关键：架构设计优先于代码执行
\`\`\`

#### 3. 并行处理：5+10 实例

\`\`\`
Boris 的多实例布局
──────────────────
终端 (iTerm2)                  Web 版
─────────────                  ──────
Tab 1: 主功能开发              浏览器 Tab × 5-10
Tab 2: Bug 修复                用于长时间任务
Tab 3: 测试编写                如文档生成
Tab 4: 代码审查                架构设计等
Tab 5: 部署脚本

→ 避免上下文切换成本
→ 真正的并行开发
\`\`\`

#### 4. Slash 命令自动化

在 \`~/.claude/commands/\` 目录下创建命令：

\`\`\`bash
# /commit-push-pr 命令
自动执行：
1. 收集 git status
2. 生成 commit message
3. 推送代码
4. 创建 PR

# 使用
> /commit-push-pr
\`\`\`

#### 5. 验证闭环机制

> "给 Claude 一个验证自己的方式，是最关键的一环。"

\`\`\`
验证闭环
────────
代码生成 → 自动测试 → 失败?
                        ↓
                    自动修复
                        ↓
                    重新测试
                        ↓
                    通过 ✓
\`\`\`

#### 6. 团队知识库

\`\`\`bash
# 将 Claude 使用规范 check in 到 git
project/
├── .claude/
│   ├── CLAUDE.md          # 项目规范
│   ├── commands/          # 团队共享命令
│   └── agents/            # Sub-agents 配置

→ "教 AI 的经验"版本化管理
→ 团队效率集体提升
\`\`\`

---

## 🚀 SuperClaude 框架

> "让 Claude Code 编程能力暴增 300%！"

SuperClaude 是一个专门为 Claude Code 设计的综合配置框架，通过结构化的配置文件和专业化的工作流程，将 Claude Code 从通用 AI 助手转变为专业的开发伙伴。

### 安装

\`\`\`bash
git clone https://github.com/NomenAK/SuperClaude.git
cd SuperClaude
./install.sh

# 验证安装
ls ~/.claude/           # 4 个主文件
ls ~/.claude/commands/  # 17 个命令文件
\`\`\`

### 19 个专业命令

| 类别 | 命令 | 功能 |
|------|------|------|
| 构建 | /build | 项目构建 |
| 开发 | /user:dev-setup | 环境配置 |
| 测试 | /user:test | 测试执行 |
| 分析 | /user:analyze | 代码分析 |
| 调试 | /user:troubleshoot | 问题排查 |
| 优化 | /user:improve | 代码改进 |
| 部署 | /user:deploy | 项目部署 |
| 迁移 | /user:migrate | 数据迁移 |
| 安全 | /user:scan | 安全扫描 |
| 设计 | /user:design | 架构设计 |

### 9 大专家角色

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                      SuperClaude 专家角色                               │
└─────────────────────────────────────────────────────────────────────────┘

  architect        frontend         backend          security
  ┌────────┐      ┌────────┐      ┌────────┐      ┌────────┐
  │系统设计 │      │用户体验 │      │API性能 │      │安全审计 │
  │可扩展性 │      │React   │      │数据库  │      │威胁建模 │
  └────────┘      └────────┘      └────────┘      └────────┘

  qa               performance      analyzer         mentor
  ┌────────┐      ┌────────┐      ┌────────┐      ┌────────┐
  │测试质量 │      │性能优化 │      │问题分析 │      │技术指导 │
  │覆盖率   │      │瓶颈定位 │      │根因定位 │      │代码教学 │
  └────────┘      └────────┘      └────────┘      └────────┘

  refactorer
  ┌────────┐
  │代码重构 │
  │质量提升 │
  └────────┘
\`\`\`

### 使用示例

\`\`\`bash
# 创建 React 项目 + TDD + 前端专家
/build --react --magic --tdd --persona-frontend

# 安全扫描 + 安全专家
/user:scan --persona-security

# 性能优化 + 性能专家
/user:improve --persona-performance
\`\`\`

---

## 📚 上下文工程（Context Engineering）

> "效果比提示工程好 10 倍，比 Vibe Coding 好 100 倍。"

上下文工程是一种新的 AI 编程方法论，强调通过精心设计的上下文（而不仅仅是提示词）来引导 AI 产出高质量代码。

### 核心方法：PRP（产品需求提示）

\`\`\`
传统方式                          PRP 方式
──────────                        ──────────
"帮我写个登录功能"                1. 功能需求定义
        │                        2. 分解为可验证步骤
        ▼                        3. 每步完成后验证
  代码质量不可控                  4. 迭代直到完美
                                         │
                                         ▼
                                   代码质量有保障
\`\`\`

---

## 🔧 更多实用技巧

### Cursor + Claude Code 组合
同时使用 IDE 插件（Cursor）和命令行工具（Claude Code），分工协作：
- Cursor：快速补全、小范围修改
- Claude Code：大规模重构、复杂任务

### Spec-Driven 开发
先写规格文档（spec.md），再让 AI 按规格实现：
1. 需求分析 → 编写 spec
2. spec 评审 → 确认无误
3. 按 spec 实现 → 一次通过

### Output Styles 功能
Claude Code 支持多种输出风格：
- **Concise**：简洁模式，减少解释
- **Verbose**：详细模式，包含推理过程
- **Learning**：学习模式，边写代码边教学

### 多 AI 协作
Claude + GPT + Gemini 组合使用，取长补短。

---

## 💡 核心经验总结

| 大佬 | 核心理念 | 关键技巧 |
|------|----------|----------|
| Boris | 开箱即用 + 复利思维 | Plan 模式、并行实例、验证闭环 |
| SuperClaude | 专业化 + 模块化 | 19命令、9角色、MCP集成 |
| 上下文工程 | 结构化 + 可验证 | PRP 方法、迭代验证 |

> 📌 **记住**：选择适合自己的方法，不要盲目复制，找到自己的最佳实践。
            `,
            ja: `
## 🎯 エキスパートの知見

AI分野トップ開発者の実践経験から学びましょう。

---

## 👤 Boris Cherny - Claude Code 創設者

> "開封即用が最強ワークフロー、複利思考で効率倍増！"

Boris は Anthropic のエンジニアで、Claude Code プロジェクトの核心的な創設者です。彼がSNSで共有したワークフローは数万のいいねと転送を獲得しました。

### コア理念：シンプルだが極致

Borisの哲学は「**AIをワークフローの自然な延長にする**」こと。

---

### 🔧 Boris の具体的な設定

#### 1. 大規模モデル優先

| 選択 | 理由 |
|------|------|
| Opus 4.5 + thinking モード | 「一発成功」率が高い |
| 小規模モデルは使わない | 修正ループを減らす |

#### 2. Plan モード駆動

\`\`\`
Boris のワークフロー
────────────────
1. Shift+Tab 2回 → Plan モードへ
2. Claude と PR 計画を詳細に議論
3. 満足したら → auto-accept モードへ
4. Claude に一括で完成させる
\`\`\`

#### 3. 並列処理：5+10 インスタンス

ターミナル 5つ + Web版 5-10個を同時実行。

#### 4. 検証クローズドループ

> "Claude に自己検証の方法を与えることが最も重要。"

---

## 🚀 SuperClaude フレームワーク

> "Claude Code のプログラミング能力を 300% 向上！"

SuperClaude は Claude Code 専用の総合設定フレームワークで、構造化された設定ファイルと専門化されたワークフローにより、Claude Code を汎用AIアシスタントから専門的な開発パートナーに変革します。

### 19の専門コマンド + 9つの専門家ロール

| ロール | 専門領域 |
|--------|---------|
| architect | システム設計 |
| frontend | ユーザー体験、React |
| backend | API、パフォーマンス |
| security | セキュリティ監査 |
| qa | テスト品質 |
| performance | パフォーマンス最適化 |
| analyzer | 根本原因分析 |
| mentor | 技術指導 |
| refactorer | コードリファクタリング |

---

## 📚 コンテキストエンジニアリング

> "プロンプトエンジニアリングより10倍効果的。"

コンテキストエンジニアリングは新しいAIプログラミング方法論で、プロンプトだけでなく、精心に設計されたコンテキストを通じてAIに高品質コードを生成させることを重視します。

### PRP（製品要件プロンプト）方式

複雑な機能を検証可能なステップに分解し、各ステップ完了後に検証。

---

## 🔧 その他の実用テクニック

### Cursor + Claude Code 組み合わせ
IDEプラグイン（Cursor）とCLIツール（Claude Code）を同時使用：
- Cursor：素早い補完、小範囲の修正
- Claude Code：大規模リファクタリング、複雑なタスク

### Spec-Driven 開発
仕様書（spec.md）を先に書き、AIに仕様通り実装させる。

### Output Styles 機能
Claude Code は複数の出力スタイルをサポート：
- **Concise**：簡潔モード
- **Verbose**：詳細モード
- **Learning**：学習モード

---

## 💡 コア経験まとめ

| エキスパート | コア理念 | キーテクニック |
|-------------|---------|---------------|
| Boris | 開封即用 + 複利思考 | Plan モード、並列インスタンス |
| SuperClaude | 専門化 + モジュール化 | 19コマンド、9ロール |
| コンテキストエンジニアリング | 構造化 + 検証可能 | PRP 方式 |
            `
          }
        }
      ]
    },
    // ============================================
    // 第四章：大模型技术深度解析
    // ============================================
    {
      id: 'chapter-1',
      number: 1,
      title: { zh: '大模型技术深度解析', ja: '大規模言語モデル技術詳解' },
      subtitle: { zh: '理解 Transformer 与微调技术', ja: 'Transformerとファインチューニング技術を理解する' },
      sections: [
        {
          id: 'ch1-intro',
          title: { zh: '引言：走进大模型的内部', ja: '序章：大規模モデルの内部へ' },
          content: {
            zh: `
当我们使用 ChatGPT、Claude、Gemini 这些 AI 时，你是否好奇过：

- 它们是如何理解我们说的话的？
- 为什么它们能生成如此流畅的文字？
- "参数"到底是什么意思？7B、70B、405B 有什么区别？

本章将带你深入大模型的技术内核，理解这些神奇能力背后的原理。

---

## 为什么要了解技术原理？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        了解原理的三大价值                                 │
└─────────────────────────────────────────────────────────────────────────┘

  价值1：更好地使用 AI                价值2：做出正确的选型
  ┌──────────────────────┐           ┌──────────────────────┐
  │                      │           │                      │
  │  • 理解 AI 的能力边界  │           │  • 知道什么模型适合    │
  │  • 写出更有效的提示词  │           │    什么任务           │
  │  • 避免常见的使用误区  │           │  • 评估成本与效果      │
  │                      │           │  • 选择合适的部署方案  │
  └──────────────────────┘           └──────────────────────┘

                    价值3：具备微调和定制能力
                    ┌──────────────────────┐
                    │                      │
                    │  • 根据需求微调模型    │
                    │  • 构建领域专属 AI     │
                    │  • 优化性能和成本      │
                    │                      │
                    └──────────────────────┘
\`\`\`

---

## 本章你将学到

| 章节 | 内容 | 收获 |
|------|------|------|
| 4.1 | Transformer 架构 | 理解大模型的核心技术 |
| 4.2 | 注意力机制 | 理解 AI 如何"理解"语言 |
| 4.3 | 主流大模型对比 | 了解 GPT、Claude、Gemini 的区别 |
| 4.4 | 大模型微调入门 | 学会定制自己的 AI |
| 4.5 | LoRA 微调实战 | 低成本微调的最佳实践 |

让我们开始这段技术探索之旅！
            `,
            ja: `
ChatGPT、Claude、Geminiなどを使う時、こんな疑問を持ったことはありませんか：

- どうやって私たちの言葉を理解しているのか？
- なぜこんなに流暢な文章を生成できるのか？
- 「パラメータ」とは何？7B、70B、405Bの違いは？

この章では大規模モデルの技術コアに迫り、その魔法のような能力の原理を理解します。

---

## 技術原理を理解する価値

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        原理理解の3つの価値                               │
└─────────────────────────────────────────────────────────────────────────┘

  価値1：AIをより良く使う            価値2：正しいモデル選択
  ┌──────────────────────┐           ┌──────────────────────┐
  │  • AIの能力境界を理解  │           │  • タスクに適したモデル │
  │  • 効果的なプロンプト  │           │  • コストと効果の評価  │
  │  • よくある誤解を回避  │           │  • 適切なデプロイ方式  │
  └──────────────────────┘           └──────────────────────┘

                    価値3：ファインチューニング能力
                    ┌──────────────────────┐
                    │  • ニーズに応じた調整  │
                    │  • ドメイン特化AI構築  │
                    │  • 性能とコスト最適化  │
                    └──────────────────────┘
\`\`\`

---

## この章で学ぶこと

| セクション | 内容 | 得られるもの |
|----------|------|------------|
| 4.1 | Transformerアーキテクチャ | 大規模モデルのコア技術理解 |
| 4.2 | アテンション機構 | AIが言語を「理解」する仕組み |
| 4.3 | 主流モデル比較 | GPT、Claude、Geminiの違い |
| 4.4 | ファインチューニング入門 | 自分のAIをカスタマイズ |
| 4.5 | LoRA実践 | 低コストチューニングのベストプラクティス |
            `
          }
        },
        {
          id: 'ch1-transformer',
          title: { zh: '1.1 Transformer 架构详解', ja: '1.1 Transformerアーキテクチャ詳解' },
          content: {
            zh: `
## Transformer：改变世界的架构

2017 年，Google 发表了著名论文《Attention Is All You Need》，提出了 Transformer 架构。今天的 GPT、Claude、Gemini 全部基于这个架构。

---

## 动手体验：Transformer 工作流程

在深入学习之前，先通过这个交互式演示直观感受 Transformer 是如何处理文本的：

::transformer-v2-viz::

---

## 文本生成的核心原理：下一个词预测

大语言模型的本质其实很简单：**预测下一个词**。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    文本生成 = 不断预测下一个词                              │
└─────────────────────────────────────────────────────────────────────────┘

  用户输入: "今天天气"

  Step 1: 模型预测下一个词
  ┌────────────────────────────────────────┐
  │ 输入: "今天天气"                        │
  │                                        │
  │ 输出概率分布:                           │
  │   "很"     → 45%                       │
  │   "不"     → 25%                       │
  │   "真"     → 15%                       │
  │   "特别"   → 10%                       │
  │   其他     → 5%                        │
  │                                        │
  │ 选择: "很" (概率最高)                   │
  └────────────────────────────────────────┘

  Step 2: 把预测的词加入输入，继续预测
  ┌────────────────────────────────────────┐
  │ 输入: "今天天气很"                      │
  │                                        │
  │ 输出概率分布:                           │
  │   "好"     → 60%                       │
  │   "热"     → 20%                       │
  │   "冷"     → 15%                       │
  │   ...                                  │
  │                                        │
  │ 选择: "好"                             │
  └────────────────────────────────────────┘

  Step 3, 4, 5... 重复直到生成结束符或达到长度限制

  最终输出: "今天天气很好，适合出门散步。"
\`\`\`

> 💡 **这就是所谓的"自回归生成"（Autoregressive Generation）**

---

## 完整处理流程：从文本到预测

文本是如何变成概率预测的？整个流程分为三大步骤：

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    Transformer 处理流程全景图                              │
└─────────────────────────────────────────────────────────────────────────┘

  "今天天气很好"
       │
       ▼
  ┌─────────────────────────────────────────────────────────────────────┐
  │  STEP 1: 嵌入层 (Embedding)                                          │
  │  ─────────────────────────────────────────────────────────────────  │
  │                                                                     │
  │  1.1 分词 (Tokenization)                                            │
  │      "今天天气很好" → ["今天", "天气", "很", "好"]                    │
  │                       或 → ["今", "天", "天", "气", "很", "好"]       │
  │                                                                     │
  │  1.2 Token Embedding (词嵌入)                                        │
  │      每个 token → 768 维向量                                         │
  │      "今天" → [0.12, -0.45, 0.78, ..., 0.33]                        │
  │                                                                     │
  │  1.3 Position Embedding (位置嵌入)                                   │
  │      位置 0 → [0.00, 1.00, 0.00, ..., 1.00]                         │
  │      位置 1 → [0.84, 0.54, 0.01, ..., 0.99]                         │
  │                                                                     │
  │  1.4 相加得到最终嵌入                                                │
  │      最终向量 = Token Embedding + Position Embedding                │
  │                                                                     │
  └─────────────────────────────────────────────────────────────────────┘
       │
       ▼
  ┌─────────────────────────────────────────────────────────────────────┐
  │  STEP 2: Transformer Block × N (GPT-2 有 12 层，GPT-4 有 120+ 层)     │
  │  ─────────────────────────────────────────────────────────────────  │
  │                                                                     │
  │  每个 Block 包含:                                                    │
  │                                                                     │
  │  ┌─────────────────────────────────────────────────────────────┐    │
  │  │  Multi-Head Self-Attention                                  │    │
  │  │  • 让每个词"看到"其他词                                      │    │
  │  │  • 计算词与词之间的关联程度                                   │    │
  │  └─────────────────────────────────────────────────────────────┘    │
  │                        │                                            │
  │                        ▼                                            │
  │  ┌─────────────────────────────────────────────────────────────┐    │
  │  │  Add & Norm (残差连接 + 层归一化)                            │    │
  │  │  • 残差: output = input + attention_output                  │    │
  │  │  • 帮助梯度流动，防止梯度消失                                 │    │
  │  └─────────────────────────────────────────────────────────────┘    │
  │                        │                                            │
  │                        ▼                                            │
  │  ┌─────────────────────────────────────────────────────────────┐    │
  │  │  Feed Forward Network (前馈神经网络)                         │    │
  │  │  • 两层全连接: 768 → 3072 → 768                             │    │
  │  │  • 对每个位置独立处理                                        │    │
  │  └─────────────────────────────────────────────────────────────┘    │
  │                        │                                            │
  │                        ▼                                            │
  │  ┌─────────────────────────────────────────────────────────────┐    │
  │  │  Add & Norm (再次残差连接 + 层归一化)                        │    │
  │  └─────────────────────────────────────────────────────────────┘    │
  │                                                                     │
  └─────────────────────────────────────────────────────────────────────┘
       │
       ▼
  ┌─────────────────────────────────────────────────────────────────────┐
  │  STEP 3: 输出层                                                     │
  │  ─────────────────────────────────────────────────────────────────  │
  │                                                                     │
  │  3.1 线性投影                                                       │
  │      768 维 → 50,257 维 (词汇表大小)                                │
  │      每个维度代表一个词的"得分"(logits)                              │
  │                                                                     │
  │  3.2 Softmax 归一化                                                 │
  │      logits → 概率分布 (所有概率之和 = 1)                            │
  │                                                                     │
  │      [3.1, 0.5, 1.8, ...] → [0.45, 0.08, 0.32, ...]                │
  │                                                                     │
  │  3.3 采样策略选择下一个词                                            │
  │      • Greedy: 选概率最高的                                         │
  │      • Top-k: 从前 k 个中随机选                                     │
  │      • Top-p: 从累积概率达到 p 的词中选                              │
  │                                                                     │
  └─────────────────────────────────────────────────────────────────────┘
       │
       ▼
  输出: "好" (下一个词的预测)
\`\`\`

---

## 深入理解：分词 (Tokenization)

分词是第一步，也是容易被忽视但非常重要的一步。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    分词方式对比                                          │
└─────────────────────────────────────────────────────────────────────────┘

  方式1: 按字符分词
  "Hello" → ["H", "e", "l", "l", "o"]
  ✗ 缺点: 序列太长，无法捕捉词义

  方式2: 按空格分词
  "I love AI" → ["I", "love", "AI"]
  ✗ 缺点: 词汇表太大，罕见词无法处理

  方式3: BPE (Byte Pair Encoding) - 现代模型采用
  "unhappiness" → ["un", "happiness"] 或 ["un", "happ", "iness"]
  ✓ 优点: 平衡词汇表大小和序列长度

  GPT-2 词汇表: 50,257 个 token
  GPT-4 词汇表: ~100,000 个 token
  Claude 词汇表: ~100,000 个 token
\`\`\`

**中文分词示例:**
\`\`\`python
# 使用 tiktoken (OpenAI 的分词器)
import tiktoken
enc = tiktoken.encoding_for_model("gpt-4")

text = "今天天气很好"
tokens = enc.encode(text)
# 可能输出: [12345, 23456, 34567, 45678]  (示意)

# 每个数字对应词汇表中的一个 token
# 中文通常 1-2 个字符 = 1 个 token
\`\`\`

---

## 深入理解：词嵌入 (Token Embedding)

每个 token 被映射到一个高维向量空间。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    词嵌入可视化                                          │
└─────────────────────────────────────────────────────────────────────────┘

  词嵌入矩阵:
  ┌─────────────────────────────────────────────────────────────────┐
  │                    50,257 × 768                                 │
  │  ─────────────────────────────────────────────────────────────  │
  │                                                                 │
  │  token 0 ("!")    → [0.12, -0.45, 0.78, ..., 0.33]  (768维)    │
  │  token 1 ("\"")   → [0.23, 0.56, -0.12, ..., 0.67]  (768维)    │
  │  token 2 ("#")    → [-0.34, 0.78, 0.45, ..., -0.89] (768维)    │
  │  ...                                                            │
  │  token 15339 ("今天") → [0.56, -0.23, 0.89, ..., 0.12]          │
  │  token 28965 ("天气") → [0.45, -0.34, 0.78, ..., 0.23]          │
  │  ...                                                            │
  │  token 50256 ("<|endoftext|>") → [0.11, 0.22, ..., 0.33]       │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  GPT-2 词嵌入矩阵参数量: 50,257 × 768 ≈ 3,860 万参数！
\`\`\`

**语义关系在向量空间中的体现:**
\`\`\`python
# 经典例子: 词向量的语义算术
king - man + woman ≈ queen

# 相似词在向量空间中距离接近
cos_similarity("cat", "dog") ≈ 0.85  # 高相似度
cos_similarity("cat", "car") ≈ 0.15  # 低相似度
\`\`\`

---

## 深入理解：位置编码 (Positional Encoding)

自注意力机制本身不包含位置信息，需要显式添加。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    为什么需要位置编码？                                   │
└─────────────────────────────────────────────────────────────────────────┘

  问题: 自注意力是"集合运算"，不区分顺序

  例子:
  句子1: "猫 追 狗"  →  注意力看到 {猫, 追, 狗}
  句子2: "狗 追 猫"  →  注意力看到 {狗, 追, 猫}

  如果不加位置信息，模型会认为这两句话是一样的！

  解决: 给每个位置一个独特的"指纹"
\`\`\`

**原始 Transformer 使用正弦/余弦位置编码:**
\`\`\`python
# 位置编码公式
PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))

# pos = 词的位置 (0, 1, 2, ...)
# i = 维度索引 (0, 1, 2, ..., d_model/2)
# d_model = 嵌入维度 (如 768)
\`\`\`

\`\`\`
位置编码可视化 (假设 d_model = 8):

位置    dim0   dim1   dim2   dim3   dim4   dim5   dim6   dim7
───────────────────────────────────────────────────────────────
pos=0   0.00   1.00   0.00   1.00   0.00   1.00   0.00   1.00
pos=1   0.84   0.54   0.10   0.99   0.01   1.00   0.00   1.00
pos=2   0.91  -0.42   0.20   0.98   0.02   1.00   0.00   1.00
pos=3   0.14  -0.99   0.30   0.95   0.03   1.00   0.00   1.00
...

特点:
• 每个位置有独特的编码
• 不同维度变化频率不同 (低维度变化快，高维度变化慢)
• 相对位置可以通过线性变换表示
\`\`\`

**现代模型 (GPT-2/3/4, Llama) 使用可学习位置编码:**
\`\`\`python
# 可学习位置编码
self.position_embedding = nn.Embedding(max_seq_len, d_model)
# max_seq_len: 最大序列长度 (GPT-2: 1024, GPT-4: 128K+)

# 位置嵌入也是训练出来的，而不是固定公式
\`\`\`

---

## 深入理解：残差连接与层归一化 (Add & Norm)

为什么每个子层后都有 Add & Norm？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    残差连接 (Residual Connection)                        │
└─────────────────────────────────────────────────────────────────────────┘

  普通网络:
  x → [子层] → y

  残差网络:
  x → [子层] → y
  │            │
  └─────(+)────┘  →  output = x + y

  为什么重要？
  1. 梯度可以直接流回早期层 (解决梯度消失)
  2. 网络可以学习"恒等映射" (如果子层不需要，可以输出 0)
  3. 让训练更稳定，可以堆叠更多层

  GPT-3 有 96 层，没有残差连接根本无法训练！
\`\`\`

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    层归一化 (Layer Normalization)                        │
└─────────────────────────────────────────────────────────────────────────┘

  作用: 将每一层的输出归一化到均值为 0、方差为 1

  LayerNorm(x) = γ * (x - μ) / σ + β

  • μ = x 的均值
  • σ = x 的标准差
  • γ, β = 可学习的缩放和偏移参数

  为什么重要？
  1. 稳定训练过程
  2. 加速收敛
  3. 允许使用更大的学习率
\`\`\`

---

## 三种 Transformer 变体

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    Transformer 架构变体                                  │
└─────────────────────────────────────────────────────────────────────────┘

     Encoder-Only              Decoder-Only              Encoder-Decoder
     ────────────              ────────────              ───────────────
     ┌──────────┐             ┌──────────┐              ┌─────┐  ┌─────┐
     │ Encoder  │             │ Decoder  │              │ Enc │→│ Dec │
     └──────────┘             └──────────┘              └─────┘  └─────┘

  注意力类型:                注意力类型:                注意力类型:
  双向 (全部可见)            单向 (只看左边)            编码双向+解码单向

  代表模型:                  代表模型:                  代表模型:
  • BERT                    • GPT-1/2/3/4             • T5
  • RoBERTa                 • Claude                  • BART
  • ALBERT                  • Llama 1/2/3             • mBART

  擅长任务:                  擅长任务:                  擅长任务:
  • 文本分类                 • 文本生成                 • 机器翻译
  • 命名实体识别              • 对话系统                 • 文本摘要
  • 句子相似度               • 代码生成                 • 问答系统
  • 情感分析                 • 创意写作                 • 语音识别

  训练目标:                  训练目标:                  训练目标:
  掩码语言模型 (MLM)         下一词预测 (NTP)           Seq2Seq

     [MASK] 填空              一个词接一个词生成          输入→输出映射
\`\`\`

> 💡 **2024-2025 年的主流**: GPT、Claude、Llama 全部是 **Decoder-Only** 架构

---

## 参数量与模型能力

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    大模型参数量全景图                                     │
└─────────────────────────────────────────────────────────────────────────┘

  模型                参数量        层数     注意力头   嵌入维度
  ─────────────────────────────────────────────────────────────────
  GPT-2 Small         124M         12        12        768
  GPT-2 Large         774M         36        20        1280
  GPT-2 XL            1.5B         48        25        1600
  ─────────────────────────────────────────────────────────────────
  GPT-3 Small         125M         12        12        768
  GPT-3 Medium        350M         24        16        1024
  GPT-3 Large         760M         24        16        1536
  GPT-3 XL            1.3B         24        24        2048
  GPT-3 6.7B          6.7B         32        32        4096
  GPT-3 13B           13B          40        40        5120
  GPT-3 175B          175B         96        96        12288
  ─────────────────────────────────────────────────────────────────
  Llama 2 7B          7B           32        32        4096
  Llama 2 13B         13B          40        40        5120
  Llama 2 70B         70B          80        64        8192
  ─────────────────────────────────────────────────────────────────
  Claude 3 Haiku      ~20B         ?         ?         ?
  Claude 3 Sonnet     ~70B         ?         ?         ?
  Claude 3 Opus       ~200B        ?         ?         ?

  参数主要分布在:
  • 词嵌入矩阵: vocab_size × d_model
  • 注意力层: 4 × n_layers × d_model²
  • 前馈层: 8 × n_layers × d_model²
  • 输出层: d_model × vocab_size
\`\`\`

**Scaling Law (扩展定律):**
\`\`\`
模型性能 ∝ (参数量)^0.076 × (数据量)^0.095 × (计算量)^0.050

核心发现:
• 参数量翻倍 → 性能提升约 5%
• 数据量翻倍 → 性能提升约 7%
• 目前还没有看到明显的天花板

这就是为什么各大公司都在疯狂扩大模型规模！
\`\`\`

---

## 本节要点

1. **文本生成本质** —— 自回归预测下一个词
2. **完整流程** —— 分词 → 嵌入 → Transformer Block × N → 输出概率
3. **关键组件** —— 词嵌入、位置编码、残差连接、层归一化
4. **三种架构** —— Encoder-Only、Decoder-Only、Encoder-Decoder
5. **现代趋势** —— GPT/Claude/Llama 都是 Decoder-Only，规模持续扩大
            `,
            ja: `
## Transformer：世界を変えたアーキテクチャ

2017年、Googleが論文「Attention Is All You Need」を発表し、Transformerを提案しました。今日のGPT、Claude、Geminiはすべてこのアーキテクチャに基づいています。

---

## 体験してみよう：Transformerの動作

詳しく学ぶ前に、このインタラクティブデモでTransformerがテキストを処理する様子を体験してみましょう：

::transformer-v2-viz::

---

## テキスト生成の核心原理：次のトークン予測

大規模言語モデルの本質はシンプル：**次の単語を予測する**こと。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    テキスト生成 = 次の単語を繰り返し予測                    │
└─────────────────────────────────────────────────────────────────────────┘

  入力: "今日は天気が"

  Step 1: 次の単語を予測
  ┌────────────────────────────────────────┐
  │ 入力: "今日は天気が"                    │
  │                                        │
  │ 出力確率分布:                           │
  │   "良い"   → 45%                       │
  │   "悪い"   → 25%                       │
  │   "いい"   → 15%                       │
  │   ...                                  │
  │                                        │
  │ 選択: "良い" (最高確率)                 │
  └────────────────────────────────────────┘

  Step 2: 予測した単語を入力に追加し、続けて予測
  ┌────────────────────────────────────────┐
  │ 入力: "今日は天気が良い"                │
  │ → 次の予測: "です" (60%)               │
  └────────────────────────────────────────┘

  最終出力: "今日は天気が良いですね。"
\`\`\`

> 💡 **これが「自己回帰生成」（Autoregressive Generation）です**

---

## 完全な処理フロー

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    Transformer処理フロー全体図                            │
└─────────────────────────────────────────────────────────────────────────┘

  "今日は天気が良い"
       │
       ▼
  ┌─────────────────────────────────────────────────────────────────────┐
  │  STEP 1: 埋め込み層 (Embedding)                                      │
  │  ─────────────────────────────────────────────────────────────────  │
  │  1.1 トークン化: "今日" "は" "天気" "が" "良い"                       │
  │  1.2 トークン埋め込み: 各トークン → 768次元ベクトル                    │
  │  1.3 位置埋め込み: 位置情報を追加                                     │
  │  1.4 最終埋め込み = トークン埋め込み + 位置埋め込み                    │
  └─────────────────────────────────────────────────────────────────────┘
       │
       ▼
  ┌─────────────────────────────────────────────────────────────────────┐
  │  STEP 2: Transformer Block × N層                                    │
  │  ─────────────────────────────────────────────────────────────────  │
  │  • Multi-Head Self-Attention: 単語間の関係を計算                     │
  │  • Add & Norm: 残差接続 + 層正規化                                   │
  │  • Feed Forward: 768 → 3072 → 768                                  │
  │  • Add & Norm: 再び残差接続 + 層正規化                               │
  └─────────────────────────────────────────────────────────────────────┘
       │
       ▼
  ┌─────────────────────────────────────────────────────────────────────┐
  │  STEP 3: 出力層                                                     │
  │  ─────────────────────────────────────────────────────────────────  │
  │  • 線形投影: 768次元 → 50,257次元（語彙サイズ）                       │
  │  • Softmax: 確率分布に変換                                          │
  │  • サンプリング: 次のトークンを選択                                   │
  └─────────────────────────────────────────────────────────────────────┘
\`\`\`

---

## トークン化 (Tokenization)

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    トークン化方式の比較                                   │
└─────────────────────────────────────────────────────────────────────────┘

  方式1: 文字単位
  "Hello" → ["H", "e", "l", "l", "o"]
  ✗ シーケンスが長すぎる

  方式2: スペース区切り
  "I love AI" → ["I", "love", "AI"]
  ✗ 語彙が大きすぎる

  方式3: BPE (現代モデルが採用)
  "unhappiness" → ["un", "happiness"]
  ✓ バランスが良い

  GPT-2語彙: 50,257トークン
  GPT-4語彙: ~100,000トークン
\`\`\`

---

## 位置エンコーディング

\`\`\`
なぜ位置情報が必要か？

セルフアテンションは順序を区別しない:
"猫が犬を追う" と "犬が猫を追う"
→ 位置情報なしでは同じに見える！

解決策: 各位置に固有の「指紋」を追加

PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))

現代モデル(GPT-2/3/4, Llama)は学習可能な位置埋め込みを使用
\`\`\`

---

## 残差接続と層正規化

\`\`\`
残差接続 (Residual Connection):
output = x + SubLayer(x)

利点:
• 勾配が直接初期層に流れる
• 深いネットワークの学習が可能に
• GPT-3は96層、残差なしでは学習不可能

層正規化 (Layer Normalization):
LayerNorm(x) = γ * (x - μ) / σ + β

利点:
• 学習の安定化
• 収束の加速
\`\`\`

---

## 3種類のTransformerアーキテクチャ

| タイプ | 代表モデル | 得意なタスク |
|--------|----------|------------|
| Encoder-Only | BERT, RoBERTa | 分類、NER、感情分析 |
| Decoder-Only | GPT, Claude, Llama | 生成、対話、コード |
| Encoder-Decoder | T5, BART | 翻訳、要約、Q&A |

> 💡 **2024-2025年の主流**: GPT、Claude、Llamaは全て **Decoder-Only**

---

## パラメータ数と規模

\`\`\`
モデル              パラメータ数    層数
─────────────────────────────────────
GPT-2 XL            1.5B         48
GPT-3 175B          175B         96
Llama 2 70B         70B          80
Claude 3 Opus       ~200B        ?

Scaling Law:
性能 ∝ (パラメータ)^0.076 × (データ)^0.095

パラメータ2倍 → 性能約5%向上
\`\`\`

---

## 本節のポイント

1. **テキスト生成の本質** —— 自己回帰で次のトークンを予測
2. **完全なフロー** —— トークン化 → 埋め込み → Transformer Block → 確率出力
3. **重要な構成要素** —— 埋め込み、位置エンコーディング、残差接続
4. **3種類のアーキテクチャ** —— Encoder-Only、Decoder-Only、Encoder-Decoder
5. **現代のトレンド** —— GPT/Claude/Llamaは全てDecoder-Only
            `
          }
        },
        {
          id: 'ch1-attention',
          title: { zh: '1.2 注意力机制深入理解', ja: '1.2 アテンション機構の深い理解' },
          content: {
            zh: `
## 注意力机制：Transformer 的灵魂

注意力机制（Attention）是 Transformer 最核心的创新。它让模型能够动态地"关注"输入中最相关的部分。

### 交互式演示：注意力机制可视化

先通过这个交互式演示探索注意力是如何工作的：

::attention-viz::

---

## 用搜索引擎类比理解 Q/K/V

理解 Q、K、V 最好的方式是把它想象成**搜索引擎**：

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    Q/K/V 的搜索引擎类比                                   │
└─────────────────────────────────────────────────────────────────────────┘

  想象你在 Google 搜索：

  ┌─────────────────────────────────────────────────────────────────────┐
  │                                                                     │
  │  🔍 Query (查询)：你输入的搜索词                                      │
  │     "如何学习机器学习"                                               │
  │                                                                     │
  │  📑 Key (键)：网页的标题/关键词                                       │
  │     - "机器学习入门教程"                                             │
  │     - "深度学习实战指南"                                             │
  │     - "Python 编程基础"                                              │
  │     - "今日新闻头条"                                                 │
  │                                                                     │
  │  📄 Value (值)：网页的实际内容                                        │
  │     - [机器学习教程的完整内容...]                                     │
  │     - [深度学习指南的完整内容...]                                     │
  │     - [Python 基础的完整内容...]                                      │
  │     - [新闻的完整内容...]                                            │
  │                                                                     │
  └─────────────────────────────────────────────────────────────────────┘

  搜索过程：
  1. Query 和每个 Key 计算相似度（点击率预估）
  2. 相似度高的 Key 对应的 Value 获得更高权重
  3. 返回加权后的结果（相关网页排在前面）

  在 Transformer 中：
  • Query = "我现在想要什么信息？"
  • Key = "我有什么信息可以提供？"
  • Value = "这些信息的具体内容"
\`\`\`

---

## Self-Attention 完整计算流程

让我们用一个具体例子走一遍完整的注意力计算。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    完整示例：计算 "猫 吃 鱼" 的注意力                       │
└─────────────────────────────────────────────────────────────────────────┘

  输入句子: "猫 吃 鱼"

  ═══════════════════════════════════════════════════════════════════════
  Step 1: 将每个词转换为向量（词嵌入）
  ═══════════════════════════════════════════════════════════════════════

  "猫" → x₁ = [0.2, 0.5, 0.1, 0.8]  (假设 4 维，实际是 768 维)
  "吃" → x₂ = [0.6, 0.1, 0.9, 0.3]
  "鱼" → x₃ = [0.3, 0.7, 0.2, 0.5]

  ═══════════════════════════════════════════════════════════════════════
  Step 2: 通过线性变换生成 Q, K, V
  ═══════════════════════════════════════════════════════════════════════

  每个词都会生成自己的 Q, K, V：

  对于 "猫":
    Q₁ = x₁ × W_Q = [0.2, 0.5, 0.1, 0.8] × W_Q = [2.2, 0.8]
    K₁ = x₁ × W_K = [0.2, 0.5, 0.1, 0.8] × W_K = [0.9, 1.1]
    V₁ = x₁ × W_V = [0.2, 0.5, 0.1, 0.8] × W_V = [0.5, 0.3]

  对于 "吃":
    Q₂ = [0.7, 1.4]    K₂ = [2.0, 0.6]    V₂ = [0.8, 0.2]

  对于 "鱼":
    Q₃ = [2.0, 0.5]    K₃ = [0.4, 1.3]    V₃ = [0.2, 0.9]

  ═══════════════════════════════════════════════════════════════════════
  Step 3: 计算注意力分数 (Q × K^T)
  ═══════════════════════════════════════════════════════════════════════

  以 "猫" 为例，计算它对每个词的注意力分数：

  score(猫→猫) = Q₁ · K₁ = [2.2, 0.8] · [0.9, 1.1] = 1.08 + 0.88 = 1.96
  score(猫→吃) = Q₁ · K₂ = [2.2, 0.8] · [2.0, 0.6] = 1.20 + 0.48 = 1.68
  score(猫→鱼) = Q₁ · K₃ = [2.2, 0.8] · [0.4, 1.3] = 0.48 + 1.04 = 1.52

  完整的注意力分数矩阵：
                K₁(猫)  K₂(吃)  K₃(鱼)
              ┌───────────────────────┐
  Q₁(猫)      │  1.96    1.68    1.52  │
  Q₂(吃)      │  2.31    2.10    1.87  │
  Q₃(鱼)      │  1.45    1.30    1.15  │
              └───────────────────────┘

  ═══════════════════════════════════════════════════════════════════════
  Step 4: 缩放 (除以 √d_k)
  ═══════════════════════════════════════════════════════════════════════

  为什么要缩放？
  • 当维度 d_k 很大时，点积的值会很大
  • 大的值经过 softmax 后会产生极端的分布（接近 one-hot）
  • 这会导致梯度消失，训练不稳定

  缩放后 (d_k = 2, √d_k = 1.414)：
                K₁(猫)  K₂(吃)  K₃(鱼)
              ┌───────────────────────┐
  Q₁(猫)      │  1.39    1.19    1.07  │
  Q₂(吃)      │  1.63    1.48    1.32  │
  Q₃(鱼)      │  1.02    0.92    0.81  │
              └───────────────────────┘

  ═══════════════════════════════════════════════════════════════════════
  Step 5: Softmax 归一化
  ═══════════════════════════════════════════════════════════════════════

  对每一行应用 softmax，使每行和为 1：

  softmax([2.39, 1.19, 1.07]) = [0.40, 0.33, 0.27]

  注意力权重矩阵：
                   猫      吃      鱼
              ┌───────────────────────┐
  猫          │  0.40    0.33    0.27  │  → 行和 = 1
  吃          │  0.42    0.35    0.23  │  → 行和 = 1
  鱼          │  0.38    0.34    0.28  │  → 行和 = 1
              └───────────────────────┘

  解读：
  • "猫" 对自己的注意力是 0.40（最高）
  • "猫" 对 "吃" 的注意力是 0.33
  • "猫" 对 "鱼" 的注意力是 0.27

  ═══════════════════════════════════════════════════════════════════════
  Step 6: 加权求和得到输出
  ═══════════════════════════════════════════════════════════════════════

  "猫" 的新表示 = 0.40×V₁ + 0.33×V₂ + 0.27×V₃
                = 0.40×[0.5,0.3] + 0.33×[0.8,0.2] + 0.27×[0.2,0.9]
                = [0.20,0.12] + [0.26,0.07] + [0.05,0.24]
                = [0.51, 0.43]

  新的表示融合了整个句子的上下文信息！
\`\`\`

---

## 核心公式详解

\`\`\`python
Attention(Q, K, V) = softmax(Q · K^T / √d_k) · V
\`\`\`

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    公式分解                                              │
└─────────────────────────────────────────────────────────────────────────┘

  Q · K^T        计算相似度（哪些词和我相关）
     ↓
  / √d_k         缩放（防止梯度消失）
     ↓
  softmax()      归一化（转换为概率分布，和为1）
     ↓
  × V            加权求和（提取相关信息）

  ═══════════════════════════════════════════════════════════════════════

  为什么是 √d_k？

  假设 Q 和 K 的元素都是均值为 0、方差为 1 的随机变量：
  • Q·K 的方差 ≈ d_k
  • 当 d_k = 64 时，Q·K 的标准差 ≈ 8
  • 除以 √64 = 8 后，标准差变回 1

  这保证了无论维度多大，softmax 的输入都在合理范围内。
\`\`\`

---

## 多头注意力（Multi-Head Attention）

单个注意力头只能学习一种关系模式。多头注意力让模型同时从多个角度理解输入。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    多头注意力可视化                                       │
└─────────────────────────────────────────────────────────────────────────┘

  句子: "The cat sat on the mat because it was tired"

  不同的头学习不同的关系：

  Head 1: 主语-动词关系          Head 2: 代词指代
  ┌─────────────────────┐       ┌─────────────────────┐
  │                     │       │                     │
  │  cat ═══════► sat   │       │  it ═══════► cat    │
  │  (谁在坐？)          │       │  (it 指的是谁？)     │
  │                     │       │                     │
  └─────────────────────┘       └─────────────────────┘

  Head 3: 介词关系               Head 4: 因果关系
  ┌─────────────────────┐       ┌─────────────────────┐
  │                     │       │                     │
  │  sat ═══► on ═══► mat│       │  because ═══► tired │
  │  (坐在哪里？)         │       │  (为什么？)          │
  │                     │       │                     │
  └─────────────────────┘       └─────────────────────┘

  GPT-2: 12 个注意力头
  GPT-3: 96 个注意力头
  GPT-4: 估计 100+ 个注意力头
\`\`\`

\`\`\`python
# 多头注意力的完整实现
import torch
import torch.nn as nn
import math

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model=768, num_heads=12):
        super().__init__()
        self.num_heads = num_heads
        self.d_k = d_model // num_heads  # 每个头的维度: 768/12 = 64

        # 一次性计算所有头的 Q, K, V（效率更高）
        self.W_Q = nn.Linear(d_model, d_model)
        self.W_K = nn.Linear(d_model, d_model)
        self.W_V = nn.Linear(d_model, d_model)
        self.W_O = nn.Linear(d_model, d_model)

    def forward(self, x, mask=None):
        batch_size, seq_len, d_model = x.shape

        # Step 1: 线性变换
        Q = self.W_Q(x)  # [batch, seq_len, d_model]
        K = self.W_K(x)
        V = self.W_V(x)

        # Step 2: 拆分成多个头
        # [batch, seq_len, d_model] → [batch, num_heads, seq_len, d_k]
        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)

        # Step 3: 计算注意力
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)

        # Step 4: 应用掩码（如果有）
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        # Step 5: Softmax
        attn_weights = torch.softmax(scores, dim=-1)

        # Step 6: 加权求和
        output = torch.matmul(attn_weights, V)

        # Step 7: 合并所有头
        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)

        # Step 8: 最终线性变换
        return self.W_O(output)
\`\`\`

---

## Masked Self-Attention（掩码自注意力）

在文本生成任务中，模型在预测第 n 个词时，不能看到第 n+1, n+2, ... 位置的词。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    掩码注意力的工作原理                                    │
└─────────────────────────────────────────────────────────────────────────┘

  生成句子: "I love AI"

  训练时，我们有完整的句子，但需要模拟生成过程：

  预测位置    能看到的词           预测目标
  ─────────────────────────────────────────
  位置 1      [I]                  love
  位置 2      [I, love]            AI
  位置 3      [I, love, AI]        <END>

  掩码矩阵（下三角矩阵）：

           I     love    AI
        ┌─────────────────────┐
  I     │  1      0       0   │   1 = 可以看到
  love  │  1      1       0   │   0 = 看不到
  AI    │  1      1       1   │
        └─────────────────────┘

  在注意力分数上应用掩码：

  原始分数：              加掩码后（-∞）：         Softmax 后：
  ┌─────────────┐       ┌─────────────┐        ┌─────────────┐
  │ 2.1 1.5 0.8 │       │ 2.1  -∞  -∞ │        │ 1.0 0.0 0.0 │
  │ 1.2 2.8 1.1 │   →   │ 1.2 2.8  -∞ │   →    │ 0.2 0.8 0.0 │
  │ 0.9 1.3 2.5 │       │ 0.9 1.3 2.5 │        │ 0.1 0.3 0.6 │
  └─────────────┘       └─────────────┘        └─────────────┘

  -∞ 经过 softmax 后变成 0，实现了"看不见"的效果
\`\`\`

---

## Temperature 与采样策略

生成文本时，如何从概率分布中选择下一个词？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    Temperature 控制输出多样性                             │
└─────────────────────────────────────────────────────────────────────────┘

  原始 logits: [3.0, 1.0, 0.5, 0.1]
  对应词汇:     好    热    冷    坏

  Temperature 的作用：logits / temperature → softmax

  ┌─────────────────────────────────────────────────────────────────────┐
  │  Temperature = 0.5（低温，更确定）                                    │
  │  ─────────────────────────────────────────────────────────────────  │
  │  logits / 0.5 = [1.0, 2.0, 1.0, 0.2]                               │
  │  softmax → [0.84, 0.11, 0.04, 0.01]                                │
  │                                                                     │
  │  "好" 的概率高达 84%！输出更确定、更保守                              │
  └─────────────────────────────────────────────────────────────────────┘

  ┌─────────────────────────────────────────────────────────────────────┐
  │  Temperature = 1.0（默认）                                           │
  │  ─────────────────────────────────────────────────────────────────  │
  │  logits / 1.0 = [3.0, 1.0, 0.5, 0.1]                               │
  │  softmax → [0.53, 0.19, 0.12, 0.08]                                │
  │                                                                     │
  │  正常的概率分布                                                      │
  └─────────────────────────────────────────────────────────────────────┘

  ┌─────────────────────────────────────────────────────────────────────┐
  │  Temperature = 2.0（高温，更随机）                                    │
  │  ─────────────────────────────────────────────────────────────────  │
  │  logits / 2.0 = [2.0, 0.5, 0.25, 0.05]                             │
  │  softmax → [0.38, 0.23, 0.18, 0.15]                                │
  │                                                                     │
  │  概率分布更平坦，输出更多样、更有创意（但可能更不准确）                  │
  └─────────────────────────────────────────────────────────────────────┘

  使用建议：
  • 代码生成 / 数学题：Temperature = 0 (贪心解码)
  • 一般对话：Temperature = 0.7 ~ 1.0
  • 创意写作：Temperature = 1.0 ~ 1.5
\`\`\`

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    Top-k 和 Top-p 采样                                   │
└─────────────────────────────────────────────────────────────────────────┘

  Top-k 采样：只从概率最高的 k 个词中选择

  原始分布: [好: 0.53, 热: 0.19, 冷: 0.12, 坏: 0.08, ...]

  Top-k = 3:
  [好: 0.53, 热: 0.19, 冷: 0.12] → 重新归一化 → [好: 0.63, 热: 0.23, 冷: 0.14]

  ─────────────────────────────────────────────────────────────────────────

  Top-p 采样（Nucleus Sampling）：选择累积概率达到 p 的最小词集

  原始分布（按概率排序）:
  好: 0.53  累积: 0.53
  热: 0.19  累积: 0.72
  冷: 0.12  累积: 0.84
  坏: 0.08  累积: 0.92  ← Top-p = 0.9 时，选到这里
  ...

  Top-p = 0.9:
  从 {好, 热, 冷, 坏} 中采样

  ─────────────────────────────────────────────────────────────────────────

  实际使用中，通常组合使用：
  • temperature = 0.7
  • top_p = 0.9
  • top_k = 50
\`\`\`

---

## 注意力可视化

\`\`\`
实际的注意力模式可以通过工具可视化：

输入: "The cat sat on the mat"

Layer 6, Head 8（学习到的句法关系）：

        The   cat   sat    on   the   mat
The     ███   ░░░   ░░░   ░░░   ░░░   ░░░
cat     ░░░   ███   ░░░   ░░░   ░░░   ░░░
sat     ░░░   ███   ██░   ░░░   ░░░   ░░░  ← "sat" 关注 "cat"（主语）
on      ░░░   ░░░   ███   ███   ░░░   ░░░
the     ░░░   ░░░   ░░░   ░░░   ███   ░░░
mat     ░░░   ░░░   ░░░   ███   ░░░   ███  ← "mat" 关注 "on"（介词）

███ = 高注意力    ░░░ = 低注意力

推荐工具：BertViz, Attention Viz, Transformer Explainer
\`\`\`

---

## 本节要点

1. **Q/K/V 类比** —— 就像搜索引擎：Query 是搜索词，Key 是标题，Value 是内容
2. **注意力公式** —— Attention = softmax(QK^T/√d_k)V，缩放防止梯度消失
3. **多头注意力** —— 12-96 个头同时学习不同类型的关系
4. **掩码注意力** —— 通过 -∞ 掩码实现"看不到未来"
5. **Temperature** —— 低温更确定，高温更随机
6. **Top-k/Top-p** —— 限制采样范围，平衡质量和多样性
            `,
            ja: `
## アテンション機構：Transformerの魂

アテンション機構（Attention）はTransformerの最も核心的なイノベーションです。入力の中で最も関連性の高い部分に動的に「注目」することができます。

### インタラクティブデモ：アテンション機構の可視化

このインタラクティブデモでアテンションがどのように機能するかを探索してください：

::attention-viz::

---

## 検索エンジンで理解する Q/K/V

Q/K/V を理解する最良の方法は**検索エンジン**のアナロジーです：

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    Q/K/V の検索エンジンアナロジー                          │
└─────────────────────────────────────────────────────────────────────────┘

  Google検索を想像してください：

  🔍 Query (クエリ)：入力した検索ワード
     "機械学習 入門"

  📑 Key (キー)：ウェブページのタイトル/キーワード
     - "機械学習入門チュートリアル"
     - "深層学習実践ガイド"
     - "Python プログラミング基礎"

  📄 Value (値)：ウェブページの実際のコンテンツ
     - [機械学習チュートリアルの完全な内容...]
     - [深層学習ガイドの完全な内容...]

  Transformerでは：
  • Query = "今、どんな情報が欲しい？"
  • Key = "どんな情報を提供できる？"
  • Value = "その情報の具体的な内容"
\`\`\`

---

## Self-Attention 完全な計算フロー

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    完全な例："猫 食べる 魚" のアテンション計算              │
└─────────────────────────────────────────────────────────────────────────┘

  Step 1: 各単語をベクトルに変換
  "猫" → x₁ = [0.2, 0.5, 0.1, 0.8]
  "食べる" → x₂ = [0.6, 0.1, 0.9, 0.3]
  "魚" → x₃ = [0.3, 0.7, 0.2, 0.5]

  Step 2: 線形変換で Q, K, V を生成
  各単語に対して Q, K, V を計算

  Step 3: アテンションスコア計算 (Q × K^T)
  各単語ペアの類似度を計算

  Step 4: スケーリング (÷ √d_k)
  勾配消失を防ぐ

  Step 5: Softmax 正規化
  各行の和が1になる確率分布に変換

  Step 6: 加重和で出力を得る
  新しい表現 = Σ(アテンション重み × V)
\`\`\`

---

## コア公式

\`\`\`python
Attention(Q, K, V) = softmax(Q · K^T / √d_k) · V
\`\`\`

\`\`\`
なぜ √d_k で割るのか？

• d_k が大きいとき、ドット積の値が大きくなる
• 大きな値は softmax 後に極端な分布（one-hotに近い）を生む
• これは勾配消失を引き起こし、学習を不安定にする

√d_k で割ることで、入力を適切な範囲に保つ
\`\`\`

---

## Multi-Head Attention

\`\`\`
異なるヘッドが異なる関係を学習：

Head 1: 主語-動詞関係    Head 2: 代名詞参照
Head 3: 前置詞関係      Head 4: 因果関係

GPT-2: 12ヘッド
GPT-3: 96ヘッド
GPT-4: 推定100+ヘッド
\`\`\`

---

## Masked Self-Attention

\`\`\`
テキスト生成では、未来の単語を「カンニング」できない：

文: "I love AI"

予測位置    見える単語        予測対象
─────────────────────────────────────
位置 1      [I]               love
位置 2      [I, love]         AI
位置 3      [I, love, AI]     <END>

マスク行列（下三角）：
      I   love  AI
  ┌─────────────────┐
I │  1    0    0   │  1 = 見える
love│  1    1    0   │  0 = 見えない
AI│  1    1    1   │
  └─────────────────┘

-∞ を適用後、softmax で 0 になり「見えない」効果を実現
\`\`\`

---

## Temperature とサンプリング戦略

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    Temperature で出力の多様性を制御                        │
└─────────────────────────────────────────────────────────────────────────┘

Temperature = 0.5（低温）→ より確定的、保守的
Temperature = 1.0（デフォルト）→ 通常の分布
Temperature = 2.0（高温）→ より多様、創造的

使用ガイドライン：
• コード生成/数学: Temperature = 0（貪欲デコーディング）
• 一般的な会話: Temperature = 0.7 ~ 1.0
• 創作活動: Temperature = 1.0 ~ 1.5

Top-k サンプリング: 上位 k 個から選択
Top-p サンプリング: 累積確率が p に達するまでの単語から選択
\`\`\`

---

## 本節のポイント

1. **Q/K/V アナロジー** —— 検索エンジン：Query=検索語、Key=タイトル、Value=内容
2. **アテンション公式** —— Attention = softmax(QK^T/√d_k)V
3. **マルチヘッド** —— 12-96個のヘッドが異なる関係を学習
4. **マスクアテンション** —— -∞マスクで「未来が見えない」を実現
5. **Temperature** —— 低温=確定的、高温=多様性
6. **Top-k/Top-p** —— サンプリング範囲を制限、品質と多様性のバランス
            `
          }
        },
        {
          id: 'ch1-finetune-intro',
          title: { zh: '1.3 大模型微调入门', ja: '1.3 大規模モデルファインチューニング入門' },
          content: {
            zh: `
## 什么是微调（Fine-tuning）？

微调是在预训练模型的基础上，使用特定领域的数据进行进一步训练，使模型更适合特定任务。

---

## 为什么需要微调？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        微调的必要性                                      │
└─────────────────────────────────────────────────────────────────────────┘

  通用大模型的局限：

  ┌─────────────────────────────────────────────────────────────────────┐
  │                                                                     │
  │  ❌ 不了解你的专业领域术语                                            │
  │  ❌ 不知道你公司的产品和流程                                          │
  │  ❌ 输出风格可能不符合要求                                            │
  │  ❌ 在特定任务上表现不够精确                                          │
  │                                                                     │
  └─────────────────────────────────────────────────────────────────────┘

  微调后：

  ┌─────────────────────────────────────────────────────────────────────┐
  │                                                                     │
  │  ✅ 掌握你的领域专业知识                                              │
  │  ✅ 了解特定的术语和表达                                              │
  │  ✅ 输出符合品牌风格                                                  │
  │  ✅ 在特定任务上表现出色                                              │
  │                                                                     │
  └─────────────────────────────────────────────────────────────────────┘
\`\`\`

---

## 微调的类型

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        微调方法对比                                      │
└─────────────────────────────────────────────────────────────────────────┘

  方法              训练参数      显存需求      效果       适用场景
  ────              ────────      ────────      ────       ────────
  全量微调           100%          极高         最好       资源充足
  LoRA              ~0.1%         低           很好       资源有限（推荐）
  QLoRA             ~0.1%         极低         较好       消费级显卡
  Adapter           ~1%           中等         好         多任务
  Prompt Tuning     ~0.01%        极低         一般       简单任务


  推荐学习路径：

  初学者 → LoRA（性价比最高）
  进阶 → QLoRA（更低显存）
  专业 → 全量微调（追求极致效果）
\`\`\`

---

## LoRA：低秩自适应

LoRA（Low-Rank Adaptation）是目前最流行的微调方法。

### 核心思想

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        LoRA 原理图解                                     │
└─────────────────────────────────────────────────────────────────────────┘

  原始权重矩阵 W（冻结，不更新）
  ┌────────────────────────────────────────┐
  │                                        │
  │           d × d 维                      │    比如 4096 × 4096
  │          （参数量巨大）                  │    = 1600 万参数
  │                                        │
  └────────────────────────────────────────┘

  LoRA 添加两个小矩阵 A 和 B：

  ┌────────┐   ┌────────┐
  │   B    │   │   A    │
  │ d × r  │ × │ r × d  │    r = 秩（通常 4~64）
  └────────┘   └────────┘
      ↓             ↓
    4096×8        8×4096        = 6.5 万参数（减少 99.6%！）

  最终输出 = W·x + B·A·x

  只训练 A 和 B，W 保持不变！
\`\`\`

### LoRA 的优势

| 优势 | 说明 |
|------|------|
| 参数高效 | 只需训练 0.1% 的参数 |
| 显存友好 | 可在消费级 GPU 上运行 |
| 效果接近 | 接近全量微调的效果 |
| 可插拔 | 可以随时加载/卸载 LoRA |
| 可组合 | 多个 LoRA 可以组合使用 |

---

## 微调数据准备

### 数据格式

\`\`\`json
// 指令微调格式（推荐）
{
  "instruction": "将以下英文翻译成中文",
  "input": "Hello, how are you?",
  "output": "你好，你怎么样？"
}

// 对话格式
{
  "conversations": [
    {"role": "user", "content": "什么是机器学习？"},
    {"role": "assistant", "content": "机器学习是..."}
  ]
}

// 补全格式
{
  "prompt": "写一首关于春天的诗：",
  "completion": "春风拂面暖阳照..."
}
\`\`\`

### 数据质量原则

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        高质量微调数据的特征                               │
└─────────────────────────────────────────────────────────────────────────┘

  ✅ 数量充足
     • 最少 100-1000 条
     • 复杂任务需要更多

  ✅ 质量高
     • 准确无误
     • 表达清晰
     • 格式一致

  ✅ 多样性
     • 覆盖各种情况
     • 包含边界案例
     • 长度分布合理

  ✅ 代表性
     • 反映真实使用场景
     • 符合目标任务需求
\`\`\`

---

## 微调工具推荐

### 1. Hugging Face Transformers + PEFT

\`\`\`bash
# 安装
pip install transformers peft datasets accelerate

# PEFT = Parameter-Efficient Fine-Tuning
# 支持 LoRA、QLoRA、Adapter 等多种方法
\`\`\`

### 2. LLaMA-Factory（推荐新手）

\`\`\`bash
# 开箱即用的微调框架
git clone https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -r requirements.txt

# 提供 Web UI，无需写代码
python src/train_web.py
\`\`\`

### 3. Axolotl

\`\`\`bash
# 配置驱动的微调框架
git clone https://github.com/OpenAccess-AI-Collective/axolotl.git
cd axolotl
pip install -e .

# 使用 YAML 配置文件
accelerate launch -m axolotl.cli.train config.yaml
\`\`\`

---

## 本节要点

1. **微调目的** —— 让通用模型适应特定领域/任务
2. **LoRA** —— 最推荐的微调方法，参数效率高
3. **数据准备** —— 质量比数量更重要
4. **工具选择** —— LLaMA-Factory 适合新手，PEFT 更灵活
5. **资源评估** —— 根据显存选择合适的微调方法
            `,
            ja: `
## ファインチューニングとは？

ファインチューニングは、事前学習済みモデルをベースに、特定ドメインのデータでさらに学習させ、特定タスクにより適したモデルにすることです。

---

## なぜファインチューニングが必要？

\`\`\`
汎用大規模モデルの限界：
❌ あなたの専門領域の用語を知らない
❌ 会社の製品やプロセスを知らない
❌ 出力スタイルが要件に合わない可能性
❌ 特定タスクで十分な精度がない

ファインチューニング後：
✅ ドメイン専門知識を習得
✅ 特定の用語と表現を理解
✅ ブランドスタイルに合った出力
✅ 特定タスクで優れたパフォーマンス
\`\`\`

---

## ファインチューニングの種類

| 方法 | 学習パラメータ | VRAM | 効果 | 適用場面 |
|------|-------------|------|------|---------|
| フルチューニング | 100% | 極高 | 最良 | リソース十分 |
| LoRA | ~0.1% | 低 | 良好 | リソース制限（推奨）|
| QLoRA | ~0.1% | 極低 | 良い | 消費者GPU |
| Prompt Tuning | ~0.01% | 極低 | 普通 | 簡単なタスク |

---

## LoRA：低ランク適応

LoRA（Low-Rank Adaptation）は現在最も人気のあるファインチューニング方法です。

### コアアイデア

\`\`\`
元の重み行列 W（凍結、更新しない）
┌────────────────────────────────┐
│        d × d 次元               │  例：4096 × 4096
│       （パラメータ数膨大）        │  = 1600万パラメータ
└────────────────────────────────┘

LoRAは2つの小さな行列 A と B を追加：

┌────────┐   ┌────────┐
│   B    │ × │   A    │    r = ランク（通常4~64）
│ d × r  │   │ r × d  │
└────────┘   └────────┘
  4096×8      8×4096     = 6.5万パラメータ（99.6%削減！）

A と B のみを学習、W は変更なし！
\`\`\`

---

## ファインチューニングツール推奨

### 1. LLaMA-Factory（初心者推奨）

\`\`\`bash
git clone https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -r requirements.txt

# Web UIを提供、コード不要
python src/train_web.py
\`\`\`

### 2. Hugging Face PEFT

\`\`\`bash
pip install transformers peft datasets accelerate
\`\`\`

---

## 本節のポイント

1. **ファインチューニングの目的** —— 汎用モデルを特定ドメイン/タスクに適応
2. **LoRA** —— 最も推奨される方法、パラメータ効率が高い
3. **データ準備** —— 量より質が重要
4. **ツール選択** —— LLaMA-Factoryは初心者向け、PEFTはより柔軟
            `
          }
        },
        {
          id: 'ch1-lora-practice',
          title: { zh: '1.4 LoRA 微调实战', ja: '1.4 LoRA ファインチューニング実践' },
          content: {
            zh: `
## LoRA 微调实战指南

本节将手把手教你使用 LoRA 微调一个大模型。

---

## 环境准备

### 硬件要求

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        不同模型的显存需求                                 │
└─────────────────────────────────────────────────────────────────────────┘

  模型大小      LoRA 微调      QLoRA 微调     全量微调
  ────────      ─────────      ──────────     ────────
  7B            16 GB          8 GB           60+ GB
  13B           24 GB          12 GB          100+ GB
  70B           80+ GB         48 GB          500+ GB

  推荐配置：
  • 入门级：RTX 3090/4090（24GB）→ 7B-13B LoRA
  • 专业级：A100（40/80GB）→ 70B LoRA
  • 云端：使用 Colab Pro 或云 GPU
\`\`\`

### 安装依赖

\`\`\`bash
# 基础依赖
pip install torch transformers datasets
pip install peft accelerate bitsandbytes
pip install wandb  # 可选，用于训练监控

# 验证 CUDA
python -c "import torch; print(torch.cuda.is_available())"
\`\`\`

---

## 使用 PEFT 进行 LoRA 微调

### 完整代码示例

\`\`\`python
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import load_dataset

# 1. 加载基础模型和分词器
model_name = "meta-llama/Llama-2-7b-hf"
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

# 2. 配置 LoRA
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=8,                      # LoRA 秩，越大效果越好但参数越多
    lora_alpha=32,            # 缩放因子
    lora_dropout=0.1,         # Dropout 防止过拟合
    target_modules=[          # 要应用 LoRA 的模块
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
    ]
)

# 3. 创建 PEFT 模型
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
# 输出：trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06%

# 4. 准备数据集
def format_instruction(sample):
    return f"""### 指令：
{sample['instruction']}

### 输入：
{sample['input']}

### 回答：
{sample['output']}"""

def tokenize(sample):
    result = tokenizer(
        format_instruction(sample),
        truncation=True,
        max_length=512,
        padding="max_length"
    )
    result["labels"] = result["input_ids"].copy()
    return result

dataset = load_dataset("json", data_files="train_data.json")
tokenized_dataset = dataset.map(tokenize)

# 5. 配置训练参数
training_args = TrainingArguments(
    output_dir="./lora_output",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    fp16=True,
)

# 6. 开始训练
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    data_collator=DataCollatorForSeq2Seq(tokenizer, padding=True),
)

trainer.train()

# 7. 保存 LoRA 权重
model.save_pretrained("./lora_weights")
\`\`\`

---

## 使用 LLaMA-Factory（推荐新手）

### 通过 Web UI 微调

\`\`\`bash
# 1. 克隆仓库
git clone https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory

# 2. 安装依赖
pip install -r requirements.txt

# 3. 启动 Web UI
python src/train_web.py
\`\`\`

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    LLaMA-Factory Web UI                                  │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  模型选择: [Llama-2-7B ▼]                                               │
│                                                                         │
│  微调方法: ○ 全量  ● LoRA  ○ QLoRA                                      │
│                                                                         │
│  训练数据: [选择文件...]  train_data.json                                │
│                                                                         │
│  LoRA 秩:    [8    ]    学习率: [2e-4  ]                                │
│  训练轮数:   [3    ]    批次大小: [4   ]                                 │
│                                                                         │
│              [开始训练]    [导出模型]                                     │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
\`\`\`

### 通过命令行微调

\`\`\`bash
# 使用配置文件
CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\
    --stage sft \\
    --model_name_or_path meta-llama/Llama-2-7b-hf \\
    --do_train \\
    --dataset your_dataset \\
    --template llama2 \\
    --finetuning_type lora \\
    --lora_target q_proj,v_proj \\
    --output_dir ./lora_output \\
    --per_device_train_batch_size 4 \\
    --gradient_accumulation_steps 4 \\
    --lr_scheduler_type cosine \\
    --logging_steps 10 \\
    --save_steps 1000 \\
    --learning_rate 5e-5 \\
    --num_train_epochs 3.0 \\
    --fp16
\`\`\`

---

## QLoRA：更低显存的选择

QLoRA = 量化 + LoRA，可以在消费级显卡上微调大模型。

\`\`\`python
from transformers import BitsAndBytesConfig

# 4-bit 量化配置
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# 加载量化模型
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto"
)

# 然后正常应用 LoRA
model = get_peft_model(model, lora_config)

# 显存对比（7B 模型）：
# - FP16: ~14GB
# - 4-bit QLoRA: ~6GB
\`\`\`

---

## 微调后的模型使用

### 加载 LoRA 权重

\`\`\`python
from peft import PeftModel

# 加载基础模型
base_model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    torch_dtype=torch.float16,
    device_map="auto"
)

# 加载 LoRA 权重
model = PeftModel.from_pretrained(
    base_model,
    "./lora_weights"
)

# 推理
inputs = tokenizer("你好，请介绍一下你自己", return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=100)
print(tokenizer.decode(outputs[0]))
\`\`\`

### 合并 LoRA 权重

\`\`\`python
# 将 LoRA 权重合并到基础模型
merged_model = model.merge_and_unload()

# 保存合并后的完整模型
merged_model.save_pretrained("./merged_model")
tokenizer.save_pretrained("./merged_model")
\`\`\`

---

## 常见问题与解决

| 问题 | 可能原因 | 解决方案 |
|------|---------|---------|
| 显存不足 | 模型太大 | 使用 QLoRA 或减小 batch size |
| 训练不收敛 | 学习率不合适 | 尝试 1e-4 ~ 5e-5 |
| 过拟合 | 数据太少 | 增加数据或加大 dropout |
| 效果不好 | 数据质量差 | 清洗数据，提高质量 |
| 速度太慢 | 没有使用混合精度 | 开启 fp16/bf16 |

---

## 本节要点

1. **环境准备** —— 显存决定能微调多大的模型
2. **PEFT 库** —— 官方推荐的参数高效微调库
3. **LLaMA-Factory** —— 新手友好，提供 Web UI
4. **QLoRA** —— 显存不足时的最佳选择
5. **模型合并** —— 可以将 LoRA 权重合并到基础模型
            `,
            ja: `
## LoRAファインチューニング実践ガイド

このセクションでは、LoRAを使用して大規模モデルをファインチューニングする方法を手順を追って説明します。

---

## 環境準備

### ハードウェア要件

| モデルサイズ | LoRA | QLoRA | フルチューニング |
|------------|------|-------|---------------|
| 7B | 16 GB | 8 GB | 60+ GB |
| 13B | 24 GB | 12 GB | 100+ GB |
| 70B | 80+ GB | 48 GB | 500+ GB |

### 依存関係インストール

\`\`\`bash
pip install torch transformers datasets
pip install peft accelerate bitsandbytes
\`\`\`

---

## PEFTを使用したLoRAファインチューニング

\`\`\`python
from peft import LoraConfig, get_peft_model, TaskType

# LoRA設定
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=8,                      # LoRAランク
    lora_alpha=32,            # スケーリング係数
    lora_dropout=0.1,         # ドロップアウト
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)

# PEFTモデル作成
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
# 出力：trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06%
\`\`\`

---

## LLaMA-Factory使用（初心者推奨）

\`\`\`bash
# 1. リポジトリをクローン
git clone https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory

# 2. 依存関係インストール
pip install -r requirements.txt

# 3. Web UI起動
python src/train_web.py
\`\`\`

Web UIで簡単にファインチューニング設定が可能です。

---

## QLoRA：より低いVRAMでの選択

\`\`\`python
from transformers import BitsAndBytesConfig

# 4-bit量子化設定
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# VRAM比較（7Bモデル）：
# - FP16: ~14GB
# - 4-bit QLoRA: ~6GB
\`\`\`

---

## 本節のポイント

1. **環境準備** —— VRAMがファインチューニング可能なモデルサイズを決定
2. **PEFTライブラリ** —— 公式推奨のパラメータ効率的なファインチューニングライブラリ
3. **LLaMA-Factory** —— 初心者向け、Web UI提供
4. **QLoRA** —— VRAM不足時の最良の選択
            `
          }
        },
        {
          id: 'ch1-case-study',
          title: { zh: '1.5 实战案例：客服机器人微调', ja: '1.5 実践例：カスタマーサービスボットのファインチューニング' },
          content: {
            zh: `
## 🎯 实战目标

微调一个专门回答电商客服问题的 AI 助手。

---

## 📊 案例背景

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                     电商客服机器人微调项目                                 │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  目标：让 AI 掌握公司产品知识，以专业客服语气回答问题                        │
│                                                                         │
│  基座模型：Qwen2-7B-Instruct（开源、中文能力强）                           │
│  微调方法：LoRA（显存友好）                                               │
│  训练数据：1000 条客服对话记录                                            │
│  硬件：RTX 4090 24GB                                                    │
│  预计时间：2-3 小时                                                      │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
\`\`\`

---

## 📝 Step 1: 准备训练数据

### 数据格式示例

\`\`\`json
{
  "conversations": [
    {
      "role": "system",
      "content": "你是XX电商的专业客服，态度友好、回答专业准确。"
    },
    {
      "role": "user",
      "content": "你们的退货政策是什么？"
    },
    {
      "role": "assistant",
      "content": "您好！我们提供7天无理由退货服务。商品需保持原包装完好，配件齐全。退货运费由我们承担，退款将在收到商品后3个工作日内原路返回。请问还有其他问题吗？"
    }
  ]
}
\`\`\`

### 数据采集来源

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        训练数据来源                                       │
└─────────────────────────────────────────────────────────────────────────┘

  1. 历史客服记录（脱敏处理）
     ├─ 筛选高质量对话
     ├─ 人工纠正错误回答
     └─ 统一格式

  2. 人工编写样本
     ├─ 覆盖常见问题
     ├─ 包含边界情况
     └─ 体现品牌调性

  3. GPT-4 辅助生成
     ├─ 提供场景描述
     ├─ 生成多样化问答
     └─ 人工审核筛选

  建议配比：真实数据 60% + 人工编写 25% + AI生成 15%
\`\`\`

### 数据质量检查清单

| 检查项 | 标准 | ✓/✗ |
|--------|------|-----|
| 格式正确 | JSON 可解析 | ✓ |
| 无空值 | 每个字段都有内容 | ✓ |
| 语气一致 | 符合客服专业调性 | ✓ |
| 信息准确 | 产品信息无误 | ✓ |
| 长度适中 | 回答 50-300 字 | ✓ |
| 覆盖全面 | 涵盖主要问题类型 | ✓ |

---

## 💻 Step 2: 使用 Unsloth 快速微调

### 为什么用 Unsloth？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    Unsloth vs 传统 PEFT 对比                             │
└─────────────────────────────────────────────────────────────────────────┘

  指标              Unsloth          传统 PEFT
  ────              ───────          ─────────
  训练速度           2-5x 更快        基准
  显存占用           减少 60%         基准
  安装难度           一行命令         需要配置
  支持模型           主流 LLM         更广泛

  结论：新手强烈推荐 Unsloth！
\`\`\`

### 安装 Unsloth

\`\`\`bash
# 一键安装（推荐）
pip install unsloth

# 或从源码安装（获取最新功能）
pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
\`\`\`

### 完整微调代码

\`\`\`python
from unsloth import FastLanguageModel
from trl import SFTTrainer
from transformers import TrainingArguments
from datasets import load_dataset
import torch

# ============================================
# 1. 加载模型（自动应用 Unsloth 优化）
# ============================================
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/Qwen2-7B-Instruct",  # 使用 Unsloth 优化版
    max_seq_length=2048,
    dtype=None,  # 自动检测
    load_in_4bit=True,  # 使用 4-bit 量化，显存友好
)

# ============================================
# 2. 添加 LoRA 适配器
# ============================================
model = FastLanguageModel.get_peft_model(
    model,
    r=16,                          # LoRA 秩，推荐 8-64
    target_modules=[               # 目标模块
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ],
    lora_alpha=16,                 # 缩放因子
    lora_dropout=0,                # Unsloth 优化，设为 0 更快
    bias="none",
    use_gradient_checkpointing="unsloth",  # 节省显存
)

print(f"可训练参数: {model.print_trainable_parameters()}")
# 输出: trainable params: 41,943,040 || all params: 7,657,616,384 || 0.55%

# ============================================
# 3. 准备数据集
# ============================================
def formatting_prompts(examples):
    """将对话格式化为模型输入"""
    texts = []
    for conv in examples["conversations"]:
        text = ""
        for msg in conv:
            if msg["role"] == "system":
                text += f"<|im_start|>system\\n{msg['content']}<|im_end|>\\n"
            elif msg["role"] == "user":
                text += f"<|im_start|>user\\n{msg['content']}<|im_end|>\\n"
            elif msg["role"] == "assistant":
                text += f"<|im_start|>assistant\\n{msg['content']}<|im_end|>\\n"
        texts.append(text)
    return {"text": texts}

# 加载数据
dataset = load_dataset("json", data_files="customer_service_data.json")
dataset = dataset.map(formatting_prompts, batched=True)

# ============================================
# 4. 配置训练参数
# ============================================
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset["train"],
    dataset_text_field="text",
    max_seq_length=2048,
    args=TrainingArguments(
        output_dir="./customer_service_lora",
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        warmup_steps=10,
        max_steps=500,               # 1000条数据约500步
        learning_rate=2e-4,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=10,
        save_steps=100,
        optim="adamw_8bit",          # 8-bit 优化器，节省显存
    ),
)

# ============================================
# 5. 开始训练！
# ============================================
print("开始训练...")
trainer.train()

# ============================================
# 6. 保存模型
# ============================================
model.save_pretrained("./customer_service_lora")
tokenizer.save_pretrained("./customer_service_lora")

print("✅ 训练完成！模型已保存。")
\`\`\`

---

## 📈 Step 3: 训练监控与调优

### 观察训练 Loss

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        训练 Loss 曲线解读                                 │
└─────────────────────────────────────────────────────────────────────────┘

  Loss
   3.0 ┤
       │╲
   2.5 ┤ ╲
       │  ╲
   2.0 ┤   ╲
       │    ╲──────
   1.5 ┤           ╲
       │            ╲
   1.0 ┤             ╲______________________
       │
   0.5 ┤
       └────────────────────────────────────────
        0   100   200   300   400   500  Step

  ✅ 健康曲线：快速下降后趋于平稳
  ⚠️ 不收敛：Loss 不下降 → 调大学习率
  ⚠️ 震荡大：上下波动剧烈 → 调小学习率
  ⚠️ 过拟合：训练 Loss 极低但验证 Loss 上升 → 减少训练步数
\`\`\`

### 超参数调优建议

| 场景 | 调整建议 |
|------|---------|
| Loss 不下降 | 增大 learning_rate 到 5e-4 |
| Loss 震荡 | 减小 learning_rate 到 1e-4 |
| 显存不足 | 减小 batch_size，增大 gradient_accumulation |
| 效果不好 | 增大 r 值（如 32 或 64） |
| 过拟合 | 减少 max_steps，增加数据 |

---

## 🧪 Step 4: 效果评估

### 自动评估

\`\`\`python
from transformers import pipeline

# 加载微调后的模型
pipe = pipeline("text-generation", model="./customer_service_lora")

# 测试用例
test_cases = [
    "你们支持花呗付款吗？",
    "我的订单还没发货，能催一下吗？",
    "商品有质量问题怎么办？",
    "能开发票吗？",
]

print("=" * 60)
print("模型效果测试")
print("=" * 60)

for question in test_cases:
    prompt = f"<|im_start|>system\\n你是XX电商的专业客服。<|im_end|>\\n<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant\\n"

    response = pipe(prompt, max_new_tokens=200, temperature=0.7)
    answer = response[0]['generated_text'].split("<|im_start|>assistant\\n")[-1]

    print(f"\\n用户: {question}")
    print(f"客服: {answer}")
\`\`\`

### 人工评估标准

| 维度 | 1分 | 3分 | 5分 |
|------|-----|-----|-----|
| 准确性 | 信息错误 | 基本正确 | 完全准确 |
| 专业度 | 口语化 | 一般 | 专业术语恰当 |
| 态度 | 生硬 | 普通 | 热情友好 |
| 完整性 | 答非所问 | 部分解答 | 完整全面 |

---

## 💰 成本估算

### 云 GPU 价格参考（2025年）

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        云 GPU 价格对比                                    │
└─────────────────────────────────────────────────────────────────────────┘

  平台              GPU              价格（$/小时）   适合模型
  ────              ───              ──────────────   ────────
  Lambda Labs       RTX 4090         $0.75            7B LoRA
  Vast.ai           RTX 4090         $0.50-0.80       7B LoRA
  RunPod            RTX 4090         $0.69            7B LoRA
  AWS               A10G             $1.00            7B-13B
  Google Cloud      A100 40GB        $3.67            13B-70B
  AutoDL (国内)     RTX 4090         ¥2.5/小时        7B LoRA

  本案例预估成本：
  ┌────────────────────────────────────────────┐
  │  RTX 4090 × 3小时 ≈ $2.25（约 ¥16）        │
  │  性价比极高！                              │
  └────────────────────────────────────────────┘
\`\`\`

---

## 🚀 Step 5: 部署上线

### 使用 Ollama 本地部署

\`\`\`bash
# 1. 安装 Ollama
curl -fsSL https://ollama.com/install.sh | sh

# 2. 创建 Modelfile
cat > Modelfile << 'EOF'
FROM ./customer_service_merged

SYSTEM "你是XX电商的专业客服，态度友好、回答专业准确。"

PARAMETER temperature 0.7
PARAMETER top_p 0.9
EOF

# 3. 创建 Ollama 模型
ollama create customer-service -f Modelfile

# 4. 运行
ollama run customer-service
\`\`\`

### 使用 vLLM 高性能部署

\`\`\`bash
# 安装 vLLM
pip install vllm

# 启动 API 服务
python -m vllm.entrypoints.openai.api_server \\
    --model ./customer_service_merged \\
    --host 0.0.0.0 \\
    --port 8000

# 调用 API
curl http://localhost:8000/v1/chat/completions \\
    -H "Content-Type: application/json" \\
    -d '{
        "model": "customer_service",
        "messages": [{"role": "user", "content": "退货怎么操作？"}]
    }'
\`\`\`

---

## ✅ 本节要点

1. **数据准备** —— 质量 > 数量，建议 1000+ 条
2. **Unsloth** —— 速度快 2-5x，显存省 60%
3. **训练监控** —— 观察 Loss 曲线，及时调参
4. **效果评估** —— 自动 + 人工双重验证
5. **成本控制** —— 云 GPU 微调成本低至 ¥20
6. **部署上线** —— Ollama（简单）或 vLLM（高性能）
            `,
            ja: `
## 🎯 実践目標

ECサイトのカスタマーサービス問い合わせに特化したAIアシスタントをファインチューニング。

---

## 📊 ケース背景

\`\`\`
目標：AIに自社製品知識を習得させ、プロの接客トーンで回答
ベースモデル：Qwen2-7B-Instruct
ファインチューニング方法：LoRA
トレーニングデータ：1000件の接客対話
ハードウェア：RTX 4090 24GB
予想時間：2-3時間
\`\`\`

---

## 💻 Unslothで高速ファインチューニング

### なぜUnsloth？

| 指標 | Unsloth | 従来のPEFT |
|------|---------|-----------|
| 学習速度 | 2-5倍高速 | 基準 |
| VRAM使用量 | 60%削減 | 基準 |
| インストール | 1行 | 設定必要 |

### インストール

\`\`\`bash
pip install unsloth
\`\`\`

### 完全なコード例

\`\`\`python
from unsloth import FastLanguageModel
from trl import SFTTrainer
from transformers import TrainingArguments

# モデル読み込み
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/Qwen2-7B-Instruct",
    max_seq_length=2048,
    load_in_4bit=True,
)

# LoRAアダプター追加
model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_alpha=16,
)

# トレーナー設定
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    args=TrainingArguments(
        output_dir="./lora_output",
        per_device_train_batch_size=2,
        max_steps=500,
        learning_rate=2e-4,
    ),
)

# 学習開始
trainer.train()
\`\`\`

---

## 💰 コスト見積もり

| プラットフォーム | GPU | 価格/時間 |
|----------------|-----|----------|
| Lambda Labs | RTX 4090 | $0.75 |
| RunPod | RTX 4090 | $0.69 |
| AWS | A10G | $1.00 |

本ケースの推定コスト：約$2-3（3時間）

---

## 🚀 デプロイ

### Ollamaでローカルデプロイ

\`\`\`bash
# インストール
curl -fsSL https://ollama.com/install.sh | sh

# モデル作成
ollama create customer-service -f Modelfile

# 実行
ollama run customer-service
\`\`\`

---

## ✅ 本節のポイント

1. **データ準備** —— 質 > 量、1000件以上推奨
2. **Unsloth** —— 2-5倍高速、60% VRAM削減
3. **コスト管理** —— クラウドGPUで約$2-3
4. **デプロイ** —— Ollama（簡単）またはvLLM（高性能）
            `
          }
        },
        {
          id: 'ch1-local-deploy',
          title: { zh: '1.6 本地部署与 Ollama', ja: '1.6 ローカルデプロイとOllama' },
          content: {
            zh: `
## 为什么要本地部署？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                     本地部署 vs 云端 API                                  │
└─────────────────────────────────────────────────────────────────────────┘

                    本地部署                    云端 API
                    ────────                    ────────
  隐私性            ✅ 数据不离开本地            ❌ 数据发送到云端
  成本              ✅ 一次投入长期免费           ❌ 按调用付费
  速度              ✅ 无网络延迟                ❌ 依赖网络
  定制性            ✅ 可以微调                  ⚠️ 部分支持
  模型选择          ⚠️ 受硬件限制               ✅ 最强模型

  适用场景：
  • 隐私敏感数据处理
  • 离线环境使用
  • 高频调用节省成本
  • 学习研究目的
\`\`\`

---

## Ollama：最简单的本地部署方案

### 安装 Ollama

\`\`\`bash
# macOS / Linux
curl -fsSL https://ollama.com/install.sh | sh

# Windows
# 下载安装包：https://ollama.com/download/windows
\`\`\`

### 运行开源模型

\`\`\`bash
# 运行 Llama 3.1 8B（推荐入门）
ollama run llama3.1

# 运行 Qwen2 7B（中文能力强）
ollama run qwen2

# 运行 CodeLlama（代码生成）
ollama run codellama

# 运行 DeepSeek Coder（代码能力强）
ollama run deepseek-coder:6.7b
\`\`\`

### 常用模型推荐

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                     Ollama 热门模型                                      │
└─────────────────────────────────────────────────────────────────────────┘

  模型名称              大小        特点                  显存需求
  ────────              ────        ────                  ────────
  llama3.1:8b           4.7GB       通用能力强             8GB+
  qwen2:7b              4.4GB       中文能力最强           8GB+
  mistral:7b            4.1GB       速度快、效果好         8GB+
  codellama:7b          3.8GB       代码生成专用           8GB+
  deepseek-coder:6.7b   3.8GB       代码能力强             8GB+
  phi3:3.8b             2.2GB       小巧但能力强           4GB+
  gemma2:2b             1.6GB       超小模型入门           4GB+

  选择建议：
  • 8GB 显存：选 7B 以下模型
  • 16GB 显存：可以跑 13B 模型
  • 24GB 显存：可以跑量化的 70B 模型
\`\`\`

### Ollama 命令速查

\`\`\`bash
# 查看已下载模型
ollama list

# 下载模型（不运行）
ollama pull llama3.1

# 删除模型
ollama rm llama3.1

# 查看模型信息
ollama show llama3.1

# 复制模型
ollama cp llama3.1 my-llama

# 启动 API 服务（默认 11434 端口）
ollama serve

# API 调用示例
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.1",
  "prompt": "为什么天空是蓝色的？",
  "stream": false
}'
\`\`\`

---

## 使用自定义模型

### 导入 GGUF 模型

\`\`\`bash
# 1. 下载 GGUF 格式模型（从 Hugging Face）
# 例如：TheBloke/Llama-2-7B-GGUF

# 2. 创建 Modelfile
cat > Modelfile << 'EOF'
FROM ./llama-2-7b.Q4_K_M.gguf

# 设置系统提示词
SYSTEM "你是一个专业的AI助手，回答问题时要准确、简洁。"

# 设置参数
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER num_ctx 4096
EOF

# 3. 创建 Ollama 模型
ollama create my-llama -f Modelfile

# 4. 运行
ollama run my-llama
\`\`\`

### 导入微调后的模型

\`\`\`bash
# 1. 合并 LoRA 权重（如果还没合并）
python -c "
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer

base = AutoModelForCausalLM.from_pretrained('Qwen/Qwen2-7B-Instruct')
model = PeftModel.from_pretrained(base, './my_lora')
merged = model.merge_and_unload()
merged.save_pretrained('./merged_model')
"

# 2. 转换为 GGUF 格式
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp

python convert_hf_to_gguf.py ../merged_model --outtype q4_k_m

# 3. 创建 Ollama 模型
ollama create my-finetuned -f Modelfile
\`\`\`

---

## 配合 Open WebUI 使用

\`\`\`bash
# 使用 Docker 一键部署
docker run -d -p 3000:8080 \\
  --add-host=host.docker.internal:host-gateway \\
  -v open-webui:/app/backend/data \\
  --name open-webui \\
  ghcr.io/open-webui/open-webui:main

# 访问 http://localhost:3000
# 自动连接本地 Ollama
\`\`\`

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                     Open WebUI 界面                                      │
├─────────────────────────────────────────────────────────────────────────┤
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  🤖 Select a model: [llama3.1 ▼]                                │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  User: 帮我写一首关于春天的诗                                     │   │
│  │  ────────────────────────────────────────────────────────────   │   │
│  │  Assistant: 春风拂柳绿意浓，                                      │   │
│  │            花开满园香飘送。                                       │   │
│  │            蝴蝶翩翩舞枝头，                                       │   │
│  │            燕子归来筑新梦。                                       │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  Type a message...                                    [Send]    │   │
│  └─────────────────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────────────────┘
\`\`\`

---

## 性能优化技巧

### GPU 加速

\`\`\`bash
# 确保 NVIDIA 驱动已安装
nvidia-smi

# Ollama 会自动检测并使用 GPU
# 如需指定 GPU：
CUDA_VISIBLE_DEVICES=0 ollama run llama3.1

# 查看 GPU 使用情况
watch -n 1 nvidia-smi
\`\`\`

### 模型量化选择

| 量化等级 | 大小 | 质量 | 速度 | 推荐场景 |
|---------|------|------|------|---------|
| Q8_0 | 100% | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | 质量优先 |
| Q6_K | 75% | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | 平衡 |
| Q4_K_M | 50% | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | **推荐** |
| Q4_0 | 45% | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ | 速度优先 |
| Q2_K | 30% | ⭐⭐ | ⭐⭐⭐⭐⭐ | 极限压缩 |

---

## 本节要点

1. **Ollama** —— 最简单的本地 LLM 部署方案
2. **模型选择** —— 根据显存选择合适大小的模型
3. **自定义模型** —— 支持导入 GGUF 格式和微调模型
4. **Open WebUI** —— 提供 ChatGPT 风格的 Web 界面
5. **性能优化** —— 使用 Q4_K_M 量化获得最佳性价比
            `,
            ja: `
## なぜローカルデプロイ？

| 項目 | ローカル | クラウドAPI |
|------|---------|------------|
| プライバシー | ✅ データはローカルに | ❌ クラウドに送信 |
| コスト | ✅ 一度の投資で長期無料 | ❌ 従量課金 |
| 速度 | ✅ ネットワーク遅延なし | ❌ ネットワーク依存 |

---

## Ollama：最もシンプルなローカルデプロイ

### インストール

\`\`\`bash
# macOS / Linux
curl -fsSL https://ollama.com/install.sh | sh
\`\`\`

### モデル実行

\`\`\`bash
# Llama 3.1 8B実行
ollama run llama3.1

# Qwen2 7B（中国語に強い）
ollama run qwen2
\`\`\`

### おすすめモデル

| モデル | サイズ | 特徴 | VRAM |
|--------|-------|------|------|
| llama3.1:8b | 4.7GB | 汎用性高い | 8GB+ |
| qwen2:7b | 4.4GB | 中国語最強 | 8GB+ |
| phi3:3.8b | 2.2GB | 小型で高性能 | 4GB+ |

---

## カスタムモデル

\`\`\`bash
# GGUFモデルをインポート
cat > Modelfile << 'EOF'
FROM ./model.gguf
SYSTEM "あなたはプロのAIアシスタントです。"
EOF

ollama create my-model -f Modelfile
\`\`\`

---

## Open WebUI

\`\`\`bash
# Docker で一発デプロイ
docker run -d -p 3000:8080 \\
  --add-host=host.docker.internal:host-gateway \\
  ghcr.io/open-webui/open-webui:main

# http://localhost:3000 でアクセス
\`\`\`

---

## 本節のポイント

1. **Ollama** —— 最もシンプルなローカルLLMデプロイ
2. **モデル選択** —— VRAMに応じて適切なサイズを選択
3. **Open WebUI** —— ChatGPT風のWebインターフェース
            `
          }
        },
        {
          id: 'ch1-summary',
          title: { zh: '1.7 本章小结', ja: '1.7 この章のまとめ' },
          content: {
            zh: `
## 大模型技术核心回顾

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    本章知识地图                                          │
└─────────────────────────────────────────────────────────────────────────┘

                        大模型技术深度解析
                              │
          ┌───────────────────┼───────────────────┐
          ▼                   ▼                   ▼
     Transformer           注意力机制            微调技术
          │                   │                   │
     ┌────┴────┐         ┌────┴────┐         ┌────┴────┐
     │ Encoder │         │  Q/K/V  │         │  LoRA   │
     │ Decoder │         │ 多头注意力│         │  QLoRA  │
     │ 位置编码 │         │ Mask注意力│         │ 数据准备 │
     └─────────┘         └─────────┘         └─────────┘
\`\`\`

---

## 关键知识点总结

### Transformer 架构

| 组件 | 作用 | 重要性 |
|------|------|--------|
| Self-Attention | 建模全局依赖 | ⭐⭐⭐⭐⭐ |
| Feed Forward | 特征变换 | ⭐⭐⭐⭐ |
| 位置编码 | 提供位置信息 | ⭐⭐⭐⭐ |
| Layer Norm | 稳定训练 | ⭐⭐⭐ |

### 注意力机制

| 类型 | 特点 | 应用 |
|------|------|------|
| Self-Attention | 序列内部关系 | 所有 Transformer |
| Cross-Attention | 序列间关系 | 翻译、问答 |
| Multi-Head | 多视角 | 增强表达能力 |
| Masked | 防止信息泄露 | 语言生成 |

### 微调方法

| 方法 | 参数量 | 显存 | 推荐度 |
|------|--------|------|--------|
| LoRA | ~0.1% | 低 | ⭐⭐⭐⭐⭐ |
| QLoRA | ~0.1% | 极低 | ⭐⭐⭐⭐ |
| 全量微调 | 100% | 极高 | ⭐⭐⭐ |

---

## 实践路线图

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    学习路线建议                                          │
└─────────────────────────────────────────────────────────────────────────┘

  第一步：理解原理
  ─────────────────
  • 阅读 "Attention Is All You Need" 论文
  • 理解 Self-Attention 计算过程
  • 了解 Encoder/Decoder 区别

  第二步：动手实践
  ─────────────────
  • 使用 LLaMA-Factory 完成第一次微调
  • 准备自己的数据集
  • 尝试不同的 LoRA 参数

  第三步：深入优化
  ─────────────────
  • 学习数据清洗和增强
  • 尝试 QLoRA 降低成本
  • 探索模型合并技术

  第四步：生产部署
  ─────────────────
  • 学习模型量化
  • 了解推理优化
  • 部署到生产环境
\`\`\`

---

## 推荐资源

### 必读论文

| 论文 | 内容 | 链接 |
|------|------|------|
| Attention Is All You Need | Transformer 原论文 | arXiv:1706.03762 |
| BERT | Encoder 代表作 | arXiv:1810.04805 |
| GPT 系列 | Decoder 代表作 | OpenAI Blog |
| LoRA | 低秩微调 | arXiv:2106.09685 |

### 实践工具

| 工具 | 用途 | 难度 |
|------|------|------|
| LLaMA-Factory | 一站式微调 | ⭐⭐ |
| PEFT | 官方参数高效库 | ⭐⭐⭐ |
| Axolotl | 配置驱动微调 | ⭐⭐⭐ |
| vLLM | 高效推理 | ⭐⭐⭐⭐ |

---

## 写在最后

理解大模型的技术原理，不是为了成为 AI 研究员，而是为了：

1. **更好地使用 AI** —— 知其然，知其所以然
2. **做出明智的选型** —— 根据需求选择合适的模型和方法
3. **定制专属 AI** —— 掌握微调能力，构建领域专属模型

技术在不断发展，但核心原理是稳定的。掌握了这些基础，你就能更好地跟上 AI 的发展步伐！

*"理解原理，才能真正驾驭工具。"*
            `,
            ja: `
## 大規模モデル技術コア復習

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    この章の知識マップ                                     │
└─────────────────────────────────────────────────────────────────────────┘

                        大規模モデル技術詳解
                              │
          ┌───────────────────┼───────────────────┐
          ▼                   ▼                   ▼
     Transformer           アテンション           ファインチューニング
          │                   │                   │
     ┌────┴────┐         ┌────┴────┐         ┌────┴────┐
     │ Encoder │         │  Q/K/V  │         │  LoRA   │
     │ Decoder │         │マルチヘッド│         │  QLoRA  │
     │位置エンコード│        │ Maskアテンション│       │ データ準備 │
     └─────────┘         └─────────┘         └─────────┘
\`\`\`

---

## 重要ポイントまとめ

### Transformerアーキテクチャ

| コンポーネント | 役割 | 重要度 |
|-------------|------|--------|
| Self-Attention | グローバル依存性モデリング | ⭐⭐⭐⭐⭐ |
| Feed Forward | 特徴変換 | ⭐⭐⭐⭐ |
| 位置エンコーディング | 位置情報提供 | ⭐⭐⭐⭐ |

### ファインチューニング方法

| 方法 | パラメータ量 | VRAM | 推奨度 |
|------|-----------|------|--------|
| LoRA | ~0.1% | 低 | ⭐⭐⭐⭐⭐ |
| QLoRA | ~0.1% | 極低 | ⭐⭐⭐⭐ |
| フルチューニング | 100% | 極高 | ⭐⭐⭐ |

---

## 実践ロードマップ

1. **原理理解** —— 論文を読み、アテンション計算を理解
2. **実践開始** —— LLaMA-Factoryで最初のファインチューニング
3. **深い最適化** —— データ準備、QLoRAでコスト削減
4. **本番デプロイ** —— 量子化、推論最適化

---

## 最後に

大規模モデルの技術原理を理解することは、AI研究者になるためではなく：

1. **AIをより良く使う** —— 原理を知れば、より効果的に活用できる
2. **賢い選択** —— ニーズに応じた適切なモデルと方法を選択
3. **専用AIをカスタマイズ** —— ファインチューニング能力でドメイン特化モデルを構築

*「原理を理解してこそ、ツールを本当に使いこなせる。」*
            `
          }
        }
      ]
    },
    // ============================================
    // 第二章：提示词工程进阶
    // ============================================
    {
      id: 'chapter-2',
      number: 2,
      title: { zh: '提示词工程进阶', ja: 'プロンプトエンジニアリング上級編' },
      subtitle: { zh: '掌握与AI对话的艺术', ja: 'AIとの対話術をマスターする' },
      sections: [
        {
          id: 'ch2-intro',
          title: { zh: '引言：提示词的力量', ja: '序章：プロンプトの力' },
          content: {
            zh: `
同样的 AI，不同的提示词，结果可能天差地别。

**提示词工程（Prompt Engineering）** 是一门让 AI 更好地理解你、更精准地完成任务的技术。掌握它，你就能让 AI 发挥出 10 倍的能力。

---

## 为什么提示词如此重要？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        提示词的影响力                                    │
└─────────────────────────────────────────────────────────────────────────┘

  普通提示词                              优秀提示词
  ┌─────────────┐                        ┌─────────────┐
  │  写一篇文章  │                        │ 角色 + 任务  │
  │             │                        │ + 格式 + 约束│
  └─────────────┘                        └─────────────┘
        │                                      │
        ▼                                      ▼
  ┌─────────────┐                        ┌─────────────┐
  │  泛泛而谈    │                        │ 专业、精准   │
  │  结构混乱    │                        │ 结构清晰     │
  │  需要多次修改│                        │ 一次到位     │
  └─────────────┘                        └─────────────┘

  效率: ★★☆☆☆                           效率: ★★★★★
\`\`\`

---

## 本章你将学到

1. **提示词的核心结构** —— 如何组织一个好的提示词
2. **常用提示词模式** —— 可以直接套用的模板
3. **角色扮演技巧** —— 让 AI 变成各种专家
4. **思维链与推理** —— 让 AI 一步步思考
5. **高级技巧与实战** —— 真实场景的应用

让我们开始掌握这门"与 AI 对话的艺术"！
            `,
            ja: `
同じAIでも、プロンプトが違えば結果は天と地ほど違います。

**プロンプトエンジニアリング（Prompt Engineering）** は、AIにあなたをより良く理解させ、より正確にタスクを完了させる技術です。これをマスターすれば、AIの能力を10倍引き出せます。

---

## なぜプロンプトがこれほど重要なのか？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        プロンプトの影響力                                │
└─────────────────────────────────────────────────────────────────────────┘

  普通のプロンプト                        優れたプロンプト
  ┌─────────────┐                        ┌─────────────┐
  │  記事を書いて │                        │ 役割 + タスク │
  │             │                        │ + 形式 + 制約 │
  └─────────────┘                        └─────────────┘
        │                                      │
        ▼                                      ▼
  ┌─────────────┐                        ┌─────────────┐
  │  漠然とした内容│                        │ 専門的で正確 │
  │  構造が乱雑   │                        │ 構造が明確   │
  │  何度も修正必要│                        │ 一発OK      │
  └─────────────┘                        └─────────────┘

  効率: ★★☆☆☆                           効率: ★★★★★
\`\`\`

---

## この章で学ぶこと

1. **プロンプトのコア構造** —— 良いプロンプトの組み立て方
2. **よく使うプロンプトパターン** —— そのまま使えるテンプレート
3. **ロールプレイテクニック** —— AIを様々な専門家に変える
4. **思考の連鎖と推論** —— AIに段階的に考えさせる
5. **上級テクニックと実践** —— 実際のシーンでの応用

「AIとの対話術」をマスターしましょう！
            `
          }
        },
        {
          id: 'ch2-structure',
          title: { zh: '2.1 提示词的核心结构', ja: '2.1 プロンプトのコア構造' },
          content: {
            zh: `
一个好的提示词通常包含这些要素：

---

## CRISPE 框架

这是业界广泛使用的提示词结构：

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                         CRISPE 提示词框架                                │
└─────────────────────────────────────────────────────────────────────────┘

  C - Capacity (角色)      "你是一位资深的..."
  ────────────────────────────────────────────────────────

  R - Role (任务)          "你的任务是..."
  ────────────────────────────────────────────────────────

  I - Insight (背景)       "背景信息是..."
  ────────────────────────────────────────────────────────

  S - Statement (要求)     "请按照以下要求..."
  ────────────────────────────────────────────────────────

  P - Personality (风格)   "语气要专业/亲切/幽默..."
  ────────────────────────────────────────────────────────

  E - Experiment (示例)    "输出格式示例：..."
\`\`\`

---

## 实际例子对比

### ❌ 不完整的提示词

\`\`\`
帮我写一封邮件
\`\`\`

AI 不知道：写给谁？什么目的？什么语气？

### ✅ 使用 CRISPE 的提示词

\`\`\`
【角色】你是一位专业的商务沟通专家

【任务】帮我写一封跟进邮件

【背景】
- 上周与客户开会讨论了新项目合作
- 客户对价格有顾虑，需要进一步说服
- 客户是科技公司的采购总监

【要求】
- 字数控制在200字以内
- 重申我们的核心优势
- 提供一个限时优惠方案
- 预约下次沟通时间

【风格】专业但亲切，不要过于推销

【格式】
主题：xxx
正文：xxx
\`\`\`

---

## 简化版：三要素法则

如果觉得 CRISPE 太复杂，至少要包含这三个要素：

\`\`\`
┌───────────────────────────────────────────────────────┐
│                三要素法则                              │
├───────────────────────────────────────────────────────┤
│                                                       │
│   1. 角色 (Role)                                      │
│      └── "你是一个..."                                │
│                                                       │
│   2. 任务 (Task)                                      │
│      └── "请帮我..."                                  │
│                                                       │
│   3. 格式 (Format)                                    │
│      └── "输出格式为..."                              │
│                                                       │
└───────────────────────────────────────────────────────┘
\`\`\`

**例子**：
\`\`\`
你是一位Python专家。
请帮我优化下面这段代码，提高运行效率。
输出格式：先给出优化后的代码，再解释改动原因。
\`\`\`

---

## 本节要点

1. **CRISPE 框架** —— 完整的提示词结构
2. **三要素法则** —— 简化版：角色、任务、格式
3. **背景信息很重要** —— 提供足够的上下文
4. **明确输出格式** —— 减少来回修改
            `,
            ja: `
良いプロンプトには通常、以下の要素が含まれます：

---

## CRISPE フレームワーク

業界で広く使われているプロンプト構造：

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                         CRISPE プロンプトフレームワーク                   │
└─────────────────────────────────────────────────────────────────────────┘

  C - Capacity (役割)      "あなたはベテランの..."
  ────────────────────────────────────────────────────────

  R - Role (タスク)        "あなたのタスクは..."
  ────────────────────────────────────────────────────────

  I - Insight (背景)       "背景情報は..."
  ────────────────────────────────────────────────────────

  S - Statement (要件)     "以下の要件に従って..."
  ────────────────────────────────────────────────────────

  P - Personality (スタイル) "トーンは専門的/親しみやすい/ユーモラス..."
  ────────────────────────────────────────────────────────

  E - Experiment (例)      "出力形式の例：..."
\`\`\`

---

## 実際の例の比較

### ❌ 不完全なプロンプト

\`\`\`
メールを書いて
\`\`\`

AIには分からない：誰に？何の目的？どんなトーン？

### ✅ CRISPE を使ったプロンプト

\`\`\`
【役割】あなたはビジネスコミュニケーションの専門家です

【タスク】フォローアップメールを書いてください

【背景】
- 先週、新プロジェクトの協力について顧客と会議
- 顧客は価格に懸念があり、さらに説得が必要
- 顧客はテック企業の調達部長

【要件】
- 200文字以内
- 当社のコア優位性を再確認
- 期間限定オファーを提供
- 次回のコミュニケーション時間を予約

【スタイル】専門的だが親しみやすく、押し売り感なし

【形式】
件名：xxx
本文：xxx
\`\`\`

---

## 簡略版：3要素ルール

CRISPE が複雑すぎると感じたら、少なくとも3要素を含めましょう：

\`\`\`
┌───────────────────────────────────────────────────────┐
│                3要素ルール                             │
├───────────────────────────────────────────────────────┤
│                                                       │
│   1. 役割 (Role)                                      │
│      └── "あなたは..."                                │
│                                                       │
│   2. タスク (Task)                                    │
│      └── "〜してください..."                          │
│                                                       │
│   3. 形式 (Format)                                    │
│      └── "出力形式は..."                              │
│                                                       │
└───────────────────────────────────────────────────────┘
\`\`\`

**例**：
\`\`\`
あなたはPythonの専門家です。
以下のコードを最適化し、実行効率を向上させてください。
出力形式：最適化後のコードを先に、次に変更理由を説明。
\`\`\`

---

## このセクションのポイント

1. **CRISPE フレームワーク** —— 完全なプロンプト構造
2. **3要素ルール** —— 簡略版：役割、タスク、形式
3. **背景情報が重要** —— 十分なコンテキストを提供
4. **出力形式を明確に** —— やり取りを減らす
            `
          }
        },
        {
          id: 'ch2-patterns',
          title: { zh: '2.2 常用提示词模式', ja: '2.2 よく使うプロンプトパターン' },
          content: {
            zh: `
掌握这些常用模式，可以直接套用到各种场景。

---

## 模式一：Few-Shot（少样本示例）

给 AI 几个例子，让它学会规律后处理新问题。

\`\`\`
请将以下中文翻译成日语，保持口语化风格：

例子1:
中文：今天天气真好
日语：今日はいい天気だね

例子2:
中文：我肚子饿了
日语：お腹空いたな

现在请翻译：
中文：这个电影太好看了
日语：
\`\`\`

**适用场景**：翻译、格式转换、风格模仿

---

## 模式二：Chain of Thought（思维链）

让 AI 一步步思考，而不是直接给答案。

\`\`\`
请一步步分析这个问题：

问题：一个商店进货价是80元，售价是100元，
打8折后还能赚多少钱？

请按以下步骤分析：
1. 首先，计算打折后的售价
2. 然后，计算每件商品的利润
3. 最后，给出结论
\`\`\`

**适用场景**：数学计算、逻辑分析、复杂决策

---

## 模式三：角色扮演

让 AI 扮演特定角色，获得更专业的回答。

\`\`\`
你现在是一位有20年经验的营养师。

一位30岁的程序员来咨询你：
- 他经常加班熬夜
- 饮食不规律，常吃外卖
- 最近感觉很疲惫

请从专业角度给出建议。
\`\`\`

**适用场景**：专业咨询、创意写作、模拟面试

---

## 模式四：约束条件

明确告诉 AI 什么不要做。

\`\`\`
写一段产品介绍，要求：

✅ 要做的：
- 突出三个核心功能
- 使用简洁的语言
- 包含一个用户案例

❌ 不要：
- 不要使用专业术语
- 不要超过200字
- 不要使用夸张的形容词
\`\`\`

**适用场景**：内容创作、代码生成、格式限制

---

## 模式五：迭代优化

让 AI 自己改进结果。

\`\`\`
第一步：请写一段产品介绍

第二步：以用户体验专家的视角，
找出上面文案的三个问题

第三步：根据问题重新改写文案
\`\`\`

**适用场景**：写作润色、方案优化、自我检查

---

## 模式速查表

| 模式 | 关键词 | 适用场景 |
|------|--------|----------|
| Few-Shot | "例如..."、"参考这个格式..." | 格式统一、风格模仿 |
| 思维链 | "一步步"、"首先...然后..." | 复杂推理、计算 |
| 角色扮演 | "你是一位..."、"作为专家..." | 专业建议、创意写作 |
| 约束条件 | "不要..."、"限制在..." | 控制输出、避免问题 |
| 迭代优化 | "先...再...然后改进" | 质量提升、自我完善 |

---

## 本节要点

1. **Few-Shot** —— 用例子教会 AI
2. **思维链** —— 让 AI 分步骤思考
3. **角色扮演** —— 获得专业视角
4. **约束条件** —— 明确什么不要做
5. **迭代优化** —— 让 AI 自我改进
            `,
            ja: `
これらの一般的なパターンをマスターすれば、様々なシーンに直接適用できます。

---

## パターン1：Few-Shot（少数例示）

AIにいくつかの例を与え、パターンを学んでから新しい問題を処理させます。

\`\`\`
以下の中国語を日本語に翻訳してください。口語スタイルを維持：

例1:
中国語：今天天气真好
日本語：今日はいい天気だね

例2:
中国語：我肚子饿了
日本語：お腹空いたな

では翻訳してください：
中国語：这个电影太好看了
日本語：
\`\`\`

**適用シーン**：翻訳、フォーマット変換、スタイル模倣

---

## パターン2：Chain of Thought（思考の連鎖）

AIに直接答えを出させず、段階的に考えさせます。

\`\`\`
この問題を段階的に分析してください：

問題：ある店の仕入れ価格は80元、販売価格は100元、
20%割引後、どれだけ利益が出ますか？

以下のステップで分析してください：
1. まず、割引後の販売価格を計算
2. 次に、各商品の利益を計算
3. 最後に、結論を出す
\`\`\`

**適用シーン**：数学計算、論理分析、複雑な意思決定

---

## パターン3：ロールプレイ

AIに特定の役割を演じさせ、より専門的な回答を得ます。

\`\`\`
あなたは20年の経験を持つ栄養士です。

30歳のプログラマーが相談に来ました：
- よく残業で夜更かし
- 食事が不規則で、よく外食
- 最近とても疲れを感じている

専門家の視点からアドバイスをください。
\`\`\`

**適用シーン**：専門相談、クリエイティブライティング、模擬面接

---

## パターン4：制約条件

AIに何をしないかを明確に伝えます。

\`\`\`
製品紹介文を書いてください。要件：

✅ すること：
- 3つのコア機能を強調
- 簡潔な言葉を使用
- ユーザー事例を1つ含める

❌ しないこと：
- 専門用語を使わない
- 200文字を超えない
- 誇張した形容詞を使わない
\`\`\`

**適用シーン**：コンテンツ作成、コード生成、フォーマット制限

---

## パターン5：反復改善

AIに結果を自己改善させます。

\`\`\`
ステップ1：製品紹介文を書いてください

ステップ2：UX専門家の視点から、
上記のコピーの3つの問題を指摘

ステップ3：問題に基づいてコピーを書き直す
\`\`\`

**適用シーン**：ライティング改善、プラン最適化、セルフチェック

---

## パターン早見表

| パターン | キーワード | 適用シーン |
|----------|------------|------------|
| Few-Shot | "例えば..."、"このフォーマットを参考に..." | フォーマット統一、スタイル模倣 |
| 思考の連鎖 | "段階的に"、"まず...次に..." | 複雑な推論、計算 |
| ロールプレイ | "あなたは..."、"専門家として..." | 専門アドバイス、創作 |
| 制約条件 | "〜しない"、"〜に制限" | 出力制御、問題回避 |
| 反復改善 | "まず...次に...改善" | 品質向上、自己完善 |

---

## このセクションのポイント

1. **Few-Shot** —— 例でAIに教える
2. **思考の連鎖** —— AIに段階的に考えさせる
3. **ロールプレイ** —— 専門家の視点を得る
4. **制約条件** —— 何をしないかを明確に
5. **反復改善** —— AIに自己改善させる
            `
          }
        },
        {
          id: 'ch2-advanced',
          title: { zh: '2.3 实战案例与高级技巧', ja: '2.3 実践例と上級テクニック' },
          content: {
            zh: `
## 真实场景：提示词实战

让我们通过几个真实场景，看看如何应用提示词技巧解决实际问题。

---

## 案例1：代码审查助手

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                          代码审查提示词                                   │
└─────────────────────────────────────────────────────────────────────────┘

你是一位资深的代码审查专家，有10年的软件开发经验。

## 任务
请审查以下代码，重点关注：
1. 代码质量和可读性
2. 潜在的 bug 和安全问题
3. 性能优化建议
4. 最佳实践遵循情况

## 输出格式
请按以下结构输出：

### 🔴 必须修复（阻塞性问题）
- [问题描述] + [具体位置] + [修复建议]

### 🟡 建议改进（非阻塞）
- [改进点] + [原因] + [改进方案]

### 🟢 做得好的地方
- [亮点]

### 代码
[粘贴代码]
\`\`\`

**这个提示词的技巧：**
- ✅ 设定专家角色
- ✅ 明确任务目标
- ✅ 结构化输出格式
- ✅ 使用emoji增强可读性

---

## 案例2：文档生成助手

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                         技术文档生成提示词                                │
└─────────────────────────────────────────────────────────────────────────┘

你是一位技术文档专家。请根据以下函数生成完整的 API 文档。

## 要求
1. 简洁的功能描述（1-2句）
2. 参数说明表格（参数名、类型、必填、说明）
3. 返回值说明
4. 使用示例（包含正常用法和边界情况）
5. 注意事项

## 输出格式：Markdown

## 函数代码
[粘贴函数]
\`\`\`

---

## 高级技巧：Self-Consistency（自洽性）

当需要更可靠的答案时，可以让 AI 多次尝试，然后综合结果。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        Self-Consistency 方法                            │
└─────────────────────────────────────────────────────────────────────────┘

                    ┌───────────────┐
                    │   同一问题     │
                    └───────┬───────┘
                            │
        ┌───────────────────┼───────────────────┐
        │                   │                   │
        ▼                   ▼                   ▼
  ┌───────────┐       ┌───────────┐       ┌───────────┐
  │  思路 A    │       │  思路 B    │       │  思路 C    │
  └─────┬─────┘       └─────┬─────┘       └─────┬─────┘
        │                   │                   │
        ▼                   ▼                   ▼
  ┌───────────┐       ┌───────────┐       ┌───────────┐
  │  答案 A    │       │  答案 B    │       │  答案 A    │
  └───────────┘       └───────────┘       └───────────┘
                            │
                            ▼
                    ┌───────────────┐
                    │  投票：答案 A  │
                    │  （出现2次）   │
                    └───────────────┘
\`\`\`

**实际应用：**
\`\`\`
请用3种不同的思路解决这个问题，然后比较各个答案，
给出你最有信心的最终答案。

问题：[你的问题]
\`\`\`

---

## 高级技巧：Tree of Thought（思维树）

对于复杂问题，让 AI 探索多个分支，评估每个分支，选择最优路径。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        Tree of Thought 示例                             │
└─────────────────────────────────────────────────────────────────────────┘

                         问题
                           │
         ┌─────────────────┼─────────────────┐
         │                 │                 │
         ▼                 ▼                 ▼
     方案 A            方案 B            方案 C
    ┌────────┐        ┌────────┐        ┌────────┐
    │可行性:高│        │可行性:中│        │可行性:低│
    │成本:低  │        │成本:高  │        │成本:中  │
    │风险:低  │        │风险:中  │        │风险:高  │
    └────┬───┘        └────────┘        └────────┘
         │
         │    ← 选择最优分支继续深入
         ▼
    ┌─────────┐
    │ 详细规划 │
    └─────────┘
\`\`\`

**提示词模板：**
\`\`\`
请使用思维树方法解决这个问题：

1. 首先，生成3个可能的解决方案
2. 对每个方案评估：可行性、成本、风险
3. 选择最优方案并详细展开
4. 给出最终的完整解决方案

问题：[你的问题]
\`\`\`

---

## 实用提示词模板库

### 学习类
\`\`\`
我想学习 [主题]，我的背景是 [你的基础]。
请设计一个 [时间] 的学习计划，包含：
1. 学习路线图
2. 推荐资源
3. 练习项目
4. 检验标准
\`\`\`

### 写作类
\`\`\`
请帮我写一篇关于 [主题] 的 [文章类型]。

目标读者：[受众]
篇幅：[字数]
风格：[专业/轻松/幽默]
重点：[核心观点]

请先给出大纲，确认后再写正文。
\`\`\`

### 分析类
\`\`\`
请分析 [主题/数据/现象]：

1. 现状概述
2. 关键因素分析
3. 主要发现（用数据支撑）
4. 可行建议
5. 潜在风险

输出格式：结构化报告，使用图表说明关键数据
\`\`\`

---

## 本节要点

1. **代码审查** —— 角色 + 任务 + 结构化输出
2. **Self-Consistency** —— 多次尝试取共识
3. **Tree of Thought** —— 探索多分支选最优
4. **模板复用** —— 积累自己的提示词库
            `,
            ja: `
## 実際のシナリオ：プロンプト実践

いくつかの実際のシナリオを通じて、プロンプトテクニックで問題を解決する方法を見てみましょう。

---

## ケース1：コードレビューアシスタント

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        コードレビュープロンプト                           │
└─────────────────────────────────────────────────────────────────────────┘

あなたは10年の経験を持つシニアコードレビュアーです。

## タスク
以下のコードをレビューしてください：
1. コード品質と可読性
2. 潜在的なバグとセキュリティ問題
3. パフォーマンス改善の提案
4. ベストプラクティスの遵守状況

## 出力形式
以下の構造で出力してください：

### 🔴 必須修正（ブロッキング問題）
- [問題] + [場所] + [修正案]

### 🟡 改善提案（非ブロッキング）
- [改善点] + [理由] + [提案]

### 🟢 良い点
- [ハイライト]

### コード
[コードを貼り付け]
\`\`\`

**このプロンプトのテクニック：**
- ✅ 専門家の役割を設定
- ✅ タスク目標を明確化
- ✅ 構造化された出力形式
- ✅ 絵文字で可読性向上

---

## ケース2：ドキュメント生成アシスタント

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        技術ドキュメント生成プロンプト                      │
└─────────────────────────────────────────────────────────────────────────┘

あなたは技術ドキュメントの専門家です。
以下の関数の完全なAPIドキュメントを生成してください。

## 要件
1. 簡潔な機能説明（1-2文）
2. パラメータ説明表（名前、型、必須、説明）
3. 戻り値の説明
4. 使用例（通常の使い方とエッジケース）
5. 注意事項

## 出力形式：Markdown

## 関数コード
[関数を貼り付け]
\`\`\`

---

## 上級テクニック：Self-Consistency（自己整合性）

より信頼性の高い回答が必要な場合、AIに複数回試行させて結果を統合します。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        Self-Consistency 方法                            │
└─────────────────────────────────────────────────────────────────────────┘

                    ┌───────────────┐
                    │   同じ問題     │
                    └───────┬───────┘
                            │
        ┌───────────────────┼───────────────────┐
        │                   │                   │
        ▼                   ▼                   ▼
  ┌───────────┐       ┌───────────┐       ┌───────────┐
  │  アプローチA │       │  アプローチB │       │  アプローチC │
  └─────┬─────┘       └─────┬─────┘       └─────┬─────┘
        │                   │                   │
        ▼                   ▼                   ▼
  ┌───────────┐       ┌───────────┐       ┌───────────┐
  │  回答 A    │       │  回答 B    │       │  回答 A    │
  └───────────┘       └───────────┘       └───────────┘
                            │
                            ▼
                    ┌───────────────┐
                    │  投票：回答 A  │
                    │ （2回出現）    │
                    └───────────────┘
\`\`\`

**実践的な使い方：**
\`\`\`
3つの異なるアプローチでこの問題を解決し、
各回答を比較して、最も確信のある最終回答を出してください。

問題：[あなたの質問]
\`\`\`

---

## 上級テクニック：Tree of Thought（思考の木）

複雑な問題に対して、AIに複数の分岐を探索させ、評価して最適なパスを選択します。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        Tree of Thought 例                               │
└─────────────────────────────────────────────────────────────────────────┘

                         問題
                           │
         ┌─────────────────┼─────────────────┐
         │                 │                 │
         ▼                 ▼                 ▼
     方案 A            方案 B            方案 C
    ┌────────┐        ┌────────┐        ┌────────┐
    │実現性:高│        │実現性:中│        │実現性:低│
    │コスト:低│        │コスト:高│        │コスト:中│
    │リスク:低│        │リスク:中│        │リスク:高│
    └────┬───┘        └────────┘        └────────┘
         │
         │    ← 最適な分岐を選んで深掘り
         ▼
    ┌─────────┐
    │ 詳細計画 │
    └─────────┘
\`\`\`

**プロンプトテンプレート：**
\`\`\`
思考の木メソッドでこの問題を解決してください：

1. まず、3つの可能な解決策を生成
2. 各方案を評価：実現性、コスト、リスク
3. 最適な方案を選択し詳細に展開
4. 最終的な完全な解決策を提示

問題：[あなたの質問]
\`\`\`

---

## 実用プロンプトテンプレート集

### 学習系
\`\`\`
[トピック]を学びたいです。私の背景は[あなたの基礎]です。
[期間]の学習計画を設計してください：
1. 学習ロードマップ
2. おすすめリソース
3. 練習プロジェクト
4. 習得度チェック基準
\`\`\`

### ライティング系
\`\`\`
[トピック]についての[記事タイプ]を書いてください。

ターゲット読者：[対象]
文字数：[長さ]
スタイル：[専門的/カジュアル/ユーモア]
重点：[コアメッセージ]

まずアウトラインを提示し、確認後に本文を書いてください。
\`\`\`

### 分析系
\`\`\`
[トピック/データ/現象]を分析してください：

1. 現状の概要
2. 主要因分析
3. 主な発見（データで裏付け）
4. 実行可能な提案
5. 潜在的リスク

出力形式：構造化レポート、図表で主要データを説明
\`\`\`

---

## このセクションのポイント

1. **コードレビュー** —— 役割 + タスク + 構造化出力
2. **Self-Consistency** —— 複数回試行で合意を得る
3. **Tree of Thought** —— 複数分岐を探索し最適を選ぶ
4. **テンプレート再利用** —— 自分のプロンプトライブラリを蓄積
            `
          }
        },
        {
          id: 'ch2-summary',
          title: { zh: '2.4 本章小结', ja: '2.4 この章のまとめ' },
          content: {
            zh: `
## 提示词工程核心要点

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                       提示词工程知识地图                                  │
└─────────────────────────────────────────────────────────────────────────┘

                              提示词工程
                                  │
            ┌─────────────────────┼─────────────────────┐
            │                     │                     │
            ▼                     ▼                     ▼
       ┌─────────┐           ┌─────────┐          ┌─────────┐
       │ 结构框架 │           │ 常用模式 │          │ 实践技巧 │
       └─────────┘           └─────────┘          └─────────┘
            │                     │                     │
     ┌──────┴──────┐       ┌──────┴──────┐       ┌──────┴──────┐
     │             │       │             │       │             │
     ▼             ▼       ▼             ▼       ▼             ▼
  CRISPE        三要素   Few-Shot    思维链    角色扮演    迭代优化
  框架          法则     少样本      推理      专家视角    持续改进
\`\`\`

---

## 快速行动清单

- [ ] 下次使用 AI 时，尝试使用"三要素法则"
- [ ] 遇到复杂问题，让 AI "一步步分析"
- [ ] 需要专业建议时，让 AI 扮演相关专家
- [ ] 保存你觉得好用的提示词模板

---

## 关键金句

> "好的提示词不是在命令 AI，而是在与 AI 协作。"

> "给 AI 足够的信息，它才能给你满意的答案。"

> "提示词工程的本质，是把模糊的需求变成清晰的指令。"

---

下一章，我们将学习更激动人心的内容：**AI Agents（智能体）** —— 让 AI 不只是回答问题，而是能够自主完成复杂任务！
            `,
            ja: `
## プロンプトエンジニアリングのコアポイント

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    プロンプトエンジニアリング知識マップ                    │
└─────────────────────────────────────────────────────────────────────────┘

                          プロンプトエンジニアリング
                                  │
            ┌─────────────────────┼─────────────────────┐
            │                     │                     │
            ▼                     ▼                     ▼
       ┌─────────┐           ┌─────────┐          ┌─────────┐
       │構造フレーム│           │一般パターン│          │実践テクニック│
       └─────────┘           └─────────┘          └─────────┘
            │                     │                     │
     ┌──────┴──────┐       ┌──────┴──────┐       ┌──────┴──────┐
     │             │       │             │       │             │
     ▼             ▼       ▼             ▼       ▼             ▼
  CRISPE        3要素   Few-Shot    思考の連鎖  ロールプレイ  反復改善
  フレーム      ルール   少数例示    推論      専門家視点    継続改善
\`\`\`

---

## クイックアクションリスト

- [ ] 次にAIを使うとき、「3要素ルール」を試す
- [ ] 複雑な問題に遭遇したら、AIに「段階的に分析」させる
- [ ] 専門アドバイスが必要なとき、AIに関連専門家を演じさせる
- [ ] 使えると思ったプロンプトテンプレートを保存する

---

## 重要な格言

> 「良いプロンプトはAIに命令するのではなく、AIと協力することです。」

> 「AIに十分な情報を与えれば、満足のいく答えが得られます。」

> 「プロンプトエンジニアリングの本質は、曖昧なニーズを明確な指示に変えることです。」

---

次の章では、さらにエキサイティングな内容を学びます：**AI Agents（インテリジェントエージェント）** —— AIが質問に答えるだけでなく、複雑なタスクを自律的に完了できるようになります！
            `
          }
        }
      ]
    },
    // ============================================
    // 第二章：AI Agents 智能体
    // ============================================
    {
      id: 'chapter-3',
      number: 3,
      title: { zh: 'AI Agents 智能体', ja: 'AI Agents インテリジェントエージェント' },
      subtitle: { zh: '让AI自主完成复杂任务', ja: 'AIに複雑なタスクを自律的に完了させる' },
      sections: [
        {
          id: 'ch3-intro',
          title: { zh: '引言：从对话到行动', ja: '序章：対話から行動へ' },
          content: {
            zh: `
到目前为止，我们学习的都是**与 AI 对话**。你问，AI 答。

但是，如果 AI 能**自己动手**做事呢？

这就是 **AI Agent（智能体）** 的概念：让 AI 不只是回答问题，而是能够**自主规划、调用工具、完成任务**。

---

## 普通 AI vs AI Agent

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                     普通 AI  vs  AI Agent                               │
└─────────────────────────────────────────────────────────────────────────┘

    普通 AI（ChatGPT 等）                    AI Agent
    ┌──────────────────────┐               ┌──────────────────────┐
    │                      │               │                      │
    │   用户提问            │               │   用户设定目标        │
    │      ↓               │               │      ↓               │
    │   AI 回答            │               │   Agent 分析任务     │
    │      ↓               │               │      ↓               │
    │   结束               │               │   制定计划           │
    │                      │               │      ↓               │
    │                      │               │   调用工具执行        │
    │                      │               │      ↓               │
    │                      │               │   检查结果           │
    │                      │               │      ↓               │
    │                      │               │   继续或完成         │
    │                      │               │                      │
    └──────────────────────┘               └──────────────────────┘

    特点：一问一答                          特点：自主规划执行
    能力：回答问题                          能力：完成任务
\`\`\`

---

## 生活中的 Agent 类比

想象你有一个**超级助理**：

**普通 AI** 像一个知识渊博的顾问：
- 你问："今天北京天气怎么样？"
- 它答："今天北京晴，25度"

**AI Agent** 像一个能干的私人助理：
- 你说："帮我安排明天去北京的出差"
- 它会：
  1. 查询明天的天气
  2. 搜索航班和价格
  3. 推荐合适的酒店
  4. 把行程整理成表格发给你
  5. 甚至帮你预订（如果有权限）

---

## 本章你将学到

1. **什么是 AI Agent** —— 核心概念和工作原理
2. **Agent 的关键能力** —— 规划、工具使用、记忆
3. **常见 Agent 类型** —— 不同场景的 Agent
4. **实用 Agent 工具** —— 现在就能用的 Agent
5. **Agent 的未来** —— 发展趋势和机遇

让我们进入 AI Agent 的世界！
            `,
            ja: `
これまで私たちが学んできたのは、**AIとの対話**でした。あなたが質問し、AIが答える。

しかし、もしAIが**自分で行動**できたら？

これが**AI Agent（インテリジェントエージェント）**の概念です：AIが質問に答えるだけでなく、**自律的に計画を立て、ツールを使い、タスクを完了**できるようにすることです。

---

## 通常のAI vs AI Agent

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                     通常のAI  vs  AI Agent                              │
└─────────────────────────────────────────────────────────────────────────┘

    通常のAI（ChatGPT等）                   AI Agent
    ┌──────────────────────┐               ┌──────────────────────┐
    │                      │               │                      │
    │   ユーザーが質問     │               │   ユーザーが目標設定  │
    │      ↓               │               │      ↓               │
    │   AIが回答           │               │   Agentがタスク分析   │
    │      ↓               │               │      ↓               │
    │   終了               │               │   計画を立てる        │
    │                      │               │      ↓               │
    │                      │               │   ツールを呼び出して実行│
    │                      │               │      ↓               │
    │                      │               │   結果をチェック      │
    │                      │               │      ↓               │
    │                      │               │   継続または完了      │
    │                      │               │                      │
    └──────────────────────┘               └──────────────────────┘

    特徴：一問一答                          特徴：自律的に計画・実行
    能力：質問に答える                      能力：タスクを完了する
\`\`\`

---

## 日常生活でのAgentの例え

**スーパーアシスタント**がいると想像してください：

**通常のAI** は知識豊富なアドバイザーのよう：
- 「今日の東京の天気は？」と聞くと
- 「今日の東京は晴れ、25度です」と答える

**AI Agent** は有能なパーソナルアシスタントのよう：
- 「明日の北京出張を手配して」と言うと
- 以下を行います：
  1. 明日の天気を確認
  2. フライトと価格を検索
  3. 適切なホテルを推薦
  4. 旅程を表にまとめて送信
  5. 権限があれば予約まで行う

---

## この章で学ぶこと

1. **AI Agentとは** —— コアコンセプトと仕組み
2. **Agentの重要な能力** —— 計画、ツール使用、記憶
3. **一般的なAgentタイプ** —— 様々なシーンのAgent
4. **実用的なAgentツール** —— 今すぐ使えるAgent
5. **Agentの未来** —— 発展トレンドと機会

AI Agentの世界に入りましょう！
            `
          }
        },
        {
          id: 'ch3-how-it-works',
          title: { zh: '3.1 Agent 的工作原理', ja: '3.1 Agentの仕組み' },
          content: {
            zh: `
AI Agent 是如何工作的？让我们拆解它的核心组件。

---

## Agent 的四大核心能力

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        AI Agent 核心架构                                │
└─────────────────────────────────────────────────────────────────────────┘

                         ┌─────────────┐
                         │   大脑      │
                         │  (LLM)     │
                         └──────┬──────┘
                                │
         ┌──────────────────────┼──────────────────────┐
         │                      │                      │
         ▼                      ▼                      ▼
  ┌──────────────┐      ┌──────────────┐      ┌──────────────┐
  │    规划      │      │    工具      │      │    记忆      │
  │  Planning   │      │   Tools     │      │   Memory    │
  └──────────────┘      └──────────────┘      └──────────────┘
         │                      │                      │
         ▼                      ▼                      ▼
   分解任务              执行动作              存储信息
   制定步骤              调用API              上下文记忆
   调整计划              读写文件              长期记忆
\`\`\`

---

## 1. 规划能力（Planning）

Agent 能够把复杂任务分解成可执行的步骤：

**例子：**"帮我写一篇关于AI的博客文章"

Agent 的规划：
\`\`\`
1. 确定文章主题和目标读者
2. 搜索相关资料和最新动态
3. 制定文章大纲
4. 撰写各个章节
5. 优化和润色
6. 生成配图建议
7. 输出最终文章
\`\`\`

---

## 2. 工具使用（Tools）

Agent 可以调用各种外部工具：

| 工具类型 | 例子 | 用途 |
|----------|------|------|
| 搜索 | Google、Bing | 获取最新信息 |
| 代码 | Python 解释器 | 计算、数据处理 |
| 文件 | 读写本地文件 | 存储和访问数据 |
| API | 天气、地图、支付 | 连接外部服务 |
| 浏览器 | 网页自动化 | 操作网页 |

---

## 3. 记忆能力（Memory）

Agent 需要记住信息来完成复杂任务：

\`\`\`
┌─────────────────────────────────────────────────────┐
│                  Agent 记忆系统                     │
├─────────────────────────────────────────────────────┤
│                                                     │
│  短期记忆（对话上下文）                              │
│  └── 当前对话的历史                                 │
│  └── 刚才执行的步骤                                 │
│                                                     │
│  长期记忆（持久存储）                                │
│  └── 用户偏好                                       │
│  └── 历史任务记录                                   │
│  └── 学到的知识                                     │
│                                                     │
└─────────────────────────────────────────────────────┘
\`\`\`

---

## 4. 反思与调整

Agent 会根据执行结果调整策略：

\`\`\`
执行步骤 ──▶ 检查结果 ──▶ 结果OK？
                              │
                    ┌─────────┴─────────┐
                    │                   │
                   是                   否
                    │                   │
                    ▼                   ▼
               继续下一步          分析问题
                                       │
                                       ▼
                                  调整策略
                                       │
                                       ▼
                                  重新执行
\`\`\`

---

## ReAct 模式

大多数 Agent 采用 **ReAct（Reasoning + Acting）** 模式：

### 交互式演示：Agent 工作循环

体验一下 Agent 是如何思考、行动和观察的：

::agent-viz::

\`\`\`
循环过程：
┌──────────────────────────────────────────────────────┐
│                                                      │
│  思考 (Thought) ──▶ 行动 (Action) ──▶ 观察 (Observe) │
│     ▲                                          │     │
│     └──────────────────────────────────────────┘     │
│                                                      │
└──────────────────────────────────────────────────────┘

例子：
Thought: 用户想知道今天的股价，我需要搜索最新数据
Action: 调用搜索工具，搜索"苹果公司今日股价"
Observe: 搜索结果显示苹果股价为 $178.50
Thought: 我已经获得了数据，可以回答用户了
Action: 输出答案给用户
\`\`\`

---

## 本节要点

1. **四大核心** —— 规划、工具、记忆、反思
2. **规划能力** —— 把复杂任务分解成步骤
3. **工具使用** —— 连接外部世界的能力
4. **记忆系统** —— 短期和长期记忆
5. **ReAct 模式** —— 思考-行动-观察的循环
            `,
            ja: `
AI Agentはどのように動作するのでしょうか？コアコンポーネントを分解してみましょう。

---

## Agentの4つのコア能力

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        AI Agent コアアーキテクチャ                       │
└─────────────────────────────────────────────────────────────────────────┘

                         ┌─────────────┐
                         │    脳       │
                         │   (LLM)    │
                         └──────┬──────┘
                                │
         ┌──────────────────────┼──────────────────────┐
         │                      │                      │
         ▼                      ▼                      ▼
  ┌──────────────┐      ┌──────────────┐      ┌──────────────┐
  │    計画      │      │   ツール    │      │    記憶      │
  │  Planning   │      │   Tools     │      │   Memory    │
  └──────────────┘      └──────────────┘      └──────────────┘
         │                      │                      │
         ▼                      ▼                      ▼
   タスク分解             アクション実行          情報保存
   ステップ策定           API呼び出し            コンテキスト記憶
   計画調整              ファイル読み書き         長期記憶
\`\`\`

---

## 1. 計画能力（Planning）

Agentは複雑なタスクを実行可能なステップに分解できます：

**例：**「AIについてのブログ記事を書いて」

Agentの計画：
\`\`\`
1. 記事のテーマとターゲット読者を決定
2. 関連資料と最新動向を検索
3. 記事のアウトラインを作成
4. 各セクションを執筆
5. 最適化と推敲
6. 配置する画像を提案
7. 最終記事を出力
\`\`\`

---

## 2. ツール使用（Tools）

Agentは様々な外部ツールを呼び出せます：

| ツールタイプ | 例 | 用途 |
|-------------|-----|------|
| 検索 | Google、Bing | 最新情報の取得 |
| コード | Pythonインタープリター | 計算、データ処理 |
| ファイル | ローカルファイルの読み書き | データの保存とアクセス |
| API | 天気、地図、決済 | 外部サービスへの接続 |
| ブラウザ | Webページ自動化 | Webページの操作 |

---

## 3. 記憶能力（Memory）

Agentは複雑なタスクを完了するために情報を記憶する必要があります：

\`\`\`
┌─────────────────────────────────────────────────────┐
│                  Agent 記憶システム                  │
├─────────────────────────────────────────────────────┤
│                                                     │
│  短期記憶（対話コンテキスト）                        │
│  └── 現在の対話履歴                                 │
│  └── 直前に実行したステップ                         │
│                                                     │
│  長期記憶（永続ストレージ）                          │
│  └── ユーザーの好み                                 │
│  └── 過去のタスク記録                               │
│  └── 学んだ知識                                     │
│                                                     │
└─────────────────────────────────────────────────────┘
\`\`\`

---

## 4. 振り返りと調整

Agentは実行結果に基づいて戦略を調整します：

\`\`\`
ステップ実行 ──▶ 結果チェック ──▶ 結果OK？
                                   │
                    ┌──────────────┴──────────────┐
                    │                             │
                   はい                           いいえ
                    │                             │
                    ▼                             ▼
               次のステップへ                問題を分析
                                               │
                                               ▼
                                           戦略を調整
                                               │
                                               ▼
                                            再実行
\`\`\`

---

## ReAct パターン

ほとんどのAgentは**ReAct（Reasoning + Acting）**パターンを採用：

### インタラクティブデモ：Agent動作ループ

Agentがどのように思考、行動、観察するかを体験してください：

::agent-viz::

\`\`\`
ループプロセス：
┌──────────────────────────────────────────────────────┐
│                                                      │
│  思考 (Thought) ──▶ 行動 (Action) ──▶ 観察 (Observe) │
│     ▲                                          │     │
│     └──────────────────────────────────────────┘     │
│                                                      │
└──────────────────────────────────────────────────────┘

例：
Thought: ユーザーは今日の株価を知りたい、最新データを検索する必要がある
Action: 検索ツールを呼び出し、「Apple社今日の株価」を検索
Observe: 検索結果はApple株価が$178.50と表示
Thought: データを取得した、ユーザーに回答できる
Action: ユーザーに答えを出力
\`\`\`

---

## このセクションのポイント

1. **4つのコア** —— 計画、ツール、記憶、振り返り
2. **計画能力** —— 複雑なタスクをステップに分解
3. **ツール使用** —— 外部世界への接続能力
4. **記憶システム** —— 短期と長期記憶
5. **ReAct パターン** —— 思考-行動-観察のループ
            `
          }
        },
        {
          id: 'ch3-tools',
          title: { zh: '3.2 实用 Agent 工具', ja: '3.2 実用的なAgentツール' },
          content: {
            zh: `
现在已经有很多可以直接使用的 AI Agent 工具，让我们来看看最实用的几个。

---

## 代码开发类 Agent

### Claude Code / Cursor / GitHub Copilot

这些工具可以：
- 自动编写代码
- 理解整个项目结构
- 自动修复 bug
- 执行命令和测试

**使用场景**：
- "帮我实现一个用户登录功能"
- "修复这个报错"
- "给这个函数写单元测试"

---

## 通用任务 Agent

### ChatGPT Plugins / GPTs

OpenAI 的插件系统让 ChatGPT 可以：
- 搜索网页
- 分析数据
- 生成图片
- 连接第三方服务

### Claude with Tools

Claude 也可以：
- 执行代码
- 搜索信息
- 分析文件

---

## 自动化工作流

### Zapier / Make

这些工具可以把 AI 集成到工作流中：

\`\`\`
触发器 ──▶ AI 处理 ──▶ 执行动作

例子：
收到邮件 ──▶ AI分析内容 ──▶ 自动分类并回复
\`\`\`

---

## 浏览器自动化

### 浏览器 Agent

可以自动操作网页的 AI：

- 自动填写表单
- 抓取信息
- 执行重复性任务

**注意**：使用时要遵守网站规则和法律法规

---

## Agent 能力对比

| 工具 | 代码 | 搜索 | 文件 | 浏览器 | 价格 |
|------|------|------|------|--------|------|
| Claude Code | ✅ | ✅ | ✅ | ✅ | 付费 |
| ChatGPT Plus | ✅ | ✅ | ✅ | ❌ | $20/月 |
| Cursor | ✅ | ✅ | ✅ | ❌ | 免费/付费 |
| Copilot | ✅ | ❌ | ✅ | ❌ | $10/月 |

---

## 如何选择 Agent 工具

\`\`\`
你的需求是什么？
      │
      ├── 写代码 ──▶ Claude Code / Cursor
      │
      ├── 日常任务 ──▶ ChatGPT / Claude
      │
      ├── 自动化工作流 ──▶ Zapier + AI
      │
      └── 数据分析 ──▶ ChatGPT 代码解释器
\`\`\`

---

## 本节要点

1. **代码 Agent** —— 自动写代码、修复 bug
2. **通用 Agent** —— ChatGPT、Claude 的工具功能
3. **自动化工具** —— 把 AI 集成到工作流
4. **根据需求选择** —— 不同工具有不同优势
            `,
            ja: `
今すぐ使えるAI Agentツールがたくさんあります。最も実用的なものをいくつか見てみましょう。

---

## コード開発系Agent

### Claude Code / Cursor / GitHub Copilot

これらのツールができること：
- 自動でコードを書く
- プロジェクト構造全体を理解
- 自動でバグを修正
- コマンドとテストを実行

**使用シーン**：
- 「ユーザーログイン機能を実装して」
- 「このエラーを修正して」
- 「この関数のユニットテストを書いて」

---

## 汎用タスクAgent

### ChatGPT Plugins / GPTs

OpenAIのプラグインシステムでChatGPTができること：
- Webページを検索
- データを分析
- 画像を生成
- サードパーティサービスに接続

### Claude with Tools

Claudeもできること：
- コードを実行
- 情報を検索
- ファイルを分析

---

## 自動化ワークフロー

### Zapier / Make

これらのツールはAIをワークフローに統合できます：

\`\`\`
トリガー ──▶ AI処理 ──▶ アクション実行

例：
メール受信 ──▶ AIが内容分析 ──▶ 自動分類と返信
\`\`\`

---

## ブラウザ自動化

### ブラウザAgent

Webページを自動操作できるAI：

- フォームを自動入力
- 情報を収集
- 繰り返しタスクを実行

**注意**：使用時はWebサイトのルールと法規制を遵守してください

---

## Agent能力比較

| ツール | コード | 検索 | ファイル | ブラウザ | 価格 |
|--------|--------|------|----------|----------|------|
| Claude Code | ✅ | ✅ | ✅ | ✅ | 有料 |
| ChatGPT Plus | ✅ | ✅ | ✅ | ❌ | $20/月 |
| Cursor | ✅ | ✅ | ✅ | ❌ | 無料/有料 |
| Copilot | ✅ | ❌ | ✅ | ❌ | $10/月 |

---

## Agentツールの選び方

\`\`\`
あなたのニーズは？
      │
      ├── コードを書く ──▶ Claude Code / Cursor
      │
      ├── 日常タスク ──▶ ChatGPT / Claude
      │
      ├── ワークフロー自動化 ──▶ Zapier + AI
      │
      └── データ分析 ──▶ ChatGPT コードインタープリター
\`\`\`

---

## このセクションのポイント

1. **コードAgent** —— 自動でコード作成、バグ修正
2. **汎用Agent** —— ChatGPT、Claudeのツール機能
3. **自動化ツール** —— AIをワークフローに統合
4. **ニーズに応じて選択** —— ツールごとに異なる強み
            `
          }
        },
        {
          id: 'ch3-mcp',
          title: { zh: '3.3 MCP：AI 的"万能接口"', ja: '3.3 MCP：AIの「万能インターフェース」' },
          content: {
            zh: `
## 什么是 MCP？

**MCP（Model Context Protocol）** 是 Anthropic 推出的开放协议，让 AI 能够安全地连接各种外部工具和数据源。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                         MCP 架构概览                                     │
└─────────────────────────────────────────────────────────────────────────┘

                           ┌───────────────┐
                           │    AI 模型    │
                           │  (Claude等)   │
                           └───────┬───────┘
                                   │
                           ┌───────▼───────┐
                           │  MCP 协议层   │
                           └───────┬───────┘
                                   │
      ┌────────────────────────────┼────────────────────────────┐
      │                            │                            │
      ▼                            ▼                            ▼
┌───────────┐              ┌───────────┐              ┌───────────┐
│ 文件系统  │              │  数据库   │              │  API服务  │
│ MCP服务器 │              │ MCP服务器 │              │ MCP服务器 │
└───────────┘              └───────────┘              └───────────┘
      │                            │                            │
      ▼                            ▼                            ▼
  本地文件                    PostgreSQL                   GitHub/Slack
                               MySQL                      Notion/Jira
\`\`\`

---

## 为什么 MCP 很重要？

之前的问题：
- 每个 AI 产品都要单独开发工具集成
- 数据源连接方式各不相同
- 难以复用和标准化

MCP 解决了：
\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                         统一的工具接口                                   │
└─────────────────────────────────────────────────────────────────────────┘

  旧模式                                新模式（MCP）
  ─────────                            ──────────────
  ChatGPT ─┬─ 文件读取                  Claude ──┐
           ├─ 网页搜索                  ChatGPT ─┼── MCP ──┬─ 任何工具
           └─ API调用                   其他AI ──┘         └─ 任何数据

  Claude ──┬─ 文件读取
           ├─ 网页搜索     ──▶         一次开发，处处可用
           └─ API调用

  重复开发 ✗                           标准化复用 ✓
\`\`\`

---

## Claude Code：MCP 的实战应用

**Claude Code** 是基于 MCP 的 AI 编程助手，可以直接操作你的开发环境。

### 核心能力

| 能力 | 说明 | 示例 |
|------|------|------|
| 🔍 代码理解 | 读取和分析整个项目 | "解释这个模块的架构" |
| ✏️ 代码修改 | 直接编辑文件 | "修复这个 bug" |
| 💻 命令执行 | 运行终端命令 | "运行测试并修复失败用例" |
| 🔗 工具集成 | 通过 MCP 扩展能力 | "查询数据库中的用户数据" |

### 实际工作流程

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                     Claude Code 工作流程                                │
└─────────────────────────────────────────────────────────────────────────┘

用户: "帮我实现用户登录功能"
                │
                ▼
        ┌───────────────┐
        │ 1. 分析项目   │  ← 读取现有代码、理解架构
        └───────┬───────┘
                │
                ▼
        ┌───────────────┐
        │ 2. 制定计划   │  ← 确定需要创建/修改的文件
        └───────┬───────┘
                │
                ▼
        ┌───────────────┐
        │ 3. 编写代码   │  ← 创建路由、控制器、模型
        └───────┬───────┘
                │
                ▼
        ┌───────────────┐
        │ 4. 测试验证   │  ← 运行测试、检查错误
        └───────┬───────┘
                │
                ▼
        ┌───────────────┐
        │ 5. 迭代改进   │  ← 根据反馈继续优化
        └───────────────┘
\`\`\`

---

## 常用 MCP 服务器

| MCP 服务器 | 功能 | 使用场景 |
|-----------|------|----------|
| filesystem | 文件读写 | 代码编辑、文档处理 |
| postgres/mysql | 数据库查询 | 数据分析、报表生成 |
| github | 代码仓库操作 | PR 审查、Issue 管理 |
| slack | 消息发送 | 团队通知、自动化 |
| browser | 网页浏览 | 信息收集、测试 |
| memory | 长期记忆 | 保存对话上下文 |

---

## 如何开始使用

### 1. 安装 Claude Code
\`\`\`bash
# macOS/Linux
npm install -g @anthropic-ai/claude-code

# 启动
claude
\`\`\`

### 2. 配置 MCP 服务器
\`\`\`json
// ~/.claude/claude_desktop_config.json
{
  "mcpServers": {
    "filesystem": {
      "command": "npx",
      "args": ["-y", "@anthropic-ai/mcp-server-filesystem", "/path/to/project"]
    },
    "github": {
      "command": "npx",
      "args": ["-y", "@anthropic-ai/mcp-server-github"],
      "env": {
        "GITHUB_TOKEN": "your-token"
      }
    }
  }
}
\`\`\`

### 3. 开始对话
\`\`\`
你: 帮我看看这个项目的代码结构
Claude: [读取项目文件...] 这是一个 React + TypeScript 项目...

你: 添加一个深色模式切换功能
Claude: [分析现有代码...] [创建 ThemeContext...] [修改组件...]
\`\`\`

---

## Boris 的高效工作流（来自 Claude Code 创始人）

Claude Code 创始人 Boris Cherny 分享了他的实战经验，这些技巧能让效率提升数倍。

### 1. 并行处理：5 个实例同时跑

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        Boris 的多实例工作流                              │
└─────────────────────────────────────────────────────────────────────────┘

  终端                                    Web 版
  ─────                                  ──────
  Tab 1: 功能开发                        Tab A: 代码审查
  Tab 2: Bug 修复                        Tab B: 文档生成
  Tab 3: 测试编写                        Tab C: 架构设计
  Tab 4: 重构优化                        ...
  Tab 5: 部署脚本

  ↓ 每个实例独立处理子任务 ↓

  并行开发 → 效率翻倍
\`\`\`

### 2. Plan 模式优先

> "大多数会话从 Plan 模式开始（Shift+Tab 两次），先设计再执行。"

\`\`\`
普通模式                              Plan 模式
────────                             ──────────
直接写代码                            先分析需求
边写边改                              设计方案
来回修改 3-5 次                       一次通过

效率: 低                              效率: 高
\`\`\`

### 3. Slash 命令自动化

将重复操作封装成命令：

\`\`\`bash
/commit-push-pr    # 一键：提交 + 推送 + 创建 PR
/verify-app        # 自动化端到端测试
/code-simplifier   # 写完代码自动简化
\`\`\`

### 4. 验证反馈循环

> "给 Claude 一个验证自己的方式，质量能提升 2-3 倍。"

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                          验证反馈循环                                    │
└─────────────────────────────────────────────────────────────────────────┘

                    ┌───────────────┐
                    │  编写代码     │
                    └───────┬───────┘
                            │
                            ▼
                    ┌───────────────┐
                    │  自动测试     │ ← 单元测试 / E2E 测试
                    └───────┬───────┘
                            │
                    ┌───────┴───────┐
                    │               │
                    ▼               ▼
              ┌─────────┐     ┌─────────┐
              │  通过   │     │  失败   │
              └─────────┘     └────┬────┘
                                   │
                                   ▼
                            ┌───────────────┐
                            │  自动修复     │
                            └───────┬───────┘
                                    │
                                    └──────▶ 重新测试
\`\`\`

---

## Sub-agents：专家团队协作

Claude Code 支持创建专门的 Sub-agents，每个专注一个领域：

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                       Sub-agents 架构                                   │
└─────────────────────────────────────────────────────────────────────────┘

                           主 Agent
                              │
        ┌─────────────────────┼─────────────────────┐
        │                     │                     │
        ▼                     ▼                     ▼
  ┌───────────┐         ┌───────────┐         ┌───────────┐
  │ 代码审查员 │         │  测试专家  │         │ 文档作者  │
  │           │         │           │         │           │
  │ - 检查质量 │         │ - 编写测试 │         │ - 生成文档 │
  │ - 安全审计 │         │ - 覆盖率   │         │ - API 说明 │
  │ - 性能建议 │         │ - 边界用例 │         │ - 示例代码 │
  └───────────┘         └───────────┘         └───────────┘
        │                     │                     │
        └─────────────────────┼─────────────────────┘
                              │
                              ▼
                        整合所有结果
\`\`\`

### 配置 Sub-agent

\`\`\`yaml
# .claude/agents/code-reviewer.md
---
name: code-reviewer
description: 专门审查代码质量和安全性
tools:
  - Read
  - Grep
  - Glob
---

你是一位资深代码审查专家。重点关注：
1. 代码质量和可读性
2. 安全漏洞
3. 性能问题
\`\`\`

---

## Spec-Driven 开发：告别 Vibe Coding

传统的 "Vibe Coding"（边想边写）效率低下，Spec-Driven 开发是更好的选择：

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                   Spec-Driven vs Vibe Coding                            │
└─────────────────────────────────────────────────────────────────────────┘

  Vibe Coding                           Spec-Driven
  ────────────                          ───────────
  ┌─────────────┐                      ┌─────────────┐
  │  想到什么    │                      │  需求分析   │
  │  写什么     │                      └──────┬──────┘
  └──────┬──────┘                             │
         │                                    ▼
         ▼                             ┌─────────────┐
  ┌─────────────┐                      │  编写规格   │
  │  出 bug     │                      │  (spec.md)  │
  └──────┬──────┘                      └──────┬──────┘
         │                                    │
         ▼                                    ▼
  ┌─────────────┐                      ┌─────────────┐
  │  反复修改   │                      │  按规格实现 │
  └──────┬──────┘                      └──────┬──────┘
         │                                    │
         ▼                                    ▼
  ┌─────────────┐                      ┌─────────────┐
  │  又出 bug   │                      │  一次通过   │
  └─────────────┘                      └─────────────┘

  效率: ★★☆☆☆                         效率: ★★★★★
\`\`\`

---

## 本节要点

1. **MCP** —— AI 与外部工具的标准化接口
2. **Claude Code** —— 基于 MCP 的 AI 编程助手
3. **并行处理** —— 多实例同时工作，效率翻倍
4. **验证循环** —— 自动测试确保代码质量
5. **Sub-agents** —— 专业分工，各司其职
6. **Spec-Driven** —— 先设计后实现，一次通过
            `,
            ja: `
## MCPとは？

**MCP（Model Context Protocol）** は、Anthropicが開発したオープンプロトコルで、AIが様々な外部ツールやデータソースに安全に接続できるようにします。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                         MCP アーキテクチャ概要                           │
└─────────────────────────────────────────────────────────────────────────┘

                           ┌───────────────┐
                           │   AIモデル    │
                           │  (Claude等)   │
                           └───────┬───────┘
                                   │
                           ┌───────▼───────┐
                           │  MCPプロトコル │
                           └───────┬───────┘
                                   │
      ┌────────────────────────────┼────────────────────────────┐
      │                            │                            │
      ▼                            ▼                            ▼
┌───────────┐              ┌───────────┐              ┌───────────┐
│ファイルシステム│              │ データベース │              │ APIサービス │
│ MCPサーバー │              │ MCPサーバー │              │ MCPサーバー │
└───────────┘              └───────────┘              └───────────┘
      │                            │                            │
      ▼                            ▼                            ▼
  ローカルファイル                PostgreSQL                  GitHub/Slack
                               MySQL                      Notion/Jira
\`\`\`

---

## なぜMCPが重要なのか？

以前の問題：
- 各AI製品が個別にツール統合を開発
- データソース接続方法がバラバラ
- 再利用や標準化が困難

MCPが解決したこと：
\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                         統一されたツールインターフェース                    │
└─────────────────────────────────────────────────────────────────────────┘

  旧モデル                              新モデル（MCP）
  ─────────                            ──────────────
  ChatGPT ─┬─ ファイル読取              Claude ──┐
           ├─ Web検索                   ChatGPT ─┼── MCP ──┬─ 任意のツール
           └─ API呼出                   他のAI ──┘         └─ 任意のデータ

  Claude ──┬─ ファイル読取
           ├─ Web検索     ──▶          一度開発、どこでも利用可能
           └─ API呼出

  重複開発 ✗                           標準化再利用 ✓
\`\`\`

---

## Claude Code：MCPの実践活用

**Claude Code** はMCPベースのAIプログラミングアシスタントで、開発環境を直接操作できます。

### コア機能

| 機能 | 説明 | 例 |
|------|------|------|
| 🔍 コード理解 | プロジェクト全体を読み取り分析 | 「このモジュールのアーキテクチャを説明して」 |
| ✏️ コード修正 | ファイルを直接編集 | 「このバグを修正して」 |
| 💻 コマンド実行 | ターミナルコマンドを実行 | 「テストを実行して失敗を修正して」 |
| 🔗 ツール統合 | MCPで機能拡張 | 「DBのユーザーデータを取得して」 |

### 実際のワークフロー

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                     Claude Code ワークフロー                             │
└─────────────────────────────────────────────────────────────────────────┘

ユーザー: 「ユーザーログイン機能を実装して」
                │
                ▼
        ┌───────────────┐
        │ 1. プロジェクト分析 │  ← 既存コードを読み取り、アーキテクチャを理解
        └───────┬───────┘
                │
                ▼
        ┌───────────────┐
        │ 2. 計画立案    │  ← 作成/修正が必要なファイルを特定
        └───────┬───────┘
                │
                ▼
        ┌───────────────┐
        │ 3. コード作成  │  ← ルート、コントローラー、モデルを作成
        └───────┬───────┘
                │
                ▼
        ┌───────────────┐
        │ 4. テスト検証  │  ← テスト実行、エラー確認
        └───────┬───────┘
                │
                ▼
        ┌───────────────┐
        │ 5. 反復改善    │  ← フィードバックに基づき最適化
        └───────────────┘
\`\`\`

---

## よく使うMCPサーバー

| MCPサーバー | 機能 | 使用シーン |
|-----------|------|----------|
| filesystem | ファイル読み書き | コード編集、ドキュメント処理 |
| postgres/mysql | DBクエリ | データ分析、レポート生成 |
| github | コードリポジトリ操作 | PRレビュー、Issue管理 |
| slack | メッセージ送信 | チーム通知、自動化 |
| browser | Webブラウジング | 情報収集、テスト |
| memory | 長期記憶 | 会話コンテキストの保存 |

---

## 始め方

### 1. Claude Codeをインストール
\`\`\`bash
# macOS/Linux
npm install -g @anthropic-ai/claude-code

# 起動
claude
\`\`\`

### 2. MCPサーバーを設定
\`\`\`json
// ~/.claude/claude_desktop_config.json
{
  "mcpServers": {
    "filesystem": {
      "command": "npx",
      "args": ["-y", "@anthropic-ai/mcp-server-filesystem", "/path/to/project"]
    },
    "github": {
      "command": "npx",
      "args": ["-y", "@anthropic-ai/mcp-server-github"],
      "env": {
        "GITHUB_TOKEN": "your-token"
      }
    }
  }
}
\`\`\`

### 3. 対話を開始
\`\`\`
あなた: このプロジェクトのコード構造を見て
Claude: [プロジェクトファイルを読み取り中...] React + TypeScriptプロジェクトです...

あなた: ダークモード切替機能を追加して
Claude: [既存コードを分析中...] [ThemeContextを作成...] [コンポーネントを修正...]
\`\`\`

---

## Borisの効率的ワークフロー（Claude Code創設者より）

Claude Code創設者Boris Chernyが実践的なテクニックを共有。効率が数倍向上します。

### 1. 並列処理：5インスタンス同時実行

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                      Borisのマルチインスタンスワークフロー                  │
└─────────────────────────────────────────────────────────────────────────┘

  ターミナル                              Web版
  ────────                              ─────
  Tab 1: 機能開発                        Tab A: コードレビュー
  Tab 2: バグ修正                        Tab B: ドキュメント生成
  Tab 3: テスト作成                      Tab C: アーキテクチャ設計
  Tab 4: リファクタリング                ...
  Tab 5: デプロイスクリプト

  ↓ 各インスタンスが独立してサブタスクを処理 ↓

  並列開発 → 効率倍増
\`\`\`

### 2. Planモード優先

> "ほとんどのセッションはPlanモードから開始（Shift+Tab 2回）、設計してから実行。"

### 3. Slashコマンド自動化

\`\`\`bash
/commit-push-pr    # ワンクリック：コミット + プッシュ + PR作成
/verify-app        # 自動E2Eテスト
/code-simplifier   # コード作成後に自動簡素化
\`\`\`

### 4. 検証フィードバックループ

> "Claudeに自己検証の方法を与えると、品質が2-3倍向上する。"

---

## Sub-agents：専門家チーム協力

Claude Codeは専門のSub-agentsをサポート、各自が1つの領域に集中：

\`\`\`yaml
# .claude/agents/code-reviewer.md
---
name: code-reviewer
description: コード品質とセキュリティを専門にレビュー
tools:
  - Read
  - Grep
  - Glob
---

あなたはシニアコードレビュアーです。重点：
1. コード品質と可読性
2. セキュリティ脆弱性
3. パフォーマンス問題
\`\`\`

---

## Spec-Driven開発：Vibe Codingからの脱却

従来の"Vibe Coding"（思いつきでコーディング）は非効率。Spec-Driven開発がより良い選択：

| アプローチ | 特徴 | 効率 |
|-----------|------|------|
| Vibe Coding | 思いつきで書く→バグ→修正の繰り返し | ★★☆☆☆ |
| Spec-Driven | 要件分析→仕様作成→実装 | ★★★★★ |

---

## このセクションのポイント

1. **MCP** —— AIと外部ツールの標準化インターフェース
2. **Claude Code** —— MCPベースのAIプログラミングアシスタント
3. **並列処理** —— 複数インスタンス同時作業で効率倍増
4. **検証ループ** —— 自動テストでコード品質を確保
5. **Sub-agents** —— 専門分業、各自の役割
6. **Spec-Driven** —— 設計先行、一発で通す
            `
          }
        },
        {
          id: 'ch3-claude-code-commands',
          title: { zh: '3.4 Claude Code 命令详解', ja: '3.4 Claude Code コマンド詳解' },
          content: {
            zh: `
## Claude Code 命令大全

Claude Code 提供了丰富的斜杠命令（Slash Commands）来提升开发效率。掌握这些命令能让你的 AI 编程体验更上一层楼。

---

## 核心命令一览

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                     Claude Code 命令分类                                 │
└─────────────────────────────────────────────────────────────────────────┘

  基础命令                  MCP 命令               会话管理
  ────────                  ────────               ────────
  /help                    /mcp                   /clear
  /status                  /mcp add               /compact
  /config                  /mcp remove            /memory
  /model                   /mcp list              /resume
                           /mcp logs

  开发命令                  文件操作               工具命令
  ────────                  ────────               ────────
  /init                    /read                  /terminal
  /plan                    /edit                  /web
  /commit                  /write                 /browser
  /pr                      /search                /cost
\`\`\`

---

## 🔧 MCP 命令详解

MCP（Model Context Protocol）命令是 Claude Code 最强大的扩展能力。

### /mcp - 查看 MCP 状态

\`\`\`bash
/mcp
# 显示当前已连接的 MCP 服务器列表和状态
\`\`\`

**使用场景**：
- 检查 MCP 服务器是否正常运行
- 查看可用的工具和资源
- 调试连接问题

### /mcp add - 添加 MCP 服务器

\`\`\`bash
# 添加 GitHub MCP 服务器
/mcp add github

# 添加数据库 MCP 服务器
/mcp add postgres

# 添加自定义 MCP 服务器
/mcp add my-custom-server --command "node server.js"
\`\`\`

**使用场景**：
- 连接 GitHub 进行 PR 管理
- 连接数据库进行数据分析
- 添加自定义工具扩展能力

### /mcp logs - 查看 MCP 日志

\`\`\`bash
/mcp logs
# 查看 MCP 服务器的运行日志，用于调试

/mcp logs github
# 查看特定服务器的日志
\`\`\`

---

## 📝 会话管理命令

### /clear - 清空会话

\`\`\`bash
/clear
# 清空当前对话历史，开始全新会话
\`\`\`

**使用场景**：
- 切换到新任务时清理上下文
- 对话过长导致响应变慢时重置
- 避免旧对话影响新任务

### /compact - 压缩上下文

\`\`\`bash
/compact
# 智能压缩对话历史，保留关键信息
\`\`\`

**使用场景**：
- 对话过长时节省 tokens
- 保留重要上下文同时减少内存占用
- 长时间编程会话中定期执行

### /memory - 长期记忆

\`\`\`bash
/memory
# 查看和管理长期记忆

/memory add "项目使用 React + TypeScript"
# 添加长期记忆，后续会话都会记住
\`\`\`

**使用场景**：
- 保存项目架构信息
- 记住编码偏好（如缩进风格）
- 跨会话保持一致性

---

## 🚀 开发命令

### /init - 初始化项目

\`\`\`bash
/init
# Claude 分析当前目录，生成 CLAUDE.md 配置文件
\`\`\`

**使用场景**：
- 新项目首次使用 Claude Code
- 让 Claude 理解项目结构和技术栈
- 生成项目特定的指令配置

### /plan - 进入计划模式

\`\`\`bash
/plan
# 切换到计划模式，只做分析和设计，不执行修改

# 快捷键：Shift + Tab 两次
\`\`\`

**使用场景**：
- 复杂功能开发前的架构设计
- 代码审查和分析
- 学习理解代码库

### /commit - 提交代码

\`\`\`bash
/commit
# 自动生成 commit message 并提交
\`\`\`

**使用场景**：
- 完成功能开发后快速提交
- AI 自动分析变更生成 commit 信息
- 保持 commit 历史清晰

### /pr - 创建 Pull Request

\`\`\`bash
/pr
# 创建 PR 并生成描述
\`\`\`

**使用场景**：
- 完成功能后自动创建 PR
- AI 生成清晰的 PR 描述
- 包含变更摘要和测试说明

---

## 🔍 文件操作命令

### /search - 搜索代码

\`\`\`bash
/search "function handleClick"
# 在项目中搜索代码

/search --type ts "useState"
# 只搜索 TypeScript 文件
\`\`\`

**使用场景**：
- 快速定位代码位置
- 查找函数定义和引用
- 分析代码模式

---

## ⚙️ 配置命令

### /config - 查看/修改配置

\`\`\`bash
/config
# 查看当前配置

/config set model claude-opus-4-5-20250514
# 设置默认模型

/config set theme dark
# 设置主题
\`\`\`

### /model - 切换模型

\`\`\`bash
/model
# 查看可用模型

/model claude-opus-4-5-20250514
# 切换到 Opus 4.5
\`\`\`

**使用场景**：
- 简单任务用 Haiku 节省成本
- 复杂任务切换到 Opus 获得最佳效果
- 根据任务类型灵活选择

---

## 💰 实用命令

### /cost - 查看使用成本

\`\`\`bash
/cost
# 显示当前会话的 token 使用和费用
\`\`\`

**使用场景**：
- 监控 API 使用量
- 优化 prompt 降低成本
- 预算管理

### /status - 查看状态

\`\`\`bash
/status
# 显示连接状态、模型信息、配置等
\`\`\`

---

## 🎯 高效工作流示例

### 示例 1：新功能开发

\`\`\`bash
# 1. 先进入计划模式，设计方案
/plan
你: 我需要实现用户认证功能

# 2. 确认方案后退出计划模式执行
# Shift + Tab 切换到 Auto 模式

# 3. 开发完成后提交
/commit

# 4. 创建 PR
/pr
\`\`\`

### 示例 2：调试 MCP 问题

\`\`\`bash
# 1. 检查 MCP 状态
/mcp

# 2. 查看日志定位问题
/mcp logs

# 3. 重新添加服务器
/mcp remove github
/mcp add github
\`\`\`

### 示例 3：长会话管理

\`\`\`bash
# 1. 对话过长时压缩
/compact

# 2. 添加重要信息到长期记忆
/memory add "使用 pnpm 而非 npm"

# 3. 切换任务时清空
/clear
\`\`\`

---

## 📋 命令速查表

| 命令 | 快捷方式 | 说明 |
|------|----------|------|
| /help | - | 显示帮助信息 |
| /clear | Cmd/Ctrl+K | 清空会话 |
| /compact | - | 压缩上下文 |
| /mcp | - | 查看 MCP 状态 |
| /plan | Shift+Tab×2 | 进入计划模式 |
| /commit | - | 提交代码 |
| /pr | - | 创建 PR |
| /cost | - | 查看费用 |
| /model | - | 切换模型 |

---

## 本节要点

1. **MCP 命令** —— 管理外部工具连接，扩展 AI 能力
2. **会话管理** —— /clear、/compact、/memory 控制对话
3. **开发命令** —— /plan、/commit、/pr 覆盖完整开发流程
4. **配置命令** —— /config、/model 自定义工作环境
5. **实用技巧** —— 组合使用命令实现高效工作流
            `,
            ja: `
## Claude Code コマンド大全

Claude Codeは豊富なスラッシュコマンドを提供し、開発効率を向上させます。これらのコマンドをマスターすれば、AIプログラミング体験がさらに向上します。

---

## コアコマンド一覧

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                     Claude Code コマンド分類                             │
└─────────────────────────────────────────────────────────────────────────┘

  基本コマンド              MCPコマンド             セッション管理
  ──────────              ──────────             ────────────
  /help                    /mcp                   /clear
  /status                  /mcp add               /compact
  /config                  /mcp remove            /memory
  /model                   /mcp list              /resume
                           /mcp logs

  開発コマンド              ファイル操作            ツールコマンド
  ──────────              ──────────              ────────────
  /init                    /read                  /terminal
  /plan                    /edit                  /web
  /commit                  /write                 /browser
  /pr                      /search                /cost
\`\`\`

---

## 🔧 MCPコマンド詳解

MCP（Model Context Protocol）コマンドはClaude Codeの最強の拡張機能です。

### /mcp - MCP状態確認

\`\`\`bash
/mcp
# 接続中のMCPサーバーリストと状態を表示
\`\`\`

**使用シーン**：
- MCPサーバーが正常に動作しているか確認
- 利用可能なツールとリソースを確認
- 接続問題のデバッグ

### /mcp add - MCPサーバー追加

\`\`\`bash
# GitHub MCPサーバーを追加
/mcp add github

# データベースMCPサーバーを追加
/mcp add postgres

# カスタムMCPサーバーを追加
/mcp add my-custom-server --command "node server.js"
\`\`\`

**使用シーン**：
- GitHubに接続してPR管理
- データベースに接続してデータ分析
- カスタムツールで機能拡張

---

## 📝 セッション管理コマンド

### /clear - セッションクリア

\`\`\`bash
/clear
# 現在の会話履歴をクリアし、新しいセッションを開始
\`\`\`

**使用シーン**：
- 新しいタスクに切り替える時にコンテキストをクリア
- 会話が長くなり応答が遅くなった時にリセット
- 古い会話が新しいタスクに影響するのを防ぐ

### /compact - コンテキスト圧縮

\`\`\`bash
/compact
# 会話履歴をスマートに圧縮、重要な情報を保持
\`\`\`

**使用シーン**：
- 会話が長くなった時にtokensを節約
- 重要なコンテキストを保持しつつメモリ使用量を削減
- 長時間のプログラミングセッションで定期的に実行

---

## 🚀 開発コマンド

### /plan - 計画モードに入る

\`\`\`bash
/plan
# 計画モードに切り替え、分析と設計のみ、変更は実行しない

# ショートカット：Shift + Tab 2回
\`\`\`

**使用シーン**：
- 複雑な機能開発前のアーキテクチャ設計
- コードレビューと分析
- コードベースの理解と学習

### /commit - コードコミット

\`\`\`bash
/commit
# 自動的にcommit messageを生成してコミット
\`\`\`

### /pr - Pull Request作成

\`\`\`bash
/pr
# PRを作成して説明を生成
\`\`\`

---

## 📋 コマンド早見表

| コマンド | ショートカット | 説明 |
|---------|--------------|------|
| /help | - | ヘルプ情報を表示 |
| /clear | Cmd/Ctrl+K | セッションをクリア |
| /compact | - | コンテキストを圧縮 |
| /mcp | - | MCP状態を確認 |
| /plan | Shift+Tab×2 | 計画モードに入る |
| /commit | - | コードをコミット |
| /pr | - | PRを作成 |
| /cost | - | 費用を確認 |
| /model | - | モデルを切り替え |

---

## 本節のポイント

1. **MCPコマンド** —— 外部ツール接続を管理、AI能力を拡張
2. **セッション管理** —— /clear、/compact、/memoryで会話を制御
3. **開発コマンド** —— /plan、/commit、/prで開発フロー全体をカバー
4. **設定コマンド** —— /config、/modelで作業環境をカスタマイズ
5. **実用テクニック** —— コマンドを組み合わせて効率的なワークフローを実現
            `
          }
        },
        {
          id: 'ch3-agent-skills',
          title: { zh: '3.5 Agent Skills 详解', ja: '3.5 Agent Skills 詳解' },
          content: {
            zh: `
## Agent Skills：让 AI 学会新技能

Agent Skills 是 Claude Code 最强大的定制化功能。通过自定义 Skills，你可以让 AI 掌握特定的工作流程，实现一键执行复杂任务。

---

## 什么是 Agent Skills？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        Agent Skills 架构                                 │
└─────────────────────────────────────────────────────────────────────────┘

  用户请求                Skills 定义               AI 执行
  ────────               ──────────               ────────
  "运行 /deploy"    →    .claude/skills/         →   自动执行
                         deploy.md                   部署流程
                              │
                              ▼
                    ┌─────────────────┐
                    │ • 触发条件       │
                    │ • 执行步骤       │
                    │ • 工具调用       │
                    │ • 输出格式       │
                    └─────────────────┘
\`\`\`

**核心概念**：
- **Skills** = 可复用的 AI 工作流程定义
- **存储位置**：\`.claude/skills/\` 目录下的 Markdown 文件
- **触发方式**：通过 \`/skillname\` 或自然语言触发
- **跨平台通用**：OpenAI Codex 已采用相同规范

---

## 创建你的第一个 Skill

### 示例 1：代码审查 Skill

\`\`\`markdown
# .claude/skills/review.md

---
name: review
description: 执行全面的代码审查
triggers:
  - /review
  - 帮我审查代码
  - code review
---

# 代码审查流程

## 执行步骤

1. **获取变更文件**
   - 运行 \`git diff --name-only HEAD~1\` 获取变更文件列表
   - 如果用户指定了文件，则只审查指定文件

2. **代码分析**
   对每个文件检查以下方面：
   - 代码逻辑正确性
   - 潜在的 Bug 和边界情况
   - 性能问题
   - 安全漏洞（SQL 注入、XSS 等）
   - 代码风格一致性

3. **生成报告**
   使用以下格式输出：

   \`\`\`
   ## 📝 代码审查报告

   ### 文件：{filename}

   #### ✅ 优点
   - ...

   #### ⚠️ 建议改进
   - 行 {line}: {issue}

   #### 🔴 必须修复
   - 行 {line}: {critical_issue}
   \`\`\`

4. **提供修复建议**
   对于每个问题，提供具体的修复代码示例
\`\`\`

### 示例 2：自动化部署 Skill

\`\`\`markdown
# .claude/skills/deploy.md

---
name: deploy
description: 一键部署到生产环境
triggers:
  - /deploy
  - 部署到生产
  - deploy to production
---

# 部署流程

## 前置检查

1. 确认当前分支是 main 或 master
2. 确认没有未提交的更改
3. 确认所有测试通过

## 执行步骤

1. **运行测试**
   \`\`\`bash
   npm test
   \`\`\`
   如果测试失败，停止部署并报告错误

2. **构建项目**
   \`\`\`bash
   npm run build
   \`\`\`

3. **版本更新**
   - 读取 package.json 中的版本号
   - 询问用户选择版本更新类型（patch/minor/major）
   - 更新版本号

4. **创建 Git Tag**
   \`\`\`bash
   git tag v{version}
   git push origin v{version}
   \`\`\`

5. **部署确认**
   输出部署摘要，等待用户确认后执行部署命令
\`\`\`

---

## Skill 高级技巧

### 1. 使用变量和参数

\`\`\`markdown
# .claude/skills/create-component.md

---
name: create-component
description: 创建 React 组件
triggers:
  - /component {name}
  - 创建组件 {name}
---

# 创建 React 组件

## 参数
- \`{name}\`: 组件名称（必需）

## 执行步骤

1. 在 \`src/components/{name}/\` 目录下创建：
   - \`{name}.tsx\` - 组件代码
   - \`{name}.test.tsx\` - 测试文件
   - \`{name}.css\` - 样式文件
   - \`index.ts\` - 导出文件

2. 组件模板：
   \`\`\`tsx
   import React from 'react';
   import './{name}.css';

   interface {name}Props {
     // 定义 props
   }

   export const {name}: React.FC<{name}Props> = (props) => {
     return (
       <div className="{name}">
         {/* 组件内容 */}
       </div>
     );
   };
   \`\`\`
\`\`\`

### 2. 条件分支执行

\`\`\`markdown
# .claude/skills/fix.md

---
name: fix
description: 智能修复问题
triggers:
  - /fix
  - 帮我修复
---

# 智能修复流程

## 问题诊断

1. **检测问题类型**
   - 如果是 TypeScript 错误：运行 \`npx tsc --noEmit\` 获取错误列表
   - 如果是 ESLint 错误：运行 \`npm run lint\` 获取警告
   - 如果是测试失败：运行 \`npm test\` 查看失败用例
   - 如果是构建错误：分析构建日志

2. **根据类型执行修复**

   ### TypeScript 错误
   - 定位错误文件和行号
   - 分析类型错误原因
   - 提供类型修复方案

   ### ESLint 错误
   - 尝试自动修复：\`npm run lint -- --fix\`
   - 手动修复无法自动处理的问题

   ### 测试失败
   - 分析失败原因
   - 修复代码或更新测试用例
\`\`\`

### 3. 团队共享 Skills

\`\`\`bash
# 项目级 Skills（团队共享）
.claude/
└── skills/
    ├── review.md      # 团队代码审查规范
    ├── deploy.md      # 部署流程
    └── onboard.md     # 新人引导

# 用户级 Skills（个人专用）
~/.claude/
└── skills/
    ├── my-shortcuts.md
    └── personal-workflow.md
\`\`\`

---

## 🔥 推荐开源 Skills 资源

### awesome-claude-skills

GitHub 上最全的 Claude Skills 资源集合：

\`\`\`bash
# 仓库地址
https://github.com/travisvn/awesome-claude-skills

# 包含内容
├── 📁 skills/           # 数百个即用 Skills
│   ├── code-review/     # 代码审查
│   ├── deployment/      # 部署自动化
│   ├── testing/         # 测试生成
│   └── documentation/   # 文档生成
├── 📁 templates/        # Skills 模板
└── 📁 examples/         # 使用示例
\`\`\`

**快速使用**：
\`\`\`bash
# 克隆仓库
git clone https://github.com/travisvn/awesome-claude-skills.git

# 复制你需要的 Skills 到项目
cp -r awesome-claude-skills/skills/code-review .claude/skills/
\`\`\`

### awesome-claude-code

Claude Code 综合资源列表：

\`\`\`bash
# 仓库地址
https://github.com/hesreallyhim/awesome-claude-code

# 包含资源
- Skills 集合
- MCP 服务器列表
- 最佳实践指南
- 社区工具推荐
\`\`\`

---

## 实战案例：完整项目 Skills 配置

\`\`\`bash
# 项目目录结构
my-project/
├── .claude/
│   ├── skills/
│   │   ├── review.md        # 代码审查
│   │   ├── deploy.md        # 部署流程
│   │   ├── component.md     # 组件生成
│   │   ├── api.md           # API 开发
│   │   └── debug.md         # 调试助手
│   └── settings.json        # Hooks 配置
├── CLAUDE.md                # 项目说明
└── src/
\`\`\`

### 推荐的 Skills 集合

| Skill | 命令 | 用途 |
|-------|------|------|
| review | /review | 代码审查 |
| deploy | /deploy | 一键部署 |
| component | /component Name | 创建组件 |
| api | /api endpoint | 创建 API 端点 |
| debug | /debug | 智能调试 |
| test | /test | 运行并修复测试 |
| doc | /doc | 生成文档 |
| refactor | /refactor | 代码重构 |

---

## 本节要点

1. **Skills 定义** —— 在 \`.claude/skills/\` 目录下创建 Markdown 文件
2. **触发方式** —— 使用 \`/skillname\` 或自然语言触发
3. **参数传递** —— 使用 \`{参数名}\` 语法接收用户输入
4. **开源资源** —— awesome-claude-skills 提供数百个即用 Skills
5. **团队共享** —— 项目级 Skills 实现团队标准化
            `,
            ja: `
## Agent Skills：AIに新しいスキルを学ばせる

Agent SkillsはClaude Codeの最も強力なカスタマイズ機能です。カスタムSkillsを通じて、AIに特定のワークフローを習得させ、複雑なタスクをワンクリックで実行できます。

---

## Agent Skillsとは？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        Agent Skills アーキテクチャ                        │
└─────────────────────────────────────────────────────────────────────────┘

  ユーザーリクエスト        Skills定義                AI実行
  ──────────────         ──────────               ────────
  "/deployを実行"    →    .claude/skills/         →   自動で
                         deploy.md                   デプロイ実行
                              │
                              ▼
                    ┌─────────────────┐
                    │ • トリガー条件   │
                    │ • 実行ステップ   │
                    │ • ツール呼び出し │
                    │ • 出力形式      │
                    └─────────────────┘
\`\`\`

**コアコンセプト**：
- **Skills** = 再利用可能なAIワークフロー定義
- **保存場所**：\`.claude/skills/\` ディレクトリ内のMarkdownファイル
- **トリガー方法**：\`/skillname\` または自然言語でトリガー
- **クロスプラットフォーム互換**：OpenAI Codexも同じ仕様を採用

---

## 最初のSkillを作成する

### 例1：コードレビュー Skill

\`\`\`markdown
# .claude/skills/review.md

---
name: review
description: 包括的なコードレビューを実行
triggers:
  - /review
  - コードレビューして
  - code review
---

# コードレビュープロセス

## 実行ステップ

1. **変更ファイルを取得**
   - \`git diff --name-only HEAD~1\` で変更ファイルリストを取得
   - ユーザーが指定した場合は、指定ファイルのみレビュー

2. **コード分析**
   各ファイルで以下をチェック：
   - コードロジックの正確性
   - 潜在的なバグと境界ケース
   - パフォーマンス問題
   - セキュリティ脆弱性（SQLインジェクション、XSSなど）
   - コードスタイルの一貫性

3. **レポート生成**
   以下の形式で出力：

   \`\`\`
   ## 📝 コードレビューレポート

   ### ファイル：{filename}

   #### ✅ 良い点
   - ...

   #### ⚠️ 改善提案
   - 行 {line}: {issue}

   #### 🔴 必須修正
   - 行 {line}: {critical_issue}
   \`\`\`
\`\`\`

---

## Skill 高度なテクニック

### 1. 変数とパラメータの使用

\`\`\`markdown
# .claude/skills/create-component.md

---
name: create-component
description: Reactコンポーネントを作成
triggers:
  - /component {name}
  - コンポーネント作成 {name}
---

# Reactコンポーネント作成

## パラメータ
- \`{name}\`: コンポーネント名（必須）

## 実行ステップ

1. \`src/components/{name}/\` ディレクトリに作成：
   - \`{name}.tsx\` - コンポーネントコード
   - \`{name}.test.tsx\` - テストファイル
   - \`{name}.css\` - スタイルファイル
   - \`index.ts\` - エクスポートファイル
\`\`\`

### 2. チーム共有Skills

\`\`\`bash
# プロジェクトレベルSkills（チーム共有）
.claude/
└── skills/
    ├── review.md      # チームコードレビュー規範
    ├── deploy.md      # デプロイフロー
    └── onboard.md     # 新人ガイド

# ユーザーレベルSkills（個人専用）
~/.claude/
└── skills/
    ├── my-shortcuts.md
    └── personal-workflow.md
\`\`\`

---

## 🔥 おすすめオープンソースSkillsリソース

### awesome-claude-skills

GitHubで最も完全なClaude Skillsリソース集：

\`\`\`bash
# リポジトリURL
https://github.com/travisvn/awesome-claude-skills

# 含まれる内容
├── 📁 skills/           # 数百の即戦力Skills
├── 📁 templates/        # Skillsテンプレート
└── 📁 examples/         # 使用例
\`\`\`

### awesome-claude-code

Claude Code総合リソースリスト：

\`\`\`bash
# リポジトリURL
https://github.com/hesreallyhim/awesome-claude-code
\`\`\`

---

## 本節のポイント

1. **Skills定義** —— \`.claude/skills/\` ディレクトリにMarkdownファイルを作成
2. **トリガー方法** —— \`/skillname\` または自然言語でトリガー
3. **パラメータ渡し** —— \`{パラメータ名}\` 構文でユーザー入力を受け取る
4. **オープンソースリソース** —— awesome-claude-skillsで数百の即戦力Skills
5. **チーム共有** —— プロジェクトレベルSkillsでチーム標準化を実現
            `
          }
        },
        {
          id: 'ch3-subagents',
          title: { zh: '3.6 Sub-agents 多 Agent 协作', ja: '3.6 Sub-agents マルチAgent協力' },
          content: {
            zh: `
## Sub-agents：专家团队协作模式

Sub-agents 是 Claude Code 的高级功能，让你可以将复杂任务分解给多个专家 Agent 并行处理，大幅提升开发效率。

---

## Sub-agents 架构

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        Sub-agents 协作架构                               │
└─────────────────────────────────────────────────────────────────────────┘

                    ┌─────────────────┐
                    │   主 Agent      │
                    │  (Coordinator)  │
                    │  协调分配任务    │
                    └────────┬────────┘
                             │
           ┌─────────────────┼─────────────────┐
           ▼                 ▼                 ▼
   ┌───────────────┐ ┌───────────────┐ ┌───────────────┐
   │   代码专家     │ │   测试专家     │ │   文档专家     │
   │  (Code Agent) │ │ (Test Agent)  │ │  (Doc Agent)  │
   │   独立上下文   │ │   独立上下文   │ │   独立上下文   │
   └───────────────┘ └───────────────┘ └───────────────┘
           │                 │                 │
           ▼                 ▼                 ▼
     编写功能代码      编写测试用例       生成 API 文档
\`\`\`

**核心优势**：
- **上下文隔离** —— 每个 Agent 独立上下文，避免信息污染
- **专业化分工** —— 专注特定任务，成功率更高
- **并行处理** —— 多任务同时进行，效率倍增

---

## 使用 Sub-agents

### 对话中触发

\`\`\`markdown
用户：请帮我完成用户认证功能，需要代码、测试和文档

Claude：我会安排三个专家 Agent 并行工作：

1. **代码专家** - 实现 JWT 认证逻辑
2. **测试专家** - 编写单元测试和集成测试
3. **文档专家** - 生成 API 文档和使用说明

[启动 Sub-agents...]

// 主 Agent 协调，各专家独立工作
// 最终汇总所有输出
\`\`\`

### 通过 Skill 定义

\`\`\`markdown
# .claude/skills/full-feature.md

---
name: full-feature
description: 完整功能开发（代码+测试+文档）
---

## 执行步骤

1. **启动代码 Agent**
   - 专注于功能实现
   - 遵循项目代码规范
   - 输出：功能代码文件

2. **启动测试 Agent**
   - 等待代码 Agent 完成
   - 编写单元测试覆盖所有函数
   - 编写集成测试覆盖主要流程
   - 输出：测试文件

3. **启动文档 Agent**
   - 分析代码结构
   - 生成 JSDoc 注释
   - 更新 README
   - 输出：文档更新

4. **汇总报告**
   合并三个 Agent 的输出，生成完成报告
\`\`\`

---

## 🔥 多 Agent 管理工具

### Claude Squad ⭐ 4.3k

管理多个 AI Agent 的终端工具，支持 Claude Code、Codex、Gemini、Aider 等：

\`\`\`bash
# 安装
go install github.com/smtg-ai/claude-squad@latest

# 或使用 Homebrew
brew install smtg-ai/tap/claude-squad

# 启动
claude-squad
\`\`\`

**核心功能**：
\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│  Claude Squad - 多 Agent 终端管理器                                      │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  Workspace 1: [Claude Code] 正在实现用户认证...                          │
│  Workspace 2: [Codex] 正在编写测试用例...                                │
│  Workspace 3: [Aider] 正在重构数据库层...                                │
│  Workspace 4: [Gemini] 正在生成文档...                                   │
│                                                                         │
│  快捷键: [n]新建 [d]删除 [Tab]切换 [Enter]进入 [q]退出                   │
└─────────────────────────────────────────────────────────────────────────┘
\`\`\`

**使用场景**：
- 同时处理多个独立任务
- 不同 AI 工具各司其职
- 快速切换工作上下文

---

## Hooks：事件触发自动化

Hooks 允许你在特定事件发生时自动执行脚本，实现工作流自动化。

### 可用的 Hooks

| Hook 名称 | 触发时机 | 用途 |
|----------|---------|------|
| PreToolUse | 工具调用前 | 拦截和验证 |
| PostToolUse | 工具调用后 | 日志和后处理 |
| Notification | 通知事件 | 自定义提醒 |
| Stop | 会话结束 | 清理和总结 |

### 配置示例

\`\`\`json
// .claude/settings.json
{
  "hooks": {
    "PreToolUse": [
      {
        "matcher": "Bash",
        "command": "echo '即将执行: $TOOL_INPUT'"
      }
    ],
    "PostToolUse": [
      {
        "matcher": "Write",
        "command": "npx prettier --write $FILE_PATH"
      }
    ],
    "Stop": [
      {
        "command": "echo '会话结束' >> ~/.claude/session.log"
      }
    ]
  }
}
\`\`\`

### 实用 Hook 示例

\`\`\`json
// 自动格式化
{
  "hooks": {
    "PostToolUse": [
      {
        "matcher": "Write",
        "command": "npx prettier --write $FILE_PATH"
      }
    ]
  }
}

// 危险命令拦截
{
  "hooks": {
    "PreToolUse": [
      {
        "matcher": "Bash",
        "command": "if echo '$TOOL_INPUT' | grep -qE 'rm -rf|drop table'; then echo '⚠️ 危险命令！'; exit 1; fi"
      }
    ]
  }
}
\`\`\`

---

## 本节要点

1. **Sub-agents 架构** —— 主 Agent 协调，专家 Agent 并行工作
2. **上下文隔离** —— 每个 Agent 独立上下文，避免污染
3. **Claude Squad** —— 多 Agent 终端管理工具，支持多种 AI
4. **Hooks 自动化** —— 事件驱动的工作流自动化
5. **实战应用** —— 大型功能开发、代码迁移、重构任务
            `,
            ja: `
## Sub-agents：エキスパートチーム協力モード

Sub-agentsはClaude Codeの高度な機能で、複雑なタスクを複数のエキスパートAgentに分解して並列処理できます。

---

## Sub-agentsアーキテクチャ

\`\`\`
                    ┌─────────────────┐
                    │   メインAgent   │
                    │  (Coordinator)  │
                    └────────┬────────┘
                             │
           ┌─────────────────┼─────────────────┐
           ▼                 ▼                 ▼
   ┌───────────────┐ ┌───────────────┐ ┌───────────────┐
   │  コードエキスパート │ │  テストエキスパート │ │  ドキュメントエキスパート │
   │   独立コンテキスト │ │   独立コンテキスト │ │   独立コンテキスト │
   └───────────────┘ └───────────────┘ └───────────────┘
\`\`\`

**コアメリット**：
- **コンテキスト分離** —— 各Agent独立、情報汚染を防止
- **専門化分業** —— 特定タスクに集中、成功率向上
- **並列処理** —— 複数タスク同時進行、効率倍増

---

## 🔥 マルチAgent管理ツール

### Claude Squad ⭐ 4.3k

複数AIエージェントを管理するターミナルツール：

\`\`\`bash
# インストール
go install github.com/smtg-ai/claude-squad@latest

# 起動
claude-squad
\`\`\`

**使用シーン**：
- 複数の独立タスクを同時処理
- 異なるAIツールを使い分け
- 作業コンテキストを素早く切り替え

---

## Hooks：イベントトリガー自動化

特定イベント発生時にスクリプトを自動実行：

| Hook名 | トリガー | 用途 |
|--------|---------|------|
| PreToolUse | ツール呼び出し前 | インターセプト |
| PostToolUse | ツール呼び出し後 | 後処理 |
| Stop | セッション終了 | クリーンアップ |

\`\`\`json
{
  "hooks": {
    "PostToolUse": [
      {
        "matcher": "Write",
        "command": "npx prettier --write $FILE_PATH"
      }
    ]
  }
}
\`\`\`

---

## 本節のポイント

1. **Sub-agentsアーキテクチャ** —— メインAgentが調整、エキスパートが並列作業
2. **コンテキスト分離** —— 各Agent独立コンテキスト
3. **Claude Squad** —— マルチAgentターミナル管理ツール
4. **Hooks自動化** —— イベント駆動のワークフロー自動化
            `
          }
        },
        {
          id: 'ch3-opensource',
          title: { zh: '3.7 Claude Code 开源生态', ja: '3.7 Claude Code オープンソースエコシステム' },
          content: {
            zh: `
## Claude Code 开源生态

Claude Code 拥有活跃的开源社区，涌现出许多优秀的增强工具和框架。掌握这些工具能让你的 AI 编程效率倍增。

---

## 🏆 明星项目一览

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    Claude Code 开源生态 TOP 项目                          │
└─────────────────────────────────────────────────────────────────────────┘

  项目名称              Stars         功能
  ────────              ─────         ────
  Task Master           ⭐ 20.9k      AI 驱动的任务管理系统
  Claude-Flow           ⭐ 6.7k       多 Agent 编排框架
  Claude Squad          ⭐ 4.3k       多 Agent 终端管理器
  SuperClaude           ⭐ 3.2k       Claude Code 增强框架
  awesome-claude-skills ⭐ 2.8k       Skills 资源集合
\`\`\`

---

## 🚀 SuperClaude Framework

增强 Claude Code 能力 300% 的配置框架：

### 安装

\`\`\`bash
# 使用 npm 安装
npm install -g @superclaude-org/superclaude

# 或直接克隆
git clone https://github.com/SuperClaude-Org/SuperClaude_Framework.git
cd SuperClaude_Framework

# 安装到项目
superclaude init
\`\`\`

### 核心功能

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│  SuperClaude 功能矩阵                                                    │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  🎯 21 个斜杠命令        📦 13 个专家 Agent      🔄 6 种行为模式          │
│  ─────────────────      ────────────────       ────────────────         │
│  /architect             code-expert            spec-driven              │
│  /debug                 test-expert            tdd-mode                 │
│  /refactor              security-expert        debug-mode               │
│  /security              docs-expert            review-mode              │
│  /performance           devops-expert          ...                      │
│  ...                    ...                                             │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
\`\`\`

### 使用示例

\`\`\`bash
# 架构设计模式
/architect "设计一个微服务电商系统"

# 安全审查模式
/security "审查 API 端点安全性"

# 性能优化模式
/performance "优化数据库查询性能"

# 调试模式
/debug "定位内存泄漏问题"
\`\`\`

**GitHub**: https://github.com/SuperClaude-Org/SuperClaude_Framework

---

## 📋 Task Master（Claude Task Master）

AI 驱动的任务管理系统，GitHub ⭐ 20.9k：

### 安装

\`\`\`bash
# 使用 npm 安装
npm install -g task-master-ai

# 或使用 npx
npx task-master-ai init
\`\`\`

### 核心功能

\`\`\`bash
# 从 PRD 生成任务列表
task-master parse ./prd.md

# 查看任务列表
task-master list

# 开始下一个任务
task-master next

# 标记任务完成
task-master done 1
\`\`\`

### 工作流程

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│  Task Master 工作流程                                                    │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  📄 PRD 文档  →  🤖 AI 解析  →  📋 任务列表  →  ✅ 逐个完成              │
│                                                                         │
│  支持的 AI 编辑器:                                                       │
│  • Cursor                                                               │
│  • Claude Code                                                          │
│  • Windsurf                                                             │
│  • Lovable                                                              │
│  • Roo Code                                                             │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
\`\`\`

**特色功能**：
- 📝 PRD 自动解析为可执行任务
- 🔗 任务依赖关系管理
- 📊 进度追踪和报告
- 🔄 与多种 AI 编辑器集成

**GitHub**: https://github.com/eyaltoledano/claude-task-master

---

## 🔗 其他推荐项目

### Claude-Flow

多 Agent 编排框架：

\`\`\`bash
# 安装
npm install claude-flow

# 创建编排配置
claude-flow init

# 运行多 Agent 任务
claude-flow run workflow.yaml
\`\`\`

**GitHub**: https://github.com/kabirsingh/claude-flow

### awesome-claude

Claude 生态资源大全：

\`\`\`bash
# 包含资源
- 官方文档和教程
- 社区工具和插件
- MCP 服务器列表
- Skills 模板
- 最佳实践指南
\`\`\`

**GitHub**: https://github.com/alvinunreal/awesome-claude

---

## 📦 项目选择指南

| 需求 | 推荐项目 | 原因 |
|------|---------|------|
| 任务管理 | Task Master | PRD 到代码的完整流程 |
| 增强命令 | SuperClaude | 21 个专业命令 + 13 个专家 |
| 多 Agent 管理 | Claude Squad | 支持多种 AI 工具 |
| Skills 资源 | awesome-claude-skills | 数百个即用 Skills |
| 综合学习 | awesome-claude | 最全资源列表 |

---

## 本节要点

1. **SuperClaude** —— 21 命令 + 13 专家，能力增强 300%
2. **Task Master** —— PRD 到代码的 AI 任务管理（20k+ stars）
3. **Claude Squad** —— 多 Agent 终端管理器
4. **awesome-* 系列** —— 社区精选资源集合
5. **选择建议** —— 根据需求组合使用多个工具
            `,
            ja: `
## Claude Code オープンソースエコシステム

Claude Codeには活発なオープンソースコミュニティがあり、多くの優れた拡張ツールとフレームワークが生まれています。

---

## 🏆 スタープロジェクト一覧

| プロジェクト | Stars | 機能 |
|------------|-------|------|
| Task Master | ⭐ 20.9k | AI駆動タスク管理 |
| Claude-Flow | ⭐ 6.7k | マルチAgentオーケストレーション |
| Claude Squad | ⭐ 4.3k | マルチAgentターミナル管理 |
| SuperClaude | ⭐ 3.2k | Claude Code拡張フレームワーク |

---

## 🚀 SuperClaude Framework

Claude Code能力を300%向上させる設定フレームワーク：

\`\`\`bash
# インストール
npm install -g @superclaude-org/superclaude

# プロジェクトに適用
superclaude init
\`\`\`

**コア機能**：
- 21個のスラッシュコマンド
- 13個のエキスパートAgent
- 6種類の動作モード

\`\`\`bash
# 使用例
/architect "マイクロサービスECシステムを設計"
/security "APIエンドポイントのセキュリティを審査"
/debug "メモリリーク問題を特定"
\`\`\`

**GitHub**: https://github.com/SuperClaude-Org/SuperClaude_Framework

---

## 📋 Task Master

AI駆動タスク管理システム、⭐ 20.9k：

\`\`\`bash
# インストール
npm install -g task-master-ai

# PRDからタスクリストを生成
task-master parse ./prd.md

# 次のタスクを開始
task-master next
\`\`\`

**特徴**：
- PRD自動解析
- タスク依存関係管理
- 進捗追跡
- 複数AIエディタと統合

**GitHub**: https://github.com/eyaltoledano/claude-task-master

---

## 📦 プロジェクト選択ガイド

| ニーズ | 推奨プロジェクト | 理由 |
|--------|----------------|------|
| タスク管理 | Task Master | PRDからコードまで |
| 機能拡張 | SuperClaude | 21コマンド + 13エキスパート |
| マルチAgent | Claude Squad | 複数AIツール対応 |
| Skills | awesome-claude-skills | 数百の即戦力Skills |

---

## 本節のポイント

1. **SuperClaude** —— 21コマンド + 13エキスパート、能力300%向上
2. **Task Master** —— PRDからコードへのAIタスク管理
3. **Claude Squad** —— マルチAgentターミナル管理
4. **awesome-*シリーズ** —— コミュニティ厳選リソース集
            `
          }
        },
        {
          id: 'ch3-gemini-cli',
          title: { zh: '3.8 Gemini CLI 使用指南', ja: '3.8 Gemini CLI 使用ガイド' },
          content: {
            zh: `
## Gemini CLI：Google 的 AI 编程助手

Gemini CLI 是 Google 推出的命令行 AI 编程工具，与 Claude Code 类似但有独特优势。

---

## 快速开始

### 安装 Gemini CLI

\`\`\`bash
# 使用 npm 安装
npm install -g @anthropic-ai/claude-code

# 或使用 Google Cloud CLI
gcloud components install gemini-cli

# 验证安装
gemini --version
\`\`\`

### 初始化配置

\`\`\`bash
# 登录 Google 账号
gemini auth login

# 配置 API 密钥（可选）
gemini config set api_key YOUR_API_KEY

# 查看配置
gemini config list
\`\`\`

---

## 核心功能

### 1. 代码生成

\`\`\`bash
# 生成代码
gemini generate "创建一个 React 计数器组件"

# 指定语言
gemini generate --lang typescript "实现快速排序算法"

# 生成到文件
gemini generate "创建 Express 服务器" -o server.js
\`\`\`

### 2. 代码解释

\`\`\`bash
# 解释代码文件
gemini explain src/utils/helper.ts

# 解释特定函数
gemini explain --function handleClick src/App.tsx

# 详细解释
gemini explain --verbose complex-algorithm.py
\`\`\`

### 3. 代码优化

\`\`\`bash
# 优化代码性能
gemini optimize src/heavy-computation.js

# 优化特定方面
gemini optimize --focus memory src/data-processing.ts

# 安全性审查
gemini optimize --security api-handler.js
\`\`\`

---

## 项目级操作

### 项目初始化

\`\`\`bash
# 初始化项目配置
gemini init

# 生成项目结构
gemini init --template react-typescript

# 从现有项目学习
gemini learn .
\`\`\`

### 代码审查

\`\`\`bash
# 审查最近更改
gemini review

# 审查特定文件
gemini review src/components/*.tsx

# 审查 PR
gemini review --pr 123
\`\`\`

---

## 交互模式

### 启动交互式会话

\`\`\`bash
# 进入交互模式
gemini chat

# 带上下文进入
gemini chat --context src/

# 指定模型
gemini chat --model gemini-2.0-flash
\`\`\`

### 交互式命令

\`\`\`
gemini> /help          # 显示帮助
gemini> /clear         # 清空会话
gemini> /save          # 保存会话
gemini> /load session.json  # 加载会话
gemini> /model         # 切换模型
gemini> /exit          # 退出
\`\`\`

---

## Gemini vs Claude Code 对比

| 特性 | Gemini CLI | Claude Code |
|------|-----------|-------------|
| 模型 | Gemini 2.0 | Claude Opus/Sonnet |
| 搜索集成 | ✅ Google 搜索 | ❌ 需要 MCP |
| 代码库分析 | ✅ 原生支持 | ✅ 原生支持 |
| MCP 协议 | ❌ 不支持 | ✅ 完整支持 |
| 多模态 | ✅ 图片/视频 | ✅ 图片 |
| 价格 | 免费额度大 | 按量付费 |

### 适用场景

**选择 Gemini CLI**：
- 需要搜索最新信息
- 处理多模态内容（图片、视频）
- 个人项目、学习用途（免费额度大）
- Google Cloud 生态整合

**选择 Claude Code**：
- 需要 MCP 扩展能力
- 复杂的代码重构任务
- 团队协作和 Skills 共享
- 长文本生成需求

---

## 高级功能

### 1. Google 搜索集成

\`\`\`bash
# 搜索最新文档
gemini search "React 19 新特性"

# 搜索并生成代码
gemini generate --search "使用最新的 Next.js 15 创建 API 路由"
\`\`\`

### 2. 多模态支持

\`\`\`bash
# 分析图片
gemini analyze image.png "描述这个UI设计"

# 从截图生成代码
gemini generate --from-image mockup.png "生成对应的 React 组件"

# 分析视频
gemini analyze video.mp4 "总结这个教程的要点"
\`\`\`

### 3. 代码库索引

\`\`\`bash
# 索引整个项目
gemini index .

# 查询代码库
gemini query "哪里处理用户认证？"

# 查找相似代码
gemini find-similar src/utils/format.ts
\`\`\`

---

## 配置文件

\`\`\`yaml
# .gemini/config.yaml
model: gemini-2.0-pro
temperature: 0.7
max_tokens: 8192

# 项目特定设置
project:
  language: typescript
  framework: react
  test_framework: jest

# 忽略文件
ignore:
  - node_modules/
  - dist/
  - "*.log"

# 自定义提示
prompts:
  review: "请用中文审查代码，关注安全性和性能"
  explain: "请用简单的语言解释这段代码的作用"
\`\`\`

---

## 实用技巧

### 1. 与 Git 集成

\`\`\`bash
# 生成 commit message
git diff | gemini generate "根据这个 diff 生成 commit message"

# 审查 staged 更改
git diff --staged | gemini review

# 生成 changelog
gemini generate "根据最近10个commit生成changelog" --context "$(git log -10)"
\`\`\`

### 2. 管道操作

\`\`\`bash
# 处理日志
tail -100 error.log | gemini analyze "分析错误模式"

# 处理 API 响应
curl api.example.com/data | gemini format --json

# 代码转换
cat old-code.js | gemini convert --to typescript > new-code.ts
\`\`\`

### 3. 批量操作

\`\`\`bash
# 批量添加注释
gemini batch comment src/**/*.ts

# 批量格式化
gemini batch format src/ --style google

# 批量测试生成
gemini batch test src/utils/*.ts -o tests/
\`\`\`

---

## 本节要点

1. **安装配置** —— npm 或 gcloud 安装，gemini auth 登录
2. **核心命令** —— generate、explain、optimize、review
3. **交互模式** —— gemini chat 进入对话式编程
4. **Google 优势** —— 搜索集成、多模态支持、免费额度
5. **项目集成** —— .gemini/config.yaml 配置项目偏好
6. **管道操作** —— 与其他命令行工具无缝配合
            `,
            ja: `
## Gemini CLI：GoogleのAIプログラミングアシスタント

Gemini CLIはGoogleが提供するコマンドラインAIプログラミングツールで、Claude Codeに似ていますが独自の利点があります。

---

## クイックスタート

### Gemini CLIのインストール

\`\`\`bash
# npmでインストール
npm install -g @google/gemini-cli

# またはGoogle Cloud CLIで
gcloud components install gemini-cli

# インストール確認
gemini --version
\`\`\`

### 初期設定

\`\`\`bash
# Googleアカウントでログイン
gemini auth login

# APIキーを設定（オプション）
gemini config set api_key YOUR_API_KEY

# 設定を確認
gemini config list
\`\`\`

---

## コア機能

### 1. コード生成

\`\`\`bash
# コードを生成
gemini generate "Reactカウンターコンポーネントを作成"

# 言語を指定
gemini generate --lang typescript "クイックソートを実装"

# ファイルに出力
gemini generate "Expressサーバーを作成" -o server.js
\`\`\`

### 2. コード説明

\`\`\`bash
# コードファイルを説明
gemini explain src/utils/helper.ts

# 特定の関数を説明
gemini explain --function handleClick src/App.tsx
\`\`\`

### 3. コード最適化

\`\`\`bash
# パフォーマンス最適化
gemini optimize src/heavy-computation.js

# セキュリティレビュー
gemini optimize --security api-handler.js
\`\`\`

---

## Gemini vs Claude Code 比較

| 特性 | Gemini CLI | Claude Code |
|------|-----------|-------------|
| モデル | Gemini 2.0 | Claude Opus/Sonnet |
| 検索統合 | ✅ Google検索 | ❌ MCP必要 |
| MCP対応 | ❌ 非対応 | ✅ 完全対応 |
| マルチモーダル | ✅ 画像/動画 | ✅ 画像 |
| 価格 | 無料枠大 | 従量課金 |

### 使い分け

**Gemini CLIを選ぶ場合**：
- 最新情報の検索が必要
- マルチモーダルコンテンツ処理
- 個人プロジェクト（無料枠大）
- Google Cloudエコシステム統合

**Claude Codeを選ぶ場合**：
- MCP拡張機能が必要
- 複雑なリファクタリング
- チーム協力とSkills共有
- 長文生成ニーズ

---

## 高度な機能

### 1. Google検索統合

\`\`\`bash
# 最新ドキュメントを検索
gemini search "React 19 新機能"

# 検索してコード生成
gemini generate --search "最新のNext.js 15でAPIルートを作成"
\`\`\`

### 2. マルチモーダル対応

\`\`\`bash
# 画像を分析
gemini analyze image.png "このUIデザインを説明"

# スクリーンショットからコード生成
gemini generate --from-image mockup.png "対応するReactコンポーネントを生成"
\`\`\`

---

## 本節のポイント

1. **インストール設定** —— npmまたはgcloudでインストール
2. **コアコマンド** —— generate、explain、optimize、review
3. **対話モード** —— gemini chatで対話式プログラミング
4. **Google優位性** —— 検索統合、マルチモーダル、無料枠
5. **プロジェクト統合** —— .gemini/config.yamlで設定
6. **パイプ操作** —— 他のCLIツールとシームレス連携
            `
          }
        },
        {
          id: 'ch3-openai-codex',
          title: { zh: '3.9 OpenAI Codex 使用指南', ja: '3.9 OpenAI Codex 使用ガイド' },
          content: {
            zh: `
## OpenAI Codex：GPT 驱动的 AI 编程助手

OpenAI Codex（又称 GPT Codex Agent）是 OpenAI 推出的 AI 编程工具，基于 GPT-5 模型，与 Claude Code 形成直接竞争。

---

## 快速开始

### 安装 Codex CLI

\`\`\`bash
# 使用 npm 安装
npm install -g @openai/codex

# 或使用 pip
pip install openai-codex

# 验证安装
codex --version
\`\`\`

### 配置 API 密钥

\`\`\`bash
# 设置环境变量
export OPENAI_API_KEY="sk-..."

# 或使用配置命令
codex config set api_key sk-...

# 登录 OpenAI 账号（推荐）
codex auth login
\`\`\`

---

## 核心功能

### 1. 自然语言编程

\`\`\`bash
# 基本代码生成
codex "创建一个 REST API 端点处理用户注册"

# 在项目上下文中生成
codex --context . "添加用户邮箱验证功能"

# 指定语言和框架
codex --lang python --framework fastapi "创建文件上传接口"
\`\`\`

### 2. Agent 模式

Codex 的 Agent 模式可以自主完成复杂任务：

\`\`\`bash
# 启动 Agent 模式
codex agent "重构这个项目的认证系统，使用 JWT"

# Agent 会自动：
# 1. 分析现有代码
# 2. 制定重构计划
# 3. 逐步实现更改
# 4. 运行测试验证
\`\`\`

### 3. 代码补全

\`\`\`bash
# 实时补全（编辑器集成）
codex complete --stream

# 批量补全
codex complete src/incomplete-file.ts

# 多候选补全
codex complete --n 3 "function calculateTotal("
\`\`\`

---

## Skills 支持

OpenAI Codex 已采用与 Claude Code 相同的 Skills 规范！

### Skills 文件结构

\`\`\`markdown
# .codex/skills/deploy.md（与 Claude 兼容）

---
name: deploy
description: 部署到生产环境
triggers:
  - /deploy
  - 部署应用
---

# 部署流程

## 步骤

1. 运行测试 \`npm test\`
2. 构建项目 \`npm run build\`
3. 推送到服务器
\`\`\`

### 跨平台 Skills

\`\`\`bash
# Skills 目录结构（通用）
.claude/skills/  # Claude Code 使用
.codex/skills/   # OpenAI Codex 使用

# 推荐：使用符号链接共享
ln -s .claude/skills .codex/skills
\`\`\`

---

## 多模态能力

### 图片理解

\`\`\`bash
# 从 UI 截图生成代码
codex vision "根据这个截图实现登录页面" --image login-mockup.png

# 分析图表
codex vision "解释这个架构图" --image architecture.png

# 调试 UI 问题
codex vision "这个页面布局有什么问题？" --image buggy-ui.png
\`\`\`

### 代码 + 图片

\`\`\`bash
# 结合代码和设计稿
codex "实现这个设计，样式要和截图一致" \
  --image design.png \
  --context src/components/
\`\`\`

---

## Codex vs Claude Code 对比

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    AI 编程工具对比表                                     │
└─────────────────────────────────────────────────────────────────────────┘

  特性               Claude Code          OpenAI Codex
  ────               ───────────          ────────────
  基础模型           Claude Opus 4.5      GPT-5.2
  Skills 支持        ✅ 原生              ✅ 兼容
  MCP 协议           ✅ 完整              ❌ 不支持
  浏览器自动化       ✅ Chrome 集成       ✅ Playwright
  多模态             图片                 图片 + 视频
  长文本生成         12000+ 字符          ~7000 字符
  数学/编程          ⭐⭐⭐⭐             ⭐⭐⭐⭐⭐
  价格               $$                   $$$
\`\`\`

### 实测对比

| 任务类型 | 推荐工具 | 原因 |
|---------|---------|------|
| 复杂重构 | Claude Code | 长文本能力强 |
| 算法实现 | Codex | 数学推理更好 |
| UI 开发 | 两者相当 | 都支持图片输入 |
| 自动化测试 | Codex | Playwright 集成好 |
| MCP 扩展 | Claude Code | 独家支持 |

---

## 高级功能

### 1. 并行任务

\`\`\`bash
# 并行执行多个任务
codex parallel \
  "优化 src/utils/*.ts 的性能" \
  "为 src/api/*.ts 添加测试" \
  "更新 README.md 文档"

# 查看并行任务状态
codex status --parallel
\`\`\`

### 2. 代码审查

\`\`\`bash
# 审查 PR
codex review --pr 123

# 审查并自动修复
codex review --fix src/

# 安全审查
codex review --security --strict
\`\`\`

### 3. 测试生成

\`\`\`bash
# 为文件生成测试
codex test generate src/utils/calculator.ts

# 为整个模块生成
codex test generate src/services/ --coverage 80

# 运行并修复失败测试
codex test fix
\`\`\`

---

## 配置文件

\`\`\`json
// .codex/config.json
{
  "model": "gpt-5.2-codex",
  "temperature": 0.3,
  "max_tokens": 8192,

  "project": {
    "language": "typescript",
    "framework": "nextjs",
    "test_framework": "jest"
  },

  "skills_dir": ".codex/skills",

  "ignore": [
    "node_modules",
    "dist",
    ".git"
  ],

  "hooks": {
    "pre_commit": "npm run lint",
    "post_generate": "npx prettier --write"
  }
}
\`\`\`

---

## 工作流示例

### 示例 1：功能开发全流程

\`\`\`bash
# 1. 进入项目目录
cd my-project

# 2. 启动 Agent 模式
codex agent

# 3. 描述需求
> 我需要添加用户头像上传功能，支持裁剪和压缩

# Codex 会自动：
# - 分析项目结构
# - 创建必要的组件和 API
# - 添加图片处理逻辑
# - 编写测试
# - 更新文档
\`\`\`

### 示例 2：代码迁移

\`\`\`bash
# JavaScript 到 TypeScript 迁移
codex migrate --from js --to ts src/

# React Class 到 Hooks 迁移
codex migrate --pattern "class-to-hooks" src/components/

# API 版本升级
codex migrate --api-version v1-to-v2 src/services/
\`\`\`

### 示例 3：性能优化

\`\`\`bash
# 分析性能问题
codex analyze performance src/

# 自动优化
codex optimize --auto \
  --focus "bundle-size,runtime" \
  src/

# 生成优化报告
codex report performance --output report.md
\`\`\`

---

## 与 Claude Code 协同使用

\`\`\`bash
# 最佳实践：结合两者优势

# 1. 用 Claude Code 做架构设计（长文本+MCP）
claude "设计用户系统的整体架构" --plan

# 2. 用 Codex 实现算法密集型代码
codex "实现推荐算法，使用协同过滤"

# 3. 用 Claude Code 做代码审查（更细致）
claude "/review"

# 4. 用 Codex 生成测试（测试覆盖更全）
codex test generate src/
\`\`\`

---

## 常见问题

### Q: Codex 和 ChatGPT 有什么区别？
A: Codex 专门针对编程任务优化，有更好的代码理解和生成能力，支持 Agent 模式自主完成复杂任务。

### Q: Skills 是否真的跨平台通用？
A: 是的，2025年5月后 OpenAI 采用了 Anthropic 的 Skills 规范，基本语法完全兼容。

### Q: 该选择 Codex 还是 Claude Code？
A: 建议都尝试。数学/算法任务用 Codex，长文本/MCP 任务用 Claude Code。

---

## 本节要点

1. **安装配置** —— npm 或 pip 安装，设置 OPENAI_API_KEY
2. **Agent 模式** —— 自主完成复杂编程任务
3. **Skills 兼容** —— 与 Claude Code 共享 Skills 定义
4. **多模态能力** —— 从截图生成代码
5. **性能对比** —— 数学推理强，长文本弱
6. **协同使用** —— 结合 Claude + Codex 发挥各自优势
            `,
            ja: `
## OpenAI Codex：GPT駆動のAIプログラミングアシスタント

OpenAI Codex（GPT Codex Agentとも呼ばれる）はOpenAIが提供するAIプログラミングツールで、GPT-5モデルをベースにしており、Claude Codeと直接競合しています。

---

## クイックスタート

### Codex CLIのインストール

\`\`\`bash
# npmでインストール
npm install -g @openai/codex

# またはpipで
pip install openai-codex

# インストール確認
codex --version
\`\`\`

### APIキーの設定

\`\`\`bash
# 環境変数を設定
export OPENAI_API_KEY="sk-..."

# または設定コマンドで
codex config set api_key sk-...

# OpenAIアカウントでログイン（推奨）
codex auth login
\`\`\`

---

## コア機能

### 1. 自然言語プログラミング

\`\`\`bash
# 基本的なコード生成
codex "ユーザー登録を処理するREST APIエンドポイントを作成"

# プロジェクトコンテキストで生成
codex --context . "メール検証機能を追加"

# 言語とフレームワークを指定
codex --lang python --framework fastapi "ファイルアップロードAPIを作成"
\`\`\`

### 2. Agentモード

CodexのAgentモードは複雑なタスクを自律的に完了できます：

\`\`\`bash
# Agentモードを起動
codex agent "このプロジェクトの認証システムをJWTを使用してリファクタリング"

# Agentは自動的に：
# 1. 既存コードを分析
# 2. リファクタリング計画を策定
# 3. 段階的に変更を実装
# 4. テストを実行して検証
\`\`\`

---

## Skills対応

OpenAI CodexはClaude Codeと同じSkills仕様を採用しています！

### Skillsファイル構造

\`\`\`markdown
# .codex/skills/deploy.md（Claudeと互換）

---
name: deploy
description: 本番環境にデプロイ
triggers:
  - /deploy
  - アプリをデプロイ
---

# デプロイプロセス

## ステップ

1. テストを実行 \`npm test\`
2. プロジェクトをビルド \`npm run build\`
3. サーバーにプッシュ
\`\`\`

---

## Codex vs Claude Code 比較

| 特性 | Claude Code | OpenAI Codex |
|-----|------------|--------------|
| ベースモデル | Claude Opus 4.5 | GPT-5.2 |
| Skills対応 | ✅ ネイティブ | ✅ 互換 |
| MCP対応 | ✅ 完全 | ❌ 非対応 |
| マルチモーダル | 画像 | 画像 + 動画 |
| 長文生成 | 12000+文字 | ~7000文字 |
| 数学/プログラミング | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ |

### 使い分け

| タスクタイプ | 推奨ツール | 理由 |
|------------|----------|------|
| 複雑なリファクタリング | Claude Code | 長文能力が強い |
| アルゴリズム実装 | Codex | 数学推論が優れている |
| UI開発 | 両方とも | 画像入力対応 |
| MCP拡張 | Claude Code | 独占対応 |

---

## 高度な機能

### 1. 並列タスク

\`\`\`bash
# 複数タスクを並列実行
codex parallel \
  "src/utils/*.tsのパフォーマンスを最適化" \
  "src/api/*.tsにテストを追加" \
  "README.mdドキュメントを更新"
\`\`\`

### 2. テスト生成

\`\`\`bash
# ファイルのテストを生成
codex test generate src/utils/calculator.ts

# モジュール全体のテストを生成
codex test generate src/services/ --coverage 80
\`\`\`

---

## Claude Codeとの協同使用

\`\`\`bash
# ベストプラクティス：両者の利点を組み合わせる

# 1. Claude Codeでアーキテクチャ設計（長文+MCP）
claude "ユーザーシステムの全体アーキテクチャを設計" --plan

# 2. Codexでアルゴリズム集約型コードを実装
codex "協調フィルタリングを使用した推薦アルゴリズムを実装"

# 3. Claude Codeでコードレビュー（より詳細）
claude "/review"

# 4. Codexでテスト生成（カバレッジが広い）
codex test generate src/
\`\`\`

---

## 本節のポイント

1. **インストール設定** —— npmまたはpipでインストール、OPENAI_API_KEYを設定
2. **Agentモード** —— 複雑なプログラミングタスクを自律完了
3. **Skills互換** —— Claude CodeとSkills定義を共有
4. **マルチモーダル能力** —— スクリーンショットからコード生成
5. **性能比較** —— 数学推論強い、長文弱い
6. **協同使用** —— Claude + Codexの組み合わせで各自の強みを発揮
            `
          }
        },
        {
          id: 'ch3-models',
          title: { zh: '3.10 AI 模型对比与选型', ja: '3.10 AIモデル比較と選定' },
          content: {
            zh: `
## 2025 年主流 AI 编程工具对比

选择合适的 AI 工具能让效率事半功倍。以下是基于实测的对比结果。

---

## Claude vs GPT：实测对比

根据 AI超元域 的深度评测，Claude 和 GPT 各有优势：

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    Claude vs GPT 能力对比                               │
└─────────────────────────────────────────────────────────────────────────┘

                    Claude Sonnet 4.5              GPT-5.1
                    ───────────────                ───────
  长文本生成            ████████████                ██████
                      ~1.2万字符                   ~6900字符

  文学创作              ████████████                ████████
                      意象精准                     存在重复

  前端UI设计            ████████████                ████████
                      配色精致                     基本可用

  知识更新              ████████████                ████████
                      截至2025.1                   截至2024.6

  数学编程              ████████                    ████████████
                      较好                         稍强

  浏览器自动化          ████████                    ████████████
                      支持                         原生集成
\`\`\`

### 推荐选择

| 任务类型 | 推荐模型 | 原因 |
|---------|--------|------|
| 长文本报告 | Claude | 输出更长、结构更好 |
| 文学创作 | Claude | 意象丰富、用词精准 |
| 前端 UI | Claude | 设计感更强 |
| 数学编程 | GPT | 略有优势 |
| 浏览器自动化 | GPT | 原生支持更好 |
| 日常对话 | 两者均可 | 差异不大 |

> 💡 **建议**：同时订阅两个服务，根据任务切换使用。

---

## GPT-5.2-Codex 与 Agent Skills

OpenAI 最新的 GPT-5.2-Codex 引入了 Agent Skills 功能，这是一个重要的行业趋势。

### 什么是 Skills？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        Skills = AI 的工作手册                           │
└─────────────────────────────────────────────────────────────────────────┘

                    ┌───────────────────┐
                    │  你的专业知识      │
                    │  + 工作流程        │
                    │  + 最佳实践        │
                    └─────────┬─────────┘
                              │
                              ▼
                    ┌───────────────────┐
                    │  打包成 Skills 文件 │
                    └─────────┬─────────┘
                              │
                    ┌─────────┴─────────┐
                    │                   │
                    ▼                   ▼
              ┌───────────┐       ┌───────────┐
              │  Claude   │       │  Codex    │
              │  可调用    │       │  可调用    │
              └───────────┘       └───────────┘

        重要：Skills 在 Claude 和 Codex 之间通用！
\`\`\`

### GPT-5.2-Codex 优缺点

**优点：**
- ✅ 编码能力确实提升
- ✅ 视觉理解（看截图写代码）不错
- ✅ Skills 让开发流程更可控
- ✅ 复杂任务完成度好

**缺点：**
- ❌ **速度慢**：简单任务 5 分钟起步
- ❌ 复杂任务 10 分钟以上
- ❌ 完整项目需要半小时

> 💡 **结论**：适合丢任务给 AI 然后去做别的事。需要快速迭代？Claude Code 更顺手。

---

## AI 编程工具生态

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                      2025 AI 编程工具全景                                │
└─────────────────────────────────────────────────────────────────────────┘

  命令行工具                    IDE 集成                   云端服务
  ──────────                   ────────                   ────────
  ┌───────────┐               ┌───────────┐              ┌───────────┐
  │ Claude Code│               │  Cursor   │              │ GitHub    │
  │ 功能最全   │               │ VSCode集成 │              │ Copilot   │
  └───────────┘               └───────────┘              └───────────┘
  ┌───────────┐               ┌───────────┐              ┌───────────┐
  │ GPT Codex │               │  Windsurf │              │ Replit    │
  │ CLI版本   │               │ 新晋选手   │              │ Agent     │
  └───────────┘               └───────────┘              └───────────┘

  辅助工具
  ────────
  ┌───────────┐  ┌───────────┐  ┌───────────┐
  │Ralph Wiggum│  │  Claudia  │  │SuperClaude│
  │自动迭代修复│  │ GUI界面   │  │ 能力增强  │
  └───────────┘  └───────────┘  └───────────┘
\`\`\`

---

## 本节要点

1. **Claude** —— 长文本、创作、设计更强
2. **GPT** —— 数学、浏览器自动化略好
3. **Skills** —— 行业标准，通用性强
4. **按需选择** —— 不要迷信单一工具
            `,
            ja: `
## 2025年主流AIプログラミングツール比較

適切なAIツールを選ぶことで効率が倍増します。以下は実測に基づく比較結果です。

---

## Claude vs GPT：実測比較

AI超元域の詳細評価によると、ClaudeとGPTにはそれぞれ長所があります：

### 推奨選択

| タスクタイプ | 推奨モデル | 理由 |
|-------------|----------|------|
| 長文レポート | Claude | 出力が長く、構造が良い |
| 文学創作 | Claude | イメージ豊か、用語が正確 |
| フロントエンドUI | Claude | デザイン感が強い |
| 数学プログラミング | GPT | やや優位 |
| ブラウザ自動化 | GPT | ネイティブサポートが良い |
| 日常会話 | どちらでも | 差は小さい |

> 💡 **提案**：両方のサービスを購読し、タスクに応じて使い分ける。

---

## GPT-5.2-CodexとAgent Skills

OpenAI最新のGPT-5.2-CodexはAgent Skills機能を導入。重要な業界トレンドです。

### Skillsとは？

Skillsは「AIの作業マニュアル」—— あなたの専門知識とワークフローをファイルにパッケージ化し、AIがいつでも呼び出せるようにします。

**重要**：SkillsはClaudeとCodex間で互換性あり！

### GPT-5.2-Codexの長所短所

**長所：**
- ✅ コーディング能力が確実に向上
- ✅ ビジュアル理解（スクショからコード生成）が良好
- ✅ Skillsで開発フローをより制御可能

**短所：**
- ❌ **遅い**：簡単なタスクでも5分から
- ❌ 複雑なタスクは10分以上
- ❌ 完全なプロジェクトは30分必要

> 💡 **結論**：タスクをAIに投げて別のことをするなら適している。迅速な反復が必要？Claude Codeがより便利。

---

## AIプログラミングツールエコシステム

\`\`\`
  コマンドラインツール           IDE統合                クラウドサービス
  ────────────────           ────────              ──────────────
  Claude Code                 Cursor                GitHub Copilot
  GPT Codex CLI               Windsurf              Replit Agent

  補助ツール
  ──────────
  Ralph Wiggum（自動反復修正）
  Claudia（GUIインターフェース）
  SuperClaude（能力強化）
\`\`\`

---

## このセクションのポイント

1. **Claude** —— 長文、創作、デザインに強い
2. **GPT** —— 数学、ブラウザ自動化がやや優位
3. **Skills** —— 業界標準、汎用性が高い
4. **ニーズに応じて選択** —— 単一ツールを盲信しない
            `
          }
        },
        {
          id: 'ch3-summary',
          title: { zh: '3.11 本章小结', ja: '3.11 この章のまとめ' },
          content: {
            zh: `
## AI Agent 核心概念回顾

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        AI Agent 知识地图                                │
└─────────────────────────────────────────────────────────────────────────┘

                              AI Agent
                                  │
         ┌────────────────────────┼────────────────────────┐
         │                        │                        │
         ▼                        ▼                        ▼
    ┌──────────┐            ┌──────────┐            ┌──────────┐
    │  核心能力 │            │  工作模式 │            │  实用工具 │
    └──────────┘            └──────────┘            └──────────┘
         │                        │                        │
    ┌────┼────┐              ReAct模式             ┌────┼────┐
    │    │    │              思考-行动-观察         │    │    │
    ▼    ▼    ▼                                   ▼    ▼    ▼
  规划  工具  记忆                             代码  通用  自动化
                                               Agent Agent 工作流
\`\`\`

---

## Agent vs 普通 AI

| 特性 | 普通 AI | AI Agent |
|------|---------|----------|
| 交互方式 | 一问一答 | 自主执行 |
| 任务复杂度 | 单一任务 | 复杂任务 |
| 工具使用 | 有限 | 丰富 |
| 自主性 | 被动响应 | 主动规划 |

---

## 快速行动清单

- [ ] 尝试使用 ChatGPT 的代码解释器功能
- [ ] 用 Claude 完成一个需要搜索的任务
- [ ] 探索一个代码 Agent（如 Cursor）
- [ ] 思考你的工作中哪些任务可以用 Agent 自动化

---

## 关键金句

> "Agent 是 AI 从'顾问'变成'助手'的关键一步。"

> "未来不是 AI 替代人，而是会用 Agent 的人替代不会用的人。"

---

下一章，我们将学习 **RAG（检索增强生成）** —— 让 AI 能够获取最新知识，解决"知识过时"的问题！
            `,
            ja: `
## AI Agentコア概念の復習

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        AI Agent 知識マップ                               │
└─────────────────────────────────────────────────────────────────────────┘

                              AI Agent
                                  │
         ┌────────────────────────┼────────────────────────┐
         │                        │                        │
         ▼                        ▼                        ▼
    ┌──────────┐            ┌──────────┐            ┌──────────┐
    │  コア能力 │            │  動作モード │            │ 実用ツール │
    └──────────┘            └──────────┘            └──────────┘
         │                        │                        │
    ┌────┼────┐              ReActパターン           ┌────┼────┐
    │    │    │              思考-行動-観察          │    │    │
    ▼    ▼    ▼                                    ▼    ▼    ▼
  計画  ツール 記憶                              コード 汎用  自動化
                                                Agent Agent ワークフロー
\`\`\`

---

## Agent vs 通常のAI

| 特性 | 通常のAI | AI Agent |
|------|----------|----------|
| インタラクション | 一問一答 | 自律実行 |
| タスクの複雑さ | 単一タスク | 複雑なタスク |
| ツール使用 | 限定的 | 豊富 |
| 自律性 | 受動的応答 | 能動的計画 |

---

## クイックアクションリスト

- [ ] ChatGPTのコードインタープリター機能を試す
- [ ] Claudeで検索が必要なタスクを完了する
- [ ] コードAgent（Cursorなど）を探索する
- [ ] 仕事でAgentで自動化できるタスクを考える

---

## 重要な格言

> 「AgentはAIが『アドバイザー』から『アシスタント』になるための重要な一歩です。」

> 「未来はAIが人を置き換えるのではなく、Agentを使える人が使えない人を置き換えるのです。」

---

次の章では、**RAG（検索拡張生成）**を学びます —— AIが最新の知識を取得し、「知識の陳腐化」問題を解決できるようにします！
            `
          }
        }
      ]
    },
    // ============================================
    // 第三章：RAG 检索增强生成
    // ============================================
    {
      id: 'chapter-4',
      number: 4,
      title: { zh: 'RAG 检索增强生成', ja: 'RAG 検索拡張生成' },
      subtitle: { zh: '让AI获取最新知识', ja: 'AIに最新知識を取得させる' },
      sections: [
        {
          id: 'ch4-intro',
          title: { zh: '引言：AI 的知识困境', ja: '序章：AIの知識ジレンマ' },
          content: {
            zh: `
你有没有遇到过这样的情况：

- 问 AI 今天的新闻，它说不知道
- 问公司内部的规章制度，它回答不了
- 问最新的产品信息，它给出过时的答案

这是因为 AI 的知识有**截止日期**，而且它不了解你的**私有数据**。

**RAG（Retrieval-Augmented Generation，检索增强生成）** 就是解决这个问题的关键技术！

---

## AI 知识的局限性

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        AI 知识的两大局限                                │
└─────────────────────────────────────────────────────────────────────────┘

  局限1：知识有截止日期                    局限2：不了解私有数据
  ┌──────────────────────┐               ┌──────────────────────┐
  │                      │               │                      │
  │  训练数据截止到       │               │  AI 不知道：          │
  │  某个时间点           │               │                      │
  │                      │               │  • 你公司的规章制度    │
  │  比如：2024年1月      │               │  • 你的产品文档        │
  │                      │               │  • 你的客户数据        │
  │  之后的事情都不知道    │               │  • 你的内部知识库      │
  │                      │               │                      │
  └──────────────────────┘               └──────────────────────┘

                           RAG 可以解决这两个问题！
\`\`\`

---

## 什么是 RAG？

RAG 的核心思想很简单：

> **先检索相关信息，再让 AI 回答**

就像一个学生考试时可以查资料一样，RAG 让 AI 在回答问题前，先从知识库中找到相关内容。

---

## 本章你将学到

1. **RAG 的工作原理** —— 它是如何运作的
2. **向量数据库** —— RAG 的核心技术
3. **RAG 的应用场景** —— 在哪些地方使用
4. **如何使用 RAG** —— 实际操作方法
5. **RAG 的优缺点** —— 了解它的边界

让我们深入了解这个让 AI "与时俱进"的技术！
            `,
            ja: `
こんな経験はありませんか：

- AIに今日のニュースを聞くと、知らないと言う
- 会社の規則を聞くと、答えられない
- 最新の製品情報を聞くと、古い答えが返ってくる

これはAIの知識に**締め切り日**があり、あなたの**プライベートデータ**を知らないからです。

**RAG（Retrieval-Augmented Generation、検索拡張生成）**がこの問題を解決する重要な技術です！

---

## AI知識の限界

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        AI知識の2つの限界                                │
└─────────────────────────────────────────────────────────────────────────┘

  限界1：知識に締め切りがある              限界2：プライベートデータを知らない
  ┌──────────────────────┐               ┌──────────────────────┐
  │                      │               │                      │
  │  トレーニングデータは  │               │  AIが知らないこと：    │
  │  ある時点で終了        │               │                      │
  │                      │               │  • あなたの会社の規則  │
  │  例：2024年1月        │               │  • あなたの製品ドキュメント │
  │                      │               │  • あなたの顧客データ   │
  │  それ以降のことは知らない│               │  • あなたの社内知識ベース │
  │                      │               │                      │
  └──────────────────────┘               └──────────────────────┘

                           RAGはこの両方を解決できます！
\`\`\`

---

## RAGとは？

RAGの核心的なアイデアはシンプル：

> **まず関連情報を検索し、それからAIに回答させる**

試験中に資料を参照できる学生のように、RAGはAIが質問に答える前に、知識ベースから関連コンテンツを見つけられるようにします。

---

## この章で学ぶこと

1. **RAGの仕組み** —— どのように動作するか
2. **ベクトルデータベース** —— RAGのコア技術
3. **RAGの応用シーン** —— どこで使われるか
4. **RAGの使い方** —— 実際の操作方法
5. **RAGの長所と短所** —— その境界を理解する

AIを「時代に追いつかせる」この技術を深く理解しましょう！
            `
          }
        },
        {
          id: 'ch4-how-it-works',
          title: { zh: '4.1 RAG 的工作原理', ja: '4.1 RAGの仕組み' },
          content: {
            zh: `
让我们看看 RAG 是如何工作的。

### 交互式演示：RAG 工作流程

先通过这个交互式演示直观感受 RAG 的完整流程：

::rag-viz::

---

## RAG 工作流程

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                          RAG 完整流程                                   │
└─────────────────────────────────────────────────────────────────────────┘

  【准备阶段】                           【使用阶段】

   文档/知识                              用户提问
      │                                     │
      ▼                                     ▼
  ┌─────────┐                          ┌─────────┐
  │ 文本分块 │                          │ 问题向量化│
  └─────────┘                          └─────────┘
      │                                     │
      ▼                                     ▼
  ┌─────────┐                          ┌─────────┐
  │ 向量化   │                          │ 相似度搜索│ ◄── 从向量数据库
  └─────────┘                          └─────────┘
      │                                     │
      ▼                                     ▼
  ┌─────────┐                          ┌─────────┐
  │向量数据库│                          │ 获取相关文档│
  └─────────┘                          └─────────┘
                                           │
                                           ▼
                                      ┌─────────┐
                                      │组合提示词│
                                      │(问题+文档)│
                                      └─────────┘
                                           │
                                           ▼
                                      ┌─────────┐
                                      │ LLM 回答 │
                                      └─────────┘
\`\`\`

---

## 核心步骤详解

### 1. 文本分块（Chunking）

把长文档切分成小块：

\`\`\`
原始文档（10000字）
    │
    ▼
┌─────┐ ┌─────┐ ┌─────┐ ┌─────┐
│块1  │ │块2  │ │块3  │ │...  │
│500字│ │500字│ │500字│ │     │
└─────┘ └─────┘ └─────┘ └─────┘
\`\`\`

为什么要分块？
- LLM 有上下文长度限制
- 小块更容易精确匹配
- 提高检索效率

---

### 2. 向量化（Embedding）

把文本转换成数字向量：

\`\`\`
"人工智能是..." ──▶ [0.12, -0.34, 0.56, ..., 0.78]
                        （高维向量）
\`\`\`

**为什么用向量？**

- 语义相似的内容，向量也相似
- 可以进行数学计算（相似度）
- 比关键词匹配更智能

---

### 3. 相似度搜索

当用户提问时：
1. 把问题也转成向量
2. 在向量数据库中找最相似的文档块
3. 返回 top-K 个最相关的结果

\`\`\`
用户问题向量  ──▶  在向量空间中找最近的邻居
     ●              ┌─────────────────────┐
                    │    ●  ●             │
                    │  ●      ●           │
                    │    ●●      找到！   │
                    │      ⬛ ◄───────    │
                    │    ●                │
                    └─────────────────────┘
\`\`\`

---

### 4. 组合提示词

把检索到的内容和用户问题组合：

\`\`\`
【系统提示】
你是一个助手，请根据以下参考资料回答问题。
如果资料中没有相关信息，请说"我没有找到相关信息"。

【参考资料】
{检索到的文档块1}
{检索到的文档块2}
{检索到的文档块3}

【用户问题】
{用户的问题}
\`\`\`

---

## 本节要点

1. **两个阶段** —— 准备阶段（建索引）+ 使用阶段（检索回答）
2. **文本分块** —— 把长文档切成小块
3. **向量化** —— 把文本转成数字向量
4. **相似度搜索** —— 找到最相关的内容
5. **组合提示** —— 把检索结果和问题一起给 LLM
            `,
            ja: `
RAGがどのように動作するか見てみましょう。

### インタラクティブデモ：RAGワークフロー

このインタラクティブデモでRAGの完全なフローを直感的に体験してください：

::rag-viz::

---

## RAGワークフロー

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                          RAG 完全フロー                                  │
└─────────────────────────────────────────────────────────────────────────┘

  【準備段階】                           【使用段階】

   ドキュメント/知識                        ユーザーの質問
      │                                     │
      ▼                                     ▼
  ┌─────────┐                          ┌─────────┐
  │テキスト分割│                          │質問をベクトル化│
  └─────────┘                          └─────────┘
      │                                     │
      ▼                                     ▼
  ┌─────────┐                          ┌─────────┐
  │ ベクトル化 │                          │類似度検索│ ◄── ベクトルDBから
  └─────────┘                          └─────────┘
      │                                     │
      ▼                                     ▼
  ┌─────────┐                          ┌─────────┐
  │ベクトルDB │                          │関連文書を取得│
  └─────────┘                          └─────────┘
                                           │
                                           ▼
                                      ┌─────────┐
                                      │プロンプト組み合わせ│
                                      │(質問+文書)│
                                      └─────────┘
                                           │
                                           ▼
                                      ┌─────────┐
                                      │ LLM が回答 │
                                      └─────────┘
\`\`\`

---

## コアステップの詳細

### 1. テキスト分割（Chunking）

長いドキュメントを小さな塊に分割：

\`\`\`
元のドキュメント（10000文字）
    │
    ▼
┌─────┐ ┌─────┐ ┌─────┐ ┌─────┐
│チャンク1│ │チャンク2│ │チャンク3│ │...  │
│500文字│ │500文字│ │500文字│ │     │
└─────┘ └─────┘ └─────┘ └─────┘
\`\`\`

なぜ分割するのか？
- LLMにはコンテキスト長の制限がある
- 小さな塊の方が正確にマッチング
- 検索効率が向上

---

### 2. ベクトル化（Embedding）

テキストを数値ベクトルに変換：

\`\`\`
"人工知能は..." ──▶ [0.12, -0.34, 0.56, ..., 0.78]
                        （高次元ベクトル）
\`\`\`

**なぜベクトルを使うのか？**

- 意味的に似た内容はベクトルも似ている
- 数学的計算（類似度）が可能
- キーワードマッチングより賢い

---

### 3. 類似度検索

ユーザーが質問したとき：
1. 質問もベクトルに変換
2. ベクトルDBで最も似た文書チャンクを探す
3. top-K個の最も関連する結果を返す

\`\`\`
質問ベクトル  ──▶  ベクトル空間で最近傍を探す
     ●              ┌─────────────────────┐
                    │    ●  ●             │
                    │  ●      ●           │
                    │    ●●      見つけた！│
                    │      ⬛ ◄───────    │
                    │    ●                │
                    └─────────────────────┘
\`\`\`

---

### 4. プロンプト組み合わせ

検索した内容とユーザーの質問を組み合わせ：

\`\`\`
【システムプロンプト】
あなたはアシスタントです。以下の参考資料に基づいて質問に答えてください。
資料に関連情報がない場合は「関連情報が見つかりませんでした」と言ってください。

【参考資料】
{検索された文書チャンク1}
{検索された文書チャンク2}
{検索された文書チャンク3}

【ユーザーの質問】
{ユーザーの質問}
\`\`\`

---

## このセクションのポイント

1. **2つの段階** —— 準備段階（インデックス作成）+ 使用段階（検索と回答）
2. **テキスト分割** —— 長いドキュメントを小さく分割
3. **ベクトル化** —— テキストを数値ベクトルに変換
4. **類似度検索** —— 最も関連するコンテンツを見つける
5. **プロンプト組み合わせ** —— 検索結果と質問をLLMに渡す
            `
          }
        },
        {
          id: 'ch4-applications',
          title: { zh: '4.2 RAG 的应用场景', ja: '4.2 RAGの応用シーン' },
          content: {
            zh: `
RAG 在很多场景中都有广泛应用。

---

## 企业知识库问答

\`\`\`
场景：员工想查询公司规章制度

传统方式：在几十个文档中搜索关键词
RAG 方式：直接问 AI，它会从知识库中找到答案

例子：
问："年假怎么申请？"
答："根据公司员工手册第5章，年假申请需要提前3天
    在OA系统提交，经主管审批后生效..."
\`\`\`

---

## 客服智能问答

\`\`\`
场景：客户咨询产品问题

优势：
• 24小时自动回答
• 答案基于官方文档
• 减少人工客服压力

例子：
问："这个产品支持 Mac 吗？"
答："根据产品说明书，本产品支持 macOS 10.15 及以上版本..."
\`\`\`

---

## 个人知识管理

\`\`\`
场景：管理你的笔记、文档、收藏

使用方式：
1. 把你的笔记、PDF、网页收藏导入
2. 用自然语言提问
3. AI 从你的知识库中找答案

例子：
问："我之前看过一篇关于时间管理的文章，核心观点是什么？"
答："根据你收藏的《高效能人士的七个习惯》笔记，
    核心观点是要事第一，区分紧急和重要..."
\`\`\`

---

## 代码文档助手

\`\`\`
场景：查询项目代码文档

优势：
• 不需要记住所有 API
• 快速找到使用方法
• 结合代码上下文回答

例子：
问："这个项目的数据库连接怎么配置？"
答："根据项目 README 和 config.py，
    数据库配置在 .env 文件中设置..."
\`\`\`

---

## 应用场景对比

| 场景 | 知识来源 | 典型问题 |
|------|----------|----------|
| 企业知识库 | 规章制度、培训材料 | 报销流程是什么？ |
| 客服问答 | 产品文档、FAQ | 如何退货？ |
| 个人知识管理 | 笔记、收藏、PDF | 我之前学过的XX是什么？ |
| 代码助手 | 代码、文档、注释 | 这个函数怎么用？ |
| 学术研究 | 论文、报告 | 某领域最新进展？ |

---

## 本节要点

1. **企业应用** —— 知识库问答、客服系统
2. **个人应用** —— 笔记管理、学习助手
3. **开发应用** —— 代码文档、API 查询
4. **共同特点** —— 基于私有/专业知识回答
            `,
            ja: `
RAGは多くのシーンで広く活用されています。

---

## 企業ナレッジベースQ&A

\`\`\`
シーン：従業員が会社の規則を調べたい

従来の方法：数十のドキュメントでキーワード検索
RAG方式：AIに直接質問、知識ベースから答えを見つける

例：
質問：「有給休暇の申請方法は？」
回答：「社員ハンドブック第5章によると、有給休暇の申請は
      3日前までにOAシステムで提出し、上司の承認後に有効になります...」
\`\`\`

---

## カスタマーサービスQ&A

\`\`\`
シーン：顧客が製品について質問

メリット：
• 24時間自動回答
• 公式ドキュメントに基づく回答
• 人間のサポート負担を軽減

例：
質問：「この製品はMacに対応していますか？」
回答：「製品マニュアルによると、本製品はmacOS 10.15以上に対応しています...」
\`\`\`

---

## 個人ナレッジ管理

\`\`\`
シーン：あなたのノート、ドキュメント、お気に入りを管理

使い方：
1. ノート、PDF、ブックマークをインポート
2. 自然言語で質問
3. AIがあなたの知識ベースから答えを見つける

例：
質問：「以前読んだ時間管理についての記事、核心的なポイントは何でしたっけ？」
回答：「あなたがブックマークした『7つの習慣』のノートによると、
      核心ポイントは重要事項を優先し、緊急と重要を区別することです...」
\`\`\`

---

## コードドキュメントアシスタント

\`\`\`
シーン：プロジェクトのコードドキュメントを照会

メリット：
• すべてのAPIを覚える必要がない
• 使用方法をすばやく見つける
• コードコンテキストと組み合わせて回答

例：
質問：「このプロジェクトのデータベース接続はどう設定する？」
回答：「プロジェクトのREADMEとconfig.pyによると、
      データベース設定は.envファイルで設定します...」
\`\`\`

---

## 応用シーン比較

| シーン | 知識ソース | 典型的な質問 |
|--------|------------|--------------|
| 企業ナレッジベース | 規則、トレーニング資料 | 経費精算のプロセスは？ |
| カスタマーサービス | 製品ドキュメント、FAQ | 返品方法は？ |
| 個人ナレッジ管理 | ノート、ブックマーク、PDF | 以前学んだXXって何？ |
| コードアシスタント | コード、ドキュメント、コメント | この関数の使い方は？ |
| 学術研究 | 論文、レポート | この分野の最新動向は？ |

---

## このセクションのポイント

1. **企業応用** —— ナレッジベースQ&A、カスタマーサービス
2. **個人応用** —— ノート管理、学習アシスタント
3. **開発応用** —— コードドキュメント、APIクエリ
4. **共通点** —— プライベート/専門知識に基づく回答
            `
          }
        },
        {
          id: 'ch4-optimization',
          title: { zh: '4.3 RAG 优化技巧', ja: '4.3 RAG 最適化テクニック' },
          content: {
            zh: `
## 让 RAG 更精准的关键技巧

RAG 系统的效果取决于每个环节的优化。以下是提升 RAG 性能的核心技巧。

---

## 分块策略：如何切分文档

分块（Chunking）是 RAG 的基础，直接影响检索质量。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                          分块策略对比                                    │
└─────────────────────────────────────────────────────────────────────────┘

  固定大小分块                         语义分块
  ────────────                        ──────────
  ┌─────────────┐                    ┌─────────────┐
  │ 500 字符    │                    │  完整段落    │
  ├─────────────┤                    ├─────────────┤
  │ 500 字符    │                    │  完整章节    │
  ├─────────────┤                    ├─────────────┤
  │ 500 字符    │                    │  完整概念    │
  └─────────────┘                    └─────────────┘
       │                                  │
       ▼                                  ▼
  可能切断语义                        保持语义完整
  简单但不精准                        复杂但更准确
\`\`\`

### 最佳实践

| 策略 | 适用场景 | 典型大小 |
|------|----------|----------|
| 固定大小 + 重叠 | 通用场景 | 512 tokens + 50 重叠 |
| 句子分块 | 精确问答 | 3-5 句 |
| 段落分块 | 文档摘要 | 按 \\n\\n 分割 |
| 递归分块 | 结构化文档 | 先章节，再段落 |

---

## 混合检索：向量 + 关键词

单一向量检索可能遗漏关键词匹配，混合检索结合两者优势。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                          混合检索架构                                    │
└─────────────────────────────────────────────────────────────────────────┘

                          用户查询
                             │
              ┌──────────────┼──────────────┐
              │                             │
              ▼                             ▼
      ┌─────────────┐               ┌─────────────┐
      │  向量检索   │               │  关键词检索  │
      │ (语义相似)  │               │ (BM25/TF-IDF)|
      └──────┬──────┘               └──────┬──────┘
             │                              │
             │  相似度: 0.85               │  匹配度: 0.72
             │  相似度: 0.82               │  匹配度: 0.68
             │  相似度: 0.79               │  匹配度: 0.65
             │                              │
             └──────────────┬──────────────┘
                            │
                    ┌───────▼───────┐
                    │   融合排序    │
                    │ (RRF / 加权)  │
                    └───────┬───────┘
                            │
                            ▼
                    ┌───────────────┐
                    │  最终结果 Top-K│
                    └───────────────┘
\`\`\`

**融合公式（RRF）：**
\`\`\`
score = Σ 1/(k + rank_i)

其中：k 通常取 60
     rank_i 是该文档在第 i 个检索结果中的排名
\`\`\`

---

## 查询改写：提升检索召回率

用户的原始查询可能不够精确，通过改写提升匹配度。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                          查询改写技巧                                    │
└─────────────────────────────────────────────────────────────────────────┘

原始查询: "公司的请假流程是什么？"
                │
                ▼
        ┌───────────────┐
        │   查询改写    │
        └───────────────┘
                │
    ┌───────────┼───────────┐
    │           │           │
    ▼           ▼           ▼
  同义扩展    问题分解    假设文档

"请假流程"   "如何请假"   "请假审批流程
"休假申请"   "请假需要    包括：提交申请、
"事假流程"    什么材料"   主管审批..."
\`\`\`

### HyDE（假设文档嵌入）

让 LLM 先生成一个"假设的答案"，用这个答案去检索：

\`\`\`
用户问题: "如何配置 Nginx 反向代理？"
                │
                ▼
┌─────────────────────────────────────────────────────────────────────────┐
│ LLM 生成假设答案:                                                        │
│ "要配置 Nginx 反向代理，需要在 server 块中使用 location 和              │
│  proxy_pass 指令。首先编辑 nginx.conf，添加 upstream 定义后端..."        │
└─────────────────────────────────────────────────────────────────────────┘
                │
                ▼
        用假设答案向量检索
                │
                ▼
        返回真正相关的文档
\`\`\`

---

## Reranker：精排提升准确率

初步检索后，用更强的模型对结果重新排序。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                          两阶段检索                                      │
└─────────────────────────────────────────────────────────────────────────┘

                    ┌───────────────┐
                    │  向量检索     │
                    │  (快速召回)   │
                    └───────┬───────┘
                            │
                    返回 Top-100
                            │
                            ▼
                    ┌───────────────┐
                    │  Reranker     │
                    │  (精确排序)   │
                    └───────┬───────┘
                            │
                    返回 Top-5
                            │
                            ▼
                    ┌───────────────┐
                    │    LLM 生成   │
                    └───────────────┘

召回阶段：快但不够准 → 精排阶段：准但较慢
\`\`\`

### 常用 Reranker

| 模型 | 特点 | 推荐场景 |
|------|------|----------|
| Cohere Rerank | 效果好，API 调用 | 生产环境 |
| BGE Reranker | 开源，可本地部署 | 私有化部署 |
| Cross-Encoder | 精确但慢 | 小规模数据 |

---

## 实战建议

### 1. 从简单开始
\`\`\`
初始配置:
├── 分块: 512 tokens + 10% 重叠
├── Embedding: text-embedding-3-small
├── 检索: Top-5
└── 无 Reranker
\`\`\`

### 2. 逐步优化
\`\`\`
发现问题 → 针对性优化:
├── 召回不全 → 混合检索 / 查询改写
├── 相关性差 → 添加 Reranker
├── 语义断裂 → 调整分块策略
└── 响应慢 → 缓存 / 预计算
\`\`\`

### 3. 评估指标
\`\`\`
检索质量:
├── Recall@K: 相关文档在 Top-K 的比例
├── MRR: 第一个相关结果的位置
└── NDCG: 排序质量评分

端到端:
├── 答案准确率
├── 用户满意度
└── 响应延迟
\`\`\`

---

## 本节要点

1. **分块策略** —— 保持语义完整性
2. **混合检索** —— 向量 + 关键词双管齐下
3. **查询改写** —— HyDE 等技巧提升召回
4. **Reranker** —— 两阶段检索提升精度
            `,
            ja: `
## RAGをより正確にするための重要なテクニック

RAGシステムの効果は各段階の最適化に依存します。以下はRAG性能を向上させるコアテクニックです。

---

## チャンキング戦略：ドキュメントの分割方法

チャンキング（Chunking）はRAGの基礎であり、検索品質に直接影響します。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                          チャンキング戦略比較                              │
└─────────────────────────────────────────────────────────────────────────┘

  固定サイズチャンキング                 セマンティックチャンキング
  ──────────────────                    ────────────────────
  ┌─────────────┐                      ┌─────────────┐
  │ 500 文字    │                      │  完全な段落  │
  ├─────────────┤                      ├─────────────┤
  │ 500 文字    │                      │  完全な章    │
  ├─────────────┤                      ├─────────────┤
  │ 500 文字    │                      │  完全な概念  │
  └─────────────┘                      └─────────────┘
       │                                    │
       ▼                                    ▼
  意味が切れる可能性                    意味の完全性を保持
  シンプルだが不正確                    複雑だが正確
\`\`\`

### ベストプラクティス

| 戦略 | 適用シーン | 典型的なサイズ |
|------|----------|----------|
| 固定サイズ + オーバーラップ | 汎用 | 512 tokens + 50 重複 |
| 文チャンキング | 精密Q&A | 3-5 文 |
| 段落チャンキング | ドキュメント要約 | \\n\\n で分割 |
| 再帰的チャンキング | 構造化文書 | 先に章、次に段落 |

---

## ハイブリッド検索：ベクトル + キーワード

単一のベクトル検索ではキーワードマッチが漏れる可能性があり、ハイブリッド検索は両方の利点を組み合わせます。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                          ハイブリッド検索アーキテクチャ                     │
└─────────────────────────────────────────────────────────────────────────┘

                          ユーザークエリ
                             │
              ┌──────────────┼──────────────┐
              │                             │
              ▼                             ▼
      ┌─────────────┐               ┌─────────────┐
      │  ベクトル検索 │               │ キーワード検索│
      │ (意味類似性) │               │ (BM25/TF-IDF)|
      └──────┬──────┘               └──────┬──────┘
             │                              │
             │  類似度: 0.85               │  マッチ度: 0.72
             │  類似度: 0.82               │  マッチ度: 0.68
             │  類似度: 0.79               │  マッチ度: 0.65
             │                              │
             └──────────────┬──────────────┘
                            │
                    ┌───────▼───────┐
                    │   融合ランキング │
                    │ (RRF / 重み付け) │
                    └───────┬───────┘
                            │
                            ▼
                    ┌───────────────┐
                    │  最終結果 Top-K│
                    └───────────────┘
\`\`\`

**融合式（RRF）：**
\`\`\`
score = Σ 1/(k + rank_i)

ここで：k は通常 60
     rank_i は i番目の検索結果でのドキュメントの順位
\`\`\`

---

## クエリ書き換え：検索リコール率の向上

ユーザーの元のクエリは十分に正確でない可能性があり、書き換えでマッチ度を向上させます。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                          クエリ書き換えテクニック                          │
└─────────────────────────────────────────────────────────────────────────┘

元のクエリ: "会社の休暇申請プロセスは？"
                │
                ▼
        ┌───────────────┐
        │   クエリ書き換え │
        └───────────────┘
                │
    ┌───────────┼───────────┐
    │           │           │
    ▼           ▼           ▼
  同義語展開   質問分解    仮想ドキュメント

"休暇申請"   "休暇の     "休暇承認プロセス
"有給申請"   取り方は？"  には：申請提出、
"欠勤届"    "必要な      上司承認..."
            書類は？"
\`\`\`

### HyDE（仮想ドキュメント埋め込み）

LLMにまず「仮想の回答」を生成させ、その回答で検索します：

\`\`\`
ユーザーの質問: "Nginxのリバースプロキシ設定方法は？"
                │
                ▼
┌─────────────────────────────────────────────────────────────────────────┐
│ LLM が仮想回答を生成:                                                     │
│ "Nginxのリバースプロキシを設定するには、serverブロック内で location と     │
│  proxy_pass ディレクティブを使用します。まず nginx.conf を編集し..."       │
└─────────────────────────────────────────────────────────────────────────┘
                │
                ▼
        仮想回答でベクトル検索
                │
                ▼
        本当に関連するドキュメントを返す
\`\`\`

---

## Reranker：精密ランキングで精度向上

初期検索後、より強力なモデルで結果を再ランキングします。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                          2段階検索                                       │
└─────────────────────────────────────────────────────────────────────────┘

                    ┌───────────────┐
                    │  ベクトル検索  │
                    │  (高速リコール) │
                    └───────┬───────┘
                            │
                    Top-100 を返す
                            │
                            ▼
                    ┌───────────────┐
                    │  Reranker     │
                    │  (精密ソート)  │
                    └───────┬───────┘
                            │
                    Top-5 を返す
                            │
                            ▼
                    ┌───────────────┐
                    │    LLM 生成   │
                    └───────────────┘

リコール段階：速いが不正確 → 精密段階：正確だが遅い
\`\`\`

### よく使う Reranker

| モデル | 特徴 | 推奨シーン |
|------|------|----------|
| Cohere Rerank | 効果良好、API呼び出し | 本番環境 |
| BGE Reranker | オープンソース、ローカル展開可 | プライベート展開 |
| Cross-Encoder | 正確だが遅い | 小規模データ |

---

## 実践的なアドバイス

### 1. シンプルから始める
\`\`\`
初期設定:
├── チャンキング: 512 tokens + 10% オーバーラップ
├── Embedding: text-embedding-3-small
├── 検索: Top-5
└── Reranker なし
\`\`\`

### 2. 段階的に最適化
\`\`\`
問題発見 → 的を絞った最適化:
├── リコール不足 → ハイブリッド検索 / クエリ書き換え
├── 関連性が低い → Reranker 追加
├── 意味が分断 → チャンキング戦略を調整
└── レスポンスが遅い → キャッシュ / 事前計算
\`\`\`

### 3. 評価指標
\`\`\`
検索品質:
├── Recall@K: Top-K での関連ドキュメントの割合
├── MRR: 最初の関連結果の位置
└── NDCG: ランキング品質スコア

エンドツーエンド:
├── 回答精度
├── ユーザー満足度
└── レスポンス遅延
\`\`\`

---

## このセクションのポイント

1. **チャンキング戦略** —— 意味の完全性を保つ
2. **ハイブリッド検索** —— ベクトル + キーワードの両方を活用
3. **クエリ書き換え** —— HyDE などでリコール向上
4. **Reranker** —— 2段階検索で精度向上
            `
          }
        },
        {
          id: 'ch4-summary',
          title: { zh: '4.4 本章小结', ja: '4.4 この章のまとめ' },
          content: {
            zh: `
## RAG 核心概念回顾

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                         RAG 知识地图                                    │
└─────────────────────────────────────────────────────────────────────────┘

                               RAG
                                │
         ┌──────────────────────┼──────────────────────┐
         │                      │                      │
         ▼                      ▼                      ▼
    ┌──────────┐          ┌──────────┐          ┌──────────┐
    │  工作流程 │          │  核心技术 │          │  应用场景 │
    └──────────┘          └──────────┘          └──────────┘
         │                      │                      │
    ┌────┼────┐          ┌──────┼──────┐          ┌────┼────┐
    │    │    │          │      │      │          │    │    │
    ▼    ▼    ▼          ▼      ▼      ▼          ▼    ▼    ▼
  检索   拡充   生成     分块   向量化   数据库    企业  个人  开发
\`\`\`

---

## RAG vs 微调

| 对比项 | RAG | 微调(Fine-tuning) |
|--------|-----|-------------------|
| 知识更新 | 只需更新文档 | 需要重新训练 |
| 成本 | 较低 | 较高 |
| 实现难度 | 简单 | 复杂 |
| 适用场景 | 知识库问答 | 特定领域专家 |
| 可解释性 | 可追溯来源 | 黑盒 |

---

## RAG 的优缺点

### 优点
- ✅ 可以实时更新知识
- ✅ 答案可追溯来源
- ✅ 不需要训练模型
- ✅ 保护私有数据

### 缺点
- ❌ 依赖检索质量
- ❌ 需要维护知识库
- ❌ 有上下文长度限制
- ❌ 可能检索到无关内容

---

## 快速行动清单

- [ ] 了解常用 AI 工具的知识截止日期
- [ ] 思考工作中有哪些知识可以用 RAG 管理
- [ ] 尝试使用一个 RAG 工具（如 Notion AI、企业知识库）
- [ ] 关注 RAG 技术的发展动态

---

## 关键金句

> "RAG 让 AI 从'博学但过时'变成'与时俱进且专业'。"

> "检索增强生成是连接 AI 与现实世界知识的桥梁。"

---

## 全书总结

恭喜你完成了这本 AI 进阶实战指南！

你学到了：
1. **提示词工程** —— 与 AI 高效对话的艺术
2. **AI Agent** —— 让 AI 自主完成复杂任务
3. **RAG 技术** —— 让 AI 获取最新、私有知识

**AI 的世界在不断快速发展，保持学习，保持好奇！**

*"最好的投资是对自己认知的投资。"*
            `,
            ja: `
## RAGコア概念の復習

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                         RAG 知識マップ                                   │
└─────────────────────────────────────────────────────────────────────────┘

                               RAG
                                │
         ┌──────────────────────┼──────────────────────┐
         │                      │                      │
         ▼                      ▼                      ▼
    ┌──────────┐          ┌──────────┐          ┌──────────┐
    │ ワークフロー │          │  コア技術  │          │ 応用シーン │
    └──────────┘          └──────────┘          └──────────┘
         │                      │                      │
    ┌────┼────┐          ┌──────┼──────┐          ┌────┼────┐
    │    │    │          │      │      │          │    │    │
    ▼    ▼    ▼          ▼      ▼      ▼          ▼    ▼    ▼
  検索   拡張   生成      分割  ベクトル化 DB      企業  個人   開発
\`\`\`

---

## RAG vs ファインチューニング

| 比較項目 | RAG | ファインチューニング |
|----------|-----|---------------------|
| 知識の更新 | ドキュメント更新のみ | 再トレーニングが必要 |
| コスト | 低い | 高い |
| 実装難易度 | 簡単 | 複雑 |
| 適用シーン | ナレッジベースQ&A | 特定分野の専門家 |
| 説明可能性 | ソースを追跡可能 | ブラックボックス |

---

## RAGの長所と短所

### 長所
- ✅ 知識をリアルタイムで更新可能
- ✅ 回答のソースを追跡可能
- ✅ モデルのトレーニング不要
- ✅ プライベートデータを保護

### 短所
- ❌ 検索品質に依存
- ❌ 知識ベースのメンテナンスが必要
- ❌ コンテキスト長の制限がある
- ❌ 無関係なコンテンツを検索する可能性

---

## クイックアクションリスト

- [ ] よく使うAIツールの知識締め切り日を確認
- [ ] 仕事でRAGで管理できる知識を考える
- [ ] RAGツール（Notion AI、企業ナレッジベースなど）を試す
- [ ] RAG技術の発展動向をフォロー

---

## 重要な格言

> 「RAGはAIを『博識だが古い』から『時代に追いついて専門的』に変えます。」

> 「検索拡張生成は、AIと現実世界の知識をつなぐ架け橋です。」

---

## 全書のまとめ

このAI進級実践ガイドを完了おめでとうございます！

学んだこと：
1. **プロンプトエンジニアリング** —— AIとの効率的な対話術
2. **AI Agent** —— AIに複雑なタスクを自律的に完了させる
3. **RAG技術** —— AIに最新・プライベート知識を取得させる

**AIの世界は急速に発展し続けています。学び続け、好奇心を持ち続けましょう！**

*「最高の投資は、自分の認知への投資です。」*
            `
          }
        }
      ]
    },
  ]
};

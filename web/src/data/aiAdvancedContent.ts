// AI 进阶实战电子书内容
// 设计理念：面向有基础的用户，深入实践技巧

import { Book } from './aiBookContent';

// ============================================
// AI 进阶实战 - 实用技巧篇
// ============================================
export const aiAdvancedBook: Book = {
  id: 'ai-advanced',
  title: {
    zh: 'AI 进阶实战',
    ja: 'AI 実践上級編'
  },
  subtitle: {
    zh: '掌握 AI 的高级使用技巧',
    ja: 'AIの上級テクニックをマスターする'
  },
  author: 'StudyForge',
  chapters: [
    // ============================================
    // 序章：AI 动态时间轴
    // ============================================
    {
      id: 'chapter-0',
      number: 0,
      title: { zh: 'AI 动态时间轴', ja: 'AI動向タイムライン' },
      subtitle: { zh: '追踪最新 AI 技术发展', ja: '最新AI技術の動向を追跡' },
      sections: [
        {
          id: 'ch0-timeline',
          title: { zh: '2025 年 AI 大事记', ja: '2025年AI大事記' },
          content: {
            zh: `
## 🚀 AI 技术动态时间轴

追踪最新 AI 发展动态，点击链接深入了解相关知识点。

---

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        2025 年 AI 技术发展时间轴                          │
└─────────────────────────────────────────────────────────────────────────┘

  2025.01                                                          2025.07
     │                                                                │
     ▼                                                                ▼
─────●────────●────────●────────●────────●────────●────────●────────●─────
     │        │        │        │        │        │        │        │
   Claude   GPT-5    Claude   Codex    Sub-     Skills   Chrome   最新
   Opus4.5  发布     Code     Agent    agents   标准化   集成     动态
   发布              MCP      Skills   功能     通用
\`\`\`

---

## 📅 详细时间轴

### 🔵 2025年1月 - Claude Opus 4.5 发布

Anthropic 发布 Claude Opus 4.5，成为当时最强大的 AI 模型。

| 特性 | 说明 |
|------|------|
| 知识截止 | 2025年1月 |
| 长文本能力 | 可输出 1.2 万+ 字符 |
| 创作能力 | 文学创作、设计感更强 |

📖 **相关知识点**：→ 本书 [3.10 AI 模型对比与选型]

---

### 🔵 2025年2月 - GPT-5 系列发布

OpenAI 陆续发布 GPT-5.0、5.1、5.2，持续迭代改进。

**实测对比要点**：
- 长文本生成：Claude 约 12000 字符，GPT-5.1 约 6900 字符
- 数学编程：GPT-5.1 更强
- 浏览器自动化：GPT-5.1 略胜

📖 **深入学习**：→ 本书 [3.10 Claude vs GPT 实测对比]

---

### 🟢 2025年3月 - Claude Code 正式发布

基于 MCP 协议的 AI 编程助手 Claude Code 发布，改变开发方式。

**核心特性**：
- ✅ 直接读写代码文件
- ✅ 运行终端命令
- ✅ MCP 协议扩展能力

**Boris 核心经验**：Opus 4.5 + Plan 模式、多实例并行、Slash 命令自动化

📖 **深入学习**：→ 本书 [3.3 MCP：AI 的"万能接口"] 和 [序章：大佬经验分享]

---

### 🟢 2025年4月 - Sub-agents 功能上线

Claude Code 支持 Sub-agents，实现专家团队协作模式。

\`\`\`
Sub-agents 架构示意
───────────────────
        主 Agent
            │
    ┌───────┼───────┐
    ▼       ▼       ▼
  代码    测试    文档
  审查    专家    生成
\`\`\`

**核心价值**：上下文隔离避免污染、专业化分工成功率更高、可复用团队共享

📖 **深入学习**：→ 本书 [3.3 Sub-agents 专家团队协作]

---

### 🟡 2025年5月 - Agent Skills 成为行业标准

OpenAI Codex 采用 Anthropic 的 Skills 规范，实现跨平台通用。

| 平台 | Skills 支持 |
|------|------------|
| Claude Code | ✅ 原生支持 |
| GPT Codex | ✅ 新增支持 |
| Cursor | ✅ 兼容 |

> 💡 你写的 Skills 可以在 Claude 和 Codex 之间通用！

**重大意义**：一次编写到处运行，团队知识资产化，AI 能力可定制

📖 **深入学习**：→ 本书 [3.9 OpenAI Codex 使用指南]

---

### 🟡 2025年6月 - Claude Code Chrome 集成

Claude Code 原生支持 Chrome 浏览器，实现端到端自动化测试。

**应用场景**：
- 前端 UI 调试
- 浏览器自动化测试
- 保留登录状态和扩展插件

**验证循环技巧**：让 Claude 通过 Chrome 自动验证 UI，直到功能和体验满意，质量可提升 2-3 倍

📖 **深入学习**：→ 本书 [3.3 Boris 的验证反馈循环]

---

### 🔴 2025年7月 - 最新动态

**最新工具一览：**

| 工具 | 功能 | 亮点 |
|------|------|------|
| Ralph Wiggum | AI 自动迭代修复 | Bug 到完美应用只需一条命令 |
| Claudia | Claude Code GUI | 告别命令行，可视化操作 |
| SuperClaude | 能力增强 300% | 19 个命令 + 9 大专家角色 |
| Claude Code PM | 并行开发 | GitHub Issues 秒变独立分支 |
| Kilo Code | 融合 Cline + Roo | 5 种智能模式切换 |

**发展趋势总结**：
- 📊 Spec-Driven 开发逐渐取代 Vibe Coding（先写规格再编码）
- 🔧 上下文工程比提示工程效果好 10 倍（给足背景信息）
- 🤝 多 AI 协作成为常态（Claude + GPT + Gemini 各取所长）

📖 **深入学习**：→ 本书 [序章：大佬经验分享] 有 SuperClaude 完整使用指南

---

## 🗺️ 本书导航

> 💡 **使用左侧目录可快速跳转到任意章节**

| 章节 | 内容 |
|------|------|
| 第0章 | AI 动态时间轴 - 追踪2025年AI发展 |
| 第2章 | 提示词工程进阶 - 与AI对话的艺术 |
| 第3章 | AI Agents 智能体 - MCP、Claude Code等 |
| 第4章 | RAG 检索增强生成 - 让AI获取新知识 |
| 第1章 | 大模型技术深度解析 - Transformer与微调 |

---

> 📌 **提示**：本时间轴会持续更新，记录 AI 领域重要发展节点。
            `,
            ja: `
## 🚀 AI技術動向タイムライン

最新のAI発展動向を追跡し、リンクをクリックして関連知識を深く理解しましょう。

---

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        2025年 AI技術発展タイムライン                       │
└─────────────────────────────────────────────────────────────────────────┘

  2025.01                                                          2025.07
     │                                                                │
─────●────────●────────●────────●────────●────────●────────●────────●─────
   Claude   GPT-5    Claude   Codex    Sub-     Skills   Chrome   最新
   Opus4.5  発表     Code     Agent    agents   標準化   統合     動向
\`\`\`

---

## 📅 詳細タイムライン

### 🔵 2025年1月 - Claude Opus 4.5 リリース

AnthropicがClaude Opus 4.5をリリース。当時最強のAIモデルに。

📖 **関連知識**：→ 本書 [3.5 AIモデル比較と選定]

---

### 🔵 2025年2月 - GPT-5シリーズ発表

OpenAIがGPT-5.0、5.1、5.2を順次リリース。

**実測比較ポイント**：長文生成はClaude優位（約12000字 vs 6900字）、数学・プログラミングはGPT-5.1が強い

📖 **詳細**：→ 本書 [3.10 Claude vs GPT 実測比較]

---

### 🟢 2025年3月 - Claude Code 正式リリース

MCPベースのAIプログラミングアシスタントがリリース。

**Borisの核心経験**：Opus 4.5 + Planモード、マルチインスタンス並列、Slashコマンド自動化

📖 **詳細**：→ 本書 [3.3 MCP] と [序章：エキスパートの知見]

---

### 🟢 2025年4月 - Sub-agents 機能リリース

Claude CodeがSub-agentsをサポート、専門家チーム協力を実現。

**コアバリュー**：コンテキスト分離で汚染防止、専門化分業で成功率向上、再利用可能

📖 **詳細**：→ 本書 [3.3 Sub-agents 専門家チーム協力]

---

### 🟡 2025年5月 - Agent Skills が業界標準に

OpenAI CodexがAnthropicのSkills仕様を採用。

**重要な意義**：一度作成すればどこでも使える、チーム知識の資産化、AIカスタマイズ可能

📖 **詳細**：→ 本書 [3.9 OpenAI Codex 使用ガイド]

---

### 🟡 2025年6月 - Claude Code Chrome 統合

Claude CodeがChromeブラウザをネイティブサポート。

**検証ループ技術**：ClaudeがChromeでUI自動検証、品質2〜3倍向上

---

### 🔴 2025年7月 - 最新動向

| ツール | 機能 | ハイライト |
|--------|------|-----------|
| Ralph Wiggum | AI自動反復修正 | バグから完璧なアプリへ一コマンド |
| Claudia | Claude Code GUI | コマンドライン不要、視覚化操作 |
| SuperClaude | 能力強化300% | 19コマンド + 9エキスパートロール |
| Kilo Code | Cline + Roo融合 | 5つのスマートモード |

**トレンド**：
- Spec-Driven開発がVibeコーディングに代わる
- コンテキストエンジニアリングはプロンプトの10倍効果的
- マルチAI協力が常態化（Claude + GPT + Gemini）

📖 **詳細**：→ 本書 [序章：エキスパートの知見] に SuperClaude 完全ガイド

---

## 🗺️ クイックナビゲーション

| 知りたいこと | ジャンプ先 |
|-------------|----------|
| モデル選び方 | → 第3章第10節 [AIモデル比較] |
| MCPとは？ | → 第3章第3節 [MCP] |
| プロンプト技術 | → 第2章 [プロンプトエンジニアリング] |
| RAG最適化 | → 第4章第3節 [RAG最適化] |
            `
          }
        },
        {
          id: 'ch0-experts',
          title: { zh: '大佬经验分享', ja: 'エキスパートの知見' },
          content: {
            zh: `
## 🎯 大佬经验分享

来自 AI 领域顶尖开发者的实战经验，帮你少走弯路。

---

## 👤 Boris Cherny - Claude Code 创始人

> "开箱即用才是最强工作流，复利工程思维让效率翻倍！"

Boris 是 Anthropic 的工程师，Claude Code 项目的核心创建者。他在社交媒体上分享了自己的工作流程，获得了数万点赞和转发。以下是他的核心经验总结。

### 核心理念：简单但极致

Boris 的哲学是"**让 AI 成为工作流的自然延伸**"，而不是过度定制。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                     Boris 的工作流核心                                   │
└─────────────────────────────────────────────────────────────────────────┘

  ┌─────────────┐    ┌─────────────┐    ┌─────────────┐
  │  开箱即用   │ ──▶│  复利思维   │ ──▶│  极致效率   │
  │  不过度配置 │    │  经验复用   │    │  一次搞定   │
  └─────────────┘    └─────────────┘    └─────────────┘
\`\`\`

---

### 🔧 Boris 的具体配置

#### 1. 大模型优先策略

| 选择 | 原因 |
|------|------|
| Opus 4.5 + thinking 模式 | "一次搞定"成功率更高 |
| 不用小模型 | 减少反复纠正的轮次 |

> 💡 **复利思维**：虽然大模型更慢，但减少了 3-5 次修改循环，总时间反而更短。

#### 2. Plan 模式驱动

\`\`\`
Boris 的工作流程
────────────────
1. Shift+Tab 两次 → 进入 Plan 模式
2. 和 Claude 把整个 PR 计划聊透
3. 满意后 → 切到 auto-accept 模式
4. 让 Claude 一次性完成所有工作

关键：架构设计优先于代码执行
\`\`\`

#### 3. 并行处理：5+10 实例

\`\`\`
Boris 的多实例布局
──────────────────
终端 (iTerm2)                  Web 版
─────────────                  ──────
Tab 1: 主功能开发              浏览器 Tab × 5-10
Tab 2: Bug 修复                用于长时间任务
Tab 3: 测试编写                如文档生成
Tab 4: 代码审查                架构设计等
Tab 5: 部署脚本

→ 避免上下文切换成本
→ 真正的并行开发
\`\`\`

#### 4. Slash 命令自动化

在 \`~/.claude/commands/\` 目录下创建命令：

\`\`\`bash
# /commit-push-pr 命令
自动执行：
1. 收集 git status
2. 生成 commit message
3. 推送代码
4. 创建 PR

# 使用
> /commit-push-pr
\`\`\`

#### 5. 验证闭环机制

> "给 Claude 一个验证自己的方式，是最关键的一环。"

\`\`\`
验证闭环
────────
代码生成 → 自动测试 → 失败?
                        ↓
                    自动修复
                        ↓
                    重新测试
                        ↓
                    通过 ✓
\`\`\`

#### 6. 团队知识库

\`\`\`bash
# 将 Claude 使用规范 check in 到 git
project/
├── .claude/
│   ├── CLAUDE.md          # 项目规范
│   ├── commands/          # 团队共享命令
│   └── agents/            # Sub-agents 配置

→ "教 AI 的经验"版本化管理
→ 团队效率集体提升
\`\`\`

---

## 🚀 SuperClaude 框架

> "让 Claude Code 编程能力暴增 300%！"

SuperClaude 是一个专门为 Claude Code 设计的综合配置框架，通过结构化的配置文件和专业化的工作流程，将 Claude Code 从通用 AI 助手转变为专业的开发伙伴。

### 安装

\`\`\`bash
git clone https://github.com/NomenAK/SuperClaude.git
cd SuperClaude
./install.sh

# 验证安装
ls ~/.claude/           # 4 个主文件
ls ~/.claude/commands/  # 17 个命令文件
\`\`\`

### 19 个专业命令

| 类别 | 命令 | 功能 |
|------|------|------|
| 构建 | /build | 项目构建 |
| 开发 | /user:dev-setup | 环境配置 |
| 测试 | /user:test | 测试执行 |
| 分析 | /user:analyze | 代码分析 |
| 调试 | /user:troubleshoot | 问题排查 |
| 优化 | /user:improve | 代码改进 |
| 部署 | /user:deploy | 项目部署 |
| 迁移 | /user:migrate | 数据迁移 |
| 安全 | /user:scan | 安全扫描 |
| 设计 | /user:design | 架构设计 |

### 9 大专家角色

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                      SuperClaude 专家角色                               │
└─────────────────────────────────────────────────────────────────────────┘

  architect        frontend         backend          security
  ┌────────┐      ┌────────┐      ┌────────┐      ┌────────┐
  │系统设计 │      │用户体验 │      │API性能 │      │安全审计 │
  │可扩展性 │      │React   │      │数据库  │      │威胁建模 │
  └────────┘      └────────┘      └────────┘      └────────┘

  qa               performance      analyzer         mentor
  ┌────────┐      ┌────────┐      ┌────────┐      ┌────────┐
  │测试质量 │      │性能优化 │      │问题分析 │      │技术指导 │
  │覆盖率   │      │瓶颈定位 │      │根因定位 │      │代码教学 │
  └────────┘      └────────┘      └────────┘      └────────┘

  refactorer
  ┌────────┐
  │代码重构 │
  │质量提升 │
  └────────┘
\`\`\`

### 使用示例

\`\`\`bash
# 创建 React 项目 + TDD + 前端专家
/build --react --magic --tdd --persona-frontend

# 安全扫描 + 安全专家
/user:scan --persona-security

# 性能优化 + 性能专家
/user:improve --persona-performance
\`\`\`

---

## 📚 上下文工程（Context Engineering）

> "效果比提示工程好 10 倍，比 Vibe Coding 好 100 倍。"

上下文工程是一种新的 AI 编程方法论，强调通过精心设计的上下文（而不仅仅是提示词）来引导 AI 产出高质量代码。

### 核心方法：PRP（产品需求提示）

\`\`\`
传统方式                          PRP 方式
──────────                        ──────────
"帮我写个登录功能"                1. 功能需求定义
        │                        2. 分解为可验证步骤
        ▼                        3. 每步完成后验证
  代码质量不可控                  4. 迭代直到完美
                                         │
                                         ▼
                                   代码质量有保障
\`\`\`

---

## 🔧 更多实用技巧

### Cursor + Claude Code 组合
同时使用 IDE 插件（Cursor）和命令行工具（Claude Code），分工协作：
- Cursor：快速补全、小范围修改
- Claude Code：大规模重构、复杂任务

### Spec-Driven 开发
先写规格文档（spec.md），再让 AI 按规格实现：
1. 需求分析 → 编写 spec
2. spec 评审 → 确认无误
3. 按 spec 实现 → 一次通过

### Output Styles 功能
Claude Code 支持多种输出风格：
- **Concise**：简洁模式，减少解释
- **Verbose**：详细模式，包含推理过程
- **Learning**：学习模式，边写代码边教学

### 多 AI 协作
Claude + GPT + Gemini 组合使用，取长补短。

---

## 💡 核心经验总结

| 大佬 | 核心理念 | 关键技巧 |
|------|----------|----------|
| Boris | 开箱即用 + 复利思维 | Plan 模式、并行实例、验证闭环 |
| SuperClaude | 专业化 + 模块化 | 19命令、9角色、MCP集成 |
| 上下文工程 | 结构化 + 可验证 | PRP 方法、迭代验证 |

> 📌 **记住**：选择适合自己的方法，不要盲目复制，找到自己的最佳实践。
            `,
            ja: `
## 🎯 エキスパートの知見

AI分野トップ開発者の実践経験から学びましょう。

---

## 👤 Boris Cherny - Claude Code 創設者

> "開封即用が最強ワークフロー、複利思考で効率倍増！"

Boris は Anthropic のエンジニアで、Claude Code プロジェクトの核心的な創設者です。彼がSNSで共有したワークフローは数万のいいねと転送を獲得しました。

### コア理念：シンプルだが極致

Borisの哲学は「**AIをワークフローの自然な延長にする**」こと。

---

### 🔧 Boris の具体的な設定

#### 1. 大規模モデル優先

| 選択 | 理由 |
|------|------|
| Opus 4.5 + thinking モード | 「一発成功」率が高い |
| 小規模モデルは使わない | 修正ループを減らす |

#### 2. Plan モード駆動

\`\`\`
Boris のワークフロー
────────────────
1. Shift+Tab 2回 → Plan モードへ
2. Claude と PR 計画を詳細に議論
3. 満足したら → auto-accept モードへ
4. Claude に一括で完成させる
\`\`\`

#### 3. 並列処理：5+10 インスタンス

ターミナル 5つ + Web版 5-10個を同時実行。

#### 4. 検証クローズドループ

> "Claude に自己検証の方法を与えることが最も重要。"

---

## 🚀 SuperClaude フレームワーク

> "Claude Code のプログラミング能力を 300% 向上！"

SuperClaude は Claude Code 専用の総合設定フレームワークで、構造化された設定ファイルと専門化されたワークフローにより、Claude Code を汎用AIアシスタントから専門的な開発パートナーに変革します。

### 19の専門コマンド + 9つの専門家ロール

| ロール | 専門領域 |
|--------|---------|
| architect | システム設計 |
| frontend | ユーザー体験、React |
| backend | API、パフォーマンス |
| security | セキュリティ監査 |
| qa | テスト品質 |
| performance | パフォーマンス最適化 |
| analyzer | 根本原因分析 |
| mentor | 技術指導 |
| refactorer | コードリファクタリング |

---

## 📚 コンテキストエンジニアリング

> "プロンプトエンジニアリングより10倍効果的。"

コンテキストエンジニアリングは新しいAIプログラミング方法論で、プロンプトだけでなく、精心に設計されたコンテキストを通じてAIに高品質コードを生成させることを重視します。

### PRP（製品要件プロンプト）方式

複雑な機能を検証可能なステップに分解し、各ステップ完了後に検証。

---

## 🔧 その他の実用テクニック

### Cursor + Claude Code 組み合わせ
IDEプラグイン（Cursor）とCLIツール（Claude Code）を同時使用：
- Cursor：素早い補完、小範囲の修正
- Claude Code：大規模リファクタリング、複雑なタスク

### Spec-Driven 開発
仕様書（spec.md）を先に書き、AIに仕様通り実装させる。

### Output Styles 機能
Claude Code は複数の出力スタイルをサポート：
- **Concise**：簡潔モード
- **Verbose**：詳細モード
- **Learning**：学習モード

---

## 💡 コア経験まとめ

| エキスパート | コア理念 | キーテクニック |
|-------------|---------|---------------|
| Boris | 開封即用 + 複利思考 | Plan モード、並列インスタンス |
| SuperClaude | 専門化 + モジュール化 | 19コマンド、9ロール |
| コンテキストエンジニアリング | 構造化 + 検証可能 | PRP 方式 |
            `
          }
        }
      ]
    },
    // ============================================
    // 第四章：大模型技术深度解析
    // ============================================
    {
      id: 'chapter-1',
      number: 1,
      title: { zh: '大模型技术深度解析', ja: '大規模言語モデル技術詳解' },
      subtitle: { zh: '理解 Transformer 与微调技术', ja: 'Transformerとファインチューニング技術を理解する' },
      sections: [
        {
          id: 'ch1-intro',
          title: { zh: '引言：走进大模型的内部', ja: '序章：大規模モデルの内部へ' },
          content: {
            zh: `
当我们使用 ChatGPT、Claude、Gemini 这些 AI 时，你是否好奇过：

- 它们是如何理解我们说的话的？
- 为什么它们能生成如此流畅的文字？
- "参数"到底是什么意思？7B、70B、405B 有什么区别？

本章将带你深入大模型的技术内核，理解这些神奇能力背后的原理。

---

## 为什么要了解技术原理？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        了解原理的三大价值                                 │
└─────────────────────────────────────────────────────────────────────────┘

  价值1：更好地使用 AI                价值2：做出正确的选型
  ┌──────────────────────┐           ┌──────────────────────┐
  │                      │           │                      │
  │  • 理解 AI 的能力边界  │           │  • 知道什么模型适合    │
  │  • 写出更有效的提示词  │           │    什么任务           │
  │  • 避免常见的使用误区  │           │  • 评估成本与效果      │
  │                      │           │  • 选择合适的部署方案  │
  └──────────────────────┘           └──────────────────────┘

                    价值3：具备微调和定制能力
                    ┌──────────────────────┐
                    │                      │
                    │  • 根据需求微调模型    │
                    │  • 构建领域专属 AI     │
                    │  • 优化性能和成本      │
                    │                      │
                    └──────────────────────┘
\`\`\`

---

## 本章你将学到

| 章节 | 内容 | 收获 |
|------|------|------|
| 4.1 | Transformer 架构 | 理解大模型的核心技术 |
| 4.2 | 注意力机制 | 理解 AI 如何"理解"语言 |
| 4.3 | 主流大模型对比 | 了解 GPT、Claude、Gemini 的区别 |
| 4.4 | 大模型微调入门 | 学会定制自己的 AI |
| 4.5 | LoRA 微调实战 | 低成本微调的最佳实践 |

让我们开始这段技术探索之旅！
            `,
            ja: `
ChatGPT、Claude、Geminiなどを使う時、こんな疑問を持ったことはありませんか：

- どうやって私たちの言葉を理解しているのか？
- なぜこんなに流暢な文章を生成できるのか？
- 「パラメータ」とは何？7B、70B、405Bの違いは？

この章では大規模モデルの技術コアに迫り、その魔法のような能力の原理を理解します。

---

## 技術原理を理解する価値

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        原理理解の3つの価値                               │
└─────────────────────────────────────────────────────────────────────────┘

  価値1：AIをより良く使う            価値2：正しいモデル選択
  ┌──────────────────────┐           ┌──────────────────────┐
  │  • AIの能力境界を理解  │           │  • タスクに適したモデル │
  │  • 効果的なプロンプト  │           │  • コストと効果の評価  │
  │  • よくある誤解を回避  │           │  • 適切なデプロイ方式  │
  └──────────────────────┘           └──────────────────────┘

                    価値3：ファインチューニング能力
                    ┌──────────────────────┐
                    │  • ニーズに応じた調整  │
                    │  • ドメイン特化AI構築  │
                    │  • 性能とコスト最適化  │
                    └──────────────────────┘
\`\`\`

---

## この章で学ぶこと

| セクション | 内容 | 得られるもの |
|----------|------|------------|
| 4.1 | Transformerアーキテクチャ | 大規模モデルのコア技術理解 |
| 4.2 | アテンション機構 | AIが言語を「理解」する仕組み |
| 4.3 | 主流モデル比較 | GPT、Claude、Geminiの違い |
| 4.4 | ファインチューニング入門 | 自分のAIをカスタマイズ |
| 4.5 | LoRA実践 | 低コストチューニングのベストプラクティス |
            `
          }
        },
        {
          id: 'ch1-transformer',
          title: { zh: '1.1 Transformer 架构详解', ja: '1.1 Transformerアーキテクチャ詳解' },
          content: {
            zh: `
## Transformer：改变世界的架构

2017 年，Google 发表了著名论文《Attention Is All You Need》，提出了 Transformer 架构。今天的 GPT、Claude、Gemini 全部基于这个架构。

---

## 动手体验：Transformer 工作流程

在深入学习之前，先通过这个交互式演示直观感受 Transformer 是如何处理文本的：

::transformer-v2-viz::

---

## 文本生成的核心原理：下一个词预测

大语言模型的本质其实很简单：**预测下一个词**。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    文本生成 = 不断预测下一个词                              │
└─────────────────────────────────────────────────────────────────────────┘

  用户输入: "今天天气"

  Step 1: 模型预测下一个词
  ┌────────────────────────────────────────┐
  │ 输入: "今天天气"                        │
  │                                        │
  │ 输出概率分布:                           │
  │   "很"     → 45%                       │
  │   "不"     → 25%                       │
  │   "真"     → 15%                       │
  │   "特别"   → 10%                       │
  │   其他     → 5%                        │
  │                                        │
  │ 选择: "很" (概率最高)                   │
  └────────────────────────────────────────┘

  Step 2: 把预测的词加入输入，继续预测
  ┌────────────────────────────────────────┐
  │ 输入: "今天天气很"                      │
  │                                        │
  │ 输出概率分布:                           │
  │   "好"     → 60%                       │
  │   "热"     → 20%                       │
  │   "冷"     → 15%                       │
  │   ...                                  │
  │                                        │
  │ 选择: "好"                             │
  └────────────────────────────────────────┘

  Step 3, 4, 5... 重复直到生成结束符或达到长度限制

  最终输出: "今天天气很好，适合出门散步。"
\`\`\`

> 💡 **这就是所谓的"自回归生成"（Autoregressive Generation）**

---

## 完整处理流程：从文本到预测

文本是如何变成概率预测的？整个流程分为三大步骤：

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    Transformer 处理流程全景图                              │
└─────────────────────────────────────────────────────────────────────────┘

  "今天天气很好"
       │
       ▼
  ┌─────────────────────────────────────────────────────────────────────┐
  │  STEP 1: 嵌入层 (Embedding)                                          │
  │  ─────────────────────────────────────────────────────────────────  │
  │                                                                     │
  │  1.1 分词 (Tokenization)                                            │
  │      "今天天气很好" → ["今天", "天气", "很", "好"]                    │
  │                       或 → ["今", "天", "天", "气", "很", "好"]       │
  │                                                                     │
  │  1.2 Token Embedding (词嵌入)                                        │
  │      每个 token → 768 维向量                                         │
  │      "今天" → [0.12, -0.45, 0.78, ..., 0.33]                        │
  │                                                                     │
  │  1.3 Position Embedding (位置嵌入)                                   │
  │      位置 0 → [0.00, 1.00, 0.00, ..., 1.00]                         │
  │      位置 1 → [0.84, 0.54, 0.01, ..., 0.99]                         │
  │                                                                     │
  │  1.4 相加得到最终嵌入                                                │
  │      最终向量 = Token Embedding + Position Embedding                │
  │                                                                     │
  └─────────────────────────────────────────────────────────────────────┘
       │
       ▼
  ┌─────────────────────────────────────────────────────────────────────┐
  │  STEP 2: Transformer Block × N (GPT-2 有 12 层，GPT-4 有 120+ 层)     │
  │  ─────────────────────────────────────────────────────────────────  │
  │                                                                     │
  │  每个 Block 包含:                                                    │
  │                                                                     │
  │  ┌─────────────────────────────────────────────────────────────┐    │
  │  │  Multi-Head Self-Attention                                  │    │
  │  │  • 让每个词"看到"其他词                                      │    │
  │  │  • 计算词与词之间的关联程度                                   │    │
  │  └─────────────────────────────────────────────────────────────┘    │
  │                        │                                            │
  │                        ▼                                            │
  │  ┌─────────────────────────────────────────────────────────────┐    │
  │  │  Add & Norm (残差连接 + 层归一化)                            │    │
  │  │  • 残差: output = input + attention_output                  │    │
  │  │  • 帮助梯度流动，防止梯度消失                                 │    │
  │  └─────────────────────────────────────────────────────────────┘    │
  │                        │                                            │
  │                        ▼                                            │
  │  ┌─────────────────────────────────────────────────────────────┐    │
  │  │  Feed Forward Network (前馈神经网络)                         │    │
  │  │  • 两层全连接: 768 → 3072 → 768                             │    │
  │  │  • 对每个位置独立处理                                        │    │
  │  └─────────────────────────────────────────────────────────────┘    │
  │                        │                                            │
  │                        ▼                                            │
  │  ┌─────────────────────────────────────────────────────────────┐    │
  │  │  Add & Norm (再次残差连接 + 层归一化)                        │    │
  │  └─────────────────────────────────────────────────────────────┘    │
  │                                                                     │
  └─────────────────────────────────────────────────────────────────────┘
       │
       ▼
  ┌─────────────────────────────────────────────────────────────────────┐
  │  STEP 3: 输出层                                                     │
  │  ─────────────────────────────────────────────────────────────────  │
  │                                                                     │
  │  3.1 线性投影                                                       │
  │      768 维 → 50,257 维 (词汇表大小)                                │
  │      每个维度代表一个词的"得分"(logits)                              │
  │                                                                     │
  │  3.2 Softmax 归一化                                                 │
  │      logits → 概率分布 (所有概率之和 = 1)                            │
  │                                                                     │
  │      [3.1, 0.5, 1.8, ...] → [0.45, 0.08, 0.32, ...]                │
  │                                                                     │
  │  3.3 采样策略选择下一个词                                            │
  │      • Greedy: 选概率最高的                                         │
  │      • Top-k: 从前 k 个中随机选                                     │
  │      • Top-p: 从累积概率达到 p 的词中选                              │
  │                                                                     │
  └─────────────────────────────────────────────────────────────────────┘
       │
       ▼
  输出: "好" (下一个词的预测)
\`\`\`

---

## 深入理解：分词 (Tokenization)

分词是第一步，也是容易被忽视但非常重要的一步。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    分词方式对比                                          │
└─────────────────────────────────────────────────────────────────────────┘

  方式1: 按字符分词
  "Hello" → ["H", "e", "l", "l", "o"]
  ✗ 缺点: 序列太长，无法捕捉词义

  方式2: 按空格分词
  "I love AI" → ["I", "love", "AI"]
  ✗ 缺点: 词汇表太大，罕见词无法处理

  方式3: BPE (Byte Pair Encoding) - 现代模型采用
  "unhappiness" → ["un", "happiness"] 或 ["un", "happ", "iness"]
  ✓ 优点: 平衡词汇表大小和序列长度

  GPT-2 词汇表: 50,257 个 token
  GPT-4 词汇表: ~100,000 个 token
  Claude 词汇表: ~100,000 个 token
\`\`\`

**中文分词示例:**
\`\`\`python
# 使用 tiktoken (OpenAI 的分词器)
import tiktoken
enc = tiktoken.encoding_for_model("gpt-4")

text = "今天天气很好"
tokens = enc.encode(text)
# 可能输出: [12345, 23456, 34567, 45678]  (示意)

# 每个数字对应词汇表中的一个 token
# 中文通常 1-2 个字符 = 1 个 token
\`\`\`

---

## 深入理解：词嵌入 (Token Embedding)

每个 token 被映射到一个高维向量空间。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    词嵌入可视化                                          │
└─────────────────────────────────────────────────────────────────────────┘

  词嵌入矩阵:
  ┌─────────────────────────────────────────────────────────────────┐
  │                    50,257 × 768                                 │
  │  ─────────────────────────────────────────────────────────────  │
  │                                                                 │
  │  token 0 ("!")    → [0.12, -0.45, 0.78, ..., 0.33]  (768维)    │
  │  token 1 ("\"")   → [0.23, 0.56, -0.12, ..., 0.67]  (768维)    │
  │  token 2 ("#")    → [-0.34, 0.78, 0.45, ..., -0.89] (768维)    │
  │  ...                                                            │
  │  token 15339 ("今天") → [0.56, -0.23, 0.89, ..., 0.12]          │
  │  token 28965 ("天气") → [0.45, -0.34, 0.78, ..., 0.23]          │
  │  ...                                                            │
  │  token 50256 ("<|endoftext|>") → [0.11, 0.22, ..., 0.33]       │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  GPT-2 词嵌入矩阵参数量: 50,257 × 768 ≈ 3,860 万参数！
\`\`\`

**语义关系在向量空间中的体现:**
\`\`\`python
# 经典例子: 词向量的语义算术
king - man + woman ≈ queen

# 相似词在向量空间中距离接近
cos_similarity("cat", "dog") ≈ 0.85  # 高相似度
cos_similarity("cat", "car") ≈ 0.15  # 低相似度
\`\`\`

---

## 深入理解：位置编码 (Positional Encoding)

自注意力机制本身不包含位置信息，需要显式添加。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    为什么需要位置编码？                                   │
└─────────────────────────────────────────────────────────────────────────┘

  问题: 自注意力是"集合运算"，不区分顺序

  例子:
  句子1: "猫 追 狗"  →  注意力看到 {猫, 追, 狗}
  句子2: "狗 追 猫"  →  注意力看到 {狗, 追, 猫}

  如果不加位置信息，模型会认为这两句话是一样的！

  解决: 给每个位置一个独特的"指纹"
\`\`\`

**原始 Transformer 使用正弦/余弦位置编码:**
\`\`\`python
# 位置编码公式
PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))

# pos = 词的位置 (0, 1, 2, ...)
# i = 维度索引 (0, 1, 2, ..., d_model/2)
# d_model = 嵌入维度 (如 768)
\`\`\`

\`\`\`
位置编码可视化 (假设 d_model = 8):

位置    dim0   dim1   dim2   dim3   dim4   dim5   dim6   dim7
───────────────────────────────────────────────────────────────
pos=0   0.00   1.00   0.00   1.00   0.00   1.00   0.00   1.00
pos=1   0.84   0.54   0.10   0.99   0.01   1.00   0.00   1.00
pos=2   0.91  -0.42   0.20   0.98   0.02   1.00   0.00   1.00
pos=3   0.14  -0.99   0.30   0.95   0.03   1.00   0.00   1.00
...

特点:
• 每个位置有独特的编码
• 不同维度变化频率不同 (低维度变化快，高维度变化慢)
• 相对位置可以通过线性变换表示
\`\`\`

**现代模型 (GPT-2/3/4, Llama) 使用可学习位置编码:**
\`\`\`python
# 可学习位置编码
self.position_embedding = nn.Embedding(max_seq_len, d_model)
# max_seq_len: 最大序列长度 (GPT-2: 1024, GPT-4: 128K+)

# 位置嵌入也是训练出来的，而不是固定公式
\`\`\`

---

## 深入理解：残差连接与层归一化 (Add & Norm)

为什么每个子层后都有 Add & Norm？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    残差连接 (Residual Connection)                        │
└─────────────────────────────────────────────────────────────────────────┘

  普通网络:
  x → [子层] → y

  残差网络:
  x → [子层] → y
  │            │
  └─────(+)────┘  →  output = x + y

  为什么重要？
  1. 梯度可以直接流回早期层 (解决梯度消失)
  2. 网络可以学习"恒等映射" (如果子层不需要，可以输出 0)
  3. 让训练更稳定，可以堆叠更多层

  GPT-3 有 96 层，没有残差连接根本无法训练！
\`\`\`

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    层归一化 (Layer Normalization)                        │
└─────────────────────────────────────────────────────────────────────────┘

  作用: 将每一层的输出归一化到均值为 0、方差为 1

  LayerNorm(x) = γ * (x - μ) / σ + β

  • μ = x 的均值
  • σ = x 的标准差
  • γ, β = 可学习的缩放和偏移参数

  为什么重要？
  1. 稳定训练过程
  2. 加速收敛
  3. 允许使用更大的学习率
\`\`\`

---

## 三种 Transformer 变体

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    Transformer 架构变体                                  │
└─────────────────────────────────────────────────────────────────────────┘

     Encoder-Only              Decoder-Only              Encoder-Decoder
     ────────────              ────────────              ───────────────
     ┌──────────┐             ┌──────────┐              ┌─────┐  ┌─────┐
     │ Encoder  │             │ Decoder  │              │ Enc │→│ Dec │
     └──────────┘             └──────────┘              └─────┘  └─────┘

  注意力类型:                注意力类型:                注意力类型:
  双向 (全部可见)            单向 (只看左边)            编码双向+解码单向

  代表模型:                  代表模型:                  代表模型:
  • BERT                    • GPT-1/2/3/4             • T5
  • RoBERTa                 • Claude                  • BART
  • ALBERT                  • Llama 1/2/3             • mBART

  擅长任务:                  擅长任务:                  擅长任务:
  • 文本分类                 • 文本生成                 • 机器翻译
  • 命名实体识别              • 对话系统                 • 文本摘要
  • 句子相似度               • 代码生成                 • 问答系统
  • 情感分析                 • 创意写作                 • 语音识别

  训练目标:                  训练目标:                  训练目标:
  掩码语言模型 (MLM)         下一词预测 (NTP)           Seq2Seq

     [MASK] 填空              一个词接一个词生成          输入→输出映射
\`\`\`

> 💡 **2024-2025 年的主流**: GPT、Claude、Llama 全部是 **Decoder-Only** 架构

---

## 参数量与模型能力

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    大模型参数量全景图                                     │
└─────────────────────────────────────────────────────────────────────────┘

  模型                参数量        层数     注意力头   嵌入维度
  ─────────────────────────────────────────────────────────────────
  GPT-2 Small         124M         12        12        768
  GPT-2 Large         774M         36        20        1280
  GPT-2 XL            1.5B         48        25        1600
  ─────────────────────────────────────────────────────────────────
  GPT-3 Small         125M         12        12        768
  GPT-3 Medium        350M         24        16        1024
  GPT-3 Large         760M         24        16        1536
  GPT-3 XL            1.3B         24        24        2048
  GPT-3 6.7B          6.7B         32        32        4096
  GPT-3 13B           13B          40        40        5120
  GPT-3 175B          175B         96        96        12288
  ─────────────────────────────────────────────────────────────────
  Llama 2 7B          7B           32        32        4096
  Llama 2 13B         13B          40        40        5120
  Llama 2 70B         70B          80        64        8192
  ─────────────────────────────────────────────────────────────────
  Claude 3 Haiku      ~20B         ?         ?         ?
  Claude 3 Sonnet     ~70B         ?         ?         ?
  Claude 3 Opus       ~200B        ?         ?         ?

  参数主要分布在:
  • 词嵌入矩阵: vocab_size × d_model
  • 注意力层: 4 × n_layers × d_model²
  • 前馈层: 8 × n_layers × d_model²
  • 输出层: d_model × vocab_size
\`\`\`

**Scaling Law (扩展定律):**
\`\`\`
模型性能 ∝ (参数量)^0.076 × (数据量)^0.095 × (计算量)^0.050

核心发现:
• 参数量翻倍 → 性能提升约 5%
• 数据量翻倍 → 性能提升约 7%
• 目前还没有看到明显的天花板

这就是为什么各大公司都在疯狂扩大模型规模！
\`\`\`

---

## 本节要点

1. **文本生成本质** —— 自回归预测下一个词
2. **完整流程** —— 分词 → 嵌入 → Transformer Block × N → 输出概率
3. **关键组件** —— 词嵌入、位置编码、残差连接、层归一化
4. **三种架构** —— Encoder-Only、Decoder-Only、Encoder-Decoder
5. **现代趋势** —— GPT/Claude/Llama 都是 Decoder-Only，规模持续扩大
            `,
            ja: `
## Transformer：世界を変えたアーキテクチャ

2017年、Googleが論文「Attention Is All You Need」を発表し、Transformerを提案しました。今日のGPT、Claude、Geminiはすべてこのアーキテクチャに基づいています。

---

## 体験してみよう：Transformerの動作

詳しく学ぶ前に、このインタラクティブデモでTransformerがテキストを処理する様子を体験してみましょう：

::transformer-v2-viz::

---

## テキスト生成の核心原理：次のトークン予測

大規模言語モデルの本質はシンプル：**次の単語を予測する**こと。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    テキスト生成 = 次の単語を繰り返し予測                    │
└─────────────────────────────────────────────────────────────────────────┘

  入力: "今日は天気が"

  Step 1: 次の単語を予測
  ┌────────────────────────────────────────┐
  │ 入力: "今日は天気が"                    │
  │                                        │
  │ 出力確率分布:                           │
  │   "良い"   → 45%                       │
  │   "悪い"   → 25%                       │
  │   "いい"   → 15%                       │
  │   ...                                  │
  │                                        │
  │ 選択: "良い" (最高確率)                 │
  └────────────────────────────────────────┘

  Step 2: 予測した単語を入力に追加し、続けて予測
  ┌────────────────────────────────────────┐
  │ 入力: "今日は天気が良い"                │
  │ → 次の予測: "です" (60%)               │
  └────────────────────────────────────────┘

  最終出力: "今日は天気が良いですね。"
\`\`\`

> 💡 **これが「自己回帰生成」（Autoregressive Generation）です**

---

## 完全な処理フロー

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    Transformer処理フロー全体図                            │
└─────────────────────────────────────────────────────────────────────────┘

  "今日は天気が良い"
       │
       ▼
  ┌─────────────────────────────────────────────────────────────────────┐
  │  STEP 1: 埋め込み層 (Embedding)                                      │
  │  ─────────────────────────────────────────────────────────────────  │
  │  1.1 トークン化: "今日" "は" "天気" "が" "良い"                       │
  │  1.2 トークン埋め込み: 各トークン → 768次元ベクトル                    │
  │  1.3 位置埋め込み: 位置情報を追加                                     │
  │  1.4 最終埋め込み = トークン埋め込み + 位置埋め込み                    │
  └─────────────────────────────────────────────────────────────────────┘
       │
       ▼
  ┌─────────────────────────────────────────────────────────────────────┐
  │  STEP 2: Transformer Block × N層                                    │
  │  ─────────────────────────────────────────────────────────────────  │
  │  • Multi-Head Self-Attention: 単語間の関係を計算                     │
  │  • Add & Norm: 残差接続 + 層正規化                                   │
  │  • Feed Forward: 768 → 3072 → 768                                  │
  │  • Add & Norm: 再び残差接続 + 層正規化                               │
  └─────────────────────────────────────────────────────────────────────┘
       │
       ▼
  ┌─────────────────────────────────────────────────────────────────────┐
  │  STEP 3: 出力層                                                     │
  │  ─────────────────────────────────────────────────────────────────  │
  │  • 線形投影: 768次元 → 50,257次元（語彙サイズ）                       │
  │  • Softmax: 確率分布に変換                                          │
  │  • サンプリング: 次のトークンを選択                                   │
  └─────────────────────────────────────────────────────────────────────┘
\`\`\`

---

## トークン化 (Tokenization)

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    トークン化方式の比較                                   │
└─────────────────────────────────────────────────────────────────────────┘

  方式1: 文字単位
  "Hello" → ["H", "e", "l", "l", "o"]
  ✗ シーケンスが長すぎる

  方式2: スペース区切り
  "I love AI" → ["I", "love", "AI"]
  ✗ 語彙が大きすぎる

  方式3: BPE (現代モデルが採用)
  "unhappiness" → ["un", "happiness"]
  ✓ バランスが良い

  GPT-2語彙: 50,257トークン
  GPT-4語彙: ~100,000トークン
\`\`\`

---

## 位置エンコーディング

\`\`\`
なぜ位置情報が必要か？

セルフアテンションは順序を区別しない:
"猫が犬を追う" と "犬が猫を追う"
→ 位置情報なしでは同じに見える！

解決策: 各位置に固有の「指紋」を追加

PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))

現代モデル(GPT-2/3/4, Llama)は学習可能な位置埋め込みを使用
\`\`\`

---

## 残差接続と層正規化

\`\`\`
残差接続 (Residual Connection):
output = x + SubLayer(x)

利点:
• 勾配が直接初期層に流れる
• 深いネットワークの学習が可能に
• GPT-3は96層、残差なしでは学習不可能

層正規化 (Layer Normalization):
LayerNorm(x) = γ * (x - μ) / σ + β

利点:
• 学習の安定化
• 収束の加速
\`\`\`

---

## 3種類のTransformerアーキテクチャ

| タイプ | 代表モデル | 得意なタスク |
|--------|----------|------------|
| Encoder-Only | BERT, RoBERTa | 分類、NER、感情分析 |
| Decoder-Only | GPT, Claude, Llama | 生成、対話、コード |
| Encoder-Decoder | T5, BART | 翻訳、要約、Q&A |

> 💡 **2024-2025年の主流**: GPT、Claude、Llamaは全て **Decoder-Only**

---

## パラメータ数と規模

\`\`\`
モデル              パラメータ数    層数
─────────────────────────────────────
GPT-2 XL            1.5B         48
GPT-3 175B          175B         96
Llama 2 70B         70B          80
Claude 3 Opus       ~200B        ?

Scaling Law:
性能 ∝ (パラメータ)^0.076 × (データ)^0.095

パラメータ2倍 → 性能約5%向上
\`\`\`

---

## 本節のポイント

1. **テキスト生成の本質** —— 自己回帰で次のトークンを予測
2. **完全なフロー** —— トークン化 → 埋め込み → Transformer Block → 確率出力
3. **重要な構成要素** —— 埋め込み、位置エンコーディング、残差接続
4. **3種類のアーキテクチャ** —— Encoder-Only、Decoder-Only、Encoder-Decoder
5. **現代のトレンド** —— GPT/Claude/Llamaは全てDecoder-Only
            `
          }
        },
        {
          id: 'ch1-attention',
          title: { zh: '1.2 注意力机制深入理解', ja: '1.2 アテンション機構の深い理解' },
          content: {
            zh: `
## 注意力机制：Transformer 的灵魂

注意力机制（Attention）是 Transformer 最核心的创新。它让模型能够动态地"关注"输入中最相关的部分。

### 交互式演示：注意力机制可视化

先通过这个交互式演示探索注意力是如何工作的：

::attention-viz::

---

## 用搜索引擎类比理解 Q/K/V

理解 Q、K、V 最好的方式是把它想象成**搜索引擎**：

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    Q/K/V 的搜索引擎类比                                   │
└─────────────────────────────────────────────────────────────────────────┘

  想象你在 Google 搜索：

  ┌─────────────────────────────────────────────────────────────────────┐
  │                                                                     │
  │  🔍 Query (查询)：你输入的搜索词                                      │
  │     "如何学习机器学习"                                               │
  │                                                                     │
  │  📑 Key (键)：网页的标题/关键词                                       │
  │     - "机器学习入门教程"                                             │
  │     - "深度学习实战指南"                                             │
  │     - "Python 编程基础"                                              │
  │     - "今日新闻头条"                                                 │
  │                                                                     │
  │  📄 Value (值)：网页的实际内容                                        │
  │     - [机器学习教程的完整内容...]                                     │
  │     - [深度学习指南的完整内容...]                                     │
  │     - [Python 基础的完整内容...]                                      │
  │     - [新闻的完整内容...]                                            │
  │                                                                     │
  └─────────────────────────────────────────────────────────────────────┘

  搜索过程：
  1. Query 和每个 Key 计算相似度（点击率预估）
  2. 相似度高的 Key 对应的 Value 获得更高权重
  3. 返回加权后的结果（相关网页排在前面）

  在 Transformer 中：
  • Query = "我现在想要什么信息？"
  • Key = "我有什么信息可以提供？"
  • Value = "这些信息的具体内容"
\`\`\`

---

## Self-Attention 完整计算流程

让我们用一个具体例子走一遍完整的注意力计算。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    完整示例：计算 "猫 吃 鱼" 的注意力                       │
└─────────────────────────────────────────────────────────────────────────┘

  输入句子: "猫 吃 鱼"

  ═══════════════════════════════════════════════════════════════════════
  Step 1: 将每个词转换为向量（词嵌入）
  ═══════════════════════════════════════════════════════════════════════

  "猫" → x₁ = [0.2, 0.5, 0.1, 0.8]  (假设 4 维，实际是 768 维)
  "吃" → x₂ = [0.6, 0.1, 0.9, 0.3]
  "鱼" → x₃ = [0.3, 0.7, 0.2, 0.5]

  ═══════════════════════════════════════════════════════════════════════
  Step 2: 通过线性变换生成 Q, K, V
  ═══════════════════════════════════════════════════════════════════════

  每个词都会生成自己的 Q, K, V：

  对于 "猫":
    Q₁ = x₁ × W_Q = [0.2, 0.5, 0.1, 0.8] × W_Q = [2.2, 0.8]
    K₁ = x₁ × W_K = [0.2, 0.5, 0.1, 0.8] × W_K = [0.9, 1.1]
    V₁ = x₁ × W_V = [0.2, 0.5, 0.1, 0.8] × W_V = [0.5, 0.3]

  对于 "吃":
    Q₂ = [0.7, 1.4]    K₂ = [2.0, 0.6]    V₂ = [0.8, 0.2]

  对于 "鱼":
    Q₃ = [2.0, 0.5]    K₃ = [0.4, 1.3]    V₃ = [0.2, 0.9]

  ═══════════════════════════════════════════════════════════════════════
  Step 3: 计算注意力分数 (Q × K^T)
  ═══════════════════════════════════════════════════════════════════════

  以 "猫" 为例，计算它对每个词的注意力分数：

  score(猫→猫) = Q₁ · K₁ = [2.2, 0.8] · [0.9, 1.1] = 1.08 + 0.88 = 1.96
  score(猫→吃) = Q₁ · K₂ = [2.2, 0.8] · [2.0, 0.6] = 1.20 + 0.48 = 1.68
  score(猫→鱼) = Q₁ · K₃ = [2.2, 0.8] · [0.4, 1.3] = 0.48 + 1.04 = 1.52

  完整的注意力分数矩阵：
                K₁(猫)  K₂(吃)  K₃(鱼)
              ┌───────────────────────┐
  Q₁(猫)      │  1.96    1.68    1.52  │
  Q₂(吃)      │  2.31    2.10    1.87  │
  Q₃(鱼)      │  1.45    1.30    1.15  │
              └───────────────────────┘

  ═══════════════════════════════════════════════════════════════════════
  Step 4: 缩放 (除以 √d_k)
  ═══════════════════════════════════════════════════════════════════════

  为什么要缩放？
  • 当维度 d_k 很大时，点积的值会很大
  • 大的值经过 softmax 后会产生极端的分布（接近 one-hot）
  • 这会导致梯度消失，训练不稳定

  缩放后 (d_k = 2, √d_k = 1.414)：
                K₁(猫)  K₂(吃)  K₃(鱼)
              ┌───────────────────────┐
  Q₁(猫)      │  1.39    1.19    1.07  │
  Q₂(吃)      │  1.63    1.48    1.32  │
  Q₃(鱼)      │  1.02    0.92    0.81  │
              └───────────────────────┘

  ═══════════════════════════════════════════════════════════════════════
  Step 5: Softmax 归一化
  ═══════════════════════════════════════════════════════════════════════

  对每一行应用 softmax，使每行和为 1：

  softmax([2.39, 1.19, 1.07]) = [0.40, 0.33, 0.27]

  注意力权重矩阵：
                   猫      吃      鱼
              ┌───────────────────────┐
  猫          │  0.40    0.33    0.27  │  → 行和 = 1
  吃          │  0.42    0.35    0.23  │  → 行和 = 1
  鱼          │  0.38    0.34    0.28  │  → 行和 = 1
              └───────────────────────┘

  解读：
  • "猫" 对自己的注意力是 0.40（最高）
  • "猫" 对 "吃" 的注意力是 0.33
  • "猫" 对 "鱼" 的注意力是 0.27

  ═══════════════════════════════════════════════════════════════════════
  Step 6: 加权求和得到输出
  ═══════════════════════════════════════════════════════════════════════

  "猫" 的新表示 = 0.40×V₁ + 0.33×V₂ + 0.27×V₃
                = 0.40×[0.5,0.3] + 0.33×[0.8,0.2] + 0.27×[0.2,0.9]
                = [0.20,0.12] + [0.26,0.07] + [0.05,0.24]
                = [0.51, 0.43]

  新的表示融合了整个句子的上下文信息！
\`\`\`

---

## 核心公式详解

\`\`\`python
Attention(Q, K, V) = softmax(Q · K^T / √d_k) · V
\`\`\`

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    公式分解                                              │
└─────────────────────────────────────────────────────────────────────────┘

  Q · K^T        计算相似度（哪些词和我相关）
     ↓
  / √d_k         缩放（防止梯度消失）
     ↓
  softmax()      归一化（转换为概率分布，和为1）
     ↓
  × V            加权求和（提取相关信息）

  ═══════════════════════════════════════════════════════════════════════

  为什么是 √d_k？

  假设 Q 和 K 的元素都是均值为 0、方差为 1 的随机变量：
  • Q·K 的方差 ≈ d_k
  • 当 d_k = 64 时，Q·K 的标准差 ≈ 8
  • 除以 √64 = 8 后，标准差变回 1

  这保证了无论维度多大，softmax 的输入都在合理范围内。
\`\`\`

---

## 多头注意力（Multi-Head Attention）

单个注意力头只能学习一种关系模式。多头注意力让模型同时从多个角度理解输入。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    多头注意力可视化                                       │
└─────────────────────────────────────────────────────────────────────────┘

  句子: "The cat sat on the mat because it was tired"

  不同的头学习不同的关系：

  Head 1: 主语-动词关系          Head 2: 代词指代
  ┌─────────────────────┐       ┌─────────────────────┐
  │                     │       │                     │
  │  cat ═══════► sat   │       │  it ═══════► cat    │
  │  (谁在坐？)          │       │  (it 指的是谁？)     │
  │                     │       │                     │
  └─────────────────────┘       └─────────────────────┘

  Head 3: 介词关系               Head 4: 因果关系
  ┌─────────────────────┐       ┌─────────────────────┐
  │                     │       │                     │
  │  sat ═══► on ═══► mat│       │  because ═══► tired │
  │  (坐在哪里？)         │       │  (为什么？)          │
  │                     │       │                     │
  └─────────────────────┘       └─────────────────────┘

  GPT-2: 12 个注意力头
  GPT-3: 96 个注意力头
  GPT-4: 估计 100+ 个注意力头
\`\`\`

\`\`\`python
# 多头注意力的完整实现
import torch
import torch.nn as nn
import math

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model=768, num_heads=12):
        super().__init__()
        self.num_heads = num_heads
        self.d_k = d_model // num_heads  # 每个头的维度: 768/12 = 64

        # 一次性计算所有头的 Q, K, V（效率更高）
        self.W_Q = nn.Linear(d_model, d_model)
        self.W_K = nn.Linear(d_model, d_model)
        self.W_V = nn.Linear(d_model, d_model)
        self.W_O = nn.Linear(d_model, d_model)

    def forward(self, x, mask=None):
        batch_size, seq_len, d_model = x.shape

        # Step 1: 线性变换
        Q = self.W_Q(x)  # [batch, seq_len, d_model]
        K = self.W_K(x)
        V = self.W_V(x)

        # Step 2: 拆分成多个头
        # [batch, seq_len, d_model] → [batch, num_heads, seq_len, d_k]
        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)

        # Step 3: 计算注意力
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)

        # Step 4: 应用掩码（如果有）
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        # Step 5: Softmax
        attn_weights = torch.softmax(scores, dim=-1)

        # Step 6: 加权求和
        output = torch.matmul(attn_weights, V)

        # Step 7: 合并所有头
        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)

        # Step 8: 最终线性变换
        return self.W_O(output)
\`\`\`

---

## Masked Self-Attention（掩码自注意力）

在文本生成任务中，模型在预测第 n 个词时，不能看到第 n+1, n+2, ... 位置的词。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    掩码注意力的工作原理                                    │
└─────────────────────────────────────────────────────────────────────────┘

  生成句子: "I love AI"

  训练时，我们有完整的句子，但需要模拟生成过程：

  预测位置    能看到的词           预测目标
  ─────────────────────────────────────────
  位置 1      [I]                  love
  位置 2      [I, love]            AI
  位置 3      [I, love, AI]        <END>

  掩码矩阵（下三角矩阵）：

           I     love    AI
        ┌─────────────────────┐
  I     │  1      0       0   │   1 = 可以看到
  love  │  1      1       0   │   0 = 看不到
  AI    │  1      1       1   │
        └─────────────────────┘

  在注意力分数上应用掩码：

  原始分数：              加掩码后（-∞）：         Softmax 后：
  ┌─────────────┐       ┌─────────────┐        ┌─────────────┐
  │ 2.1 1.5 0.8 │       │ 2.1  -∞  -∞ │        │ 1.0 0.0 0.0 │
  │ 1.2 2.8 1.1 │   →   │ 1.2 2.8  -∞ │   →    │ 0.2 0.8 0.0 │
  │ 0.9 1.3 2.5 │       │ 0.9 1.3 2.5 │        │ 0.1 0.3 0.6 │
  └─────────────┘       └─────────────┘        └─────────────┘

  -∞ 经过 softmax 后变成 0，实现了"看不见"的效果
\`\`\`

---

## Temperature 与采样策略

生成文本时，如何从概率分布中选择下一个词？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    Temperature 控制输出多样性                             │
└─────────────────────────────────────────────────────────────────────────┘

  原始 logits: [3.0, 1.0, 0.5, 0.1]
  对应词汇:     好    热    冷    坏

  Temperature 的作用：logits / temperature → softmax

  ┌─────────────────────────────────────────────────────────────────────┐
  │  Temperature = 0.5（低温，更确定）                                    │
  │  ─────────────────────────────────────────────────────────────────  │
  │  logits / 0.5 = [1.0, 2.0, 1.0, 0.2]                               │
  │  softmax → [0.84, 0.11, 0.04, 0.01]                                │
  │                                                                     │
  │  "好" 的概率高达 84%！输出更确定、更保守                              │
  └─────────────────────────────────────────────────────────────────────┘

  ┌─────────────────────────────────────────────────────────────────────┐
  │  Temperature = 1.0（默认）                                           │
  │  ─────────────────────────────────────────────────────────────────  │
  │  logits / 1.0 = [3.0, 1.0, 0.5, 0.1]                               │
  │  softmax → [0.53, 0.19, 0.12, 0.08]                                │
  │                                                                     │
  │  正常的概率分布                                                      │
  └─────────────────────────────────────────────────────────────────────┘

  ┌─────────────────────────────────────────────────────────────────────┐
  │  Temperature = 2.0（高温，更随机）                                    │
  │  ─────────────────────────────────────────────────────────────────  │
  │  logits / 2.0 = [2.0, 0.5, 0.25, 0.05]                             │
  │  softmax → [0.38, 0.23, 0.18, 0.15]                                │
  │                                                                     │
  │  概率分布更平坦，输出更多样、更有创意（但可能更不准确）                  │
  └─────────────────────────────────────────────────────────────────────┘

  使用建议：
  • 代码生成 / 数学题：Temperature = 0 (贪心解码)
  • 一般对话：Temperature = 0.7 ~ 1.0
  • 创意写作：Temperature = 1.0 ~ 1.5
\`\`\`

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    Top-k 和 Top-p 采样                                   │
└─────────────────────────────────────────────────────────────────────────┘

  Top-k 采样：只从概率最高的 k 个词中选择

  原始分布: [好: 0.53, 热: 0.19, 冷: 0.12, 坏: 0.08, ...]

  Top-k = 3:
  [好: 0.53, 热: 0.19, 冷: 0.12] → 重新归一化 → [好: 0.63, 热: 0.23, 冷: 0.14]

  ─────────────────────────────────────────────────────────────────────────

  Top-p 采样（Nucleus Sampling）：选择累积概率达到 p 的最小词集

  原始分布（按概率排序）:
  好: 0.53  累积: 0.53
  热: 0.19  累积: 0.72
  冷: 0.12  累积: 0.84
  坏: 0.08  累积: 0.92  ← Top-p = 0.9 时，选到这里
  ...

  Top-p = 0.9:
  从 {好, 热, 冷, 坏} 中采样

  ─────────────────────────────────────────────────────────────────────────

  实际使用中，通常组合使用：
  • temperature = 0.7
  • top_p = 0.9
  • top_k = 50
\`\`\`

---

## 注意力可视化

\`\`\`
实际的注意力模式可以通过工具可视化：

输入: "The cat sat on the mat"

Layer 6, Head 8（学习到的句法关系）：

        The   cat   sat    on   the   mat
The     ███   ░░░   ░░░   ░░░   ░░░   ░░░
cat     ░░░   ███   ░░░   ░░░   ░░░   ░░░
sat     ░░░   ███   ██░   ░░░   ░░░   ░░░  ← "sat" 关注 "cat"（主语）
on      ░░░   ░░░   ███   ███   ░░░   ░░░
the     ░░░   ░░░   ░░░   ░░░   ███   ░░░
mat     ░░░   ░░░   ░░░   ███   ░░░   ███  ← "mat" 关注 "on"（介词）

███ = 高注意力    ░░░ = 低注意力

推荐工具：BertViz, Attention Viz, Transformer Explainer
\`\`\`

---

## 本节要点

1. **Q/K/V 类比** —— 就像搜索引擎：Query 是搜索词，Key 是标题，Value 是内容
2. **注意力公式** —— Attention = softmax(QK^T/√d_k)V，缩放防止梯度消失
3. **多头注意力** —— 12-96 个头同时学习不同类型的关系
4. **掩码注意力** —— 通过 -∞ 掩码实现"看不到未来"
5. **Temperature** —— 低温更确定，高温更随机
6. **Top-k/Top-p** —— 限制采样范围，平衡质量和多样性
            `,
            ja: `
## アテンション機構：Transformerの魂

アテンション機構（Attention）はTransformerの最も核心的なイノベーションです。入力の中で最も関連性の高い部分に動的に「注目」することができます。

### インタラクティブデモ：アテンション機構の可視化

このインタラクティブデモでアテンションがどのように機能するかを探索してください：

::attention-viz::

---

## 検索エンジンで理解する Q/K/V

Q/K/V を理解する最良の方法は**検索エンジン**のアナロジーです：

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    Q/K/V の検索エンジンアナロジー                          │
└─────────────────────────────────────────────────────────────────────────┘

  Google検索を想像してください：

  🔍 Query (クエリ)：入力した検索ワード
     "機械学習 入門"

  📑 Key (キー)：ウェブページのタイトル/キーワード
     - "機械学習入門チュートリアル"
     - "深層学習実践ガイド"
     - "Python プログラミング基礎"

  📄 Value (値)：ウェブページの実際のコンテンツ
     - [機械学習チュートリアルの完全な内容...]
     - [深層学習ガイドの完全な内容...]

  Transformerでは：
  • Query = "今、どんな情報が欲しい？"
  • Key = "どんな情報を提供できる？"
  • Value = "その情報の具体的な内容"
\`\`\`

---

## Self-Attention 完全な計算フロー

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    完全な例："猫 食べる 魚" のアテンション計算              │
└─────────────────────────────────────────────────────────────────────────┘

  Step 1: 各単語をベクトルに変換
  "猫" → x₁ = [0.2, 0.5, 0.1, 0.8]
  "食べる" → x₂ = [0.6, 0.1, 0.9, 0.3]
  "魚" → x₃ = [0.3, 0.7, 0.2, 0.5]

  Step 2: 線形変換で Q, K, V を生成
  各単語に対して Q, K, V を計算

  Step 3: アテンションスコア計算 (Q × K^T)
  各単語ペアの類似度を計算

  Step 4: スケーリング (÷ √d_k)
  勾配消失を防ぐ

  Step 5: Softmax 正規化
  各行の和が1になる確率分布に変換

  Step 6: 加重和で出力を得る
  新しい表現 = Σ(アテンション重み × V)
\`\`\`

---

## コア公式

\`\`\`python
Attention(Q, K, V) = softmax(Q · K^T / √d_k) · V
\`\`\`

\`\`\`
なぜ √d_k で割るのか？

• d_k が大きいとき、ドット積の値が大きくなる
• 大きな値は softmax 後に極端な分布（one-hotに近い）を生む
• これは勾配消失を引き起こし、学習を不安定にする

√d_k で割ることで、入力を適切な範囲に保つ
\`\`\`

---

## Multi-Head Attention

\`\`\`
異なるヘッドが異なる関係を学習：

Head 1: 主語-動詞関係    Head 2: 代名詞参照
Head 3: 前置詞関係      Head 4: 因果関係

GPT-2: 12ヘッド
GPT-3: 96ヘッド
GPT-4: 推定100+ヘッド
\`\`\`

---

## Masked Self-Attention

\`\`\`
テキスト生成では、未来の単語を「カンニング」できない：

文: "I love AI"

予測位置    見える単語        予測対象
─────────────────────────────────────
位置 1      [I]               love
位置 2      [I, love]         AI
位置 3      [I, love, AI]     <END>

マスク行列（下三角）：
      I   love  AI
  ┌─────────────────┐
I │  1    0    0   │  1 = 見える
love│  1    1    0   │  0 = 見えない
AI│  1    1    1   │
  └─────────────────┘

-∞ を適用後、softmax で 0 になり「見えない」効果を実現
\`\`\`

---

## Temperature とサンプリング戦略

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    Temperature で出力の多様性を制御                        │
└─────────────────────────────────────────────────────────────────────────┘

Temperature = 0.5（低温）→ より確定的、保守的
Temperature = 1.0（デフォルト）→ 通常の分布
Temperature = 2.0（高温）→ より多様、創造的

使用ガイドライン：
• コード生成/数学: Temperature = 0（貪欲デコーディング）
• 一般的な会話: Temperature = 0.7 ~ 1.0
• 創作活動: Temperature = 1.0 ~ 1.5

Top-k サンプリング: 上位 k 個から選択
Top-p サンプリング: 累積確率が p に達するまでの単語から選択
\`\`\`

---

## 本節のポイント

1. **Q/K/V アナロジー** —— 検索エンジン：Query=検索語、Key=タイトル、Value=内容
2. **アテンション公式** —— Attention = softmax(QK^T/√d_k)V
3. **マルチヘッド** —— 12-96個のヘッドが異なる関係を学習
4. **マスクアテンション** —— -∞マスクで「未来が見えない」を実現
5. **Temperature** —— 低温=確定的、高温=多様性
6. **Top-k/Top-p** —— サンプリング範囲を制限、品質と多様性のバランス
            `
          }
        },
        {
          id: 'ch1-finetune-intro',
          title: { zh: '1.3 大模型微调入门', ja: '1.3 大規模モデルファインチューニング入門' },
          content: {
            zh: `
## 什么是微调（Fine-tuning）？

微调是在预训练模型的基础上，使用特定领域的数据进行进一步训练，使模型更适合特定任务。

---

## 为什么需要微调？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        微调的必要性                                      │
└─────────────────────────────────────────────────────────────────────────┘

  通用大模型的局限：

  ┌─────────────────────────────────────────────────────────────────────┐
  │                                                                     │
  │  ❌ 不了解你的专业领域术语                                            │
  │  ❌ 不知道你公司的产品和流程                                          │
  │  ❌ 输出风格可能不符合要求                                            │
  │  ❌ 在特定任务上表现不够精确                                          │
  │                                                                     │
  └─────────────────────────────────────────────────────────────────────┘

  微调后：

  ┌─────────────────────────────────────────────────────────────────────┐
  │                                                                     │
  │  ✅ 掌握你的领域专业知识                                              │
  │  ✅ 了解特定的术语和表达                                              │
  │  ✅ 输出符合品牌风格                                                  │
  │  ✅ 在特定任务上表现出色                                              │
  │                                                                     │
  └─────────────────────────────────────────────────────────────────────┘
\`\`\`

---

## 微调的类型

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        微调方法对比                                      │
└─────────────────────────────────────────────────────────────────────────┘

  方法              训练参数      显存需求      效果       适用场景
  ────              ────────      ────────      ────       ────────
  全量微调           100%          极高         最好       资源充足
  LoRA              ~0.1%         低           很好       资源有限（推荐）
  QLoRA             ~0.1%         极低         较好       消费级显卡
  Adapter           ~1%           中等         好         多任务
  Prompt Tuning     ~0.01%        极低         一般       简单任务


  推荐学习路径：

  初学者 → LoRA（性价比最高）
  进阶 → QLoRA（更低显存）
  专业 → 全量微调（追求极致效果）
\`\`\`

---

## LoRA：低秩自适应

LoRA（Low-Rank Adaptation）是目前最流行的微调方法。

### 核心思想

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        LoRA 原理图解                                     │
└─────────────────────────────────────────────────────────────────────────┘

  原始权重矩阵 W（冻结，不更新）
  ┌────────────────────────────────────────┐
  │                                        │
  │           d × d 维                      │    比如 4096 × 4096
  │          （参数量巨大）                  │    = 1600 万参数
  │                                        │
  └────────────────────────────────────────┘

  LoRA 添加两个小矩阵 A 和 B：

  ┌────────┐   ┌────────┐
  │   B    │   │   A    │
  │ d × r  │ × │ r × d  │    r = 秩（通常 4~64）
  └────────┘   └────────┘
      ↓             ↓
    4096×8        8×4096        = 6.5 万参数（减少 99.6%！）

  最终输出 = W·x + B·A·x

  只训练 A 和 B，W 保持不变！
\`\`\`

### LoRA 的优势

| 优势 | 说明 |
|------|------|
| 参数高效 | 只需训练 0.1% 的参数 |
| 显存友好 | 可在消费级 GPU 上运行 |
| 效果接近 | 接近全量微调的效果 |
| 可插拔 | 可以随时加载/卸载 LoRA |
| 可组合 | 多个 LoRA 可以组合使用 |

---

## 微调数据准备

### 数据格式

\`\`\`json
// 指令微调格式（推荐）
{
  "instruction": "将以下英文翻译成中文",
  "input": "Hello, how are you?",
  "output": "你好，你怎么样？"
}

// 对话格式
{
  "conversations": [
    {"role": "user", "content": "什么是机器学习？"},
    {"role": "assistant", "content": "机器学习是..."}
  ]
}

// 补全格式
{
  "prompt": "写一首关于春天的诗：",
  "completion": "春风拂面暖阳照..."
}
\`\`\`

### 数据质量原则

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        高质量微调数据的特征                               │
└─────────────────────────────────────────────────────────────────────────┘

  ✅ 数量充足
     • 最少 100-1000 条
     • 复杂任务需要更多

  ✅ 质量高
     • 准确无误
     • 表达清晰
     • 格式一致

  ✅ 多样性
     • 覆盖各种情况
     • 包含边界案例
     • 长度分布合理

  ✅ 代表性
     • 反映真实使用场景
     • 符合目标任务需求
\`\`\`

---

## 微调工具推荐

### 1. Hugging Face Transformers + PEFT

\`\`\`bash
# 安装
pip install transformers peft datasets accelerate

# PEFT = Parameter-Efficient Fine-Tuning
# 支持 LoRA、QLoRA、Adapter 等多种方法
\`\`\`

### 2. LLaMA-Factory（推荐新手）

\`\`\`bash
# 开箱即用的微调框架
git clone https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -r requirements.txt

# 提供 Web UI，无需写代码
python src/train_web.py
\`\`\`

### 3. Axolotl

\`\`\`bash
# 配置驱动的微调框架
git clone https://github.com/OpenAccess-AI-Collective/axolotl.git
cd axolotl
pip install -e .

# 使用 YAML 配置文件
accelerate launch -m axolotl.cli.train config.yaml
\`\`\`

---

## 本节要点

1. **微调目的** —— 让通用模型适应特定领域/任务
2. **LoRA** —— 最推荐的微调方法，参数效率高
3. **数据准备** —— 质量比数量更重要
4. **工具选择** —— LLaMA-Factory 适合新手，PEFT 更灵活
5. **资源评估** —— 根据显存选择合适的微调方法
            `,
            ja: `
## ファインチューニングとは？

ファインチューニングは、事前学習済みモデルをベースに、特定ドメインのデータでさらに学習させ、特定タスクにより適したモデルにすることです。

---

## なぜファインチューニングが必要？

\`\`\`
汎用大規模モデルの限界：
❌ あなたの専門領域の用語を知らない
❌ 会社の製品やプロセスを知らない
❌ 出力スタイルが要件に合わない可能性
❌ 特定タスクで十分な精度がない

ファインチューニング後：
✅ ドメイン専門知識を習得
✅ 特定の用語と表現を理解
✅ ブランドスタイルに合った出力
✅ 特定タスクで優れたパフォーマンス
\`\`\`

---

## ファインチューニングの種類

| 方法 | 学習パラメータ | VRAM | 効果 | 適用場面 |
|------|-------------|------|------|---------|
| フルチューニング | 100% | 極高 | 最良 | リソース十分 |
| LoRA | ~0.1% | 低 | 良好 | リソース制限（推奨）|
| QLoRA | ~0.1% | 極低 | 良い | 消費者GPU |
| Prompt Tuning | ~0.01% | 極低 | 普通 | 簡単なタスク |

---

## LoRA：低ランク適応

LoRA（Low-Rank Adaptation）は現在最も人気のあるファインチューニング方法です。

### コアアイデア

\`\`\`
元の重み行列 W（凍結、更新しない）
┌────────────────────────────────┐
│        d × d 次元               │  例：4096 × 4096
│       （パラメータ数膨大）        │  = 1600万パラメータ
└────────────────────────────────┘

LoRAは2つの小さな行列 A と B を追加：

┌────────┐   ┌────────┐
│   B    │ × │   A    │    r = ランク（通常4~64）
│ d × r  │   │ r × d  │
└────────┘   └────────┘
  4096×8      8×4096     = 6.5万パラメータ（99.6%削減！）

A と B のみを学習、W は変更なし！
\`\`\`

---

## ファインチューニングツール推奨

### 1. LLaMA-Factory（初心者推奨）

\`\`\`bash
git clone https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -r requirements.txt

# Web UIを提供、コード不要
python src/train_web.py
\`\`\`

### 2. Hugging Face PEFT

\`\`\`bash
pip install transformers peft datasets accelerate
\`\`\`

---

## 本節のポイント

1. **ファインチューニングの目的** —— 汎用モデルを特定ドメイン/タスクに適応
2. **LoRA** —— 最も推奨される方法、パラメータ効率が高い
3. **データ準備** —— 量より質が重要
4. **ツール選択** —— LLaMA-Factoryは初心者向け、PEFTはより柔軟
            `
          }
        },
        {
          id: 'ch1-lora-practice',
          title: { zh: '1.4 LoRA 微调实战', ja: '1.4 LoRA ファインチューニング実践' },
          content: {
            zh: `
## LoRA 微调实战指南

本节将手把手教你使用 LoRA 微调一个大模型。

---

## 环境准备

### 硬件要求

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        不同模型的显存需求                                 │
└─────────────────────────────────────────────────────────────────────────┘

  模型大小      LoRA 微调      QLoRA 微调     全量微调
  ────────      ─────────      ──────────     ────────
  7B            16 GB          8 GB           60+ GB
  13B           24 GB          12 GB          100+ GB
  70B           80+ GB         48 GB          500+ GB

  推荐配置：
  • 入门级：RTX 3090/4090（24GB）→ 7B-13B LoRA
  • 专业级：A100（40/80GB）→ 70B LoRA
  • 云端：使用 Colab Pro 或云 GPU
\`\`\`

### 安装依赖

\`\`\`bash
# 基础依赖
pip install torch transformers datasets
pip install peft accelerate bitsandbytes
pip install wandb  # 可选，用于训练监控

# 验证 CUDA
python -c "import torch; print(torch.cuda.is_available())"
\`\`\`

---

## 使用 PEFT 进行 LoRA 微调

### 完整代码示例

\`\`\`python
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import load_dataset

# 1. 加载基础模型和分词器
model_name = "meta-llama/Llama-2-7b-hf"
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

# 2. 配置 LoRA
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=8,                      # LoRA 秩，越大效果越好但参数越多
    lora_alpha=32,            # 缩放因子
    lora_dropout=0.1,         # Dropout 防止过拟合
    target_modules=[          # 要应用 LoRA 的模块
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
    ]
)

# 3. 创建 PEFT 模型
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
# 输出：trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06%

# 4. 准备数据集
def format_instruction(sample):
    return f"""### 指令：
{sample['instruction']}

### 输入：
{sample['input']}

### 回答：
{sample['output']}"""

def tokenize(sample):
    result = tokenizer(
        format_instruction(sample),
        truncation=True,
        max_length=512,
        padding="max_length"
    )
    result["labels"] = result["input_ids"].copy()
    return result

dataset = load_dataset("json", data_files="train_data.json")
tokenized_dataset = dataset.map(tokenize)

# 5. 配置训练参数
training_args = TrainingArguments(
    output_dir="./lora_output",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    fp16=True,
)

# 6. 开始训练
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    data_collator=DataCollatorForSeq2Seq(tokenizer, padding=True),
)

trainer.train()

# 7. 保存 LoRA 权重
model.save_pretrained("./lora_weights")
\`\`\`

---

## 使用 LLaMA-Factory（推荐新手）

### 通过 Web UI 微调

\`\`\`bash
# 1. 克隆仓库
git clone https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory

# 2. 安装依赖
pip install -r requirements.txt

# 3. 启动 Web UI
python src/train_web.py
\`\`\`

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    LLaMA-Factory Web UI                                  │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  模型选择: [Llama-2-7B ▼]                                               │
│                                                                         │
│  微调方法: ○ 全量  ● LoRA  ○ QLoRA                                      │
│                                                                         │
│  训练数据: [选择文件...]  train_data.json                                │
│                                                                         │
│  LoRA 秩:    [8    ]    学习率: [2e-4  ]                                │
│  训练轮数:   [3    ]    批次大小: [4   ]                                 │
│                                                                         │
│              [开始训练]    [导出模型]                                     │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
\`\`\`

### 通过命令行微调

\`\`\`bash
# 使用配置文件
CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\
    --stage sft \\
    --model_name_or_path meta-llama/Llama-2-7b-hf \\
    --do_train \\
    --dataset your_dataset \\
    --template llama2 \\
    --finetuning_type lora \\
    --lora_target q_proj,v_proj \\
    --output_dir ./lora_output \\
    --per_device_train_batch_size 4 \\
    --gradient_accumulation_steps 4 \\
    --lr_scheduler_type cosine \\
    --logging_steps 10 \\
    --save_steps 1000 \\
    --learning_rate 5e-5 \\
    --num_train_epochs 3.0 \\
    --fp16
\`\`\`

---

## QLoRA：更低显存的选择

QLoRA = 量化 + LoRA，可以在消费级显卡上微调大模型。

\`\`\`python
from transformers import BitsAndBytesConfig

# 4-bit 量化配置
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# 加载量化模型
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto"
)

# 然后正常应用 LoRA
model = get_peft_model(model, lora_config)

# 显存对比（7B 模型）：
# - FP16: ~14GB
# - 4-bit QLoRA: ~6GB
\`\`\`

---

## 微调后的模型使用

### 加载 LoRA 权重

\`\`\`python
from peft import PeftModel

# 加载基础模型
base_model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    torch_dtype=torch.float16,
    device_map="auto"
)

# 加载 LoRA 权重
model = PeftModel.from_pretrained(
    base_model,
    "./lora_weights"
)

# 推理
inputs = tokenizer("你好，请介绍一下你自己", return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=100)
print(tokenizer.decode(outputs[0]))
\`\`\`

### 合并 LoRA 权重

\`\`\`python
# 将 LoRA 权重合并到基础模型
merged_model = model.merge_and_unload()

# 保存合并后的完整模型
merged_model.save_pretrained("./merged_model")
tokenizer.save_pretrained("./merged_model")
\`\`\`

---

## 常见问题与解决

| 问题 | 可能原因 | 解决方案 |
|------|---------|---------|
| 显存不足 | 模型太大 | 使用 QLoRA 或减小 batch size |
| 训练不收敛 | 学习率不合适 | 尝试 1e-4 ~ 5e-5 |
| 过拟合 | 数据太少 | 增加数据或加大 dropout |
| 效果不好 | 数据质量差 | 清洗数据，提高质量 |
| 速度太慢 | 没有使用混合精度 | 开启 fp16/bf16 |

---

## 本节要点

1. **环境准备** —— 显存决定能微调多大的模型
2. **PEFT 库** —— 官方推荐的参数高效微调库
3. **LLaMA-Factory** —— 新手友好，提供 Web UI
4. **QLoRA** —— 显存不足时的最佳选择
5. **模型合并** —— 可以将 LoRA 权重合并到基础模型
            `,
            ja: `
## LoRAファインチューニング実践ガイド

このセクションでは、LoRAを使用して大規模モデルをファインチューニングする方法を手順を追って説明します。

---

## 環境準備

### ハードウェア要件

| モデルサイズ | LoRA | QLoRA | フルチューニング |
|------------|------|-------|---------------|
| 7B | 16 GB | 8 GB | 60+ GB |
| 13B | 24 GB | 12 GB | 100+ GB |
| 70B | 80+ GB | 48 GB | 500+ GB |

### 依存関係インストール

\`\`\`bash
pip install torch transformers datasets
pip install peft accelerate bitsandbytes
\`\`\`

---

## PEFTを使用したLoRAファインチューニング

\`\`\`python
from peft import LoraConfig, get_peft_model, TaskType

# LoRA設定
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=8,                      # LoRAランク
    lora_alpha=32,            # スケーリング係数
    lora_dropout=0.1,         # ドロップアウト
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)

# PEFTモデル作成
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
# 出力：trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06%
\`\`\`

---

## LLaMA-Factory使用（初心者推奨）

\`\`\`bash
# 1. リポジトリをクローン
git clone https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory

# 2. 依存関係インストール
pip install -r requirements.txt

# 3. Web UI起動
python src/train_web.py
\`\`\`

Web UIで簡単にファインチューニング設定が可能です。

---

## QLoRA：より低いVRAMでの選択

\`\`\`python
from transformers import BitsAndBytesConfig

# 4-bit量子化設定
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# VRAM比較（7Bモデル）：
# - FP16: ~14GB
# - 4-bit QLoRA: ~6GB
\`\`\`

---

## 本節のポイント

1. **環境準備** —— VRAMがファインチューニング可能なモデルサイズを決定
2. **PEFTライブラリ** —— 公式推奨のパラメータ効率的なファインチューニングライブラリ
3. **LLaMA-Factory** —— 初心者向け、Web UI提供
4. **QLoRA** —— VRAM不足時の最良の選択
            `
          }
        },
        {
          id: 'ch1-case-study',
          title: { zh: '1.5 实战案例：客服机器人微调', ja: '1.5 実践例：カスタマーサービスボットのファインチューニング' },
          content: {
            zh: `
## 🎯 实战目标

微调一个专门回答电商客服问题的 AI 助手。

---

## 📊 案例背景

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                     电商客服机器人微调项目                                 │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  目标：让 AI 掌握公司产品知识，以专业客服语气回答问题                        │
│                                                                         │
│  基座模型：Qwen2-7B-Instruct（开源、中文能力强）                           │
│  微调方法：LoRA（显存友好）                                               │
│  训练数据：1000 条客服对话记录                                            │
│  硬件：RTX 4090 24GB                                                    │
│  预计时间：2-3 小时                                                      │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
\`\`\`

---

## 📝 Step 1: 准备训练数据

### 数据格式示例

\`\`\`json
{
  "conversations": [
    {
      "role": "system",
      "content": "你是XX电商的专业客服，态度友好、回答专业准确。"
    },
    {
      "role": "user",
      "content": "你们的退货政策是什么？"
    },
    {
      "role": "assistant",
      "content": "您好！我们提供7天无理由退货服务。商品需保持原包装完好，配件齐全。退货运费由我们承担，退款将在收到商品后3个工作日内原路返回。请问还有其他问题吗？"
    }
  ]
}
\`\`\`

### 数据采集来源

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        训练数据来源                                       │
└─────────────────────────────────────────────────────────────────────────┘

  1. 历史客服记录（脱敏处理）
     ├─ 筛选高质量对话
     ├─ 人工纠正错误回答
     └─ 统一格式

  2. 人工编写样本
     ├─ 覆盖常见问题
     ├─ 包含边界情况
     └─ 体现品牌调性

  3. GPT-4 辅助生成
     ├─ 提供场景描述
     ├─ 生成多样化问答
     └─ 人工审核筛选

  建议配比：真实数据 60% + 人工编写 25% + AI生成 15%
\`\`\`

### 数据质量检查清单

| 检查项 | 标准 | ✓/✗ |
|--------|------|-----|
| 格式正确 | JSON 可解析 | ✓ |
| 无空值 | 每个字段都有内容 | ✓ |
| 语气一致 | 符合客服专业调性 | ✓ |
| 信息准确 | 产品信息无误 | ✓ |
| 长度适中 | 回答 50-300 字 | ✓ |
| 覆盖全面 | 涵盖主要问题类型 | ✓ |

---

## 💻 Step 2: 使用 Unsloth 快速微调

### 为什么用 Unsloth？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    Unsloth vs 传统 PEFT 对比                             │
└─────────────────────────────────────────────────────────────────────────┘

  指标              Unsloth          传统 PEFT
  ────              ───────          ─────────
  训练速度           2-5x 更快        基准
  显存占用           减少 60%         基准
  安装难度           一行命令         需要配置
  支持模型           主流 LLM         更广泛

  结论：新手强烈推荐 Unsloth！
\`\`\`

### 安装 Unsloth

\`\`\`bash
# 一键安装（推荐）
pip install unsloth

# 或从源码安装（获取最新功能）
pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
\`\`\`

### 完整微调代码

\`\`\`python
from unsloth import FastLanguageModel
from trl import SFTTrainer
from transformers import TrainingArguments
from datasets import load_dataset
import torch

# ============================================
# 1. 加载模型（自动应用 Unsloth 优化）
# ============================================
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/Qwen2-7B-Instruct",  # 使用 Unsloth 优化版
    max_seq_length=2048,
    dtype=None,  # 自动检测
    load_in_4bit=True,  # 使用 4-bit 量化，显存友好
)

# ============================================
# 2. 添加 LoRA 适配器
# ============================================
model = FastLanguageModel.get_peft_model(
    model,
    r=16,                          # LoRA 秩，推荐 8-64
    target_modules=[               # 目标模块
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ],
    lora_alpha=16,                 # 缩放因子
    lora_dropout=0,                # Unsloth 优化，设为 0 更快
    bias="none",
    use_gradient_checkpointing="unsloth",  # 节省显存
)

print(f"可训练参数: {model.print_trainable_parameters()}")
# 输出: trainable params: 41,943,040 || all params: 7,657,616,384 || 0.55%

# ============================================
# 3. 准备数据集
# ============================================
def formatting_prompts(examples):
    """将对话格式化为模型输入"""
    texts = []
    for conv in examples["conversations"]:
        text = ""
        for msg in conv:
            if msg["role"] == "system":
                text += f"<|im_start|>system\\n{msg['content']}<|im_end|>\\n"
            elif msg["role"] == "user":
                text += f"<|im_start|>user\\n{msg['content']}<|im_end|>\\n"
            elif msg["role"] == "assistant":
                text += f"<|im_start|>assistant\\n{msg['content']}<|im_end|>\\n"
        texts.append(text)
    return {"text": texts}

# 加载数据
dataset = load_dataset("json", data_files="customer_service_data.json")
dataset = dataset.map(formatting_prompts, batched=True)

# ============================================
# 4. 配置训练参数
# ============================================
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset["train"],
    dataset_text_field="text",
    max_seq_length=2048,
    args=TrainingArguments(
        output_dir="./customer_service_lora",
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        warmup_steps=10,
        max_steps=500,               # 1000条数据约500步
        learning_rate=2e-4,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=10,
        save_steps=100,
        optim="adamw_8bit",          # 8-bit 优化器，节省显存
    ),
)

# ============================================
# 5. 开始训练！
# ============================================
print("开始训练...")
trainer.train()

# ============================================
# 6. 保存模型
# ============================================
model.save_pretrained("./customer_service_lora")
tokenizer.save_pretrained("./customer_service_lora")

print("✅ 训练完成！模型已保存。")
\`\`\`

---

## 📈 Step 3: 训练监控与调优

### 观察训练 Loss

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        训练 Loss 曲线解读                                 │
└─────────────────────────────────────────────────────────────────────────┘

  Loss
   3.0 ┤
       │╲
   2.5 ┤ ╲
       │  ╲
   2.0 ┤   ╲
       │    ╲──────
   1.5 ┤           ╲
       │            ╲
   1.0 ┤             ╲______________________
       │
   0.5 ┤
       └────────────────────────────────────────
        0   100   200   300   400   500  Step

  ✅ 健康曲线：快速下降后趋于平稳
  ⚠️ 不收敛：Loss 不下降 → 调大学习率
  ⚠️ 震荡大：上下波动剧烈 → 调小学习率
  ⚠️ 过拟合：训练 Loss 极低但验证 Loss 上升 → 减少训练步数
\`\`\`

### 超参数调优建议

| 场景 | 调整建议 |
|------|---------|
| Loss 不下降 | 增大 learning_rate 到 5e-4 |
| Loss 震荡 | 减小 learning_rate 到 1e-4 |
| 显存不足 | 减小 batch_size，增大 gradient_accumulation |
| 效果不好 | 增大 r 值（如 32 或 64） |
| 过拟合 | 减少 max_steps，增加数据 |

---

## 🧪 Step 4: 效果评估

### 自动评估

\`\`\`python
from transformers import pipeline

# 加载微调后的模型
pipe = pipeline("text-generation", model="./customer_service_lora")

# 测试用例
test_cases = [
    "你们支持花呗付款吗？",
    "我的订单还没发货，能催一下吗？",
    "商品有质量问题怎么办？",
    "能开发票吗？",
]

print("=" * 60)
print("模型效果测试")
print("=" * 60)

for question in test_cases:
    prompt = f"<|im_start|>system\\n你是XX电商的专业客服。<|im_end|>\\n<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant\\n"

    response = pipe(prompt, max_new_tokens=200, temperature=0.7)
    answer = response[0]['generated_text'].split("<|im_start|>assistant\\n")[-1]

    print(f"\\n用户: {question}")
    print(f"客服: {answer}")
\`\`\`

### 人工评估标准

| 维度 | 1分 | 3分 | 5分 |
|------|-----|-----|-----|
| 准确性 | 信息错误 | 基本正确 | 完全准确 |
| 专业度 | 口语化 | 一般 | 专业术语恰当 |
| 态度 | 生硬 | 普通 | 热情友好 |
| 完整性 | 答非所问 | 部分解答 | 完整全面 |

---

## 💰 成本估算

### 云 GPU 价格参考（2025年）

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        云 GPU 价格对比                                    │
└─────────────────────────────────────────────────────────────────────────┘

  平台              GPU              价格（$/小时）   适合模型
  ────              ───              ──────────────   ────────
  Lambda Labs       RTX 4090         $0.75            7B LoRA
  Vast.ai           RTX 4090         $0.50-0.80       7B LoRA
  RunPod            RTX 4090         $0.69            7B LoRA
  AWS               A10G             $1.00            7B-13B
  Google Cloud      A100 40GB        $3.67            13B-70B
  AutoDL (国内)     RTX 4090         ¥2.5/小时        7B LoRA

  本案例预估成本：
  ┌────────────────────────────────────────────┐
  │  RTX 4090 × 3小时 ≈ $2.25（约 ¥16）        │
  │  性价比极高！                              │
  └────────────────────────────────────────────┘
\`\`\`

---

## 🚀 Step 5: 部署上线

### 使用 Ollama 本地部署

\`\`\`bash
# 1. 安装 Ollama
curl -fsSL https://ollama.com/install.sh | sh

# 2. 创建 Modelfile
cat > Modelfile << 'EOF'
FROM ./customer_service_merged

SYSTEM "你是XX电商的专业客服，态度友好、回答专业准确。"

PARAMETER temperature 0.7
PARAMETER top_p 0.9
EOF

# 3. 创建 Ollama 模型
ollama create customer-service -f Modelfile

# 4. 运行
ollama run customer-service
\`\`\`

### 使用 vLLM 高性能部署

\`\`\`bash
# 安装 vLLM
pip install vllm

# 启动 API 服务
python -m vllm.entrypoints.openai.api_server \\
    --model ./customer_service_merged \\
    --host 0.0.0.0 \\
    --port 8000

# 调用 API
curl http://localhost:8000/v1/chat/completions \\
    -H "Content-Type: application/json" \\
    -d '{
        "model": "customer_service",
        "messages": [{"role": "user", "content": "退货怎么操作？"}]
    }'
\`\`\`

---

## ✅ 本节要点

1. **数据准备** —— 质量 > 数量，建议 1000+ 条
2. **Unsloth** —— 速度快 2-5x，显存省 60%
3. **训练监控** —— 观察 Loss 曲线，及时调参
4. **效果评估** —— 自动 + 人工双重验证
5. **成本控制** —— 云 GPU 微调成本低至 ¥20
6. **部署上线** —— Ollama（简单）或 vLLM（高性能）
            `,
            ja: `
## 🎯 実践目標

ECサイトのカスタマーサービス問い合わせに特化したAIアシスタントをファインチューニング。

---

## 📊 ケース背景

\`\`\`
目標：AIに自社製品知識を習得させ、プロの接客トーンで回答
ベースモデル：Qwen2-7B-Instruct
ファインチューニング方法：LoRA
トレーニングデータ：1000件の接客対話
ハードウェア：RTX 4090 24GB
予想時間：2-3時間
\`\`\`

---

## 💻 Unslothで高速ファインチューニング

### なぜUnsloth？

| 指標 | Unsloth | 従来のPEFT |
|------|---------|-----------|
| 学習速度 | 2-5倍高速 | 基準 |
| VRAM使用量 | 60%削減 | 基準 |
| インストール | 1行 | 設定必要 |

### インストール

\`\`\`bash
pip install unsloth
\`\`\`

### 完全なコード例

\`\`\`python
from unsloth import FastLanguageModel
from trl import SFTTrainer
from transformers import TrainingArguments

# モデル読み込み
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/Qwen2-7B-Instruct",
    max_seq_length=2048,
    load_in_4bit=True,
)

# LoRAアダプター追加
model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_alpha=16,
)

# トレーナー設定
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    args=TrainingArguments(
        output_dir="./lora_output",
        per_device_train_batch_size=2,
        max_steps=500,
        learning_rate=2e-4,
    ),
)

# 学習開始
trainer.train()
\`\`\`

---

## 💰 コスト見積もり

| プラットフォーム | GPU | 価格/時間 |
|----------------|-----|----------|
| Lambda Labs | RTX 4090 | $0.75 |
| RunPod | RTX 4090 | $0.69 |
| AWS | A10G | $1.00 |

本ケースの推定コスト：約$2-3（3時間）

---

## 🚀 デプロイ

### Ollamaでローカルデプロイ

\`\`\`bash
# インストール
curl -fsSL https://ollama.com/install.sh | sh

# モデル作成
ollama create customer-service -f Modelfile

# 実行
ollama run customer-service
\`\`\`

---

## ✅ 本節のポイント

1. **データ準備** —— 質 > 量、1000件以上推奨
2. **Unsloth** —— 2-5倍高速、60% VRAM削減
3. **コスト管理** —— クラウドGPUで約$2-3
4. **デプロイ** —— Ollama（簡単）またはvLLM（高性能）
            `
          }
        },
        {
          id: 'ch1-local-deploy',
          title: { zh: '1.6 本地部署与 Ollama', ja: '1.6 ローカルデプロイとOllama' },
          content: {
            zh: `
## 为什么要本地部署？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                     本地部署 vs 云端 API                                  │
└─────────────────────────────────────────────────────────────────────────┘

                    本地部署                    云端 API
                    ────────                    ────────
  隐私性            ✅ 数据不离开本地            ❌ 数据发送到云端
  成本              ✅ 一次投入长期免费           ❌ 按调用付费
  速度              ✅ 无网络延迟                ❌ 依赖网络
  定制性            ✅ 可以微调                  ⚠️ 部分支持
  模型选择          ⚠️ 受硬件限制               ✅ 最强模型

  适用场景：
  • 隐私敏感数据处理
  • 离线环境使用
  • 高频调用节省成本
  • 学习研究目的
\`\`\`

---

## Ollama：最简单的本地部署方案

### 安装 Ollama

\`\`\`bash
# macOS / Linux
curl -fsSL https://ollama.com/install.sh | sh

# Windows
# 下载安装包：https://ollama.com/download/windows
\`\`\`

### 运行开源模型

\`\`\`bash
# 运行 Llama 3.1 8B（推荐入门）
ollama run llama3.1

# 运行 Qwen2 7B（中文能力强）
ollama run qwen2

# 运行 CodeLlama（代码生成）
ollama run codellama

# 运行 DeepSeek Coder（代码能力强）
ollama run deepseek-coder:6.7b
\`\`\`

### 常用模型推荐

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                     Ollama 热门模型                                      │
└─────────────────────────────────────────────────────────────────────────┘

  模型名称              大小        特点                  显存需求
  ────────              ────        ────                  ────────
  llama3.1:8b           4.7GB       通用能力强             8GB+
  qwen2:7b              4.4GB       中文能力最强           8GB+
  mistral:7b            4.1GB       速度快、效果好         8GB+
  codellama:7b          3.8GB       代码生成专用           8GB+
  deepseek-coder:6.7b   3.8GB       代码能力强             8GB+
  phi3:3.8b             2.2GB       小巧但能力强           4GB+
  gemma2:2b             1.6GB       超小模型入门           4GB+

  选择建议：
  • 8GB 显存：选 7B 以下模型
  • 16GB 显存：可以跑 13B 模型
  • 24GB 显存：可以跑量化的 70B 模型
\`\`\`

### Ollama 命令速查

\`\`\`bash
# 查看已下载模型
ollama list

# 下载模型（不运行）
ollama pull llama3.1

# 删除模型
ollama rm llama3.1

# 查看模型信息
ollama show llama3.1

# 复制模型
ollama cp llama3.1 my-llama

# 启动 API 服务（默认 11434 端口）
ollama serve

# API 调用示例
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.1",
  "prompt": "为什么天空是蓝色的？",
  "stream": false
}'
\`\`\`

---

## 使用自定义模型

### 导入 GGUF 模型

\`\`\`bash
# 1. 下载 GGUF 格式模型（从 Hugging Face）
# 例如：TheBloke/Llama-2-7B-GGUF

# 2. 创建 Modelfile
cat > Modelfile << 'EOF'
FROM ./llama-2-7b.Q4_K_M.gguf

# 设置系统提示词
SYSTEM "你是一个专业的AI助手，回答问题时要准确、简洁。"

# 设置参数
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER num_ctx 4096
EOF

# 3. 创建 Ollama 模型
ollama create my-llama -f Modelfile

# 4. 运行
ollama run my-llama
\`\`\`

### 导入微调后的模型

\`\`\`bash
# 1. 合并 LoRA 权重（如果还没合并）
python -c "
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer

base = AutoModelForCausalLM.from_pretrained('Qwen/Qwen2-7B-Instruct')
model = PeftModel.from_pretrained(base, './my_lora')
merged = model.merge_and_unload()
merged.save_pretrained('./merged_model')
"

# 2. 转换为 GGUF 格式
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp

python convert_hf_to_gguf.py ../merged_model --outtype q4_k_m

# 3. 创建 Ollama 模型
ollama create my-finetuned -f Modelfile
\`\`\`

---

## 配合 Open WebUI 使用

\`\`\`bash
# 使用 Docker 一键部署
docker run -d -p 3000:8080 \\
  --add-host=host.docker.internal:host-gateway \\
  -v open-webui:/app/backend/data \\
  --name open-webui \\
  ghcr.io/open-webui/open-webui:main

# 访问 http://localhost:3000
# 自动连接本地 Ollama
\`\`\`

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                     Open WebUI 界面                                      │
├─────────────────────────────────────────────────────────────────────────┤
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  🤖 Select a model: [llama3.1 ▼]                                │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  User: 帮我写一首关于春天的诗                                     │   │
│  │  ────────────────────────────────────────────────────────────   │   │
│  │  Assistant: 春风拂柳绿意浓，                                      │   │
│  │            花开满园香飘送。                                       │   │
│  │            蝴蝶翩翩舞枝头，                                       │   │
│  │            燕子归来筑新梦。                                       │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  Type a message...                                    [Send]    │   │
│  └─────────────────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────────────────┘
\`\`\`

---

## 性能优化技巧

### GPU 加速

\`\`\`bash
# 确保 NVIDIA 驱动已安装
nvidia-smi

# Ollama 会自动检测并使用 GPU
# 如需指定 GPU：
CUDA_VISIBLE_DEVICES=0 ollama run llama3.1

# 查看 GPU 使用情况
watch -n 1 nvidia-smi
\`\`\`

### 模型量化选择

| 量化等级 | 大小 | 质量 | 速度 | 推荐场景 |
|---------|------|------|------|---------|
| Q8_0 | 100% | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | 质量优先 |
| Q6_K | 75% | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | 平衡 |
| Q4_K_M | 50% | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | **推荐** |
| Q4_0 | 45% | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ | 速度优先 |
| Q2_K | 30% | ⭐⭐ | ⭐⭐⭐⭐⭐ | 极限压缩 |

---

## 本节要点

1. **Ollama** —— 最简单的本地 LLM 部署方案
2. **模型选择** —— 根据显存选择合适大小的模型
3. **自定义模型** —— 支持导入 GGUF 格式和微调模型
4. **Open WebUI** —— 提供 ChatGPT 风格的 Web 界面
5. **性能优化** —— 使用 Q4_K_M 量化获得最佳性价比
            `,
            ja: `
## なぜローカルデプロイ？

| 項目 | ローカル | クラウドAPI |
|------|---------|------------|
| プライバシー | ✅ データはローカルに | ❌ クラウドに送信 |
| コスト | ✅ 一度の投資で長期無料 | ❌ 従量課金 |
| 速度 | ✅ ネットワーク遅延なし | ❌ ネットワーク依存 |

---

## Ollama：最もシンプルなローカルデプロイ

### インストール

\`\`\`bash
# macOS / Linux
curl -fsSL https://ollama.com/install.sh | sh
\`\`\`

### モデル実行

\`\`\`bash
# Llama 3.1 8B実行
ollama run llama3.1

# Qwen2 7B（中国語に強い）
ollama run qwen2
\`\`\`

### おすすめモデル

| モデル | サイズ | 特徴 | VRAM |
|--------|-------|------|------|
| llama3.1:8b | 4.7GB | 汎用性高い | 8GB+ |
| qwen2:7b | 4.4GB | 中国語最強 | 8GB+ |
| phi3:3.8b | 2.2GB | 小型で高性能 | 4GB+ |

---

## カスタムモデル

\`\`\`bash
# GGUFモデルをインポート
cat > Modelfile << 'EOF'
FROM ./model.gguf
SYSTEM "あなたはプロのAIアシスタントです。"
EOF

ollama create my-model -f Modelfile
\`\`\`

---

## Open WebUI

\`\`\`bash
# Docker で一発デプロイ
docker run -d -p 3000:8080 \\
  --add-host=host.docker.internal:host-gateway \\
  ghcr.io/open-webui/open-webui:main

# http://localhost:3000 でアクセス
\`\`\`

---

## 本節のポイント

1. **Ollama** —— 最もシンプルなローカルLLMデプロイ
2. **モデル選択** —— VRAMに応じて適切なサイズを選択
3. **Open WebUI** —— ChatGPT風のWebインターフェース
            `
          }
        },
        {
          id: 'ch1-quantization',
          title: { zh: '1.7 模型量化压缩', ja: '1.7 モデル量子化圧縮' },
          content: {
            zh: `
## 让大模型"瘦身"：量化技术详解

当你想在消费级显卡上运行 70B 大模型时，量化就是你的救星。

---

## 🎯 什么是量化？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        模型量化原理                                       │
└─────────────────────────────────────────────────────────────────────────┘

  原始模型 (FP32)                    量化后模型 (INT4)
  ┌────────────────┐                ┌────────────────┐
  │ 参数: 3.14159  │   ───量化───▶   │ 参数: 3        │
  │ 占用: 32 bits  │                │ 占用: 4 bits   │
  │ 精度: 最高     │                │ 精度: 略降     │
  └────────────────┘                └────────────────┘
         │                                  │
         ▼                                  ▼
   70B 模型 ≈ 140GB                  70B 模型 ≈ 35GB
   需要 A100 80G×2                   可用 RTX 4090 24G
\`\`\`

**核心思想**：用更少的 bit 数存储参数，牺牲少量精度换取巨大的内存节省。

---

## 📊 主流量化方法对比

| 方法 | 精度 | 速度 | VRAM占用 | 适用场景 |
|------|------|------|----------|----------|
| **FP32** | ⭐⭐⭐⭐⭐ | ⭐⭐ | 100% | 训练基准 |
| **FP16** | ⭐⭐⭐⭐ | ⭐⭐⭐ | 50% | 推理标准 |
| **INT8** | ⭐⭐⭐ | ⭐⭐⭐⭐ | 25% | 部署优化 |
| **INT4** | ⭐⭐ | ⭐⭐⭐⭐⭐ | 12.5% | 极限压缩 |

---

## 🔧 主流量化格式详解

### 1️⃣ GGUF (原 GGML)

**最流行的 CPU/GPU 混合推理格式**，由 llama.cpp 开发者创建。

\`\`\`bash
# 使用 llama.cpp 运行 GGUF 模型
./llama-cli -m llama-3-8b-Q4_K_M.gguf -p "你好" -n 100

# 常见量化级别
# Q2_K  - 极限压缩，质量损失大
# Q4_K_M - 推荐！平衡压缩和质量
# Q5_K_M - 高质量，稍大
# Q8_0  - 几乎无损
\`\`\`

**GGUF 量化级别选择指南**：

| 量化级别 | 7B 模型大小 | 质量 | 推荐用途 |
|----------|-------------|------|----------|
| Q2_K | ~2.5GB | ⭐⭐ | 极限内存场景 |
| Q4_K_M | ~4GB | ⭐⭐⭐⭐ | **日常使用首选** |
| Q5_K_M | ~5GB | ⭐⭐⭐⭐⭐ | 质量优先 |
| Q8_0 | ~7GB | ⭐⭐⭐⭐⭐ | 几乎无损 |

---

### 2️⃣ GPTQ

**GPU 专用量化**，推理速度快，需要 GPU。

\`\`\`python
from transformers import AutoModelForCausalLM, AutoTokenizer

# 加载 GPTQ 量化模型
model_id = "TheBloke/Llama-2-7B-GPTQ"
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    trust_remote_code=True
)
tokenizer = AutoTokenizer.from_pretrained(model_id)

# 正常使用
output = model.generate(
    tokenizer("Hello", return_tensors="pt").input_ids.cuda(),
    max_new_tokens=50
)
\`\`\`

---

### 3️⃣ AWQ (Activation-aware Weight Quantization)

**更智能的量化**：根据激活值重要性决定量化精度。

\`\`\`python
from awq import AutoAWQForCausalLM
from transformers import AutoTokenizer

# 加载 AWQ 模型
model_id = "TheBloke/Llama-2-7B-AWQ"
model = AutoAWQForCausalLM.from_quantized(
    model_id,
    fuse_layers=True,  # 融合层加速
    device_map="auto"
)

# AWQ 优势：
# - 比 GPTQ 更好的精度保持
# - 更快的推理速度
# - 更低的内存占用
\`\`\`

---

### 4️⃣ bitsandbytes (QLoRA)

**训练和推理都能用**，HuggingFace 官方支持。

\`\`\`python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 4-bit 量化配置
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",        # NormalFloat4
    bnb_4bit_compute_dtype="float16",
    bnb_4bit_use_double_quant=True    # 双重量化
)

# 加载模型
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3-8B",
    quantization_config=bnb_config,
    device_map="auto"
)

# 8-bit 量化 (更简单)
model_8bit = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3-8B",
    load_in_8bit=True,
    device_map="auto"
)
\`\`\`

---

## 🎮 实战：不同显存的最佳选择

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    显存 vs 可运行模型                                     │
└─────────────────────────────────────────────────────────────────────────┘

  显存容量          推荐模型 & 量化方案
  ─────────────────────────────────────────────────────

  4GB  (GTX 1650)   → Qwen2-1.5B (FP16) 或 7B (Q2_K)
       │
       ▼
  8GB  (RTX 3060)   → Llama3-8B (Q4_K_M) 或 Qwen2-7B (Q5_K_M)
       │
       ▼
  12GB (RTX 3060Ti) → Llama3-8B (Q8_0) 或 13B (Q4_K_M)
       │
       ▼
  24GB (RTX 4090)   → Llama3-70B (Q4_K_M) 或 Qwen2-72B (Q4_K_M)
       │
       ▼
  48GB (A6000×2)    → Llama3-70B (FP16) 或 更大模型
\`\`\`

---

## 🛠️ 自己动手量化

### 使用 llama.cpp 量化

\`\`\`bash
# 1. 克隆 llama.cpp
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp && make -j

# 2. 转换 HuggingFace 模型到 GGUF
python convert_hf_to_gguf.py /path/to/model --outfile model.gguf

# 3. 量化
./llama-quantize model.gguf model-Q4_K_M.gguf Q4_K_M
\`\`\`

### 使用 AutoGPTQ 量化

\`\`\`python
from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig

# 配置量化参数
quantize_config = BaseQuantizeConfig(
    bits=4,
    group_size=128,
    damp_percent=0.1,
    desc_act=False
)

# 加载模型并量化
model = AutoGPTQForCausalLM.from_pretrained(
    "meta-llama/Llama-3-8B",
    quantize_config
)

# 准备校准数据
examples = [tokenizer(text, return_tensors="pt") for text in calibration_texts]

# 执行量化
model.quantize(examples)

# 保存
model.save_quantized("llama3-8b-gptq-4bit")
\`\`\`

---

## ⚖️ 如何选择量化方案？

\`\`\`
                        选择决策树
                            │
              ┌─────────────┴─────────────┐
              │      你有 GPU 吗？         │
              └─────────────┬─────────────┘
                   ╱                ╲
                 是                  否
                 │                   │
        ┌────────┴────────┐         │
        │  推理还是训练？  │         ▼
        └────────┬────────┘      GGUF
             ╱         ╲        (CPU推理)
           推理        训练
            │           │
     ┌──────┴──────┐    ▼
     │ 追求速度？  │  bitsandbytes
     └──────┬──────┘  (QLoRA训练)
        ╱        ╲
      是          否
       │           │
       ▼           ▼
     AWQ        GPTQ
  (最快推理)   (兼容性好)
\`\`\`

---

## 💡 小贴士

> 🎯 **推荐组合**：
> - 本地部署：Ollama + GGUF Q4_K_M
> - GPU 推理：vLLM + AWQ
> - 微调训练：bitsandbytes + QLoRA
> - 生产环境：TensorRT-LLM + INT8
            `,
            ja: `
## 大規模モデルを「スリム化」：量子化技術詳解

消費者向けGPUで70Bモデルを動かしたい時、量子化があなたの救世主です。

---

## 🎯 量子化とは？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        モデル量子化の原理                                  │
└─────────────────────────────────────────────────────────────────────────┘

  元のモデル (FP32)                  量子化後 (INT4)
  ┌────────────────┐                ┌────────────────┐
  │ パラメータ: 3.14159 │  ──量子化──▶  │ パラメータ: 3   │
  │ 容量: 32 bits  │                │ 容量: 4 bits   │
  │ 精度: 最高     │                │ 精度: やや低下  │
  └────────────────┘                └────────────────┘
         │                                  │
         ▼                                  ▼
   70B モデル ≈ 140GB                70B モデル ≈ 35GB
   A100 80G×2 が必要                 RTX 4090 24G で動作
\`\`\`

**核心思想**：少ないbit数でパラメータを保存し、わずかな精度を犠牲に大きなメモリ節約を実現。

---

## 📊 主要な量子化手法の比較

| 手法 | 精度 | 速度 | VRAM使用 | 適用シーン |
|------|------|------|----------|------------|
| **FP32** | ⭐⭐⭐⭐⭐ | ⭐⭐ | 100% | 学習基準 |
| **FP16** | ⭐⭐⭐⭐ | ⭐⭐⭐ | 50% | 推論標準 |
| **INT8** | ⭐⭐⭐ | ⭐⭐⭐⭐ | 25% | デプロイ最適化 |
| **INT4** | ⭐⭐ | ⭐⭐⭐⭐⭐ | 12.5% | 極限圧縮 |

---

## 🔧 主要な量子化フォーマット詳解

### 1️⃣ GGUF（旧 GGML）

**最も人気のCPU/GPUハイブリッド推論形式**、llama.cpp開発者が作成。

\`\`\`bash
# llama.cpp で GGUF モデルを実行
./llama-cli -m llama-3-8b-Q4_K_M.gguf -p "こんにちは" -n 100

# 一般的な量子化レベル
# Q2_K  - 極限圧縮、品質低下大
# Q4_K_M - おすすめ！圧縮と品質のバランス
# Q5_K_M - 高品質、やや大きい
# Q8_0  - ほぼ無損失
\`\`\`

**GGUF 量子化レベル選択ガイド**：

| 量子化レベル | 7B モデルサイズ | 品質 | 推奨用途 |
|--------------|-----------------|------|----------|
| Q2_K | ~2.5GB | ⭐⭐ | 極限メモリ制限 |
| Q4_K_M | ~4GB | ⭐⭐⭐⭐ | **日常使用に最適** |
| Q5_K_M | ~5GB | ⭐⭐⭐⭐⭐ | 品質優先 |
| Q8_0 | ~7GB | ⭐⭐⭐⭐⭐ | ほぼ無損失 |

---

### 2️⃣ GPTQ

**GPU専用量子化**、高速推論、GPUが必要。

\`\`\`python
from transformers import AutoModelForCausalLM, AutoTokenizer

# GPTQ量子化モデルをロード
model_id = "TheBloke/Llama-2-7B-GPTQ"
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    trust_remote_code=True
)
tokenizer = AutoTokenizer.from_pretrained(model_id)

# 通常通り使用
output = model.generate(
    tokenizer("Hello", return_tensors="pt").input_ids.cuda(),
    max_new_tokens=50
)
\`\`\`

---

### 3️⃣ AWQ（Activation-aware Weight Quantization）

**よりスマートな量子化**：活性化値の重要性に基づいて量子化精度を決定。

\`\`\`python
from awq import AutoAWQForCausalLM
from transformers import AutoTokenizer

# AWQモデルをロード
model_id = "TheBloke/Llama-2-7B-AWQ"
model = AutoAWQForCausalLM.from_quantized(
    model_id,
    fuse_layers=True,  # レイヤー融合で高速化
    device_map="auto"
)

# AWQの利点：
# - GPTQより良い精度維持
# - より速い推論速度
# - より少ないメモリ使用
\`\`\`

---

### 4️⃣ bitsandbytes（QLoRA）

**学習と推論両方で使用可能**、HuggingFace公式サポート。

\`\`\`python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 4-bit量子化設定
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",        # NormalFloat4
    bnb_4bit_compute_dtype="float16",
    bnb_4bit_use_double_quant=True    # 二重量子化
)

# モデルをロード
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3-8B",
    quantization_config=bnb_config,
    device_map="auto"
)
\`\`\`

---

## 🎮 実践：VRAM別の最適な選択

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    VRAM vs 実行可能モデル                                 │
└─────────────────────────────────────────────────────────────────────────┘

  VRAM容量           推奨モデル & 量子化方式
  ─────────────────────────────────────────────────────

  4GB  (GTX 1650)   → Qwen2-1.5B (FP16) または 7B (Q2_K)
       │
       ▼
  8GB  (RTX 3060)   → Llama3-8B (Q4_K_M) または Qwen2-7B (Q5_K_M)
       │
       ▼
  24GB (RTX 4090)   → Llama3-70B (Q4_K_M) または Qwen2-72B (Q4_K_M)
\`\`\`

---

## 💡 ヒント

> 🎯 **おすすめの組み合わせ**：
> - ローカルデプロイ：Ollama + GGUF Q4_K_M
> - GPU推論：vLLM + AWQ
> - ファインチューニング：bitsandbytes + QLoRA
> - 本番環境：TensorRT-LLM + INT8
            `
          }
        },
        {
          id: 'ch1-summary',
          title: { zh: '1.8 本章小结', ja: '1.8 この章のまとめ' },
          content: {
            zh: `
## 大模型技术核心回顾

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    本章知识地图                                          │
└─────────────────────────────────────────────────────────────────────────┘

                        大模型技术深度解析
                              │
          ┌───────────────────┼───────────────────┐
          ▼                   ▼                   ▼
     Transformer           注意力机制            微调技术
          │                   │                   │
     ┌────┴────┐         ┌────┴────┐         ┌────┴────┐
     │ Encoder │         │  Q/K/V  │         │  LoRA   │
     │ Decoder │         │ 多头注意力│         │  QLoRA  │
     │ 位置编码 │         │ Mask注意力│         │ 数据准备 │
     └─────────┘         └─────────┘         └─────────┘
\`\`\`

---

## 关键知识点总结

### Transformer 架构

| 组件 | 作用 | 重要性 |
|------|------|--------|
| Self-Attention | 建模全局依赖 | ⭐⭐⭐⭐⭐ |
| Feed Forward | 特征变换 | ⭐⭐⭐⭐ |
| 位置编码 | 提供位置信息 | ⭐⭐⭐⭐ |
| Layer Norm | 稳定训练 | ⭐⭐⭐ |

### 注意力机制

| 类型 | 特点 | 应用 |
|------|------|------|
| Self-Attention | 序列内部关系 | 所有 Transformer |
| Cross-Attention | 序列间关系 | 翻译、问答 |
| Multi-Head | 多视角 | 增强表达能力 |
| Masked | 防止信息泄露 | 语言生成 |

### 微调方法

| 方法 | 参数量 | 显存 | 推荐度 |
|------|--------|------|--------|
| LoRA | ~0.1% | 低 | ⭐⭐⭐⭐⭐ |
| QLoRA | ~0.1% | 极低 | ⭐⭐⭐⭐ |
| 全量微调 | 100% | 极高 | ⭐⭐⭐ |

---

## 实践路线图

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    学习路线建议                                          │
└─────────────────────────────────────────────────────────────────────────┘

  第一步：理解原理
  ─────────────────
  • 阅读 "Attention Is All You Need" 论文
  • 理解 Self-Attention 计算过程
  • 了解 Encoder/Decoder 区别

  第二步：动手实践
  ─────────────────
  • 使用 LLaMA-Factory 完成第一次微调
  • 准备自己的数据集
  • 尝试不同的 LoRA 参数

  第三步：深入优化
  ─────────────────
  • 学习数据清洗和增强
  • 尝试 QLoRA 降低成本
  • 探索模型合并技术

  第四步：生产部署
  ─────────────────
  • 学习模型量化
  • 了解推理优化
  • 部署到生产环境
\`\`\`

---

## 推荐资源

### 必读论文

| 论文 | 内容 | 链接 |
|------|------|------|
| Attention Is All You Need | Transformer 原论文 | arXiv:1706.03762 |
| BERT | Encoder 代表作 | arXiv:1810.04805 |
| GPT 系列 | Decoder 代表作 | OpenAI Blog |
| LoRA | 低秩微调 | arXiv:2106.09685 |

### 实践工具

| 工具 | 用途 | 难度 |
|------|------|------|
| LLaMA-Factory | 一站式微调 | ⭐⭐ |
| PEFT | 官方参数高效库 | ⭐⭐⭐ |
| Axolotl | 配置驱动微调 | ⭐⭐⭐ |
| vLLM | 高效推理 | ⭐⭐⭐⭐ |

---

## 写在最后

理解大模型的技术原理，不是为了成为 AI 研究员，而是为了：

1. **更好地使用 AI** —— 知其然，知其所以然
2. **做出明智的选型** —— 根据需求选择合适的模型和方法
3. **定制专属 AI** —— 掌握微调能力，构建领域专属模型

技术在不断发展，但核心原理是稳定的。掌握了这些基础，你就能更好地跟上 AI 的发展步伐！

*"理解原理，才能真正驾驭工具。"*
            `,
            ja: `
## 大規模モデル技術コア復習

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    この章の知識マップ                                     │
└─────────────────────────────────────────────────────────────────────────┘

                        大規模モデル技術詳解
                              │
          ┌───────────────────┼───────────────────┐
          ▼                   ▼                   ▼
     Transformer           アテンション           ファインチューニング
          │                   │                   │
     ┌────┴────┐         ┌────┴────┐         ┌────┴────┐
     │ Encoder │         │  Q/K/V  │         │  LoRA   │
     │ Decoder │         │マルチヘッド│         │  QLoRA  │
     │位置エンコード│        │ Maskアテンション│       │ データ準備 │
     └─────────┘         └─────────┘         └─────────┘
\`\`\`

---

## 重要ポイントまとめ

### Transformerアーキテクチャ

| コンポーネント | 役割 | 重要度 |
|-------------|------|--------|
| Self-Attention | グローバル依存性モデリング | ⭐⭐⭐⭐⭐ |
| Feed Forward | 特徴変換 | ⭐⭐⭐⭐ |
| 位置エンコーディング | 位置情報提供 | ⭐⭐⭐⭐ |

### ファインチューニング方法

| 方法 | パラメータ量 | VRAM | 推奨度 |
|------|-----------|------|--------|
| LoRA | ~0.1% | 低 | ⭐⭐⭐⭐⭐ |
| QLoRA | ~0.1% | 極低 | ⭐⭐⭐⭐ |
| フルチューニング | 100% | 極高 | ⭐⭐⭐ |

---

## 実践ロードマップ

1. **原理理解** —— 論文を読み、アテンション計算を理解
2. **実践開始** —— LLaMA-Factoryで最初のファインチューニング
3. **深い最適化** —— データ準備、QLoRAでコスト削減
4. **本番デプロイ** —— 量子化、推論最適化

---

## 最後に

大規模モデルの技術原理を理解することは、AI研究者になるためではなく：

1. **AIをより良く使う** —— 原理を知れば、より効果的に活用できる
2. **賢い選択** —— ニーズに応じた適切なモデルと方法を選択
3. **専用AIをカスタマイズ** —— ファインチューニング能力でドメイン特化モデルを構築

*「原理を理解してこそ、ツールを本当に使いこなせる。」*
            `
          }
        }
      ]
    },
    // ============================================
    // 第二章：提示词工程进阶
    // ============================================
    {
      id: 'chapter-2',
      number: 2,
      title: { zh: '提示词工程进阶', ja: 'プロンプトエンジニアリング上級編' },
      subtitle: { zh: '掌握与AI对话的艺术', ja: 'AIとの対話術をマスターする' },
      sections: [
        {
          id: 'ch2-intro',
          title: { zh: '引言：提示词的力量', ja: '序章：プロンプトの力' },
          content: {
            zh: `
同样的 AI，不同的提示词，结果可能天差地别。

**提示词工程（Prompt Engineering）** 是一门让 AI 更好地理解你、更精准地完成任务的技术。掌握它，你就能让 AI 发挥出 10 倍的能力。

---

## 为什么提示词如此重要？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        提示词的影响力                                    │
└─────────────────────────────────────────────────────────────────────────┘

  普通提示词                              优秀提示词
  ┌─────────────┐                        ┌─────────────┐
  │  写一篇文章  │                        │ 角色 + 任务  │
  │             │                        │ + 格式 + 约束│
  └─────────────┘                        └─────────────┘
        │                                      │
        ▼                                      ▼
  ┌─────────────┐                        ┌─────────────┐
  │  泛泛而谈    │                        │ 专业、精准   │
  │  结构混乱    │                        │ 结构清晰     │
  │  需要多次修改│                        │ 一次到位     │
  └─────────────┘                        └─────────────┘

  效率: ★★☆☆☆                           效率: ★★★★★
\`\`\`

---

## 本章你将学到

1. **提示词的核心结构** —— 如何组织一个好的提示词
2. **常用提示词模式** —— 可以直接套用的模板
3. **角色扮演技巧** —— 让 AI 变成各种专家
4. **思维链与推理** —— 让 AI 一步步思考
5. **高级技巧与实战** —— 真实场景的应用

让我们开始掌握这门"与 AI 对话的艺术"！
            `,
            ja: `
同じAIでも、プロンプトが違えば結果は天と地ほど違います。

**プロンプトエンジニアリング（Prompt Engineering）** は、AIにあなたをより良く理解させ、より正確にタスクを完了させる技術です。これをマスターすれば、AIの能力を10倍引き出せます。

---

## なぜプロンプトがこれほど重要なのか？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        プロンプトの影響力                                │
└─────────────────────────────────────────────────────────────────────────┘

  普通のプロンプト                        優れたプロンプト
  ┌─────────────┐                        ┌─────────────┐
  │  記事を書いて │                        │ 役割 + タスク │
  │             │                        │ + 形式 + 制約 │
  └─────────────┘                        └─────────────┘
        │                                      │
        ▼                                      ▼
  ┌─────────────┐                        ┌─────────────┐
  │  漠然とした内容│                        │ 専門的で正確 │
  │  構造が乱雑   │                        │ 構造が明確   │
  │  何度も修正必要│                        │ 一発OK      │
  └─────────────┘                        └─────────────┘

  効率: ★★☆☆☆                           効率: ★★★★★
\`\`\`

---

## この章で学ぶこと

1. **プロンプトのコア構造** —— 良いプロンプトの組み立て方
2. **よく使うプロンプトパターン** —— そのまま使えるテンプレート
3. **ロールプレイテクニック** —— AIを様々な専門家に変える
4. **思考の連鎖と推論** —— AIに段階的に考えさせる
5. **上級テクニックと実践** —— 実際のシーンでの応用

「AIとの対話術」をマスターしましょう！
            `
          }
        },
        {
          id: 'ch2-structure',
          title: { zh: '2.1 提示词的核心结构', ja: '2.1 プロンプトのコア構造' },
          content: {
            zh: `
一个好的提示词通常包含这些要素：

---

## CRISPE 框架

这是业界广泛使用的提示词结构：

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                         CRISPE 提示词框架                                │
└─────────────────────────────────────────────────────────────────────────┘

  C - Capacity (角色)      "你是一位资深的..."
  ────────────────────────────────────────────────────────

  R - Role (任务)          "你的任务是..."
  ────────────────────────────────────────────────────────

  I - Insight (背景)       "背景信息是..."
  ────────────────────────────────────────────────────────

  S - Statement (要求)     "请按照以下要求..."
  ────────────────────────────────────────────────────────

  P - Personality (风格)   "语气要专业/亲切/幽默..."
  ────────────────────────────────────────────────────────

  E - Experiment (示例)    "输出格式示例：..."
\`\`\`

---

## 实际例子对比

### ❌ 不完整的提示词

\`\`\`
帮我写一封邮件
\`\`\`

AI 不知道：写给谁？什么目的？什么语气？

### ✅ 使用 CRISPE 的提示词

\`\`\`
【角色】你是一位专业的商务沟通专家

【任务】帮我写一封跟进邮件

【背景】
- 上周与客户开会讨论了新项目合作
- 客户对价格有顾虑，需要进一步说服
- 客户是科技公司的采购总监

【要求】
- 字数控制在200字以内
- 重申我们的核心优势
- 提供一个限时优惠方案
- 预约下次沟通时间

【风格】专业但亲切，不要过于推销

【格式】
主题：xxx
正文：xxx
\`\`\`

---

## 简化版：三要素法则

如果觉得 CRISPE 太复杂，至少要包含这三个要素：

\`\`\`
┌───────────────────────────────────────────────────────┐
│                三要素法则                              │
├───────────────────────────────────────────────────────┤
│                                                       │
│   1. 角色 (Role)                                      │
│      └── "你是一个..."                                │
│                                                       │
│   2. 任务 (Task)                                      │
│      └── "请帮我..."                                  │
│                                                       │
│   3. 格式 (Format)                                    │
│      └── "输出格式为..."                              │
│                                                       │
└───────────────────────────────────────────────────────┘
\`\`\`

**例子**：
\`\`\`
你是一位Python专家。
请帮我优化下面这段代码，提高运行效率。
输出格式：先给出优化后的代码，再解释改动原因。
\`\`\`

---

## 本节要点

1. **CRISPE 框架** —— 完整的提示词结构
2. **三要素法则** —— 简化版：角色、任务、格式
3. **背景信息很重要** —— 提供足够的上下文
4. **明确输出格式** —— 减少来回修改
            `,
            ja: `
良いプロンプトには通常、以下の要素が含まれます：

---

## CRISPE フレームワーク

業界で広く使われているプロンプト構造：

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                         CRISPE プロンプトフレームワーク                   │
└─────────────────────────────────────────────────────────────────────────┘

  C - Capacity (役割)      "あなたはベテランの..."
  ────────────────────────────────────────────────────────

  R - Role (タスク)        "あなたのタスクは..."
  ────────────────────────────────────────────────────────

  I - Insight (背景)       "背景情報は..."
  ────────────────────────────────────────────────────────

  S - Statement (要件)     "以下の要件に従って..."
  ────────────────────────────────────────────────────────

  P - Personality (スタイル) "トーンは専門的/親しみやすい/ユーモラス..."
  ────────────────────────────────────────────────────────

  E - Experiment (例)      "出力形式の例：..."
\`\`\`

---

## 実際の例の比較

### ❌ 不完全なプロンプト

\`\`\`
メールを書いて
\`\`\`

AIには分からない：誰に？何の目的？どんなトーン？

### ✅ CRISPE を使ったプロンプト

\`\`\`
【役割】あなたはビジネスコミュニケーションの専門家です

【タスク】フォローアップメールを書いてください

【背景】
- 先週、新プロジェクトの協力について顧客と会議
- 顧客は価格に懸念があり、さらに説得が必要
- 顧客はテック企業の調達部長

【要件】
- 200文字以内
- 当社のコア優位性を再確認
- 期間限定オファーを提供
- 次回のコミュニケーション時間を予約

【スタイル】専門的だが親しみやすく、押し売り感なし

【形式】
件名：xxx
本文：xxx
\`\`\`

---

## 簡略版：3要素ルール

CRISPE が複雑すぎると感じたら、少なくとも3要素を含めましょう：

\`\`\`
┌───────────────────────────────────────────────────────┐
│                3要素ルール                             │
├───────────────────────────────────────────────────────┤
│                                                       │
│   1. 役割 (Role)                                      │
│      └── "あなたは..."                                │
│                                                       │
│   2. タスク (Task)                                    │
│      └── "〜してください..."                          │
│                                                       │
│   3. 形式 (Format)                                    │
│      └── "出力形式は..."                              │
│                                                       │
└───────────────────────────────────────────────────────┘
\`\`\`

**例**：
\`\`\`
あなたはPythonの専門家です。
以下のコードを最適化し、実行効率を向上させてください。
出力形式：最適化後のコードを先に、次に変更理由を説明。
\`\`\`

---

## このセクションのポイント

1. **CRISPE フレームワーク** —— 完全なプロンプト構造
2. **3要素ルール** —— 簡略版：役割、タスク、形式
3. **背景情報が重要** —— 十分なコンテキストを提供
4. **出力形式を明確に** —— やり取りを減らす
            `
          }
        },
        {
          id: 'ch2-patterns',
          title: { zh: '2.2 常用提示词模式', ja: '2.2 よく使うプロンプトパターン' },
          content: {
            zh: `
掌握这些常用模式，可以直接套用到各种场景。

---

## 模式一：Few-Shot（少样本示例）

给 AI 几个例子，让它学会规律后处理新问题。

\`\`\`
请将以下中文翻译成日语，保持口语化风格：

例子1:
中文：今天天气真好
日语：今日はいい天気だね

例子2:
中文：我肚子饿了
日语：お腹空いたな

现在请翻译：
中文：这个电影太好看了
日语：
\`\`\`

**适用场景**：翻译、格式转换、风格模仿

---

## 模式二：Chain of Thought（思维链）

让 AI 一步步思考，而不是直接给答案。

\`\`\`
请一步步分析这个问题：

问题：一个商店进货价是80元，售价是100元，
打8折后还能赚多少钱？

请按以下步骤分析：
1. 首先，计算打折后的售价
2. 然后，计算每件商品的利润
3. 最后，给出结论
\`\`\`

**适用场景**：数学计算、逻辑分析、复杂决策

---

## 模式三：角色扮演

让 AI 扮演特定角色，获得更专业的回答。

\`\`\`
你现在是一位有20年经验的营养师。

一位30岁的程序员来咨询你：
- 他经常加班熬夜
- 饮食不规律，常吃外卖
- 最近感觉很疲惫

请从专业角度给出建议。
\`\`\`

**适用场景**：专业咨询、创意写作、模拟面试

---

## 模式四：约束条件

明确告诉 AI 什么不要做。

\`\`\`
写一段产品介绍，要求：

✅ 要做的：
- 突出三个核心功能
- 使用简洁的语言
- 包含一个用户案例

❌ 不要：
- 不要使用专业术语
- 不要超过200字
- 不要使用夸张的形容词
\`\`\`

**适用场景**：内容创作、代码生成、格式限制

---

## 模式五：迭代优化

让 AI 自己改进结果。

\`\`\`
第一步：请写一段产品介绍

第二步：以用户体验专家的视角，
找出上面文案的三个问题

第三步：根据问题重新改写文案
\`\`\`

**适用场景**：写作润色、方案优化、自我检查

---

## 模式速查表

| 模式 | 关键词 | 适用场景 |
|------|--------|----------|
| Few-Shot | "例如..."、"参考这个格式..." | 格式统一、风格模仿 |
| 思维链 | "一步步"、"首先...然后..." | 复杂推理、计算 |
| 角色扮演 | "你是一位..."、"作为专家..." | 专业建议、创意写作 |
| 约束条件 | "不要..."、"限制在..." | 控制输出、避免问题 |
| 迭代优化 | "先...再...然后改进" | 质量提升、自我完善 |

---

## 本节要点

1. **Few-Shot** —— 用例子教会 AI
2. **思维链** —— 让 AI 分步骤思考
3. **角色扮演** —— 获得专业视角
4. **约束条件** —— 明确什么不要做
5. **迭代优化** —— 让 AI 自我改进
            `,
            ja: `
これらの一般的なパターンをマスターすれば、様々なシーンに直接適用できます。

---

## パターン1：Few-Shot（少数例示）

AIにいくつかの例を与え、パターンを学んでから新しい問題を処理させます。

\`\`\`
以下の中国語を日本語に翻訳してください。口語スタイルを維持：

例1:
中国語：今天天气真好
日本語：今日はいい天気だね

例2:
中国語：我肚子饿了
日本語：お腹空いたな

では翻訳してください：
中国語：这个电影太好看了
日本語：
\`\`\`

**適用シーン**：翻訳、フォーマット変換、スタイル模倣

---

## パターン2：Chain of Thought（思考の連鎖）

AIに直接答えを出させず、段階的に考えさせます。

\`\`\`
この問題を段階的に分析してください：

問題：ある店の仕入れ価格は80元、販売価格は100元、
20%割引後、どれだけ利益が出ますか？

以下のステップで分析してください：
1. まず、割引後の販売価格を計算
2. 次に、各商品の利益を計算
3. 最後に、結論を出す
\`\`\`

**適用シーン**：数学計算、論理分析、複雑な意思決定

---

## パターン3：ロールプレイ

AIに特定の役割を演じさせ、より専門的な回答を得ます。

\`\`\`
あなたは20年の経験を持つ栄養士です。

30歳のプログラマーが相談に来ました：
- よく残業で夜更かし
- 食事が不規則で、よく外食
- 最近とても疲れを感じている

専門家の視点からアドバイスをください。
\`\`\`

**適用シーン**：専門相談、クリエイティブライティング、模擬面接

---

## パターン4：制約条件

AIに何をしないかを明確に伝えます。

\`\`\`
製品紹介文を書いてください。要件：

✅ すること：
- 3つのコア機能を強調
- 簡潔な言葉を使用
- ユーザー事例を1つ含める

❌ しないこと：
- 専門用語を使わない
- 200文字を超えない
- 誇張した形容詞を使わない
\`\`\`

**適用シーン**：コンテンツ作成、コード生成、フォーマット制限

---

## パターン5：反復改善

AIに結果を自己改善させます。

\`\`\`
ステップ1：製品紹介文を書いてください

ステップ2：UX専門家の視点から、
上記のコピーの3つの問題を指摘

ステップ3：問題に基づいてコピーを書き直す
\`\`\`

**適用シーン**：ライティング改善、プラン最適化、セルフチェック

---

## パターン早見表

| パターン | キーワード | 適用シーン |
|----------|------------|------------|
| Few-Shot | "例えば..."、"このフォーマットを参考に..." | フォーマット統一、スタイル模倣 |
| 思考の連鎖 | "段階的に"、"まず...次に..." | 複雑な推論、計算 |
| ロールプレイ | "あなたは..."、"専門家として..." | 専門アドバイス、創作 |
| 制約条件 | "〜しない"、"〜に制限" | 出力制御、問題回避 |
| 反復改善 | "まず...次に...改善" | 品質向上、自己完善 |

---

## このセクションのポイント

1. **Few-Shot** —— 例でAIに教える
2. **思考の連鎖** —— AIに段階的に考えさせる
3. **ロールプレイ** —— 専門家の視点を得る
4. **制約条件** —— 何をしないかを明確に
5. **反復改善** —— AIに自己改善させる
            `
          }
        },
        {
          id: 'ch2-advanced',
          title: { zh: '2.3 实战案例与高级技巧', ja: '2.3 実践例と上級テクニック' },
          content: {
            zh: `
## 真实场景：提示词实战

让我们通过几个真实场景，看看如何应用提示词技巧解决实际问题。

---

## 案例1：代码审查助手

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                          代码审查提示词                                   │
└─────────────────────────────────────────────────────────────────────────┘

你是一位资深的代码审查专家，有10年的软件开发经验。

## 任务
请审查以下代码，重点关注：
1. 代码质量和可读性
2. 潜在的 bug 和安全问题
3. 性能优化建议
4. 最佳实践遵循情况

## 输出格式
请按以下结构输出：

### 🔴 必须修复（阻塞性问题）
- [问题描述] + [具体位置] + [修复建议]

### 🟡 建议改进（非阻塞）
- [改进点] + [原因] + [改进方案]

### 🟢 做得好的地方
- [亮点]

### 代码
[粘贴代码]
\`\`\`

**这个提示词的技巧：**
- ✅ 设定专家角色
- ✅ 明确任务目标
- ✅ 结构化输出格式
- ✅ 使用emoji增强可读性

---

## 案例2：文档生成助手

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                         技术文档生成提示词                                │
└─────────────────────────────────────────────────────────────────────────┘

你是一位技术文档专家。请根据以下函数生成完整的 API 文档。

## 要求
1. 简洁的功能描述（1-2句）
2. 参数说明表格（参数名、类型、必填、说明）
3. 返回值说明
4. 使用示例（包含正常用法和边界情况）
5. 注意事项

## 输出格式：Markdown

## 函数代码
[粘贴函数]
\`\`\`

---

## 高级技巧：Self-Consistency（自洽性）

当需要更可靠的答案时，可以让 AI 多次尝试，然后综合结果。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        Self-Consistency 方法                            │
└─────────────────────────────────────────────────────────────────────────┘

                    ┌───────────────┐
                    │   同一问题     │
                    └───────┬───────┘
                            │
        ┌───────────────────┼───────────────────┐
        │                   │                   │
        ▼                   ▼                   ▼
  ┌───────────┐       ┌───────────┐       ┌───────────┐
  │  思路 A    │       │  思路 B    │       │  思路 C    │
  └─────┬─────┘       └─────┬─────┘       └─────┬─────┘
        │                   │                   │
        ▼                   ▼                   ▼
  ┌───────────┐       ┌───────────┐       ┌───────────┐
  │  答案 A    │       │  答案 B    │       │  答案 A    │
  └───────────┘       └───────────┘       └───────────┘
                            │
                            ▼
                    ┌───────────────┐
                    │  投票：答案 A  │
                    │  （出现2次）   │
                    └───────────────┘
\`\`\`

**实际应用：**
\`\`\`
请用3种不同的思路解决这个问题，然后比较各个答案，
给出你最有信心的最终答案。

问题：[你的问题]
\`\`\`

---

## 高级技巧：Tree of Thought（思维树）

对于复杂问题，让 AI 探索多个分支，评估每个分支，选择最优路径。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        Tree of Thought 示例                             │
└─────────────────────────────────────────────────────────────────────────┘

                         问题
                           │
         ┌─────────────────┼─────────────────┐
         │                 │                 │
         ▼                 ▼                 ▼
     方案 A            方案 B            方案 C
    ┌────────┐        ┌────────┐        ┌────────┐
    │可行性:高│        │可行性:中│        │可行性:低│
    │成本:低  │        │成本:高  │        │成本:中  │
    │风险:低  │        │风险:中  │        │风险:高  │
    └────┬───┘        └────────┘        └────────┘
         │
         │    ← 选择最优分支继续深入
         ▼
    ┌─────────┐
    │ 详细规划 │
    └─────────┘
\`\`\`

**提示词模板：**
\`\`\`
请使用思维树方法解决这个问题：

1. 首先，生成3个可能的解决方案
2. 对每个方案评估：可行性、成本、风险
3. 选择最优方案并详细展开
4. 给出最终的完整解决方案

问题：[你的问题]
\`\`\`

---

## 实用提示词模板库

### 学习类
\`\`\`
我想学习 [主题]，我的背景是 [你的基础]。
请设计一个 [时间] 的学习计划，包含：
1. 学习路线图
2. 推荐资源
3. 练习项目
4. 检验标准
\`\`\`

### 写作类
\`\`\`
请帮我写一篇关于 [主题] 的 [文章类型]。

目标读者：[受众]
篇幅：[字数]
风格：[专业/轻松/幽默]
重点：[核心观点]

请先给出大纲，确认后再写正文。
\`\`\`

### 分析类
\`\`\`
请分析 [主题/数据/现象]：

1. 现状概述
2. 关键因素分析
3. 主要发现（用数据支撑）
4. 可行建议
5. 潜在风险

输出格式：结构化报告，使用图表说明关键数据
\`\`\`

---

## 本节要点

1. **代码审查** —— 角色 + 任务 + 结构化输出
2. **Self-Consistency** —— 多次尝试取共识
3. **Tree of Thought** —— 探索多分支选最优
4. **模板复用** —— 积累自己的提示词库
            `,
            ja: `
## 実際のシナリオ：プロンプト実践

いくつかの実際のシナリオを通じて、プロンプトテクニックで問題を解決する方法を見てみましょう。

---

## ケース1：コードレビューアシスタント

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        コードレビュープロンプト                           │
└─────────────────────────────────────────────────────────────────────────┘

あなたは10年の経験を持つシニアコードレビュアーです。

## タスク
以下のコードをレビューしてください：
1. コード品質と可読性
2. 潜在的なバグとセキュリティ問題
3. パフォーマンス改善の提案
4. ベストプラクティスの遵守状況

## 出力形式
以下の構造で出力してください：

### 🔴 必須修正（ブロッキング問題）
- [問題] + [場所] + [修正案]

### 🟡 改善提案（非ブロッキング）
- [改善点] + [理由] + [提案]

### 🟢 良い点
- [ハイライト]

### コード
[コードを貼り付け]
\`\`\`

**このプロンプトのテクニック：**
- ✅ 専門家の役割を設定
- ✅ タスク目標を明確化
- ✅ 構造化された出力形式
- ✅ 絵文字で可読性向上

---

## ケース2：ドキュメント生成アシスタント

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        技術ドキュメント生成プロンプト                      │
└─────────────────────────────────────────────────────────────────────────┘

あなたは技術ドキュメントの専門家です。
以下の関数の完全なAPIドキュメントを生成してください。

## 要件
1. 簡潔な機能説明（1-2文）
2. パラメータ説明表（名前、型、必須、説明）
3. 戻り値の説明
4. 使用例（通常の使い方とエッジケース）
5. 注意事項

## 出力形式：Markdown

## 関数コード
[関数を貼り付け]
\`\`\`

---

## 上級テクニック：Self-Consistency（自己整合性）

より信頼性の高い回答が必要な場合、AIに複数回試行させて結果を統合します。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        Self-Consistency 方法                            │
└─────────────────────────────────────────────────────────────────────────┘

                    ┌───────────────┐
                    │   同じ問題     │
                    └───────┬───────┘
                            │
        ┌───────────────────┼───────────────────┐
        │                   │                   │
        ▼                   ▼                   ▼
  ┌───────────┐       ┌───────────┐       ┌───────────┐
  │  アプローチA │       │  アプローチB │       │  アプローチC │
  └─────┬─────┘       └─────┬─────┘       └─────┬─────┘
        │                   │                   │
        ▼                   ▼                   ▼
  ┌───────────┐       ┌───────────┐       ┌───────────┐
  │  回答 A    │       │  回答 B    │       │  回答 A    │
  └───────────┘       └───────────┘       └───────────┘
                            │
                            ▼
                    ┌───────────────┐
                    │  投票：回答 A  │
                    │ （2回出現）    │
                    └───────────────┘
\`\`\`

**実践的な使い方：**
\`\`\`
3つの異なるアプローチでこの問題を解決し、
各回答を比較して、最も確信のある最終回答を出してください。

問題：[あなたの質問]
\`\`\`

---

## 上級テクニック：Tree of Thought（思考の木）

複雑な問題に対して、AIに複数の分岐を探索させ、評価して最適なパスを選択します。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        Tree of Thought 例                               │
└─────────────────────────────────────────────────────────────────────────┘

                         問題
                           │
         ┌─────────────────┼─────────────────┐
         │                 │                 │
         ▼                 ▼                 ▼
     方案 A            方案 B            方案 C
    ┌────────┐        ┌────────┐        ┌────────┐
    │実現性:高│        │実現性:中│        │実現性:低│
    │コスト:低│        │コスト:高│        │コスト:中│
    │リスク:低│        │リスク:中│        │リスク:高│
    └────┬───┘        └────────┘        └────────┘
         │
         │    ← 最適な分岐を選んで深掘り
         ▼
    ┌─────────┐
    │ 詳細計画 │
    └─────────┘
\`\`\`

**プロンプトテンプレート：**
\`\`\`
思考の木メソッドでこの問題を解決してください：

1. まず、3つの可能な解決策を生成
2. 各方案を評価：実現性、コスト、リスク
3. 最適な方案を選択し詳細に展開
4. 最終的な完全な解決策を提示

問題：[あなたの質問]
\`\`\`

---

## 実用プロンプトテンプレート集

### 学習系
\`\`\`
[トピック]を学びたいです。私の背景は[あなたの基礎]です。
[期間]の学習計画を設計してください：
1. 学習ロードマップ
2. おすすめリソース
3. 練習プロジェクト
4. 習得度チェック基準
\`\`\`

### ライティング系
\`\`\`
[トピック]についての[記事タイプ]を書いてください。

ターゲット読者：[対象]
文字数：[長さ]
スタイル：[専門的/カジュアル/ユーモア]
重点：[コアメッセージ]

まずアウトラインを提示し、確認後に本文を書いてください。
\`\`\`

### 分析系
\`\`\`
[トピック/データ/現象]を分析してください：

1. 現状の概要
2. 主要因分析
3. 主な発見（データで裏付け）
4. 実行可能な提案
5. 潜在的リスク

出力形式：構造化レポート、図表で主要データを説明
\`\`\`

---

## このセクションのポイント

1. **コードレビュー** —— 役割 + タスク + 構造化出力
2. **Self-Consistency** —— 複数回試行で合意を得る
3. **Tree of Thought** —— 複数分岐を探索し最適を選ぶ
4. **テンプレート再利用** —— 自分のプロンプトライブラリを蓄積
            `
          }
        },
        {
          id: 'ch2-claude-tips',
          title: { zh: '2.4 Claude 专属技巧', ja: '2.4 Claude専用テクニック' },
          content: {
            zh: `
## Claude 的独特能力

Claude 有一些独特的功能，掌握这些技巧可以大幅提升使用效果。

---

## 1. Extended Thinking（深度思考）

Claude 可以在回答前进行深度思考，特别适合复杂问题。

**触发方式**：
\`\`\`
请仔细思考这个问题，考虑多种可能性，然后给出你的分析。

问题：[你的复杂问题]
\`\`\`

**适用场景**：数学证明、代码Debug、复杂业务分析、策略规划

---

## 2. Artifacts（可视化输出）

Claude 可以生成独立的可视化输出。

| 类型 | 说明 | 示例 |
|------|------|------|
| 代码 | 可运行的代码片段 | React组件、Python脚本 |
| SVG | 矢量图形 | 图标、插图 |
| HTML | 网页 | 落地页、表单 |
| Mermaid | 流程图 | 架构图、流程图 |

---

## 3. System Prompt 最佳实践

\`\`\`
## 身份定义
你是 [角色名称]，专注于 [领域]。

## 核心能力
- 能力1
- 能力2

## 行为规范
- 总是 [正面行为]
- 永远不要 [禁止行为]

## 输出格式
回答时请遵循以下格式：
1. 简要总结
2. 详细分析
3. 行动建议
\`\`\`

---

## 4. Claude Code 集成技巧

### Slash 命令
\`\`\`bash
/init      # 初始化项目记忆
/compact   # 紧凑对话，节省 token
/clear     # 清除上下文
/help      # 查看帮助
\`\`\`

### CLAUDE.md 项目配置

在项目根目录创建 CLAUDE.md 文件，Claude Code 会自动读取：

\`\`\`markdown
# CLAUDE.md

## 项目概述
这是一个 [项目类型] 项目，使用 [技术栈]。

## 常用命令
- npm run dev - 启动开发服务器
- npm run build - 构建生产版本

## 代码规范
- 使用 TypeScript
- 组件使用函数式写法
\`\`\`

---

## 5. 多轮对话策略

- **第1轮**：建立上下文（项目背景、技术栈）
- **第2轮**：具体任务（实现功能、解决问题）
- **第3轮**：迭代优化（基于上面的代码再添加...）
- **第4轮**：总结确认（请总结完成的功能）

**技巧**：保持上下文连贯，适时用 /compact 压缩

---

## 6. 输出控制技巧

### 控制长度
\`\`\`
请用3句话概括。
请控制在100字以内。
\`\`\`

### 控制格式
\`\`\`
请用 Markdown 表格输出。
请用代码块输出。
\`\`\`

### 控制语言
\`\`\`
请用中文回答。
请用技术性语言回答（面向开发者）。
\`\`\`

---

## 本节要点

1. **Extended Thinking** —— 复杂问题让 Claude 深度思考
2. **Artifacts** —— 生成可视化、可运行的输出
3. **System Prompt** —— 精心设计系统提示词
4. **Claude Code** —— 利用 /命令 和 CLAUDE.md
5. **多轮对话** —— 保持上下文连贯，适时总结
6. **输出控制** —— 明确指定长度、格式、语言
            `,
            ja: `
## Claudeの独自機能

Claudeにはいくつかの独自機能があり、これらのテクニックをマスターすると使用効果が大幅に向上します。

---

## 1. Extended Thinking（深い思考）

Claudeは回答前に深い思考を行うことができ、複雑な問題に特に適しています。

**トリガー方法**：
\`\`\`
この問題について慎重に考え、複数の可能性を検討してから分析を提供してください。

質問：[あなたの複雑な質問]
\`\`\`

**適用シーン**：数学的証明、コードデバッグ、複雑なビジネス分析、戦略プランニング

---

## 2. Artifacts（視覚的出力）

Claudeは独立した視覚的出力を生成できます。

| タイプ | 説明 | 例 |
|--------|------|-----|
| コード | 実行可能なコードスニペット | Reactコンポーネント、Pythonスクリプト |
| SVG | ベクターグラフィックス | アイコン、イラスト |
| HTML | ウェブページ | ランディングページ、フォーム |
| Mermaid | フローチャート | アーキテクチャ図、プロセスフロー |

---

## 3. System Prompt のベストプラクティス

\`\`\`
## アイデンティティ定義
あなたは[役割名]で、[分野]に専門性があります。

## コア能力
- 能力1
- 能力2

## 行動規範
- 常に[ポジティブな行動]
- 決して[禁止行動]しない

## 出力形式
回答する際は以下の形式に従ってください：
1. 簡潔な要約
2. 詳細な分析
3. アクションの提案
\`\`\`

---

## 4. Claude Code 統合テクニック

### Slashコマンド
\`\`\`bash
/init      # プロジェクトメモリの初期化
/compact   # 会話をコンパクトに
/clear     # コンテキストをクリア
/help      # ヘルプを表示
\`\`\`

### CLAUDE.md プロジェクト設定

プロジェクトルートにCLAUDE.mdファイルを作成すると、Claude Codeが自動的に読み取ります。

---

## 5. マルチターン対話戦略

- **第1ターン**：コンテキストの確立（プロジェクト背景、技術スタック）
- **第2ターン**：具体的なタスク（機能実装、問題解決）
- **第3ターン**：反復改善（上のコードに基づいて追加...）
- **第4ターン**：まとめと確認（完成した機能をまとめて）

**テクニック**：コンテキストの一貫性を維持、適時 /compact で圧縮

---

## 6. 出力制御テクニック

### 長さの制御
\`\`\`
3文で要約してください。
100文字以内で。
\`\`\`

### 形式の制御
\`\`\`
Markdownテーブルで出力してください。
コードブロックで出力してください。
\`\`\`

---

## このセクションのポイント

1. **Extended Thinking** —— 複雑な問題にはClaudeに深く考えさせる
2. **Artifacts** —— 視覚的で実行可能な出力を生成
3. **System Prompt** —— システムプロンプトを慎重に設計
4. **Claude Code** —— /コマンドとCLAUDE.mdを活用
5. **マルチターン対話** —— コンテキストを一貫させ、適時まとめる
6. **出力制御** —— 長さ、形式、言語を明確に指定
            `
          }
        },
        {
          id: 'ch2-structured-output',
          title: { zh: '2.5 结构化输出：JSON Mode', ja: '2.5 構造化出力：JSON Mode' },
          content: {
            zh: `
## 让 AI 输出可解析的数据

当你需要程序自动处理 AI 的输出时，结构化输出是必备技能。

---

## 🎯 为什么需要结构化输出？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    传统输出 vs 结构化输出                                  │
└─────────────────────────────────────────────────────────────────────────┘

  传统文本输出                          结构化 JSON 输出
  ┌────────────────────────┐           ┌────────────────────────┐
  │ "这个产品很好，我给     │           │ {                      │
  │  5颗星，推荐购买。"    │           │   "sentiment": "正面", │
  │                        │           │   "rating": 5,         │
  │  ❌ 难以程序化解析      │           │   "recommend": true    │
  │  ❌ 格式不稳定          │           │ }                      │
  │  ❌ 需要正则提取        │           │                        │
  └────────────────────────┘           │  ✅ 直接 JSON.parse()  │
                                       │  ✅ 格式稳定可靠       │
                                       │  ✅ 类型安全           │
                                       └────────────────────────┘
\`\`\`

---

## 📊 主流 API 的 JSON Mode 对比

| 平台 | 方式 | 可靠性 | 备注 |
|------|------|--------|------|
| **OpenAI** | response_format | ⭐⭐⭐⭐⭐ | 原生支持，强制 JSON |
| **Claude** | Prompt 引导 | ⭐⭐⭐⭐ | 通过指令实现 |
| **Gemini** | response_schema | ⭐⭐⭐⭐⭐ | 支持 Schema 验证 |

---

## 🔧 OpenAI JSON Mode

\`\`\`python
from openai import OpenAI
import json

client = OpenAI()

# 方法1：简单 JSON Mode
response = client.chat.completions.create(
    model="gpt-4o",
    response_format={"type": "json_object"},  # 开启 JSON Mode
    messages=[
        {"role": "system", "content": "你是一个情感分析助手，以 JSON 格式输出分析结果。"},
        {"role": "user", "content": "分析这条评论的情感：这个产品太棒了，物超所值！"}
    ]
)

result = json.loads(response.choices[0].message.content)
print(result)
# {"sentiment": "positive", "score": 0.95, "keywords": ["棒", "物超所值"]}
\`\`\`

### Structured Output (更严格)

\`\`\`python
from pydantic import BaseModel
from openai import OpenAI

# 定义数据结构
class SentimentAnalysis(BaseModel):
    sentiment: str  # positive, negative, neutral
    score: float    # 0.0 - 1.0
    keywords: list[str]
    summary: str

client = OpenAI()

# 使用 Structured Output
response = client.beta.chat.completions.parse(
    model="gpt-4o-2024-08-06",
    messages=[
        {"role": "system", "content": "分析用户评论的情感"},
        {"role": "user", "content": "这个产品质量一般，但价格很便宜"}
    ],
    response_format=SentimentAnalysis  # 传入 Pydantic 模型
)

# 自动解析为 Python 对象
result = response.choices[0].message.parsed
print(f"情感: {result.sentiment}")
print(f"评分: {result.score}")
print(f"关键词: {result.keywords}")
\`\`\`

---

## 🔧 Claude 结构化输出

Claude 没有原生 JSON Mode，但可以通过 Prompt 实现：

\`\`\`python
import anthropic
import json

client = anthropic.Anthropic()

# 方法1：Prompt 引导
response = client.messages.create(
    model="claude-sonnet-4-20250514",
    max_tokens=1024,
    system="""你是一个数据提取助手。
请严格按照以下 JSON Schema 输出，不要包含任何其他文字：

{
  "name": "string",
  "email": "string",
  "phone": "string",
  "address": "string"
}""",
    messages=[
        {"role": "user", "content": "请从以下文本提取联系信息：张三，邮箱 zhangsan@example.com，电话 13800138000，地址：北京市朝阳区xxx"}
    ]
)

# 解析结果
result = json.loads(response.content[0].text)
print(result)
\`\`\`

### Claude Tool Use (更可靠)

\`\`\`python
import anthropic

client = anthropic.Anthropic()

# 定义工具 Schema
tools = [
    {
        "name": "extract_contact",
        "description": "提取联系人信息",
        "input_schema": {
            "type": "object",
            "properties": {
                "name": {"type": "string", "description": "姓名"},
                "email": {"type": "string", "description": "邮箱"},
                "phone": {"type": "string", "description": "电话"},
                "address": {"type": "string", "description": "地址"}
            },
            "required": ["name"]
        }
    }
]

response = client.messages.create(
    model="claude-sonnet-4-20250514",
    max_tokens=1024,
    tools=tools,
    tool_choice={"type": "tool", "name": "extract_contact"},  # 强制使用工具
    messages=[
        {"role": "user", "content": "提取联系信息：张三，zhangsan@example.com"}
    ]
)

# 获取结构化结果
tool_use = response.content[0]
result = tool_use.input
print(result)  # {"name": "张三", "email": "zhangsan@example.com"}
\`\`\`

---

## 🔧 Gemini 结构化输出

\`\`\`python
import google.generativeai as genai
from google.generativeai import types

genai.configure(api_key="YOUR_API_KEY")

# 定义输出 Schema
response_schema = {
    "type": "object",
    "properties": {
        "sentiment": {"type": "string", "enum": ["positive", "negative", "neutral"]},
        "score": {"type": "number"},
        "keywords": {"type": "array", "items": {"type": "string"}}
    },
    "required": ["sentiment", "score"]
}

model = genai.GenerativeModel(
    "gemini-1.5-pro",
    generation_config=types.GenerationConfig(
        response_mime_type="application/json",
        response_schema=response_schema
    )
)

response = model.generate_content("分析：这个电影太精彩了！")
print(response.text)  # 保证符合 Schema 的 JSON
\`\`\`

---

## 💡 通用 Prompt 技巧

当 API 不支持原生 JSON Mode 时：

\`\`\`markdown
## 输出格式要求

请严格按照以下 JSON 格式输出，不要包含任何解释性文字：

\`\`\`json
{
  "field1": "说明1",
  "field2": "说明2",
  "field3": ["数组", "示例"]
}
\`\`\`

重要规则：
1. 只输出 JSON，不要输出其他内容
2. 确保 JSON 格式正确，可以被解析
3. 所有字段都是必填的
4. 字符串使用双引号
\`\`\`

---

## 🛡️ 错误处理最佳实践

\`\`\`python
import json
from typing import TypeVar, Type
from pydantic import BaseModel, ValidationError

T = TypeVar('T', bound=BaseModel)

def parse_ai_response(response: str, model: Type[T]) -> T | None:
    """安全解析 AI 返回的 JSON"""
    try:
        # 尝试提取 JSON 块
        if '\`\`\`json' in response:
            json_str = response.split('\`\`\`json')[1].split('\`\`\`')[0]
        elif '\`\`\`' in response:
            json_str = response.split('\`\`\`')[1].split('\`\`\`')[0]
        else:
            json_str = response

        # 解析并验证
        data = json.loads(json_str.strip())
        return model.model_validate(data)

    except json.JSONDecodeError as e:
        print(f"JSON 解析失败: {e}")
        return None
    except ValidationError as e:
        print(f"数据验证失败: {e}")
        return None

# 使用示例
class ProductInfo(BaseModel):
    name: str
    price: float
    in_stock: bool

result = parse_ai_response(ai_response, ProductInfo)
if result:
    print(f"产品: {result.name}, 价格: {result.price}")
\`\`\`

---

## 📊 选择决策树

\`\`\`
                需要结构化输出？
                      │
            ┌─────────┴─────────┐
            │   使用什么 API？   │
            └─────────┬─────────┘
               ╱      │      ╲
           OpenAI  Claude  Gemini
              │       │       │
              ▼       ▼       ▼
        Structured  Tool   response_
          Output    Use    schema
              │       │       │
              └───────┼───────┘
                      ▼
              Pydantic 验证
                      │
                      ▼
                 业务逻辑
\`\`\`
            `,
            ja: `
## AIの出力をパース可能なデータに

プログラムでAIの出力を自動処理する必要がある場合、構造化出力は必須スキルです。

---

## 🎯 なぜ構造化出力が必要？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    従来の出力 vs 構造化出力                               │
└─────────────────────────────────────────────────────────────────────────┘

  従来のテキスト出力                    構造化JSON出力
  ┌────────────────────────┐           ┌────────────────────────┐
  │ "この商品は素晴らしい、 │           │ {                      │
  │  星5つ、おすすめ！"    │           │   "sentiment": "正面", │
  │                        │           │   "rating": 5,         │
  │  ❌ プログラム解析困難  │           │   "recommend": true    │
  │  ❌ フォーマット不安定  │           │ }                      │
  │  ❌ 正規表現が必要     │           │                        │
  └────────────────────────┘           │  ✅ JSON.parse()可能  │
                                       │  ✅ フォーマット安定   │
                                       │  ✅ 型安全            │
                                       └────────────────────────┘
\`\`\`

---

## 📊 主要APIのJSON Mode比較

| プラットフォーム | 方式 | 信頼性 | 備考 |
|------------------|------|--------|------|
| **OpenAI** | response_format | ⭐⭐⭐⭐⭐ | ネイティブ対応 |
| **Claude** | Prompt誘導 | ⭐⭐⭐⭐ | 指示で実現 |
| **Gemini** | response_schema | ⭐⭐⭐⭐⭐ | Schema検証対応 |

---

## 🔧 OpenAI JSON Mode

\`\`\`python
from openai import OpenAI
import json

client = OpenAI()

# 方法1：シンプルなJSON Mode
response = client.chat.completions.create(
    model="gpt-4o",
    response_format={"type": "json_object"},  # JSON Mode有効化
    messages=[
        {"role": "system", "content": "感情分析アシスタントとして、JSON形式で結果を出力してください。"},
        {"role": "user", "content": "このレビューの感情を分析：この商品は最高、コスパ抜群！"}
    ]
)

result = json.loads(response.choices[0].message.content)
print(result)
\`\`\`

### Structured Output（より厳密）

\`\`\`python
from pydantic import BaseModel
from openai import OpenAI

# データ構造を定義
class SentimentAnalysis(BaseModel):
    sentiment: str  # positive, negative, neutral
    score: float    # 0.0 - 1.0
    keywords: list[str]
    summary: str

client = OpenAI()

# Structured Outputを使用
response = client.beta.chat.completions.parse(
    model="gpt-4o-2024-08-06",
    messages=[
        {"role": "system", "content": "ユーザーレビューの感情を分析"},
        {"role": "user", "content": "品質は普通だが、価格は安い"}
    ],
    response_format=SentimentAnalysis
)

# 自動的にPythonオブジェクトに解析
result = response.choices[0].message.parsed
print(f"感情: {result.sentiment}")
\`\`\`

---

## 🔧 Claude 構造化出力

ClaudeにはネイティブのJSON Modeがありませんが、Promptで実現可能：

\`\`\`python
import anthropic
import json

client = anthropic.Anthropic()

response = client.messages.create(
    model="claude-sonnet-4-20250514",
    max_tokens=1024,
    system="""データ抽出アシスタントです。
以下のJSON Schemaに厳密に従って出力してください：

{
  "name": "string",
  "email": "string",
  "phone": "string"
}""",
    messages=[
        {"role": "user", "content": "連絡先を抽出：田中太郎、tanaka@example.com"}
    ]
)

result = json.loads(response.content[0].text)
\`\`\`

### Claude Tool Use（より信頼性が高い）

\`\`\`python
import anthropic

client = anthropic.Anthropic()

tools = [
    {
        "name": "extract_contact",
        "description": "連絡先情報を抽出",
        "input_schema": {
            "type": "object",
            "properties": {
                "name": {"type": "string"},
                "email": {"type": "string"}
            },
            "required": ["name"]
        }
    }
]

response = client.messages.create(
    model="claude-sonnet-4-20250514",
    max_tokens=1024,
    tools=tools,
    tool_choice={"type": "tool", "name": "extract_contact"},
    messages=[
        {"role": "user", "content": "連絡先を抽出：田中太郎、tanaka@example.com"}
    ]
)

result = response.content[0].input
\`\`\`

---

## 💡 汎用Promptテクニック

APIがネイティブJSON Modeをサポートしていない場合：

\`\`\`markdown
## 出力フォーマット要件

以下のJSON形式で厳密に出力し、説明文は含めないでください：

\`\`\`json
{
  "field1": "説明1",
  "field2": "説明2"
}
\`\`\`

重要なルール：
1. JSONのみを出力
2. JSON形式が正しいことを確認
3. すべてのフィールドは必須
\`\`\`

---

## 📊 選択決定木

\`\`\`
                構造化出力が必要？
                      │
            ┌─────────┴─────────┐
            │   どのAPIを使用？  │
            └─────────┬─────────┘
               ╱      │      ╲
           OpenAI  Claude  Gemini
              │       │       │
              ▼       ▼       ▼
        Structured  Tool   response_
          Output    Use    schema
              │       │       │
              └───────┼───────┘
                      ▼
              Pydantic検証
\`\`\`
            `
          }
        },
        {
          id: 'ch2-summary',
          title: { zh: '2.6 本章小结', ja: '2.6 この章のまとめ' },
          content: {
            zh: `
## 提示词工程核心要点

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                       提示词工程知识地图                                  │
└─────────────────────────────────────────────────────────────────────────┘

                              提示词工程
                                  │
            ┌─────────────────────┼─────────────────────┐
            │                     │                     │
            ▼                     ▼                     ▼
       ┌─────────┐           ┌─────────┐          ┌─────────┐
       │ 结构框架 │           │ 常用模式 │          │ 实践技巧 │
       └─────────┘           └─────────┘          └─────────┘
            │                     │                     │
     ┌──────┴──────┐       ┌──────┴──────┐       ┌──────┴──────┐
     │             │       │             │       │             │
     ▼             ▼       ▼             ▼       ▼             ▼
  CRISPE        三要素   Few-Shot    思维链    角色扮演    迭代优化
  框架          法则     少样本      推理      专家视角    持续改进
\`\`\`

---

## 快速行动清单

- [ ] 下次使用 AI 时，尝试使用"三要素法则"
- [ ] 遇到复杂问题，让 AI "一步步分析"
- [ ] 需要专业建议时，让 AI 扮演相关专家
- [ ] 保存你觉得好用的提示词模板

---

## 关键金句

> "好的提示词不是在命令 AI，而是在与 AI 协作。"

> "给 AI 足够的信息，它才能给你满意的答案。"

> "提示词工程的本质，是把模糊的需求变成清晰的指令。"

---

下一章，我们将学习更激动人心的内容：**AI Agents（智能体）** —— 让 AI 不只是回答问题，而是能够自主完成复杂任务！
            `,
            ja: `
## プロンプトエンジニアリングのコアポイント

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    プロンプトエンジニアリング知識マップ                    │
└─────────────────────────────────────────────────────────────────────────┘

                          プロンプトエンジニアリング
                                  │
            ┌─────────────────────┼─────────────────────┐
            │                     │                     │
            ▼                     ▼                     ▼
       ┌─────────┐           ┌─────────┐          ┌─────────┐
       │構造フレーム│           │一般パターン│          │実践テクニック│
       └─────────┘           └─────────┘          └─────────┘
            │                     │                     │
     ┌──────┴──────┐       ┌──────┴──────┐       ┌──────┴──────┐
     │             │       │             │       │             │
     ▼             ▼       ▼             ▼       ▼             ▼
  CRISPE        3要素   Few-Shot    思考の連鎖  ロールプレイ  反復改善
  フレーム      ルール   少数例示    推論      専門家視点    継続改善
\`\`\`

---

## クイックアクションリスト

- [ ] 次にAIを使うとき、「3要素ルール」を試す
- [ ] 複雑な問題に遭遇したら、AIに「段階的に分析」させる
- [ ] 専門アドバイスが必要なとき、AIに関連専門家を演じさせる
- [ ] 使えると思ったプロンプトテンプレートを保存する

---

## 重要な格言

> 「良いプロンプトはAIに命令するのではなく、AIと協力することです。」

> 「AIに十分な情報を与えれば、満足のいく答えが得られます。」

> 「プロンプトエンジニアリングの本質は、曖昧なニーズを明確な指示に変えることです。」

---

次の章では、さらにエキサイティングな内容を学びます：**AI Agents（インテリジェントエージェント）** —— AIが質問に答えるだけでなく、複雑なタスクを自律的に完了できるようになります！
            `
          }
        }
      ]
    },
    // ============================================
    // 第二章：AI Agents 智能体
    // ============================================
    {
      id: 'chapter-3',
      number: 3,
      title: { zh: 'AI Agents 智能体', ja: 'AI Agents インテリジェントエージェント' },
      subtitle: { zh: '让AI自主完成复杂任务', ja: 'AIに複雑なタスクを自律的に完了させる' },
      sections: [
        {
          id: 'ch3-intro',
          title: { zh: '引言：从对话到行动', ja: '序章：対話から行動へ' },
          content: {
            zh: `
到目前为止，我们学习的都是**与 AI 对话**。你问，AI 答。

但是，如果 AI 能**自己动手**做事呢？

这就是 **AI Agent（智能体）** 的概念：让 AI 不只是回答问题，而是能够**自主规划、调用工具、完成任务**。

---

## 普通 AI vs AI Agent

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                     普通 AI  vs  AI Agent                               │
└─────────────────────────────────────────────────────────────────────────┘

    普通 AI（ChatGPT 等）                    AI Agent
    ┌──────────────────────┐               ┌──────────────────────┐
    │                      │               │                      │
    │   用户提问            │               │   用户设定目标        │
    │      ↓               │               │      ↓               │
    │   AI 回答            │               │   Agent 分析任务     │
    │      ↓               │               │      ↓               │
    │   结束               │               │   制定计划           │
    │                      │               │      ↓               │
    │                      │               │   调用工具执行        │
    │                      │               │      ↓               │
    │                      │               │   检查结果           │
    │                      │               │      ↓               │
    │                      │               │   继续或完成         │
    │                      │               │                      │
    └──────────────────────┘               └──────────────────────┘

    特点：一问一答                          特点：自主规划执行
    能力：回答问题                          能力：完成任务
\`\`\`

---

## 生活中的 Agent 类比

想象你有一个**超级助理**：

**普通 AI** 像一个知识渊博的顾问：
- 你问："今天北京天气怎么样？"
- 它答："今天北京晴，25度"

**AI Agent** 像一个能干的私人助理：
- 你说："帮我安排明天去北京的出差"
- 它会：
  1. 查询明天的天气
  2. 搜索航班和价格
  3. 推荐合适的酒店
  4. 把行程整理成表格发给你
  5. 甚至帮你预订（如果有权限）

---

## 本章你将学到

1. **什么是 AI Agent** —— 核心概念和工作原理
2. **Agent 的关键能力** —— 规划、工具使用、记忆
3. **常见 Agent 类型** —— 不同场景的 Agent
4. **实用 Agent 工具** —— 现在就能用的 Agent
5. **Agent 的未来** —— 发展趋势和机遇

让我们进入 AI Agent 的世界！
            `,
            ja: `
これまで私たちが学んできたのは、**AIとの対話**でした。あなたが質問し、AIが答える。

しかし、もしAIが**自分で行動**できたら？

これが**AI Agent（インテリジェントエージェント）**の概念です：AIが質問に答えるだけでなく、**自律的に計画を立て、ツールを使い、タスクを完了**できるようにすることです。

---

## 通常のAI vs AI Agent

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                     通常のAI  vs  AI Agent                              │
└─────────────────────────────────────────────────────────────────────────┘

    通常のAI（ChatGPT等）                   AI Agent
    ┌──────────────────────┐               ┌──────────────────────┐
    │                      │               │                      │
    │   ユーザーが質問     │               │   ユーザーが目標設定  │
    │      ↓               │               │      ↓               │
    │   AIが回答           │               │   Agentがタスク分析   │
    │      ↓               │               │      ↓               │
    │   終了               │               │   計画を立てる        │
    │                      │               │      ↓               │
    │                      │               │   ツールを呼び出して実行│
    │                      │               │      ↓               │
    │                      │               │   結果をチェック      │
    │                      │               │      ↓               │
    │                      │               │   継続または完了      │
    │                      │               │                      │
    └──────────────────────┘               └──────────────────────┘

    特徴：一問一答                          特徴：自律的に計画・実行
    能力：質問に答える                      能力：タスクを完了する
\`\`\`

---

## 日常生活でのAgentの例え

**スーパーアシスタント**がいると想像してください：

**通常のAI** は知識豊富なアドバイザーのよう：
- 「今日の東京の天気は？」と聞くと
- 「今日の東京は晴れ、25度です」と答える

**AI Agent** は有能なパーソナルアシスタントのよう：
- 「明日の北京出張を手配して」と言うと
- 以下を行います：
  1. 明日の天気を確認
  2. フライトと価格を検索
  3. 適切なホテルを推薦
  4. 旅程を表にまとめて送信
  5. 権限があれば予約まで行う

---

## この章で学ぶこと

1. **AI Agentとは** —— コアコンセプトと仕組み
2. **Agentの重要な能力** —— 計画、ツール使用、記憶
3. **一般的なAgentタイプ** —— 様々なシーンのAgent
4. **実用的なAgentツール** —— 今すぐ使えるAgent
5. **Agentの未来** —— 発展トレンドと機会

AI Agentの世界に入りましょう！
            `
          }
        },
        {
          id: 'ch3-how-it-works',
          title: { zh: '3.1 Agent 的工作原理', ja: '3.1 Agentの仕組み' },
          content: {
            zh: `
AI Agent 是如何工作的？让我们拆解它的核心组件。

---

## Agent 的四大核心能力

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        AI Agent 核心架构                                │
└─────────────────────────────────────────────────────────────────────────┘

                         ┌─────────────┐
                         │   大脑      │
                         │  (LLM)     │
                         └──────┬──────┘
                                │
         ┌──────────────────────┼──────────────────────┐
         │                      │                      │
         ▼                      ▼                      ▼
  ┌──────────────┐      ┌──────────────┐      ┌──────────────┐
  │    规划      │      │    工具      │      │    记忆      │
  │  Planning   │      │   Tools     │      │   Memory    │
  └──────────────┘      └──────────────┘      └──────────────┘
         │                      │                      │
         ▼                      ▼                      ▼
   分解任务              执行动作              存储信息
   制定步骤              调用API              上下文记忆
   调整计划              读写文件              长期记忆
\`\`\`

---

## 1. 规划能力（Planning）

Agent 能够把复杂任务分解成可执行的步骤：

**例子：**"帮我写一篇关于AI的博客文章"

Agent 的规划：
\`\`\`
1. 确定文章主题和目标读者
2. 搜索相关资料和最新动态
3. 制定文章大纲
4. 撰写各个章节
5. 优化和润色
6. 生成配图建议
7. 输出最终文章
\`\`\`

---

## 2. 工具使用（Tools）

Agent 可以调用各种外部工具：

| 工具类型 | 例子 | 用途 |
|----------|------|------|
| 搜索 | Google、Bing | 获取最新信息 |
| 代码 | Python 解释器 | 计算、数据处理 |
| 文件 | 读写本地文件 | 存储和访问数据 |
| API | 天气、地图、支付 | 连接外部服务 |
| 浏览器 | 网页自动化 | 操作网页 |

---

## 3. 记忆能力（Memory）

Agent 需要记住信息来完成复杂任务：

\`\`\`
┌─────────────────────────────────────────────────────┐
│                  Agent 记忆系统                     │
├─────────────────────────────────────────────────────┤
│                                                     │
│  短期记忆（对话上下文）                              │
│  └── 当前对话的历史                                 │
│  └── 刚才执行的步骤                                 │
│                                                     │
│  长期记忆（持久存储）                                │
│  └── 用户偏好                                       │
│  └── 历史任务记录                                   │
│  └── 学到的知识                                     │
│                                                     │
└─────────────────────────────────────────────────────┘
\`\`\`

---

## 4. 反思与调整

Agent 会根据执行结果调整策略：

\`\`\`
执行步骤 ──▶ 检查结果 ──▶ 结果OK？
                              │
                    ┌─────────┴─────────┐
                    │                   │
                   是                   否
                    │                   │
                    ▼                   ▼
               继续下一步          分析问题
                                       │
                                       ▼
                                  调整策略
                                       │
                                       ▼
                                  重新执行
\`\`\`

---

## ReAct 模式

大多数 Agent 采用 **ReAct（Reasoning + Acting）** 模式：

### 交互式演示：Agent 工作循环

体验一下 Agent 是如何思考、行动和观察的：

::agent-viz::

\`\`\`
循环过程：
┌──────────────────────────────────────────────────────┐
│                                                      │
│  思考 (Thought) ──▶ 行动 (Action) ──▶ 观察 (Observe) │
│     ▲                                          │     │
│     └──────────────────────────────────────────┘     │
│                                                      │
└──────────────────────────────────────────────────────┘

例子：
Thought: 用户想知道今天的股价，我需要搜索最新数据
Action: 调用搜索工具，搜索"苹果公司今日股价"
Observe: 搜索结果显示苹果股价为 $178.50
Thought: 我已经获得了数据，可以回答用户了
Action: 输出答案给用户
\`\`\`

---

## 本节要点

1. **四大核心** —— 规划、工具、记忆、反思
2. **规划能力** —— 把复杂任务分解成步骤
3. **工具使用** —— 连接外部世界的能力
4. **记忆系统** —— 短期和长期记忆
5. **ReAct 模式** —— 思考-行动-观察的循环
            `,
            ja: `
AI Agentはどのように動作するのでしょうか？コアコンポーネントを分解してみましょう。

---

## Agentの4つのコア能力

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        AI Agent コアアーキテクチャ                       │
└─────────────────────────────────────────────────────────────────────────┘

                         ┌─────────────┐
                         │    脳       │
                         │   (LLM)    │
                         └──────┬──────┘
                                │
         ┌──────────────────────┼──────────────────────┐
         │                      │                      │
         ▼                      ▼                      ▼
  ┌──────────────┐      ┌──────────────┐      ┌──────────────┐
  │    計画      │      │   ツール    │      │    記憶      │
  │  Planning   │      │   Tools     │      │   Memory    │
  └──────────────┘      └──────────────┘      └──────────────┘
         │                      │                      │
         ▼                      ▼                      ▼
   タスク分解             アクション実行          情報保存
   ステップ策定           API呼び出し            コンテキスト記憶
   計画調整              ファイル読み書き         長期記憶
\`\`\`

---

## 1. 計画能力（Planning）

Agentは複雑なタスクを実行可能なステップに分解できます：

**例：**「AIについてのブログ記事を書いて」

Agentの計画：
\`\`\`
1. 記事のテーマとターゲット読者を決定
2. 関連資料と最新動向を検索
3. 記事のアウトラインを作成
4. 各セクションを執筆
5. 最適化と推敲
6. 配置する画像を提案
7. 最終記事を出力
\`\`\`

---

## 2. ツール使用（Tools）

Agentは様々な外部ツールを呼び出せます：

| ツールタイプ | 例 | 用途 |
|-------------|-----|------|
| 検索 | Google、Bing | 最新情報の取得 |
| コード | Pythonインタープリター | 計算、データ処理 |
| ファイル | ローカルファイルの読み書き | データの保存とアクセス |
| API | 天気、地図、決済 | 外部サービスへの接続 |
| ブラウザ | Webページ自動化 | Webページの操作 |

---

## 3. 記憶能力（Memory）

Agentは複雑なタスクを完了するために情報を記憶する必要があります：

\`\`\`
┌─────────────────────────────────────────────────────┐
│                  Agent 記憶システム                  │
├─────────────────────────────────────────────────────┤
│                                                     │
│  短期記憶（対話コンテキスト）                        │
│  └── 現在の対話履歴                                 │
│  └── 直前に実行したステップ                         │
│                                                     │
│  長期記憶（永続ストレージ）                          │
│  └── ユーザーの好み                                 │
│  └── 過去のタスク記録                               │
│  └── 学んだ知識                                     │
│                                                     │
└─────────────────────────────────────────────────────┘
\`\`\`

---

## 4. 振り返りと調整

Agentは実行結果に基づいて戦略を調整します：

\`\`\`
ステップ実行 ──▶ 結果チェック ──▶ 結果OK？
                                   │
                    ┌──────────────┴──────────────┐
                    │                             │
                   はい                           いいえ
                    │                             │
                    ▼                             ▼
               次のステップへ                問題を分析
                                               │
                                               ▼
                                           戦略を調整
                                               │
                                               ▼
                                            再実行
\`\`\`

---

## ReAct パターン

ほとんどのAgentは**ReAct（Reasoning + Acting）**パターンを採用：

### インタラクティブデモ：Agent動作ループ

Agentがどのように思考、行動、観察するかを体験してください：

::agent-viz::

\`\`\`
ループプロセス：
┌──────────────────────────────────────────────────────┐
│                                                      │
│  思考 (Thought) ──▶ 行動 (Action) ──▶ 観察 (Observe) │
│     ▲                                          │     │
│     └──────────────────────────────────────────┘     │
│                                                      │
└──────────────────────────────────────────────────────┘

例：
Thought: ユーザーは今日の株価を知りたい、最新データを検索する必要がある
Action: 検索ツールを呼び出し、「Apple社今日の株価」を検索
Observe: 検索結果はApple株価が$178.50と表示
Thought: データを取得した、ユーザーに回答できる
Action: ユーザーに答えを出力
\`\`\`

---

## このセクションのポイント

1. **4つのコア** —— 計画、ツール、記憶、振り返り
2. **計画能力** —— 複雑なタスクをステップに分解
3. **ツール使用** —— 外部世界への接続能力
4. **記憶システム** —— 短期と長期記憶
5. **ReAct パターン** —— 思考-行動-観察のループ
            `
          }
        },
        {
          id: 'ch3-tools',
          title: { zh: '3.2 实用 Agent 工具', ja: '3.2 実用的なAgentツール' },
          content: {
            zh: `
现在已经有很多可以直接使用的 AI Agent 工具，让我们来看看最实用的几个。

---

## 代码开发类 Agent

### Claude Code / Cursor / GitHub Copilot

这些工具可以：
- 自动编写代码
- 理解整个项目结构
- 自动修复 bug
- 执行命令和测试

**使用场景**：
- "帮我实现一个用户登录功能"
- "修复这个报错"
- "给这个函数写单元测试"

---

## 通用任务 Agent

### ChatGPT Plugins / GPTs

OpenAI 的插件系统让 ChatGPT 可以：
- 搜索网页
- 分析数据
- 生成图片
- 连接第三方服务

### Claude with Tools

Claude 也可以：
- 执行代码
- 搜索信息
- 分析文件

---

## 自动化工作流

### Zapier / Make

这些工具可以把 AI 集成到工作流中：

\`\`\`
触发器 ──▶ AI 处理 ──▶ 执行动作

例子：
收到邮件 ──▶ AI分析内容 ──▶ 自动分类并回复
\`\`\`

---

## 浏览器自动化

### 浏览器 Agent

可以自动操作网页的 AI：

- 自动填写表单
- 抓取信息
- 执行重复性任务

**注意**：使用时要遵守网站规则和法律法规

---

## Agent 能力对比

| 工具 | 代码 | 搜索 | 文件 | 浏览器 | 价格 |
|------|------|------|------|--------|------|
| Claude Code | ✅ | ✅ | ✅ | ✅ | 付费 |
| ChatGPT Plus | ✅ | ✅ | ✅ | ❌ | $20/月 |
| Cursor | ✅ | ✅ | ✅ | ❌ | 免费/付费 |
| Copilot | ✅ | ❌ | ✅ | ❌ | $10/月 |

---

## 如何选择 Agent 工具

\`\`\`
你的需求是什么？
      │
      ├── 写代码 ──▶ Claude Code / Cursor
      │
      ├── 日常任务 ──▶ ChatGPT / Claude
      │
      ├── 自动化工作流 ──▶ Zapier + AI
      │
      └── 数据分析 ──▶ ChatGPT 代码解释器
\`\`\`

---

## 本节要点

1. **代码 Agent** —— 自动写代码、修复 bug
2. **通用 Agent** —— ChatGPT、Claude 的工具功能
3. **自动化工具** —— 把 AI 集成到工作流
4. **根据需求选择** —— 不同工具有不同优势
            `,
            ja: `
今すぐ使えるAI Agentツールがたくさんあります。最も実用的なものをいくつか見てみましょう。

---

## コード開発系Agent

### Claude Code / Cursor / GitHub Copilot

これらのツールができること：
- 自動でコードを書く
- プロジェクト構造全体を理解
- 自動でバグを修正
- コマンドとテストを実行

**使用シーン**：
- 「ユーザーログイン機能を実装して」
- 「このエラーを修正して」
- 「この関数のユニットテストを書いて」

---

## 汎用タスクAgent

### ChatGPT Plugins / GPTs

OpenAIのプラグインシステムでChatGPTができること：
- Webページを検索
- データを分析
- 画像を生成
- サードパーティサービスに接続

### Claude with Tools

Claudeもできること：
- コードを実行
- 情報を検索
- ファイルを分析

---

## 自動化ワークフロー

### Zapier / Make

これらのツールはAIをワークフローに統合できます：

\`\`\`
トリガー ──▶ AI処理 ──▶ アクション実行

例：
メール受信 ──▶ AIが内容分析 ──▶ 自動分類と返信
\`\`\`

---

## ブラウザ自動化

### ブラウザAgent

Webページを自動操作できるAI：

- フォームを自動入力
- 情報を収集
- 繰り返しタスクを実行

**注意**：使用時はWebサイトのルールと法規制を遵守してください

---

## Agent能力比較

| ツール | コード | 検索 | ファイル | ブラウザ | 価格 |
|--------|--------|------|----------|----------|------|
| Claude Code | ✅ | ✅ | ✅ | ✅ | 有料 |
| ChatGPT Plus | ✅ | ✅ | ✅ | ❌ | $20/月 |
| Cursor | ✅ | ✅ | ✅ | ❌ | 無料/有料 |
| Copilot | ✅ | ❌ | ✅ | ❌ | $10/月 |

---

## Agentツールの選び方

\`\`\`
あなたのニーズは？
      │
      ├── コードを書く ──▶ Claude Code / Cursor
      │
      ├── 日常タスク ──▶ ChatGPT / Claude
      │
      ├── ワークフロー自動化 ──▶ Zapier + AI
      │
      └── データ分析 ──▶ ChatGPT コードインタープリター
\`\`\`

---

## このセクションのポイント

1. **コードAgent** —— 自動でコード作成、バグ修正
2. **汎用Agent** —— ChatGPT、Claudeのツール機能
3. **自動化ツール** —— AIをワークフローに統合
4. **ニーズに応じて選択** —— ツールごとに異なる強み
            `
          }
        },
        {
          id: 'ch3-mcp',
          title: { zh: '3.3 MCP：AI 的"万能接口"', ja: '3.3 MCP：AIの「万能インターフェース」' },
          content: {
            zh: `
## 什么是 MCP？

**MCP（Model Context Protocol）** 是 Anthropic 推出的开放协议，让 AI 能够安全地连接各种外部工具和数据源。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                         MCP 架构概览                                     │
└─────────────────────────────────────────────────────────────────────────┘

                           ┌───────────────┐
                           │    AI 模型    │
                           │  (Claude等)   │
                           └───────┬───────┘
                                   │
                           ┌───────▼───────┐
                           │  MCP 协议层   │
                           └───────┬───────┘
                                   │
      ┌────────────────────────────┼────────────────────────────┐
      │                            │                            │
      ▼                            ▼                            ▼
┌───────────┐              ┌───────────┐              ┌───────────┐
│ 文件系统  │              │  数据库   │              │  API服务  │
│ MCP服务器 │              │ MCP服务器 │              │ MCP服务器 │
└───────────┘              └───────────┘              └───────────┘
      │                            │                            │
      ▼                            ▼                            ▼
  本地文件                    PostgreSQL                   GitHub/Slack
                               MySQL                      Notion/Jira
\`\`\`

---

## 为什么 MCP 很重要？

之前的问题：
- 每个 AI 产品都要单独开发工具集成
- 数据源连接方式各不相同
- 难以复用和标准化

MCP 解决了：
\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                         统一的工具接口                                   │
└─────────────────────────────────────────────────────────────────────────┘

  旧模式                                新模式（MCP）
  ─────────                            ──────────────
  ChatGPT ─┬─ 文件读取                  Claude ──┐
           ├─ 网页搜索                  ChatGPT ─┼── MCP ──┬─ 任何工具
           └─ API调用                   其他AI ──┘         └─ 任何数据

  Claude ──┬─ 文件读取
           ├─ 网页搜索     ──▶         一次开发，处处可用
           └─ API调用

  重复开发 ✗                           标准化复用 ✓
\`\`\`

---

## Claude Code：MCP 的实战应用

**Claude Code** 是基于 MCP 的 AI 编程助手，可以直接操作你的开发环境。

### 核心能力

| 能力 | 说明 | 示例 |
|------|------|------|
| 🔍 代码理解 | 读取和分析整个项目 | "解释这个模块的架构" |
| ✏️ 代码修改 | 直接编辑文件 | "修复这个 bug" |
| 💻 命令执行 | 运行终端命令 | "运行测试并修复失败用例" |
| 🔗 工具集成 | 通过 MCP 扩展能力 | "查询数据库中的用户数据" |

### 实际工作流程

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                     Claude Code 工作流程                                │
└─────────────────────────────────────────────────────────────────────────┘

用户: "帮我实现用户登录功能"
                │
                ▼
        ┌───────────────┐
        │ 1. 分析项目   │  ← 读取现有代码、理解架构
        └───────┬───────┘
                │
                ▼
        ┌───────────────┐
        │ 2. 制定计划   │  ← 确定需要创建/修改的文件
        └───────┬───────┘
                │
                ▼
        ┌───────────────┐
        │ 3. 编写代码   │  ← 创建路由、控制器、模型
        └───────┬───────┘
                │
                ▼
        ┌───────────────┐
        │ 4. 测试验证   │  ← 运行测试、检查错误
        └───────┬───────┘
                │
                ▼
        ┌───────────────┐
        │ 5. 迭代改进   │  ← 根据反馈继续优化
        └───────────────┘
\`\`\`

---

## 常用 MCP 服务器

| MCP 服务器 | 功能 | 使用场景 |
|-----------|------|----------|
| filesystem | 文件读写 | 代码编辑、文档处理 |
| postgres/mysql | 数据库查询 | 数据分析、报表生成 |
| github | 代码仓库操作 | PR 审查、Issue 管理 |
| slack | 消息发送 | 团队通知、自动化 |
| browser | 网页浏览 | 信息收集、测试 |
| memory | 长期记忆 | 保存对话上下文 |

---

## 如何开始使用

### 1. 安装 Claude Code
\`\`\`bash
# macOS/Linux
npm install -g @anthropic-ai/claude-code

# 启动
claude
\`\`\`

### 2. 配置 MCP 服务器
\`\`\`json
// ~/.claude/claude_desktop_config.json
{
  "mcpServers": {
    "filesystem": {
      "command": "npx",
      "args": ["-y", "@anthropic-ai/mcp-server-filesystem", "/path/to/project"]
    },
    "github": {
      "command": "npx",
      "args": ["-y", "@anthropic-ai/mcp-server-github"],
      "env": {
        "GITHUB_TOKEN": "your-token"
      }
    }
  }
}
\`\`\`

### 3. 开始对话
\`\`\`
你: 帮我看看这个项目的代码结构
Claude: [读取项目文件...] 这是一个 React + TypeScript 项目...

你: 添加一个深色模式切换功能
Claude: [分析现有代码...] [创建 ThemeContext...] [修改组件...]
\`\`\`

---

## Boris 的高效工作流（来自 Claude Code 创始人）

Claude Code 创始人 Boris Cherny 分享了他的实战经验，这些技巧能让效率提升数倍。

### 1. 并行处理：5 个实例同时跑

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        Boris 的多实例工作流                              │
└─────────────────────────────────────────────────────────────────────────┘

  终端                                    Web 版
  ─────                                  ──────
  Tab 1: 功能开发                        Tab A: 代码审查
  Tab 2: Bug 修复                        Tab B: 文档生成
  Tab 3: 测试编写                        Tab C: 架构设计
  Tab 4: 重构优化                        ...
  Tab 5: 部署脚本

  ↓ 每个实例独立处理子任务 ↓

  并行开发 → 效率翻倍
\`\`\`

### 2. Plan 模式优先

> "大多数会话从 Plan 模式开始（Shift+Tab 两次），先设计再执行。"

\`\`\`
普通模式                              Plan 模式
────────                             ──────────
直接写代码                            先分析需求
边写边改                              设计方案
来回修改 3-5 次                       一次通过

效率: 低                              效率: 高
\`\`\`

### 3. Slash 命令自动化

将重复操作封装成命令：

\`\`\`bash
/commit-push-pr    # 一键：提交 + 推送 + 创建 PR
/verify-app        # 自动化端到端测试
/code-simplifier   # 写完代码自动简化
\`\`\`

### 4. 验证反馈循环

> "给 Claude 一个验证自己的方式，质量能提升 2-3 倍。"

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                          验证反馈循环                                    │
└─────────────────────────────────────────────────────────────────────────┘

                    ┌───────────────┐
                    │  编写代码     │
                    └───────┬───────┘
                            │
                            ▼
                    ┌───────────────┐
                    │  自动测试     │ ← 单元测试 / E2E 测试
                    └───────┬───────┘
                            │
                    ┌───────┴───────┐
                    │               │
                    ▼               ▼
              ┌─────────┐     ┌─────────┐
              │  通过   │     │  失败   │
              └─────────┘     └────┬────┘
                                   │
                                   ▼
                            ┌───────────────┐
                            │  自动修复     │
                            └───────┬───────┘
                                    │
                                    └──────▶ 重新测试
\`\`\`

---

## Sub-agents：专家团队协作

Claude Code 支持创建专门的 Sub-agents，每个专注一个领域：

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                       Sub-agents 架构                                   │
└─────────────────────────────────────────────────────────────────────────┘

                           主 Agent
                              │
        ┌─────────────────────┼─────────────────────┐
        │                     │                     │
        ▼                     ▼                     ▼
  ┌───────────┐         ┌───────────┐         ┌───────────┐
  │ 代码审查员 │         │  测试专家  │         │ 文档作者  │
  │           │         │           │         │           │
  │ - 检查质量 │         │ - 编写测试 │         │ - 生成文档 │
  │ - 安全审计 │         │ - 覆盖率   │         │ - API 说明 │
  │ - 性能建议 │         │ - 边界用例 │         │ - 示例代码 │
  └───────────┘         └───────────┘         └───────────┘
        │                     │                     │
        └─────────────────────┼─────────────────────┘
                              │
                              ▼
                        整合所有结果
\`\`\`

### 配置 Sub-agent

\`\`\`yaml
# .claude/agents/code-reviewer.md
---
name: code-reviewer
description: 专门审查代码质量和安全性
tools:
  - Read
  - Grep
  - Glob
---

你是一位资深代码审查专家。重点关注：
1. 代码质量和可读性
2. 安全漏洞
3. 性能问题
\`\`\`

---

## Spec-Driven 开发：告别 Vibe Coding

传统的 "Vibe Coding"（边想边写）效率低下，Spec-Driven 开发是更好的选择：

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                   Spec-Driven vs Vibe Coding                            │
└─────────────────────────────────────────────────────────────────────────┘

  Vibe Coding                           Spec-Driven
  ────────────                          ───────────
  ┌─────────────┐                      ┌─────────────┐
  │  想到什么    │                      │  需求分析   │
  │  写什么     │                      └──────┬──────┘
  └──────┬──────┘                             │
         │                                    ▼
         ▼                             ┌─────────────┐
  ┌─────────────┐                      │  编写规格   │
  │  出 bug     │                      │  (spec.md)  │
  └──────┬──────┘                      └──────┬──────┘
         │                                    │
         ▼                                    ▼
  ┌─────────────┐                      ┌─────────────┐
  │  反复修改   │                      │  按规格实现 │
  └──────┬──────┘                      └──────┬──────┘
         │                                    │
         ▼                                    ▼
  ┌─────────────┐                      ┌─────────────┐
  │  又出 bug   │                      │  一次通过   │
  └─────────────┘                      └─────────────┘

  效率: ★★☆☆☆                         效率: ★★★★★
\`\`\`

---

## 本节要点

1. **MCP** —— AI 与外部工具的标准化接口
2. **Claude Code** —— 基于 MCP 的 AI 编程助手
3. **并行处理** —— 多实例同时工作，效率翻倍
4. **验证循环** —— 自动测试确保代码质量
5. **Sub-agents** —— 专业分工，各司其职
6. **Spec-Driven** —— 先设计后实现，一次通过
            `,
            ja: `
## MCPとは？

**MCP（Model Context Protocol）** は、Anthropicが開発したオープンプロトコルで、AIが様々な外部ツールやデータソースに安全に接続できるようにします。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                         MCP アーキテクチャ概要                           │
└─────────────────────────────────────────────────────────────────────────┘

                           ┌───────────────┐
                           │   AIモデル    │
                           │  (Claude等)   │
                           └───────┬───────┘
                                   │
                           ┌───────▼───────┐
                           │  MCPプロトコル │
                           └───────┬───────┘
                                   │
      ┌────────────────────────────┼────────────────────────────┐
      │                            │                            │
      ▼                            ▼                            ▼
┌───────────┐              ┌───────────┐              ┌───────────┐
│ファイルシステム│              │ データベース │              │ APIサービス │
│ MCPサーバー │              │ MCPサーバー │              │ MCPサーバー │
└───────────┘              └───────────┘              └───────────┘
      │                            │                            │
      ▼                            ▼                            ▼
  ローカルファイル                PostgreSQL                  GitHub/Slack
                               MySQL                      Notion/Jira
\`\`\`

---

## なぜMCPが重要なのか？

以前の問題：
- 各AI製品が個別にツール統合を開発
- データソース接続方法がバラバラ
- 再利用や標準化が困難

MCPが解決したこと：
\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                         統一されたツールインターフェース                    │
└─────────────────────────────────────────────────────────────────────────┘

  旧モデル                              新モデル（MCP）
  ─────────                            ──────────────
  ChatGPT ─┬─ ファイル読取              Claude ──┐
           ├─ Web検索                   ChatGPT ─┼── MCP ──┬─ 任意のツール
           └─ API呼出                   他のAI ──┘         └─ 任意のデータ

  Claude ──┬─ ファイル読取
           ├─ Web検索     ──▶          一度開発、どこでも利用可能
           └─ API呼出

  重複開発 ✗                           標準化再利用 ✓
\`\`\`

---

## Claude Code：MCPの実践活用

**Claude Code** はMCPベースのAIプログラミングアシスタントで、開発環境を直接操作できます。

### コア機能

| 機能 | 説明 | 例 |
|------|------|------|
| 🔍 コード理解 | プロジェクト全体を読み取り分析 | 「このモジュールのアーキテクチャを説明して」 |
| ✏️ コード修正 | ファイルを直接編集 | 「このバグを修正して」 |
| 💻 コマンド実行 | ターミナルコマンドを実行 | 「テストを実行して失敗を修正して」 |
| 🔗 ツール統合 | MCPで機能拡張 | 「DBのユーザーデータを取得して」 |

### 実際のワークフロー

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                     Claude Code ワークフロー                             │
└─────────────────────────────────────────────────────────────────────────┘

ユーザー: 「ユーザーログイン機能を実装して」
                │
                ▼
        ┌───────────────┐
        │ 1. プロジェクト分析 │  ← 既存コードを読み取り、アーキテクチャを理解
        └───────┬───────┘
                │
                ▼
        ┌───────────────┐
        │ 2. 計画立案    │  ← 作成/修正が必要なファイルを特定
        └───────┬───────┘
                │
                ▼
        ┌───────────────┐
        │ 3. コード作成  │  ← ルート、コントローラー、モデルを作成
        └───────┬───────┘
                │
                ▼
        ┌───────────────┐
        │ 4. テスト検証  │  ← テスト実行、エラー確認
        └───────┬───────┘
                │
                ▼
        ┌───────────────┐
        │ 5. 反復改善    │  ← フィードバックに基づき最適化
        └───────────────┘
\`\`\`

---

## よく使うMCPサーバー

| MCPサーバー | 機能 | 使用シーン |
|-----------|------|----------|
| filesystem | ファイル読み書き | コード編集、ドキュメント処理 |
| postgres/mysql | DBクエリ | データ分析、レポート生成 |
| github | コードリポジトリ操作 | PRレビュー、Issue管理 |
| slack | メッセージ送信 | チーム通知、自動化 |
| browser | Webブラウジング | 情報収集、テスト |
| memory | 長期記憶 | 会話コンテキストの保存 |

---

## 始め方

### 1. Claude Codeをインストール
\`\`\`bash
# macOS/Linux
npm install -g @anthropic-ai/claude-code

# 起動
claude
\`\`\`

### 2. MCPサーバーを設定
\`\`\`json
// ~/.claude/claude_desktop_config.json
{
  "mcpServers": {
    "filesystem": {
      "command": "npx",
      "args": ["-y", "@anthropic-ai/mcp-server-filesystem", "/path/to/project"]
    },
    "github": {
      "command": "npx",
      "args": ["-y", "@anthropic-ai/mcp-server-github"],
      "env": {
        "GITHUB_TOKEN": "your-token"
      }
    }
  }
}
\`\`\`

### 3. 対話を開始
\`\`\`
あなた: このプロジェクトのコード構造を見て
Claude: [プロジェクトファイルを読み取り中...] React + TypeScriptプロジェクトです...

あなた: ダークモード切替機能を追加して
Claude: [既存コードを分析中...] [ThemeContextを作成...] [コンポーネントを修正...]
\`\`\`

---

## Borisの効率的ワークフロー（Claude Code創設者より）

Claude Code創設者Boris Chernyが実践的なテクニックを共有。効率が数倍向上します。

### 1. 並列処理：5インスタンス同時実行

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                      Borisのマルチインスタンスワークフロー                  │
└─────────────────────────────────────────────────────────────────────────┘

  ターミナル                              Web版
  ────────                              ─────
  Tab 1: 機能開発                        Tab A: コードレビュー
  Tab 2: バグ修正                        Tab B: ドキュメント生成
  Tab 3: テスト作成                      Tab C: アーキテクチャ設計
  Tab 4: リファクタリング                ...
  Tab 5: デプロイスクリプト

  ↓ 各インスタンスが独立してサブタスクを処理 ↓

  並列開発 → 効率倍増
\`\`\`

### 2. Planモード優先

> "ほとんどのセッションはPlanモードから開始（Shift+Tab 2回）、設計してから実行。"

### 3. Slashコマンド自動化

\`\`\`bash
/commit-push-pr    # ワンクリック：コミット + プッシュ + PR作成
/verify-app        # 自動E2Eテスト
/code-simplifier   # コード作成後に自動簡素化
\`\`\`

### 4. 検証フィードバックループ

> "Claudeに自己検証の方法を与えると、品質が2-3倍向上する。"

---

## Sub-agents：専門家チーム協力

Claude Codeは専門のSub-agentsをサポート、各自が1つの領域に集中：

\`\`\`yaml
# .claude/agents/code-reviewer.md
---
name: code-reviewer
description: コード品質とセキュリティを専門にレビュー
tools:
  - Read
  - Grep
  - Glob
---

あなたはシニアコードレビュアーです。重点：
1. コード品質と可読性
2. セキュリティ脆弱性
3. パフォーマンス問題
\`\`\`

---

## Spec-Driven開発：Vibe Codingからの脱却

従来の"Vibe Coding"（思いつきでコーディング）は非効率。Spec-Driven開発がより良い選択：

| アプローチ | 特徴 | 効率 |
|-----------|------|------|
| Vibe Coding | 思いつきで書く→バグ→修正の繰り返し | ★★☆☆☆ |
| Spec-Driven | 要件分析→仕様作成→実装 | ★★★★★ |

---

## このセクションのポイント

1. **MCP** —— AIと外部ツールの標準化インターフェース
2. **Claude Code** —— MCPベースのAIプログラミングアシスタント
3. **並列処理** —— 複数インスタンス同時作業で効率倍増
4. **検証ループ** —— 自動テストでコード品質を確保
5. **Sub-agents** —— 専門分業、各自の役割
6. **Spec-Driven** —— 設計先行、一発で通す
            `
          }
        },
        {
          id: 'ch3-claude-code-commands',
          title: { zh: '3.4 Claude Code 命令详解', ja: '3.4 Claude Code コマンド詳解' },
          content: {
            zh: `
## Claude Code 命令大全

Claude Code 提供了丰富的斜杠命令（Slash Commands）来提升开发效率。掌握这些命令能让你的 AI 编程体验更上一层楼。

---

## 核心命令一览

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                     Claude Code 命令分类                                 │
└─────────────────────────────────────────────────────────────────────────┘

  基础命令                  MCP 命令               会话管理
  ────────                  ────────               ────────
  /help                    /mcp                   /clear
  /status                  /mcp add               /compact
  /config                  /mcp remove            /memory
  /model                   /mcp list              /resume
                           /mcp logs

  开发命令                  文件操作               工具命令
  ────────                  ────────               ────────
  /init                    /read                  /terminal
  /plan                    /edit                  /web
  /commit                  /write                 /browser
  /pr                      /search                /cost
\`\`\`

---

## 🔧 MCP 命令详解

MCP（Model Context Protocol）命令是 Claude Code 最强大的扩展能力。

### /mcp - 查看 MCP 状态

\`\`\`bash
/mcp
# 显示当前已连接的 MCP 服务器列表和状态
\`\`\`

**使用场景**：
- 检查 MCP 服务器是否正常运行
- 查看可用的工具和资源
- 调试连接问题

### /mcp add - 添加 MCP 服务器

\`\`\`bash
# 添加 GitHub MCP 服务器
/mcp add github

# 添加数据库 MCP 服务器
/mcp add postgres

# 添加自定义 MCP 服务器
/mcp add my-custom-server --command "node server.js"
\`\`\`

**使用场景**：
- 连接 GitHub 进行 PR 管理
- 连接数据库进行数据分析
- 添加自定义工具扩展能力

### /mcp logs - 查看 MCP 日志

\`\`\`bash
/mcp logs
# 查看 MCP 服务器的运行日志，用于调试

/mcp logs github
# 查看特定服务器的日志
\`\`\`

---

## 📝 会话管理命令

### /clear - 清空会话

\`\`\`bash
/clear
# 清空当前对话历史，开始全新会话
\`\`\`

**使用场景**：
- 切换到新任务时清理上下文
- 对话过长导致响应变慢时重置
- 避免旧对话影响新任务

### /compact - 压缩上下文

\`\`\`bash
/compact
# 智能压缩对话历史，保留关键信息
\`\`\`

**使用场景**：
- 对话过长时节省 tokens
- 保留重要上下文同时减少内存占用
- 长时间编程会话中定期执行

### /memory - 长期记忆

\`\`\`bash
/memory
# 查看和管理长期记忆

/memory add "项目使用 React + TypeScript"
# 添加长期记忆，后续会话都会记住
\`\`\`

**使用场景**：
- 保存项目架构信息
- 记住编码偏好（如缩进风格）
- 跨会话保持一致性

---

## 🚀 开发命令

### /init - 初始化项目

\`\`\`bash
/init
# Claude 分析当前目录，生成 CLAUDE.md 配置文件
\`\`\`

**使用场景**：
- 新项目首次使用 Claude Code
- 让 Claude 理解项目结构和技术栈
- 生成项目特定的指令配置

### /plan - 进入计划模式

\`\`\`bash
/plan
# 切换到计划模式，只做分析和设计，不执行修改

# 快捷键：Shift + Tab 两次
\`\`\`

**使用场景**：
- 复杂功能开发前的架构设计
- 代码审查和分析
- 学习理解代码库

### /commit - 提交代码

\`\`\`bash
/commit
# 自动生成 commit message 并提交
\`\`\`

**使用场景**：
- 完成功能开发后快速提交
- AI 自动分析变更生成 commit 信息
- 保持 commit 历史清晰

### /pr - 创建 Pull Request

\`\`\`bash
/pr
# 创建 PR 并生成描述
\`\`\`

**使用场景**：
- 完成功能后自动创建 PR
- AI 生成清晰的 PR 描述
- 包含变更摘要和测试说明

---

## 🔍 文件操作命令

### /search - 搜索代码

\`\`\`bash
/search "function handleClick"
# 在项目中搜索代码

/search --type ts "useState"
# 只搜索 TypeScript 文件
\`\`\`

**使用场景**：
- 快速定位代码位置
- 查找函数定义和引用
- 分析代码模式

---

## ⚙️ 配置命令

### /config - 查看/修改配置

\`\`\`bash
/config
# 查看当前配置

/config set model claude-opus-4-5-20250514
# 设置默认模型

/config set theme dark
# 设置主题
\`\`\`

### /model - 切换模型

\`\`\`bash
/model
# 查看可用模型

/model claude-opus-4-5-20250514
# 切换到 Opus 4.5
\`\`\`

**使用场景**：
- 简单任务用 Haiku 节省成本
- 复杂任务切换到 Opus 获得最佳效果
- 根据任务类型灵活选择

---

## 💰 实用命令

### /cost - 查看使用成本

\`\`\`bash
/cost
# 显示当前会话的 token 使用和费用
\`\`\`

**使用场景**：
- 监控 API 使用量
- 优化 prompt 降低成本
- 预算管理

### /status - 查看状态

\`\`\`bash
/status
# 显示连接状态、模型信息、配置等
\`\`\`

---

## 🎯 高效工作流示例

### 示例 1：新功能开发

\`\`\`bash
# 1. 先进入计划模式，设计方案
/plan
你: 我需要实现用户认证功能

# 2. 确认方案后退出计划模式执行
# Shift + Tab 切换到 Auto 模式

# 3. 开发完成后提交
/commit

# 4. 创建 PR
/pr
\`\`\`

### 示例 2：调试 MCP 问题

\`\`\`bash
# 1. 检查 MCP 状态
/mcp

# 2. 查看日志定位问题
/mcp logs

# 3. 重新添加服务器
/mcp remove github
/mcp add github
\`\`\`

### 示例 3：长会话管理

\`\`\`bash
# 1. 对话过长时压缩
/compact

# 2. 添加重要信息到长期记忆
/memory add "使用 pnpm 而非 npm"

# 3. 切换任务时清空
/clear
\`\`\`

---

## 📋 命令速查表

| 命令 | 快捷方式 | 说明 |
|------|----------|------|
| /help | - | 显示帮助信息 |
| /clear | Cmd/Ctrl+K | 清空会话 |
| /compact | - | 压缩上下文 |
| /mcp | - | 查看 MCP 状态 |
| /plan | Shift+Tab×2 | 进入计划模式 |
| /commit | - | 提交代码 |
| /pr | - | 创建 PR |
| /cost | - | 查看费用 |
| /model | - | 切换模型 |

---

## 本节要点

1. **MCP 命令** —— 管理外部工具连接，扩展 AI 能力
2. **会话管理** —— /clear、/compact、/memory 控制对话
3. **开发命令** —— /plan、/commit、/pr 覆盖完整开发流程
4. **配置命令** —— /config、/model 自定义工作环境
5. **实用技巧** —— 组合使用命令实现高效工作流
            `,
            ja: `
## Claude Code コマンド大全

Claude Codeは豊富なスラッシュコマンドを提供し、開発効率を向上させます。これらのコマンドをマスターすれば、AIプログラミング体験がさらに向上します。

---

## コアコマンド一覧

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                     Claude Code コマンド分類                             │
└─────────────────────────────────────────────────────────────────────────┘

  基本コマンド              MCPコマンド             セッション管理
  ──────────              ──────────             ────────────
  /help                    /mcp                   /clear
  /status                  /mcp add               /compact
  /config                  /mcp remove            /memory
  /model                   /mcp list              /resume
                           /mcp logs

  開発コマンド              ファイル操作            ツールコマンド
  ──────────              ──────────              ────────────
  /init                    /read                  /terminal
  /plan                    /edit                  /web
  /commit                  /write                 /browser
  /pr                      /search                /cost
\`\`\`

---

## 🔧 MCPコマンド詳解

MCP（Model Context Protocol）コマンドはClaude Codeの最強の拡張機能です。

### /mcp - MCP状態確認

\`\`\`bash
/mcp
# 接続中のMCPサーバーリストと状態を表示
\`\`\`

**使用シーン**：
- MCPサーバーが正常に動作しているか確認
- 利用可能なツールとリソースを確認
- 接続問題のデバッグ

### /mcp add - MCPサーバー追加

\`\`\`bash
# GitHub MCPサーバーを追加
/mcp add github

# データベースMCPサーバーを追加
/mcp add postgres

# カスタムMCPサーバーを追加
/mcp add my-custom-server --command "node server.js"
\`\`\`

**使用シーン**：
- GitHubに接続してPR管理
- データベースに接続してデータ分析
- カスタムツールで機能拡張

---

## 📝 セッション管理コマンド

### /clear - セッションクリア

\`\`\`bash
/clear
# 現在の会話履歴をクリアし、新しいセッションを開始
\`\`\`

**使用シーン**：
- 新しいタスクに切り替える時にコンテキストをクリア
- 会話が長くなり応答が遅くなった時にリセット
- 古い会話が新しいタスクに影響するのを防ぐ

### /compact - コンテキスト圧縮

\`\`\`bash
/compact
# 会話履歴をスマートに圧縮、重要な情報を保持
\`\`\`

**使用シーン**：
- 会話が長くなった時にtokensを節約
- 重要なコンテキストを保持しつつメモリ使用量を削減
- 長時間のプログラミングセッションで定期的に実行

---

## 🚀 開発コマンド

### /plan - 計画モードに入る

\`\`\`bash
/plan
# 計画モードに切り替え、分析と設計のみ、変更は実行しない

# ショートカット：Shift + Tab 2回
\`\`\`

**使用シーン**：
- 複雑な機能開発前のアーキテクチャ設計
- コードレビューと分析
- コードベースの理解と学習

### /commit - コードコミット

\`\`\`bash
/commit
# 自動的にcommit messageを生成してコミット
\`\`\`

### /pr - Pull Request作成

\`\`\`bash
/pr
# PRを作成して説明を生成
\`\`\`

---

## 📋 コマンド早見表

| コマンド | ショートカット | 説明 |
|---------|--------------|------|
| /help | - | ヘルプ情報を表示 |
| /clear | Cmd/Ctrl+K | セッションをクリア |
| /compact | - | コンテキストを圧縮 |
| /mcp | - | MCP状態を確認 |
| /plan | Shift+Tab×2 | 計画モードに入る |
| /commit | - | コードをコミット |
| /pr | - | PRを作成 |
| /cost | - | 費用を確認 |
| /model | - | モデルを切り替え |

---

## 本節のポイント

1. **MCPコマンド** —— 外部ツール接続を管理、AI能力を拡張
2. **セッション管理** —— /clear、/compact、/memoryで会話を制御
3. **開発コマンド** —— /plan、/commit、/prで開発フロー全体をカバー
4. **設定コマンド** —— /config、/modelで作業環境をカスタマイズ
5. **実用テクニック** —— コマンドを組み合わせて効率的なワークフローを実現
            `
          }
        },
        {
          id: 'ch3-agent-skills',
          title: { zh: '3.5 Agent Skills 详解', ja: '3.5 Agent Skills 詳解' },
          content: {
            zh: `
## Agent Skills：让 AI 学会新技能

Agent Skills 是 Claude Code 最强大的定制化功能。通过自定义 Skills，你可以让 AI 掌握特定的工作流程，实现一键执行复杂任务。

---

## 什么是 Agent Skills？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        Agent Skills 架构                                 │
└─────────────────────────────────────────────────────────────────────────┘

  用户请求                Skills 定义               AI 执行
  ────────               ──────────               ────────
  "运行 /deploy"    →    .claude/skills/         →   自动执行
                         deploy.md                   部署流程
                              │
                              ▼
                    ┌─────────────────┐
                    │ • 触发条件       │
                    │ • 执行步骤       │
                    │ • 工具调用       │
                    │ • 输出格式       │
                    └─────────────────┘
\`\`\`

**核心概念**：
- **Skills** = 可复用的 AI 工作流程定义
- **存储位置**：\`.claude/skills/\` 目录下的 Markdown 文件
- **触发方式**：通过 \`/skillname\` 或自然语言触发
- **跨平台通用**：OpenAI Codex 已采用相同规范

---

## 创建你的第一个 Skill

### 示例 1：代码审查 Skill

\`\`\`markdown
# .claude/skills/review.md

---
name: review
description: 执行全面的代码审查
triggers:
  - /review
  - 帮我审查代码
  - code review
---

# 代码审查流程

## 执行步骤

1. **获取变更文件**
   - 运行 \`git diff --name-only HEAD~1\` 获取变更文件列表
   - 如果用户指定了文件，则只审查指定文件

2. **代码分析**
   对每个文件检查以下方面：
   - 代码逻辑正确性
   - 潜在的 Bug 和边界情况
   - 性能问题
   - 安全漏洞（SQL 注入、XSS 等）
   - 代码风格一致性

3. **生成报告**
   使用以下格式输出：

   \`\`\`
   ## 📝 代码审查报告

   ### 文件：{filename}

   #### ✅ 优点
   - ...

   #### ⚠️ 建议改进
   - 行 {line}: {issue}

   #### 🔴 必须修复
   - 行 {line}: {critical_issue}
   \`\`\`

4. **提供修复建议**
   对于每个问题，提供具体的修复代码示例
\`\`\`

### 示例 2：自动化部署 Skill

\`\`\`markdown
# .claude/skills/deploy.md

---
name: deploy
description: 一键部署到生产环境
triggers:
  - /deploy
  - 部署到生产
  - deploy to production
---

# 部署流程

## 前置检查

1. 确认当前分支是 main 或 master
2. 确认没有未提交的更改
3. 确认所有测试通过

## 执行步骤

1. **运行测试**
   \`\`\`bash
   npm test
   \`\`\`
   如果测试失败，停止部署并报告错误

2. **构建项目**
   \`\`\`bash
   npm run build
   \`\`\`

3. **版本更新**
   - 读取 package.json 中的版本号
   - 询问用户选择版本更新类型（patch/minor/major）
   - 更新版本号

4. **创建 Git Tag**
   \`\`\`bash
   git tag v{version}
   git push origin v{version}
   \`\`\`

5. **部署确认**
   输出部署摘要，等待用户确认后执行部署命令
\`\`\`

---

## Skill 高级技巧

### 1. 使用变量和参数

\`\`\`markdown
# .claude/skills/create-component.md

---
name: create-component
description: 创建 React 组件
triggers:
  - /component {name}
  - 创建组件 {name}
---

# 创建 React 组件

## 参数
- \`{name}\`: 组件名称（必需）

## 执行步骤

1. 在 \`src/components/{name}/\` 目录下创建：
   - \`{name}.tsx\` - 组件代码
   - \`{name}.test.tsx\` - 测试文件
   - \`{name}.css\` - 样式文件
   - \`index.ts\` - 导出文件

2. 组件模板：
   \`\`\`tsx
   import React from 'react';
   import './{name}.css';

   interface {name}Props {
     // 定义 props
   }

   export const {name}: React.FC<{name}Props> = (props) => {
     return (
       <div className="{name}">
         {/* 组件内容 */}
       </div>
     );
   };
   \`\`\`
\`\`\`

### 2. 条件分支执行

\`\`\`markdown
# .claude/skills/fix.md

---
name: fix
description: 智能修复问题
triggers:
  - /fix
  - 帮我修复
---

# 智能修复流程

## 问题诊断

1. **检测问题类型**
   - 如果是 TypeScript 错误：运行 \`npx tsc --noEmit\` 获取错误列表
   - 如果是 ESLint 错误：运行 \`npm run lint\` 获取警告
   - 如果是测试失败：运行 \`npm test\` 查看失败用例
   - 如果是构建错误：分析构建日志

2. **根据类型执行修复**

   ### TypeScript 错误
   - 定位错误文件和行号
   - 分析类型错误原因
   - 提供类型修复方案

   ### ESLint 错误
   - 尝试自动修复：\`npm run lint -- --fix\`
   - 手动修复无法自动处理的问题

   ### 测试失败
   - 分析失败原因
   - 修复代码或更新测试用例
\`\`\`

### 3. 团队共享 Skills

\`\`\`bash
# 项目级 Skills（团队共享）
.claude/
└── skills/
    ├── review.md      # 团队代码审查规范
    ├── deploy.md      # 部署流程
    └── onboard.md     # 新人引导

# 用户级 Skills（个人专用）
~/.claude/
└── skills/
    ├── my-shortcuts.md
    └── personal-workflow.md
\`\`\`

---

## 🔥 推荐开源 Skills 资源

### awesome-claude-skills

GitHub 上最全的 Claude Skills 资源集合：

\`\`\`bash
# 仓库地址
https://github.com/travisvn/awesome-claude-skills

# 包含内容
├── 📁 skills/           # 数百个即用 Skills
│   ├── code-review/     # 代码审查
│   ├── deployment/      # 部署自动化
│   ├── testing/         # 测试生成
│   └── documentation/   # 文档生成
├── 📁 templates/        # Skills 模板
└── 📁 examples/         # 使用示例
\`\`\`

**快速使用**：
\`\`\`bash
# 克隆仓库
git clone https://github.com/travisvn/awesome-claude-skills.git

# 复制你需要的 Skills 到项目
cp -r awesome-claude-skills/skills/code-review .claude/skills/
\`\`\`

### awesome-claude-code

Claude Code 综合资源列表：

\`\`\`bash
# 仓库地址
https://github.com/hesreallyhim/awesome-claude-code

# 包含资源
- Skills 集合
- MCP 服务器列表
- 最佳实践指南
- 社区工具推荐
\`\`\`

---

## 实战案例：完整项目 Skills 配置

\`\`\`bash
# 项目目录结构
my-project/
├── .claude/
│   ├── skills/
│   │   ├── review.md        # 代码审查
│   │   ├── deploy.md        # 部署流程
│   │   ├── component.md     # 组件生成
│   │   ├── api.md           # API 开发
│   │   └── debug.md         # 调试助手
│   └── settings.json        # Hooks 配置
├── CLAUDE.md                # 项目说明
└── src/
\`\`\`

### 推荐的 Skills 集合

| Skill | 命令 | 用途 |
|-------|------|------|
| review | /review | 代码审查 |
| deploy | /deploy | 一键部署 |
| component | /component Name | 创建组件 |
| api | /api endpoint | 创建 API 端点 |
| debug | /debug | 智能调试 |
| test | /test | 运行并修复测试 |
| doc | /doc | 生成文档 |
| refactor | /refactor | 代码重构 |

---

## 本节要点

1. **Skills 定义** —— 在 \`.claude/skills/\` 目录下创建 Markdown 文件
2. **触发方式** —— 使用 \`/skillname\` 或自然语言触发
3. **参数传递** —— 使用 \`{参数名}\` 语法接收用户输入
4. **开源资源** —— awesome-claude-skills 提供数百个即用 Skills
5. **团队共享** —— 项目级 Skills 实现团队标准化
            `,
            ja: `
## Agent Skills：AIに新しいスキルを学ばせる

Agent SkillsはClaude Codeの最も強力なカスタマイズ機能です。カスタムSkillsを通じて、AIに特定のワークフローを習得させ、複雑なタスクをワンクリックで実行できます。

---

## Agent Skillsとは？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        Agent Skills アーキテクチャ                        │
└─────────────────────────────────────────────────────────────────────────┘

  ユーザーリクエスト        Skills定義                AI実行
  ──────────────         ──────────               ────────
  "/deployを実行"    →    .claude/skills/         →   自動で
                         deploy.md                   デプロイ実行
                              │
                              ▼
                    ┌─────────────────┐
                    │ • トリガー条件   │
                    │ • 実行ステップ   │
                    │ • ツール呼び出し │
                    │ • 出力形式      │
                    └─────────────────┘
\`\`\`

**コアコンセプト**：
- **Skills** = 再利用可能なAIワークフロー定義
- **保存場所**：\`.claude/skills/\` ディレクトリ内のMarkdownファイル
- **トリガー方法**：\`/skillname\` または自然言語でトリガー
- **クロスプラットフォーム互換**：OpenAI Codexも同じ仕様を採用

---

## 最初のSkillを作成する

### 例1：コードレビュー Skill

\`\`\`markdown
# .claude/skills/review.md

---
name: review
description: 包括的なコードレビューを実行
triggers:
  - /review
  - コードレビューして
  - code review
---

# コードレビュープロセス

## 実行ステップ

1. **変更ファイルを取得**
   - \`git diff --name-only HEAD~1\` で変更ファイルリストを取得
   - ユーザーが指定した場合は、指定ファイルのみレビュー

2. **コード分析**
   各ファイルで以下をチェック：
   - コードロジックの正確性
   - 潜在的なバグと境界ケース
   - パフォーマンス問題
   - セキュリティ脆弱性（SQLインジェクション、XSSなど）
   - コードスタイルの一貫性

3. **レポート生成**
   以下の形式で出力：

   \`\`\`
   ## 📝 コードレビューレポート

   ### ファイル：{filename}

   #### ✅ 良い点
   - ...

   #### ⚠️ 改善提案
   - 行 {line}: {issue}

   #### 🔴 必須修正
   - 行 {line}: {critical_issue}
   \`\`\`
\`\`\`

---

## Skill 高度なテクニック

### 1. 変数とパラメータの使用

\`\`\`markdown
# .claude/skills/create-component.md

---
name: create-component
description: Reactコンポーネントを作成
triggers:
  - /component {name}
  - コンポーネント作成 {name}
---

# Reactコンポーネント作成

## パラメータ
- \`{name}\`: コンポーネント名（必須）

## 実行ステップ

1. \`src/components/{name}/\` ディレクトリに作成：
   - \`{name}.tsx\` - コンポーネントコード
   - \`{name}.test.tsx\` - テストファイル
   - \`{name}.css\` - スタイルファイル
   - \`index.ts\` - エクスポートファイル
\`\`\`

### 2. チーム共有Skills

\`\`\`bash
# プロジェクトレベルSkills（チーム共有）
.claude/
└── skills/
    ├── review.md      # チームコードレビュー規範
    ├── deploy.md      # デプロイフロー
    └── onboard.md     # 新人ガイド

# ユーザーレベルSkills（個人専用）
~/.claude/
└── skills/
    ├── my-shortcuts.md
    └── personal-workflow.md
\`\`\`

---

## 🔥 おすすめオープンソースSkillsリソース

### awesome-claude-skills

GitHubで最も完全なClaude Skillsリソース集：

\`\`\`bash
# リポジトリURL
https://github.com/travisvn/awesome-claude-skills

# 含まれる内容
├── 📁 skills/           # 数百の即戦力Skills
├── 📁 templates/        # Skillsテンプレート
└── 📁 examples/         # 使用例
\`\`\`

### awesome-claude-code

Claude Code総合リソースリスト：

\`\`\`bash
# リポジトリURL
https://github.com/hesreallyhim/awesome-claude-code
\`\`\`

---

## 本節のポイント

1. **Skills定義** —— \`.claude/skills/\` ディレクトリにMarkdownファイルを作成
2. **トリガー方法** —— \`/skillname\` または自然言語でトリガー
3. **パラメータ渡し** —— \`{パラメータ名}\` 構文でユーザー入力を受け取る
4. **オープンソースリソース** —— awesome-claude-skillsで数百の即戦力Skills
5. **チーム共有** —— プロジェクトレベルSkillsでチーム標準化を実現
            `
          }
        },
        {
          id: 'ch3-subagents',
          title: { zh: '3.6 Sub-agents 多 Agent 协作', ja: '3.6 Sub-agents マルチAgent協力' },
          content: {
            zh: `
## Sub-agents：专家团队协作模式

Sub-agents 是 Claude Code 的高级功能，让你可以将复杂任务分解给多个专家 Agent 并行处理，大幅提升开发效率。

---

## Sub-agents 架构

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        Sub-agents 协作架构                               │
└─────────────────────────────────────────────────────────────────────────┘

                    ┌─────────────────┐
                    │   主 Agent      │
                    │  (Coordinator)  │
                    │  协调分配任务    │
                    └────────┬────────┘
                             │
           ┌─────────────────┼─────────────────┐
           ▼                 ▼                 ▼
   ┌───────────────┐ ┌───────────────┐ ┌───────────────┐
   │   代码专家     │ │   测试专家     │ │   文档专家     │
   │  (Code Agent) │ │ (Test Agent)  │ │  (Doc Agent)  │
   │   独立上下文   │ │   独立上下文   │ │   独立上下文   │
   └───────────────┘ └───────────────┘ └───────────────┘
           │                 │                 │
           ▼                 ▼                 ▼
     编写功能代码      编写测试用例       生成 API 文档
\`\`\`

**核心优势**：
- **上下文隔离** —— 每个 Agent 独立上下文，避免信息污染
- **专业化分工** —— 专注特定任务，成功率更高
- **并行处理** —— 多任务同时进行，效率倍增

---

## 使用 Sub-agents

### 对话中触发

\`\`\`markdown
用户：请帮我完成用户认证功能，需要代码、测试和文档

Claude：我会安排三个专家 Agent 并行工作：

1. **代码专家** - 实现 JWT 认证逻辑
2. **测试专家** - 编写单元测试和集成测试
3. **文档专家** - 生成 API 文档和使用说明

[启动 Sub-agents...]

// 主 Agent 协调，各专家独立工作
// 最终汇总所有输出
\`\`\`

### 通过 Skill 定义

\`\`\`markdown
# .claude/skills/full-feature.md

---
name: full-feature
description: 完整功能开发（代码+测试+文档）
---

## 执行步骤

1. **启动代码 Agent**
   - 专注于功能实现
   - 遵循项目代码规范
   - 输出：功能代码文件

2. **启动测试 Agent**
   - 等待代码 Agent 完成
   - 编写单元测试覆盖所有函数
   - 编写集成测试覆盖主要流程
   - 输出：测试文件

3. **启动文档 Agent**
   - 分析代码结构
   - 生成 JSDoc 注释
   - 更新 README
   - 输出：文档更新

4. **汇总报告**
   合并三个 Agent 的输出，生成完成报告
\`\`\`

---

## 🔥 多 Agent 管理工具

### Claude Squad ⭐ 4.3k

管理多个 AI Agent 的终端工具，支持 Claude Code、Codex、Gemini、Aider 等：

\`\`\`bash
# 安装
go install github.com/smtg-ai/claude-squad@latest

# 或使用 Homebrew
brew install smtg-ai/tap/claude-squad

# 启动
claude-squad
\`\`\`

**核心功能**：
\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│  Claude Squad - 多 Agent 终端管理器                                      │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  Workspace 1: [Claude Code] 正在实现用户认证...                          │
│  Workspace 2: [Codex] 正在编写测试用例...                                │
│  Workspace 3: [Aider] 正在重构数据库层...                                │
│  Workspace 4: [Gemini] 正在生成文档...                                   │
│                                                                         │
│  快捷键: [n]新建 [d]删除 [Tab]切换 [Enter]进入 [q]退出                   │
└─────────────────────────────────────────────────────────────────────────┘
\`\`\`

**使用场景**：
- 同时处理多个独立任务
- 不同 AI 工具各司其职
- 快速切换工作上下文

---

## Hooks：事件触发自动化

Hooks 允许你在特定事件发生时自动执行脚本，实现工作流自动化。

### 可用的 Hooks

| Hook 名称 | 触发时机 | 用途 |
|----------|---------|------|
| PreToolUse | 工具调用前 | 拦截和验证 |
| PostToolUse | 工具调用后 | 日志和后处理 |
| Notification | 通知事件 | 自定义提醒 |
| Stop | 会话结束 | 清理和总结 |

### 配置示例

\`\`\`json
// .claude/settings.json
{
  "hooks": {
    "PreToolUse": [
      {
        "matcher": "Bash",
        "command": "echo '即将执行: $TOOL_INPUT'"
      }
    ],
    "PostToolUse": [
      {
        "matcher": "Write",
        "command": "npx prettier --write $FILE_PATH"
      }
    ],
    "Stop": [
      {
        "command": "echo '会话结束' >> ~/.claude/session.log"
      }
    ]
  }
}
\`\`\`

### 实用 Hook 示例

\`\`\`json
// 自动格式化
{
  "hooks": {
    "PostToolUse": [
      {
        "matcher": "Write",
        "command": "npx prettier --write $FILE_PATH"
      }
    ]
  }
}

// 危险命令拦截
{
  "hooks": {
    "PreToolUse": [
      {
        "matcher": "Bash",
        "command": "if echo '$TOOL_INPUT' | grep -qE 'rm -rf|drop table'; then echo '⚠️ 危险命令！'; exit 1; fi"
      }
    ]
  }
}
\`\`\`

---

## 本节要点

1. **Sub-agents 架构** —— 主 Agent 协调，专家 Agent 并行工作
2. **上下文隔离** —— 每个 Agent 独立上下文，避免污染
3. **Claude Squad** —— 多 Agent 终端管理工具，支持多种 AI
4. **Hooks 自动化** —— 事件驱动的工作流自动化
5. **实战应用** —— 大型功能开发、代码迁移、重构任务
            `,
            ja: `
## Sub-agents：エキスパートチーム協力モード

Sub-agentsはClaude Codeの高度な機能で、複雑なタスクを複数のエキスパートAgentに分解して並列処理できます。

---

## Sub-agentsアーキテクチャ

\`\`\`
                    ┌─────────────────┐
                    │   メインAgent   │
                    │  (Coordinator)  │
                    └────────┬────────┘
                             │
           ┌─────────────────┼─────────────────┐
           ▼                 ▼                 ▼
   ┌───────────────┐ ┌───────────────┐ ┌───────────────┐
   │  コードエキスパート │ │  テストエキスパート │ │  ドキュメントエキスパート │
   │   独立コンテキスト │ │   独立コンテキスト │ │   独立コンテキスト │
   └───────────────┘ └───────────────┘ └───────────────┘
\`\`\`

**コアメリット**：
- **コンテキスト分離** —— 各Agent独立、情報汚染を防止
- **専門化分業** —— 特定タスクに集中、成功率向上
- **並列処理** —— 複数タスク同時進行、効率倍増

---

## 🔥 マルチAgent管理ツール

### Claude Squad ⭐ 4.3k

複数AIエージェントを管理するターミナルツール：

\`\`\`bash
# インストール
go install github.com/smtg-ai/claude-squad@latest

# 起動
claude-squad
\`\`\`

**使用シーン**：
- 複数の独立タスクを同時処理
- 異なるAIツールを使い分け
- 作業コンテキストを素早く切り替え

---

## Hooks：イベントトリガー自動化

特定イベント発生時にスクリプトを自動実行：

| Hook名 | トリガー | 用途 |
|--------|---------|------|
| PreToolUse | ツール呼び出し前 | インターセプト |
| PostToolUse | ツール呼び出し後 | 後処理 |
| Stop | セッション終了 | クリーンアップ |

\`\`\`json
{
  "hooks": {
    "PostToolUse": [
      {
        "matcher": "Write",
        "command": "npx prettier --write $FILE_PATH"
      }
    ]
  }
}
\`\`\`

---

## 本節のポイント

1. **Sub-agentsアーキテクチャ** —— メインAgentが調整、エキスパートが並列作業
2. **コンテキスト分離** —— 各Agent独立コンテキスト
3. **Claude Squad** —— マルチAgentターミナル管理ツール
4. **Hooks自動化** —— イベント駆動のワークフロー自動化
            `
          }
        },
        {
          id: 'ch3-opensource',
          title: { zh: '3.7 Claude Code 开源生态', ja: '3.7 Claude Code オープンソースエコシステム' },
          content: {
            zh: `
## Claude Code 开源生态

Claude Code 拥有活跃的开源社区，涌现出许多优秀的增强工具和框架。掌握这些工具能让你的 AI 编程效率倍增。

---

## 🏆 明星项目一览

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    Claude Code 开源生态 TOP 项目                          │
└─────────────────────────────────────────────────────────────────────────┘

  项目名称              Stars         功能
  ────────              ─────         ────
  Task Master           ⭐ 20.9k      AI 驱动的任务管理系统
  Claude-Flow           ⭐ 6.7k       多 Agent 编排框架
  Claude Squad          ⭐ 4.3k       多 Agent 终端管理器
  SuperClaude           ⭐ 3.2k       Claude Code 增强框架
  awesome-claude-skills ⭐ 2.8k       Skills 资源集合
\`\`\`

---

## 🚀 SuperClaude Framework

增强 Claude Code 能力 300% 的配置框架：

### 安装

\`\`\`bash
# 使用 npm 安装
npm install -g @superclaude-org/superclaude

# 或直接克隆
git clone https://github.com/SuperClaude-Org/SuperClaude_Framework.git
cd SuperClaude_Framework

# 安装到项目
superclaude init
\`\`\`

### 核心功能

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│  SuperClaude 功能矩阵                                                    │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  🎯 21 个斜杠命令        📦 13 个专家 Agent      🔄 6 种行为模式          │
│  ─────────────────      ────────────────       ────────────────         │
│  /architect             code-expert            spec-driven              │
│  /debug                 test-expert            tdd-mode                 │
│  /refactor              security-expert        debug-mode               │
│  /security              docs-expert            review-mode              │
│  /performance           devops-expert          ...                      │
│  ...                    ...                                             │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
\`\`\`

### 使用示例

\`\`\`bash
# 架构设计模式
/architect "设计一个微服务电商系统"

# 安全审查模式
/security "审查 API 端点安全性"

# 性能优化模式
/performance "优化数据库查询性能"

# 调试模式
/debug "定位内存泄漏问题"
\`\`\`

**GitHub**: https://github.com/SuperClaude-Org/SuperClaude_Framework

---

## 📋 Task Master（Claude Task Master）

AI 驱动的任务管理系统，GitHub ⭐ 20.9k：

### 安装

\`\`\`bash
# 使用 npm 安装
npm install -g task-master-ai

# 或使用 npx
npx task-master-ai init
\`\`\`

### 核心功能

\`\`\`bash
# 从 PRD 生成任务列表
task-master parse ./prd.md

# 查看任务列表
task-master list

# 开始下一个任务
task-master next

# 标记任务完成
task-master done 1
\`\`\`

### 工作流程

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│  Task Master 工作流程                                                    │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  📄 PRD 文档  →  🤖 AI 解析  →  📋 任务列表  →  ✅ 逐个完成              │
│                                                                         │
│  支持的 AI 编辑器:                                                       │
│  • Cursor                                                               │
│  • Claude Code                                                          │
│  • Windsurf                                                             │
│  • Lovable                                                              │
│  • Roo Code                                                             │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
\`\`\`

**特色功能**：
- 📝 PRD 自动解析为可执行任务
- 🔗 任务依赖关系管理
- 📊 进度追踪和报告
- 🔄 与多种 AI 编辑器集成

**GitHub**: https://github.com/eyaltoledano/claude-task-master

---

## 🔗 其他推荐项目

### Claude-Flow

多 Agent 编排框架：

\`\`\`bash
# 安装
npm install claude-flow

# 创建编排配置
claude-flow init

# 运行多 Agent 任务
claude-flow run workflow.yaml
\`\`\`

**GitHub**: https://github.com/kabirsingh/claude-flow

### awesome-claude

Claude 生态资源大全：

\`\`\`bash
# 包含资源
- 官方文档和教程
- 社区工具和插件
- MCP 服务器列表
- Skills 模板
- 最佳实践指南
\`\`\`

**GitHub**: https://github.com/alvinunreal/awesome-claude

---

## 📦 项目选择指南

| 需求 | 推荐项目 | 原因 |
|------|---------|------|
| 任务管理 | Task Master | PRD 到代码的完整流程 |
| 增强命令 | SuperClaude | 21 个专业命令 + 13 个专家 |
| 多 Agent 管理 | Claude Squad | 支持多种 AI 工具 |
| Skills 资源 | awesome-claude-skills | 数百个即用 Skills |
| 综合学习 | awesome-claude | 最全资源列表 |

---

## 本节要点

1. **SuperClaude** —— 21 命令 + 13 专家，能力增强 300%
2. **Task Master** —— PRD 到代码的 AI 任务管理（20k+ stars）
3. **Claude Squad** —— 多 Agent 终端管理器
4. **awesome-* 系列** —— 社区精选资源集合
5. **选择建议** —— 根据需求组合使用多个工具
            `,
            ja: `
## Claude Code オープンソースエコシステム

Claude Codeには活発なオープンソースコミュニティがあり、多くの優れた拡張ツールとフレームワークが生まれています。

---

## 🏆 スタープロジェクト一覧

| プロジェクト | Stars | 機能 |
|------------|-------|------|
| Task Master | ⭐ 20.9k | AI駆動タスク管理 |
| Claude-Flow | ⭐ 6.7k | マルチAgentオーケストレーション |
| Claude Squad | ⭐ 4.3k | マルチAgentターミナル管理 |
| SuperClaude | ⭐ 3.2k | Claude Code拡張フレームワーク |

---

## 🚀 SuperClaude Framework

Claude Code能力を300%向上させる設定フレームワーク：

\`\`\`bash
# インストール
npm install -g @superclaude-org/superclaude

# プロジェクトに適用
superclaude init
\`\`\`

**コア機能**：
- 21個のスラッシュコマンド
- 13個のエキスパートAgent
- 6種類の動作モード

\`\`\`bash
# 使用例
/architect "マイクロサービスECシステムを設計"
/security "APIエンドポイントのセキュリティを審査"
/debug "メモリリーク問題を特定"
\`\`\`

**GitHub**: https://github.com/SuperClaude-Org/SuperClaude_Framework

---

## 📋 Task Master

AI駆動タスク管理システム、⭐ 20.9k：

\`\`\`bash
# インストール
npm install -g task-master-ai

# PRDからタスクリストを生成
task-master parse ./prd.md

# 次のタスクを開始
task-master next
\`\`\`

**特徴**：
- PRD自動解析
- タスク依存関係管理
- 進捗追跡
- 複数AIエディタと統合

**GitHub**: https://github.com/eyaltoledano/claude-task-master

---

## 📦 プロジェクト選択ガイド

| ニーズ | 推奨プロジェクト | 理由 |
|--------|----------------|------|
| タスク管理 | Task Master | PRDからコードまで |
| 機能拡張 | SuperClaude | 21コマンド + 13エキスパート |
| マルチAgent | Claude Squad | 複数AIツール対応 |
| Skills | awesome-claude-skills | 数百の即戦力Skills |

---

## 本節のポイント

1. **SuperClaude** —— 21コマンド + 13エキスパート、能力300%向上
2. **Task Master** —— PRDからコードへのAIタスク管理
3. **Claude Squad** —— マルチAgentターミナル管理
4. **awesome-*シリーズ** —— コミュニティ厳選リソース集
            `
          }
        },
        {
          id: 'ch3-gemini-cli',
          title: { zh: '3.8 Gemini CLI 使用指南', ja: '3.8 Gemini CLI 使用ガイド' },
          content: {
            zh: `
## Gemini CLI：Google 的 AI 编程助手

Gemini CLI 是 Google 推出的命令行 AI 编程工具，与 Claude Code 类似但有独特优势。

---

## 快速开始

### 安装 Gemini CLI

\`\`\`bash
# 使用 npm 安装
npm install -g @anthropic-ai/claude-code

# 或使用 Google Cloud CLI
gcloud components install gemini-cli

# 验证安装
gemini --version
\`\`\`

### 初始化配置

\`\`\`bash
# 登录 Google 账号
gemini auth login

# 配置 API 密钥（可选）
gemini config set api_key YOUR_API_KEY

# 查看配置
gemini config list
\`\`\`

---

## 核心功能

### 1. 代码生成

\`\`\`bash
# 生成代码
gemini generate "创建一个 React 计数器组件"

# 指定语言
gemini generate --lang typescript "实现快速排序算法"

# 生成到文件
gemini generate "创建 Express 服务器" -o server.js
\`\`\`

### 2. 代码解释

\`\`\`bash
# 解释代码文件
gemini explain src/utils/helper.ts

# 解释特定函数
gemini explain --function handleClick src/App.tsx

# 详细解释
gemini explain --verbose complex-algorithm.py
\`\`\`

### 3. 代码优化

\`\`\`bash
# 优化代码性能
gemini optimize src/heavy-computation.js

# 优化特定方面
gemini optimize --focus memory src/data-processing.ts

# 安全性审查
gemini optimize --security api-handler.js
\`\`\`

---

## 项目级操作

### 项目初始化

\`\`\`bash
# 初始化项目配置
gemini init

# 生成项目结构
gemini init --template react-typescript

# 从现有项目学习
gemini learn .
\`\`\`

### 代码审查

\`\`\`bash
# 审查最近更改
gemini review

# 审查特定文件
gemini review src/components/*.tsx

# 审查 PR
gemini review --pr 123
\`\`\`

---

## 交互模式

### 启动交互式会话

\`\`\`bash
# 进入交互模式
gemini chat

# 带上下文进入
gemini chat --context src/

# 指定模型
gemini chat --model gemini-2.0-flash
\`\`\`

### 交互式命令

\`\`\`
gemini> /help          # 显示帮助
gemini> /clear         # 清空会话
gemini> /save          # 保存会话
gemini> /load session.json  # 加载会话
gemini> /model         # 切换模型
gemini> /exit          # 退出
\`\`\`

---

## Gemini vs Claude Code 对比

| 特性 | Gemini CLI | Claude Code |
|------|-----------|-------------|
| 模型 | Gemini 2.0 | Claude Opus/Sonnet |
| 搜索集成 | ✅ Google 搜索 | ❌ 需要 MCP |
| 代码库分析 | ✅ 原生支持 | ✅ 原生支持 |
| MCP 协议 | ❌ 不支持 | ✅ 完整支持 |
| 多模态 | ✅ 图片/视频 | ✅ 图片 |
| 价格 | 免费额度大 | 按量付费 |

### 适用场景

**选择 Gemini CLI**：
- 需要搜索最新信息
- 处理多模态内容（图片、视频）
- 个人项目、学习用途（免费额度大）
- Google Cloud 生态整合

**选择 Claude Code**：
- 需要 MCP 扩展能力
- 复杂的代码重构任务
- 团队协作和 Skills 共享
- 长文本生成需求

---

## 高级功能

### 1. Google 搜索集成

\`\`\`bash
# 搜索最新文档
gemini search "React 19 新特性"

# 搜索并生成代码
gemini generate --search "使用最新的 Next.js 15 创建 API 路由"
\`\`\`

### 2. 多模态支持

\`\`\`bash
# 分析图片
gemini analyze image.png "描述这个UI设计"

# 从截图生成代码
gemini generate --from-image mockup.png "生成对应的 React 组件"

# 分析视频
gemini analyze video.mp4 "总结这个教程的要点"
\`\`\`

### 3. 代码库索引

\`\`\`bash
# 索引整个项目
gemini index .

# 查询代码库
gemini query "哪里处理用户认证？"

# 查找相似代码
gemini find-similar src/utils/format.ts
\`\`\`

---

## 配置文件

\`\`\`yaml
# .gemini/config.yaml
model: gemini-2.0-pro
temperature: 0.7
max_tokens: 8192

# 项目特定设置
project:
  language: typescript
  framework: react
  test_framework: jest

# 忽略文件
ignore:
  - node_modules/
  - dist/
  - "*.log"

# 自定义提示
prompts:
  review: "请用中文审查代码，关注安全性和性能"
  explain: "请用简单的语言解释这段代码的作用"
\`\`\`

---

## 实用技巧

### 1. 与 Git 集成

\`\`\`bash
# 生成 commit message
git diff | gemini generate "根据这个 diff 生成 commit message"

# 审查 staged 更改
git diff --staged | gemini review

# 生成 changelog
gemini generate "根据最近10个commit生成changelog" --context "$(git log -10)"
\`\`\`

### 2. 管道操作

\`\`\`bash
# 处理日志
tail -100 error.log | gemini analyze "分析错误模式"

# 处理 API 响应
curl api.example.com/data | gemini format --json

# 代码转换
cat old-code.js | gemini convert --to typescript > new-code.ts
\`\`\`

### 3. 批量操作

\`\`\`bash
# 批量添加注释
gemini batch comment src/**/*.ts

# 批量格式化
gemini batch format src/ --style google

# 批量测试生成
gemini batch test src/utils/*.ts -o tests/
\`\`\`

---

## 本节要点

1. **安装配置** —— npm 或 gcloud 安装，gemini auth 登录
2. **核心命令** —— generate、explain、optimize、review
3. **交互模式** —— gemini chat 进入对话式编程
4. **Google 优势** —— 搜索集成、多模态支持、免费额度
5. **项目集成** —— .gemini/config.yaml 配置项目偏好
6. **管道操作** —— 与其他命令行工具无缝配合
            `,
            ja: `
## Gemini CLI：GoogleのAIプログラミングアシスタント

Gemini CLIはGoogleが提供するコマンドラインAIプログラミングツールで、Claude Codeに似ていますが独自の利点があります。

---

## クイックスタート

### Gemini CLIのインストール

\`\`\`bash
# npmでインストール
npm install -g @google/gemini-cli

# またはGoogle Cloud CLIで
gcloud components install gemini-cli

# インストール確認
gemini --version
\`\`\`

### 初期設定

\`\`\`bash
# Googleアカウントでログイン
gemini auth login

# APIキーを設定（オプション）
gemini config set api_key YOUR_API_KEY

# 設定を確認
gemini config list
\`\`\`

---

## コア機能

### 1. コード生成

\`\`\`bash
# コードを生成
gemini generate "Reactカウンターコンポーネントを作成"

# 言語を指定
gemini generate --lang typescript "クイックソートを実装"

# ファイルに出力
gemini generate "Expressサーバーを作成" -o server.js
\`\`\`

### 2. コード説明

\`\`\`bash
# コードファイルを説明
gemini explain src/utils/helper.ts

# 特定の関数を説明
gemini explain --function handleClick src/App.tsx
\`\`\`

### 3. コード最適化

\`\`\`bash
# パフォーマンス最適化
gemini optimize src/heavy-computation.js

# セキュリティレビュー
gemini optimize --security api-handler.js
\`\`\`

---

## Gemini vs Claude Code 比較

| 特性 | Gemini CLI | Claude Code |
|------|-----------|-------------|
| モデル | Gemini 2.0 | Claude Opus/Sonnet |
| 検索統合 | ✅ Google検索 | ❌ MCP必要 |
| MCP対応 | ❌ 非対応 | ✅ 完全対応 |
| マルチモーダル | ✅ 画像/動画 | ✅ 画像 |
| 価格 | 無料枠大 | 従量課金 |

### 使い分け

**Gemini CLIを選ぶ場合**：
- 最新情報の検索が必要
- マルチモーダルコンテンツ処理
- 個人プロジェクト（無料枠大）
- Google Cloudエコシステム統合

**Claude Codeを選ぶ場合**：
- MCP拡張機能が必要
- 複雑なリファクタリング
- チーム協力とSkills共有
- 長文生成ニーズ

---

## 高度な機能

### 1. Google検索統合

\`\`\`bash
# 最新ドキュメントを検索
gemini search "React 19 新機能"

# 検索してコード生成
gemini generate --search "最新のNext.js 15でAPIルートを作成"
\`\`\`

### 2. マルチモーダル対応

\`\`\`bash
# 画像を分析
gemini analyze image.png "このUIデザインを説明"

# スクリーンショットからコード生成
gemini generate --from-image mockup.png "対応するReactコンポーネントを生成"
\`\`\`

---

## 本節のポイント

1. **インストール設定** —— npmまたはgcloudでインストール
2. **コアコマンド** —— generate、explain、optimize、review
3. **対話モード** —— gemini chatで対話式プログラミング
4. **Google優位性** —— 検索統合、マルチモーダル、無料枠
5. **プロジェクト統合** —— .gemini/config.yamlで設定
6. **パイプ操作** —— 他のCLIツールとシームレス連携
            `
          }
        },
        {
          id: 'ch3-openai-codex',
          title: { zh: '3.9 OpenAI Codex 使用指南', ja: '3.9 OpenAI Codex 使用ガイド' },
          content: {
            zh: `
## OpenAI Codex：GPT 驱动的 AI 编程助手

OpenAI Codex（又称 GPT Codex Agent）是 OpenAI 推出的 AI 编程工具，基于 GPT-5 模型，与 Claude Code 形成直接竞争。

---

## 快速开始

### 安装 Codex CLI

\`\`\`bash
# 使用 npm 安装
npm install -g @openai/codex

# 或使用 pip
pip install openai-codex

# 验证安装
codex --version
\`\`\`

### 配置 API 密钥

\`\`\`bash
# 设置环境变量
export OPENAI_API_KEY="sk-..."

# 或使用配置命令
codex config set api_key sk-...

# 登录 OpenAI 账号（推荐）
codex auth login
\`\`\`

---

## 核心功能

### 1. 自然语言编程

\`\`\`bash
# 基本代码生成
codex "创建一个 REST API 端点处理用户注册"

# 在项目上下文中生成
codex --context . "添加用户邮箱验证功能"

# 指定语言和框架
codex --lang python --framework fastapi "创建文件上传接口"
\`\`\`

### 2. Agent 模式

Codex 的 Agent 模式可以自主完成复杂任务：

\`\`\`bash
# 启动 Agent 模式
codex agent "重构这个项目的认证系统，使用 JWT"

# Agent 会自动：
# 1. 分析现有代码
# 2. 制定重构计划
# 3. 逐步实现更改
# 4. 运行测试验证
\`\`\`

### 3. 代码补全

\`\`\`bash
# 实时补全（编辑器集成）
codex complete --stream

# 批量补全
codex complete src/incomplete-file.ts

# 多候选补全
codex complete --n 3 "function calculateTotal("
\`\`\`

---

## Skills 支持

OpenAI Codex 已采用与 Claude Code 相同的 Skills 规范！

### Skills 文件结构

\`\`\`markdown
# .codex/skills/deploy.md（与 Claude 兼容）

---
name: deploy
description: 部署到生产环境
triggers:
  - /deploy
  - 部署应用
---

# 部署流程

## 步骤

1. 运行测试 \`npm test\`
2. 构建项目 \`npm run build\`
3. 推送到服务器
\`\`\`

### 跨平台 Skills

\`\`\`bash
# Skills 目录结构（通用）
.claude/skills/  # Claude Code 使用
.codex/skills/   # OpenAI Codex 使用

# 推荐：使用符号链接共享
ln -s .claude/skills .codex/skills
\`\`\`

---

## 多模态能力

### 图片理解

\`\`\`bash
# 从 UI 截图生成代码
codex vision "根据这个截图实现登录页面" --image login-mockup.png

# 分析图表
codex vision "解释这个架构图" --image architecture.png

# 调试 UI 问题
codex vision "这个页面布局有什么问题？" --image buggy-ui.png
\`\`\`

### 代码 + 图片

\`\`\`bash
# 结合代码和设计稿
codex "实现这个设计，样式要和截图一致" \
  --image design.png \
  --context src/components/
\`\`\`

---

## Codex vs Claude Code 对比

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    AI 编程工具对比表                                     │
└─────────────────────────────────────────────────────────────────────────┘

  特性               Claude Code          OpenAI Codex
  ────               ───────────          ────────────
  基础模型           Claude Opus 4.5      GPT-5.2
  Skills 支持        ✅ 原生              ✅ 兼容
  MCP 协议           ✅ 完整              ❌ 不支持
  浏览器自动化       ✅ Chrome 集成       ✅ Playwright
  多模态             图片                 图片 + 视频
  长文本生成         12000+ 字符          ~7000 字符
  数学/编程          ⭐⭐⭐⭐             ⭐⭐⭐⭐⭐
  价格               $$                   $$$
\`\`\`

### 实测对比

| 任务类型 | 推荐工具 | 原因 |
|---------|---------|------|
| 复杂重构 | Claude Code | 长文本能力强 |
| 算法实现 | Codex | 数学推理更好 |
| UI 开发 | 两者相当 | 都支持图片输入 |
| 自动化测试 | Codex | Playwright 集成好 |
| MCP 扩展 | Claude Code | 独家支持 |

---

## 高级功能

### 1. 并行任务

\`\`\`bash
# 并行执行多个任务
codex parallel \
  "优化 src/utils/*.ts 的性能" \
  "为 src/api/*.ts 添加测试" \
  "更新 README.md 文档"

# 查看并行任务状态
codex status --parallel
\`\`\`

### 2. 代码审查

\`\`\`bash
# 审查 PR
codex review --pr 123

# 审查并自动修复
codex review --fix src/

# 安全审查
codex review --security --strict
\`\`\`

### 3. 测试生成

\`\`\`bash
# 为文件生成测试
codex test generate src/utils/calculator.ts

# 为整个模块生成
codex test generate src/services/ --coverage 80

# 运行并修复失败测试
codex test fix
\`\`\`

---

## 配置文件

\`\`\`json
// .codex/config.json
{
  "model": "gpt-5.2-codex",
  "temperature": 0.3,
  "max_tokens": 8192,

  "project": {
    "language": "typescript",
    "framework": "nextjs",
    "test_framework": "jest"
  },

  "skills_dir": ".codex/skills",

  "ignore": [
    "node_modules",
    "dist",
    ".git"
  ],

  "hooks": {
    "pre_commit": "npm run lint",
    "post_generate": "npx prettier --write"
  }
}
\`\`\`

---

## 工作流示例

### 示例 1：功能开发全流程

\`\`\`bash
# 1. 进入项目目录
cd my-project

# 2. 启动 Agent 模式
codex agent

# 3. 描述需求
> 我需要添加用户头像上传功能，支持裁剪和压缩

# Codex 会自动：
# - 分析项目结构
# - 创建必要的组件和 API
# - 添加图片处理逻辑
# - 编写测试
# - 更新文档
\`\`\`

### 示例 2：代码迁移

\`\`\`bash
# JavaScript 到 TypeScript 迁移
codex migrate --from js --to ts src/

# React Class 到 Hooks 迁移
codex migrate --pattern "class-to-hooks" src/components/

# API 版本升级
codex migrate --api-version v1-to-v2 src/services/
\`\`\`

### 示例 3：性能优化

\`\`\`bash
# 分析性能问题
codex analyze performance src/

# 自动优化
codex optimize --auto \
  --focus "bundle-size,runtime" \
  src/

# 生成优化报告
codex report performance --output report.md
\`\`\`

---

## 与 Claude Code 协同使用

\`\`\`bash
# 最佳实践：结合两者优势

# 1. 用 Claude Code 做架构设计（长文本+MCP）
claude "设计用户系统的整体架构" --plan

# 2. 用 Codex 实现算法密集型代码
codex "实现推荐算法，使用协同过滤"

# 3. 用 Claude Code 做代码审查（更细致）
claude "/review"

# 4. 用 Codex 生成测试（测试覆盖更全）
codex test generate src/
\`\`\`

---

## 常见问题

### Q: Codex 和 ChatGPT 有什么区别？
A: Codex 专门针对编程任务优化，有更好的代码理解和生成能力，支持 Agent 模式自主完成复杂任务。

### Q: Skills 是否真的跨平台通用？
A: 是的，2025年5月后 OpenAI 采用了 Anthropic 的 Skills 规范，基本语法完全兼容。

### Q: 该选择 Codex 还是 Claude Code？
A: 建议都尝试。数学/算法任务用 Codex，长文本/MCP 任务用 Claude Code。

---

## 本节要点

1. **安装配置** —— npm 或 pip 安装，设置 OPENAI_API_KEY
2. **Agent 模式** —— 自主完成复杂编程任务
3. **Skills 兼容** —— 与 Claude Code 共享 Skills 定义
4. **多模态能力** —— 从截图生成代码
5. **性能对比** —— 数学推理强，长文本弱
6. **协同使用** —— 结合 Claude + Codex 发挥各自优势
            `,
            ja: `
## OpenAI Codex：GPT駆動のAIプログラミングアシスタント

OpenAI Codex（GPT Codex Agentとも呼ばれる）はOpenAIが提供するAIプログラミングツールで、GPT-5モデルをベースにしており、Claude Codeと直接競合しています。

---

## クイックスタート

### Codex CLIのインストール

\`\`\`bash
# npmでインストール
npm install -g @openai/codex

# またはpipで
pip install openai-codex

# インストール確認
codex --version
\`\`\`

### APIキーの設定

\`\`\`bash
# 環境変数を設定
export OPENAI_API_KEY="sk-..."

# または設定コマンドで
codex config set api_key sk-...

# OpenAIアカウントでログイン（推奨）
codex auth login
\`\`\`

---

## コア機能

### 1. 自然言語プログラミング

\`\`\`bash
# 基本的なコード生成
codex "ユーザー登録を処理するREST APIエンドポイントを作成"

# プロジェクトコンテキストで生成
codex --context . "メール検証機能を追加"

# 言語とフレームワークを指定
codex --lang python --framework fastapi "ファイルアップロードAPIを作成"
\`\`\`

### 2. Agentモード

CodexのAgentモードは複雑なタスクを自律的に完了できます：

\`\`\`bash
# Agentモードを起動
codex agent "このプロジェクトの認証システムをJWTを使用してリファクタリング"

# Agentは自動的に：
# 1. 既存コードを分析
# 2. リファクタリング計画を策定
# 3. 段階的に変更を実装
# 4. テストを実行して検証
\`\`\`

---

## Skills対応

OpenAI CodexはClaude Codeと同じSkills仕様を採用しています！

### Skillsファイル構造

\`\`\`markdown
# .codex/skills/deploy.md（Claudeと互換）

---
name: deploy
description: 本番環境にデプロイ
triggers:
  - /deploy
  - アプリをデプロイ
---

# デプロイプロセス

## ステップ

1. テストを実行 \`npm test\`
2. プロジェクトをビルド \`npm run build\`
3. サーバーにプッシュ
\`\`\`

---

## Codex vs Claude Code 比較

| 特性 | Claude Code | OpenAI Codex |
|-----|------------|--------------|
| ベースモデル | Claude Opus 4.5 | GPT-5.2 |
| Skills対応 | ✅ ネイティブ | ✅ 互換 |
| MCP対応 | ✅ 完全 | ❌ 非対応 |
| マルチモーダル | 画像 | 画像 + 動画 |
| 長文生成 | 12000+文字 | ~7000文字 |
| 数学/プログラミング | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ |

### 使い分け

| タスクタイプ | 推奨ツール | 理由 |
|------------|----------|------|
| 複雑なリファクタリング | Claude Code | 長文能力が強い |
| アルゴリズム実装 | Codex | 数学推論が優れている |
| UI開発 | 両方とも | 画像入力対応 |
| MCP拡張 | Claude Code | 独占対応 |

---

## 高度な機能

### 1. 並列タスク

\`\`\`bash
# 複数タスクを並列実行
codex parallel \
  "src/utils/*.tsのパフォーマンスを最適化" \
  "src/api/*.tsにテストを追加" \
  "README.mdドキュメントを更新"
\`\`\`

### 2. テスト生成

\`\`\`bash
# ファイルのテストを生成
codex test generate src/utils/calculator.ts

# モジュール全体のテストを生成
codex test generate src/services/ --coverage 80
\`\`\`

---

## Claude Codeとの協同使用

\`\`\`bash
# ベストプラクティス：両者の利点を組み合わせる

# 1. Claude Codeでアーキテクチャ設計（長文+MCP）
claude "ユーザーシステムの全体アーキテクチャを設計" --plan

# 2. Codexでアルゴリズム集約型コードを実装
codex "協調フィルタリングを使用した推薦アルゴリズムを実装"

# 3. Claude Codeでコードレビュー（より詳細）
claude "/review"

# 4. Codexでテスト生成（カバレッジが広い）
codex test generate src/
\`\`\`

---

## 本節のポイント

1. **インストール設定** —— npmまたはpipでインストール、OPENAI_API_KEYを設定
2. **Agentモード** —— 複雑なプログラミングタスクを自律完了
3. **Skills互換** —— Claude CodeとSkills定義を共有
4. **マルチモーダル能力** —— スクリーンショットからコード生成
5. **性能比較** —— 数学推論強い、長文弱い
6. **協同使用** —— Claude + Codexの組み合わせで各自の強みを発揮
            `
          }
        },
        {
          id: 'ch3-computer-use',
          title: { zh: '3.10 Computer Use：AI 的机器臂', ja: '3.10 Computer Use：AIの機械アーム' },
          content: {
            zh: `
## AI 如何操控物理世界？

传统 AI 只能"说话"，而 **Computer Use** 让 AI 拥有了"手臂"——可以像人一样操作电脑、控制浏览器、甚至操控机器人。

---

## 什么是 Computer Use？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                      Computer Use 概念                                   │
└─────────────────────────────────────────────────────────────────────────┘

  传统 AI                               Computer Use AI
  ┌───────────────┐                    ┌───────────────┐
  │  只能对话     │                    │  看屏幕       │
  │  输出文字     │                    │  移动鼠标     │
  │  ─────────── │                    │  点击按钮     │
  │  "我建议..."  │                    │  输入文字     │
  │  "你可以..."  │                    │  执行操作     │
  └───────────────┘                    └───────────────┘
        │                                     │
        ▼                                     ▼
   需要人工执行                           AI 自动完成
\`\`\`

**核心能力**：
- **视觉理解**：看懂屏幕上的内容
- **坐标定位**：精确定位按钮和输入框
- **操作执行**：模拟鼠标和键盘操作
- **反馈循环**：根据结果调整行动

---

## Claude Computer Use

Anthropic 的 Computer Use 让 Claude 可以直接操作你的电脑桌面。

### 工作原理

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    Claude Computer Use 流程                              │
└─────────────────────────────────────────────────────────────────────────┘

                    ┌──────────────┐
                    │   用户指令   │
                    │ "帮我订机票" │
                    └──────┬───────┘
                           │
                           ▼
                    ┌──────────────┐
                    │  Claude 规划 │
                    │  分解任务    │
                    └──────┬───────┘
                           │
         ┌─────────────────┼─────────────────┐
         │                 │                 │
         ▼                 ▼                 ▼
    ┌─────────┐      ┌─────────┐      ┌─────────┐
    │ 截图屏幕 │      │ 分析内容 │      │ 执行操作 │
    └─────────┘      └─────────┘      └─────────┘
                           │
                           ▼
                    ┌──────────────┐
                    │  观察结果    │
                    │  调整策略    │
                    └──────────────┘
                           │
                           ▼
                    ┌──────────────┐
                    │   任务完成   │
                    └──────────────┘
\`\`\`

### 使用方式

\`\`\`bash
# 使用 Docker 运行 Computer Use
docker run -p 8080:8080 anthropic/computer-use-demo

# 或使用 API
pip install anthropic
\`\`\`

### API 调用示例

\`\`\`python
import anthropic

client = anthropic.Anthropic()

# Computer Use 需要特殊的 tool 配置
response = client.messages.create(
    model="claude-sonnet-4-20250514",
    max_tokens=1024,
    tools=[
        {
            "type": "computer_20241022",
            "name": "computer",
            "display_width_px": 1920,
            "display_height_px": 1080
        }
    ],
    messages=[
        {
            "role": "user",
            "content": "打开浏览器，搜索今天的天气"
        }
    ]
)

# Claude 会返回要执行的操作
for block in response.content:
    if block.type == "tool_use":
        print(f"操作: {block.input}")
        # 执行操作并返回截图
\`\`\`

---

## Chrome 浏览器控制

通过 MCP 插件，Claude Code 可以直接控制 Chrome 浏览器。

### 安装 Chrome MCP

\`\`\`json
// .claude/settings.json
{
  "mcpServers": {
    "chrome": {
      "command": "npx",
      "args": ["-y", "@anthropic/mcp-chrome"]
    }
  }
}
\`\`\`

### 使用场景

| 场景 | 描述 | 命令示例 |
|------|------|----------|
| 网页测试 | 自动化 UI 测试 | "测试登录功能是否正常" |
| 数据抓取 | 收集网页信息 | "获取这个页面的产品列表" |
| 表单填写 | 自动填写表单 | "帮我填写这个注册表单" |
| 截图对比 | UI 回归测试 | "对比新旧版本的差异" |

### 实际示例

\`\`\`
用户：帮我测试登录页面，输入用户名 test@example.com

Claude：我来帮你测试登录页面。

[截图分析] 我看到了登录页面，有用户名和密码输入框。

[执行操作]
1. 点击用户名输入框 (坐标: 500, 300)
2. 输入: test@example.com
3. 点击密码输入框
4. 等待用户输入密码...

[结果] 用户名已填写，请告诉我密码或下一步操作。
\`\`\`

---

## Playwright 自动化测试

结合 Playwright MCP，实现专业的端到端测试。

### 配置

\`\`\`json
{
  "mcpServers": {
    "playwright": {
      "command": "npx",
      "args": ["-y", "@anthropic/mcp-playwright"]
    }
  }
}
\`\`\`

### 自动化测试示例

\`\`\`
用户：帮我写一个自动化测试，验证购物车功能

Claude：我来创建购物车功能的端到端测试。

[生成测试代码]
\`\`\`typescript
import { test, expect } from '@playwright/test';

test('购物车功能测试', async ({ page }) => {
  // 1. 访问商品页
  await page.goto('/products');

  // 2. 添加商品到购物车
  await page.click('[data-testid="add-to-cart"]');

  // 3. 验证购物车数量
  const cartCount = page.locator('.cart-count');
  await expect(cartCount).toHaveText('1');

  // 4. 打开购物车
  await page.click('.cart-icon');

  // 5. 验证商品存在
  await expect(page.locator('.cart-item')).toBeVisible();
});
\`\`\`

[执行测试] 运行 npx playwright test

[结果] ✅ 测试通过！购物车功能正常工作。
\`\`\`

---

## 机器人控制（未来展望）

Computer Use 的技术可以扩展到物理机器人控制。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                      AI + 机器人 架构                                    │
└─────────────────────────────────────────────────────────────────────────┘

          用户指令
              │
              ▼
      ┌───────────────┐
      │   Claude AI   │
      │   理解 + 规划  │
      └───────┬───────┘
              │
    ┌─────────┴─────────┐
    ▼                   ▼
┌─────────┐       ┌─────────┐
│ 虚拟世界 │       │ 物理世界 │
│         │       │         │
│ 电脑操作 │       │ 机器臂   │
│ 浏览器   │       │ 无人机   │
│ 代码编写 │       │ 机器人   │
└─────────┘       └─────────┘
\`\`\`

### 已有项目

| 项目 | 描述 | 链接 |
|------|------|------|
| Figure + OpenAI | 人形机器人 + GPT | figure.ai |
| Google RT-2 | 机器人 Transformer | robotics-transformer |
| 1X Technologies | 通用人形机器人 | 1x.tech |
| Boston Dynamics | 机器狗 + AI | bostondynamics.com |

---

## 安全注意事项

Computer Use 功能强大，但需要谨慎使用：

### 风险等级

| 操作类型 | 风险等级 | 建议 |
|----------|----------|------|
| 只读浏览 | 🟢 低 | 可以自由使用 |
| 表单填写 | 🟡 中 | 验证后再提交 |
| 文件操作 | 🟠 较高 | 先备份 |
| 系统设置 | 🔴 高 | 手动确认 |
| 付款操作 | 🔴 高 | 绝对禁止 |

### 最佳实践

1. **沙箱环境**：使用 Docker 或虚拟机隔离
2. **权限最小化**：只授予必要权限
3. **人工确认**：关键操作前暂停确认
4. **日志记录**：记录所有操作便于审计
5. **紧急停止**：设置快速中断机制

---

## 本节要点

1. **Computer Use** —— 让 AI 像人一样操作电脑
2. **Claude 桌面控制** —— 截图 + 分析 + 操作循环
3. **Chrome MCP** —— 浏览器自动化测试
4. **Playwright** —— 专业端到端测试
5. **机器人扩展** —— 从虚拟到物理世界
6. **安全第一** —— 沙箱隔离、权限最小化
            `,
            ja: `
## AI はどのように物理世界を操作するのか？

従来のAIは「話す」ことしかできませんでしたが、**Computer Use**によりAIは「腕」を持つようになりました——人間のようにコンピューターを操作し、ブラウザを制御し、さらにはロボットを操作できるようになりました。

---

## Computer Use とは？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                      Computer Use コンセプト                             │
└─────────────────────────────────────────────────────────────────────────┘

  従来のAI                               Computer Use AI
  ┌───────────────┐                    ┌───────────────┐
  │  対話のみ     │                    │  画面を見る   │
  │  テキスト出力 │                    │  マウス移動   │
  │  ─────────── │                    │  ボタンクリック│
  │  「提案します」│                    │  テキスト入力 │
  │  「できます」 │                    │  操作実行     │
  └───────────────┘                    └───────────────┘
        │                                     │
        ▼                                     ▼
   人間が実行する必要                      AI が自動完了
\`\`\`

**コア能力**：
- **視覚理解**：画面上の内容を理解
- **座標定位**：ボタンや入力フィールドを正確に特定
- **操作実行**：マウスとキーボード操作をシミュレート
- **フィードバックループ**：結果に基づいて行動を調整

---

## Claude Computer Use

AnthropicのComputer Useにより、Claudeはデスクトップを直接操作できます。

### 動作原理

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    Claude Computer Use フロー                            │
└─────────────────────────────────────────────────────────────────────────┘

                    ┌──────────────┐
                    │ ユーザー指示 │
                    │「航空券を予約」│
                    └──────┬───────┘
                           │
                           ▼
                    ┌──────────────┐
                    │ Claude 計画  │
                    │ タスク分解   │
                    └──────┬───────┘
                           │
         ┌─────────────────┼─────────────────┐
         │                 │                 │
         ▼                 ▼                 ▼
    ┌─────────┐      ┌─────────┐      ┌─────────┐
    │スクリーンショット│  │ 内容分析 │      │ 操作実行 │
    └─────────┘      └─────────┘      └─────────┘
                           │
                           ▼
                    ┌──────────────┐
                    │  結果観察    │
                    │  戦略調整    │
                    └──────────────┘
                           │
                           ▼
                    ┌──────────────┐
                    │  タスク完了  │
                    └──────────────┘
\`\`\`

### 使用方法

\`\`\`bash
# Docker で Computer Use を実行
docker run -p 8080:8080 anthropic/computer-use-demo

# または API を使用
pip install anthropic
\`\`\`

### API 呼び出し例

\`\`\`python
import anthropic

client = anthropic.Anthropic()

# Computer Use は特別な tool 設定が必要
response = client.messages.create(
    model="claude-sonnet-4-20250514",
    max_tokens=1024,
    tools=[
        {
            "type": "computer_20241022",
            "name": "computer",
            "display_width_px": 1920,
            "display_height_px": 1080
        }
    ],
    messages=[
        {
            "role": "user",
            "content": "ブラウザを開いて今日の天気を検索"
        }
    ]
)

# Claude は実行する操作を返す
for block in response.content:
    if block.type == "tool_use":
        print(f"操作: {block.input}")
        # 操作を実行してスクリーンショットを返す
\`\`\`

---

## Chrome ブラウザ制御

MCP プラグインを通じて、Claude Code は Chrome ブラウザを直接制御できます。

### Chrome MCP のインストール

\`\`\`json
// .claude/settings.json
{
  "mcpServers": {
    "chrome": {
      "command": "npx",
      "args": ["-y", "@anthropic/mcp-chrome"]
    }
  }
}
\`\`\`

### 使用シーン

| シーン | 説明 | コマンド例 |
|--------|------|------------|
| Webテスト | 自動化UIテスト | 「ログイン機能が正常か確認」 |
| データ収集 | Webページ情報収集 | 「このページの製品リストを取得」 |
| フォーム入力 | 自動フォーム入力 | 「この登録フォームを入力」 |
| スクリーンショット比較 | UI回帰テスト | 「新旧バージョンの違いを比較」 |

---

## Playwright 自動化テスト

Playwright MCP と組み合わせて、プロフェッショナルなE2Eテストを実現。

### 設定

\`\`\`json
{
  "mcpServers": {
    "playwright": {
      "command": "npx",
      "args": ["-y", "@anthropic/mcp-playwright"]
    }
  }
}
\`\`\`

---

## ロボット制御（将来展望）

Computer Use 技術は物理ロボット制御に拡張できます。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                      AI + ロボット アーキテクチャ                        │
└─────────────────────────────────────────────────────────────────────────┘

          ユーザー指示
              │
              ▼
      ┌───────────────┐
      │   Claude AI   │
      │  理解 + 計画   │
      └───────┬───────┘
              │
    ┌─────────┴─────────┐
    ▼                   ▼
┌─────────┐       ┌─────────┐
│ 仮想世界 │       │ 物理世界 │
│         │       │         │
│ PC操作  │       │ ロボットアーム │
│ ブラウザ │       │ ドローン  │
│ コード  │       │ ロボット  │
└─────────┘       └─────────┘
\`\`\`

### 既存プロジェクト

| プロジェクト | 説明 | リンク |
|--------------|------|--------|
| Figure + OpenAI | ヒューマノイドロボット + GPT | figure.ai |
| Google RT-2 | ロボット Transformer | robotics-transformer |
| 1X Technologies | 汎用ヒューマノイド | 1x.tech |
| Boston Dynamics | ロボット犬 + AI | bostondynamics.com |

---

## セキュリティ注意事項

Computer Use は強力ですが、慎重に使用する必要があります：

### リスクレベル

| 操作タイプ | リスクレベル | 推奨事項 |
|------------|--------------|----------|
| 読み取り専用 | 🟢 低 | 自由に使用可能 |
| フォーム入力 | 🟡 中 | 確認後に送信 |
| ファイル操作 | 🟠 やや高 | 先にバックアップ |
| システム設定 | 🔴 高 | 手動確認 |
| 支払い操作 | 🔴 高 | 絶対禁止 |

### ベストプラクティス

1. **サンドボックス環境**：Docker または VM で隔離
2. **最小権限**：必要な権限のみ付与
3. **人間の確認**：重要な操作前に一時停止
4. **ログ記録**：すべての操作を記録
5. **緊急停止**：迅速な中断メカニズムを設定

---

## このセクションのポイント

1. **Computer Use** —— AI が人間のようにコンピューターを操作
2. **Claude デスクトップ制御** —— スクリーンショット + 分析 + 操作ループ
3. **Chrome MCP** —— ブラウザ自動化テスト
4. **Playwright** —— プロフェッショナルE2Eテスト
5. **ロボット拡張** —— 仮想から物理世界へ
6. **セキュリティ優先** —— サンドボックス隔離、最小権限
            `
          }
        },
        {
          id: 'ch3-a2a',
          title: { zh: '3.11 A2A 协议：Agent 互联', ja: '3.11 A2Aプロトコル：Agent連携' },
          content: {
            zh: `
## Agent-to-Agent：AI 代理互联互通

当单个 Agent 不够用时，让多个 Agent 协作是未来趋势。

---

## 🎯 什么是 A2A 协议？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        A2A 协议概念                                       │
└─────────────────────────────────────────────────────────────────────────┘

  传统模式：单一 Agent                 A2A 模式：多 Agent 协作
  ┌────────────────────┐              ┌────────────────────┐
  │     用户请求        │              │     用户请求        │
  │         │          │              │         │          │
  │         ▼          │              │         ▼          │
  │   ┌──────────┐     │              │   ┌──────────┐     │
  │   │ Agent A  │     │              │   │ 协调 Agent │     │
  │   │ (全能型) │     │              │   └─────┬────┘     │
  │   └──────────┘     │              │    ╱    │    ╲     │
  │         │          │              │   ▼     ▼     ▼    │
  │         ▼          │              │ 代码   数据   搜索  │
  │      结果          │              │ Agent Agent Agent  │
  │                    │              │   └────┬────┘      │
  │  ❌ 能力有限       │              │        ▼           │
  │  ❌ 难以扩展       │              │     汇总结果        │
  └────────────────────┘              │                    │
                                      │  ✅ 专业分工        │
                                      │  ✅ 能力无限扩展    │
                                      └────────────────────┘
\`\`\`

---

## 🌐 Google A2A 协议

Google 发布的开放标准，定义 Agent 间通信规范。

### 核心概念

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                      A2A 协议架构                                         │
└─────────────────────────────────────────────────────────────────────────┘

         ┌─────────────────────────────────────────────┐
         │              Agent Card (名片)               │
         │  - 名称、描述、能力                          │
         │  - 支持的输入/输出格式                       │
         │  - 认证方式                                  │
         └─────────────────────────────────────────────┘
                              │
                              ▼
  ┌─────────────┐    JSON-RPC 2.0    ┌─────────────┐
  │   Client    │◄─────────────────►│   Server    │
  │   Agent     │    over HTTP(S)    │   Agent     │
  └─────────────┘                    └─────────────┘
        │                                   │
        └───────────┬───────────────────────┘
                    │
                    ▼
            ┌───────────────┐
            │    Task       │
            │  - 任务ID     │
            │  - 状态       │
            │  - 输入/输出  │
            │  - 历史记录   │
            └───────────────┘
\`\`\`

### Agent Card 示例

\`\`\`json
{
  "name": "代码审查 Agent",
  "description": "专业的代码审查和安全检测",
  "url": "https://code-review.example.com/a2a",
  "version": "1.0.0",
  "capabilities": {
    "streaming": true,
    "pushNotifications": false
  },
  "inputModes": ["text/plain", "application/json"],
  "outputModes": ["text/plain", "text/markdown"],
  "skills": [
    {
      "id": "code-review",
      "name": "代码审查",
      "description": "检查代码质量和潜在问题",
      "inputSchema": {
        "type": "object",
        "properties": {
          "code": { "type": "string" },
          "language": { "type": "string" }
        }
      }
    },
    {
      "id": "security-scan",
      "name": "安全扫描",
      "description": "检测安全漏洞"
    }
  ]
}
\`\`\`

---

## 🔧 A2A 通信流程

\`\`\`python
import httpx
import json

class A2AClient:
    def __init__(self, agent_url: str):
        self.agent_url = agent_url
        self.client = httpx.Client()

    def discover(self):
        """发现 Agent 能力"""
        response = self.client.get(f"{self.agent_url}/.well-known/agent.json")
        return response.json()

    def send_task(self, skill_id: str, input_data: dict):
        """发送任务"""
        payload = {
            "jsonrpc": "2.0",
            "id": "task-001",
            "method": "tasks/send",
            "params": {
                "message": {
                    "role": "user",
                    "parts": [{"text": json.dumps(input_data)}]
                },
                "skillId": skill_id
            }
        }
        response = self.client.post(
            f"{self.agent_url}/a2a",
            json=payload
        )
        return response.json()

    def get_task_status(self, task_id: str):
        """查询任务状态"""
        payload = {
            "jsonrpc": "2.0",
            "id": "status-001",
            "method": "tasks/get",
            "params": {"taskId": task_id}
        }
        response = self.client.post(
            f"{self.agent_url}/a2a",
            json=payload
        )
        return response.json()

# 使用示例
client = A2AClient("https://code-review.example.com")

# 1. 发现能力
agent_card = client.discover()
print(f"Agent: {agent_card['name']}")
print(f"Skills: {[s['name'] for s in agent_card['skills']]}")

# 2. 发送任务
result = client.send_task("code-review", {
    "code": "def hello(): print('world')",
    "language": "python"
})
print(f"Task ID: {result['result']['taskId']}")
\`\`\`

---

## 🔗 A2A vs MCP 对比

| 特性 | A2A | MCP |
|------|-----|-----|
| **定位** | Agent 间通信 | Agent 与工具通信 |
| **协议** | JSON-RPC over HTTP | JSON-RPC over stdio/SSE |
| **发现机制** | Agent Card | 工具描述 |
| **适用场景** | 分布式 Agent 网络 | 单机工具集成 |
| **异步支持** | 任务队列 + 回调 | 流式响应 |
| **状态管理** | 有状态 (Task) | 无状态 |

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                   A2A + MCP 组合使用                                      │
└─────────────────────────────────────────────────────────────────────────┘

                           用户请求
                              │
                              ▼
                    ┌──────────────────┐
                    │   协调 Agent      │
                    │   (Claude Code)  │
                    └────────┬─────────┘
                             │
              ┌──────────────┼──────────────┐
              │ A2A          │ A2A          │ MCP
              ▼              ▼              ▼
       ┌──────────┐   ┌──────────┐   ┌──────────┐
       │ 代码审查  │   │ 文档生成  │   │ 本地工具  │
       │ Agent    │   │ Agent    │   │ (文件/DB) │
       └──────────┘   └──────────┘   └──────────┘
            │              │              │
            │ A2A 远程     │ A2A 远程     │ MCP 本地
            └──────────────┼──────────────┘
                           ▼
                        汇总结果
\`\`\`

---

## 🚀 实际应用场景

### 1. 企业工作流自动化

\`\`\`
用户请求："帮我准备下周一的客户演示"
     │
     ├─► 日程 Agent: 查询会议安排
     ├─► CRM Agent: 获取客户信息
     ├─► 文档 Agent: 生成 PPT
     ├─► 邮件 Agent: 发送提醒
     └─► 协调 Agent: 汇总确认
\`\`\`

### 2. 代码开发协作

\`\`\`
用户请求："实现用户登录功能"
     │
     ├─► 设计 Agent: 生成技术方案
     ├─► 编码 Agent: 编写代码
     ├─► 审查 Agent: 代码审查
     ├─► 测试 Agent: 编写测试
     └─► 部署 Agent: 准备上线
\`\`\`

---

## 💡 未来展望

\`\`\`
                    AI Agent 网络化趋势
                           │
         ┌─────────────────┼─────────────────┐
         │                 │                 │
         ▼                 ▼                 ▼
    Agent 市场         Agent 评分        Agent 经济
   (发现&订阅)       (信誉系统)        (自动付费)
         │                 │                 │
         └─────────────────┼─────────────────┘
                           ▼
                   去中心化 Agent 网络
                   (类似 App Store)
\`\`\`

> 🔮 **预测**：未来 5 年内，A2A 协议将成为 AI Agent 互联的标准，
> 就像 HTTP 成为 Web 的标准一样。
            `,
            ja: `
## Agent-to-Agent：AIエージェント相互連携

単一のAgentでは不十分な場合、複数のAgent協調が未来のトレンドです。

---

## 🎯 A2Aプロトコルとは？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        A2A プロトコル概念                                  │
└─────────────────────────────────────────────────────────────────────────┘

  従来モード：単一Agent                A2Aモード：複数Agent協調
  ┌────────────────────┐              ┌────────────────────┐
  │     ユーザー要求    │              │     ユーザー要求    │
  │         │          │              │         │          │
  │         ▼          │              │         ▼          │
  │   ┌──────────┐     │              │   ┌──────────┐     │
  │   │ Agent A  │     │              │   │ 調整Agent │     │
  │   │(万能型) │     │              │   └─────┬────┘     │
  │   └──────────┘     │              │    ╱    │    ╲     │
  │         │          │              │   ▼     ▼     ▼    │
  │         ▼          │              │ コード データ 検索  │
  │      結果          │              │ Agent Agent Agent  │
  │                    │              │   └────┬────┘      │
  │  ❌ 能力限定       │              │        ▼           │
  │  ❌ 拡張困難       │              │     結果統合        │
  └────────────────────┘              │                    │
                                      │  ✅ 専門分業        │
                                      │  ✅ 無限拡張可能    │
                                      └────────────────────┘
\`\`\`

---

## 🌐 Google A2A プロトコル

Googleが発表したオープン標準、Agent間通信規範を定義。

### コア概念

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                      A2A プロトコルアーキテクチャ                          │
└─────────────────────────────────────────────────────────────────────────┘

         ┌─────────────────────────────────────────────┐
         │              Agent Card (名刺)               │
         │  - 名前、説明、機能                          │
         │  - サポートする入出力形式                    │
         │  - 認証方式                                  │
         └─────────────────────────────────────────────┘
                              │
                              ▼
  ┌─────────────┐    JSON-RPC 2.0    ┌─────────────┐
  │   Client    │◄─────────────────►│   Server    │
  │   Agent     │    over HTTP(S)    │   Agent     │
  └─────────────┘                    └─────────────┘
\`\`\`

### Agent Card 例

\`\`\`json
{
  "name": "コードレビューAgent",
  "description": "専門的なコードレビューとセキュリティ検出",
  "url": "https://code-review.example.com/a2a",
  "version": "1.0.0",
  "skills": [
    {
      "id": "code-review",
      "name": "コードレビュー",
      "description": "コード品質と潜在的問題をチェック"
    }
  ]
}
\`\`\`

---

## 🔗 A2A vs MCP 比較

| 特性 | A2A | MCP |
|------|-----|-----|
| **位置づけ** | Agent間通信 | Agentとツール通信 |
| **プロトコル** | JSON-RPC over HTTP | JSON-RPC over stdio/SSE |
| **発見機構** | Agent Card | ツール説明 |
| **適用シーン** | 分散Agentネットワーク | ローカルツール統合 |
| **状態管理** | ステートフル (Task) | ステートレス |

---

## 🚀 実際の応用シーン

### 1. 企業ワークフロー自動化

\`\`\`
ユーザー要求："来週の顧客プレゼン準備して"
     │
     ├─► カレンダーAgent: 会議予定確認
     ├─► CRM Agent: 顧客情報取得
     ├─► ドキュメントAgent: PPT生成
     └─► 調整Agent: 結果統合
\`\`\`

### 2. コード開発協調

\`\`\`
ユーザー要求："ユーザーログイン機能を実装"
     │
     ├─► 設計Agent: 技術設計書生成
     ├─► コーディングAgent: コード作成
     ├─► レビューAgent: コードレビュー
     └─► テストAgent: テスト作成
\`\`\`

---

## 💡 将来展望

> 🔮 **予測**：今後5年以内に、A2AプロトコルはAI Agent相互接続の標準になる、
> HTTPがWebの標準になったように。
            `
          }
        },
        {
          id: 'ch3-models',
          title: { zh: '3.12 AI 模型对比与选型', ja: '3.12 AIモデル比較と選定' },
          content: {
            zh: `
## 2025 年主流 AI 编程工具对比

选择合适的 AI 工具能让效率事半功倍。以下是基于实测的对比结果。

---

## Claude vs GPT：实测对比

根据 AI超元域 的深度评测，Claude 和 GPT 各有优势：

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    Claude vs GPT 能力对比                               │
└─────────────────────────────────────────────────────────────────────────┘

                    Claude Sonnet 4.5              GPT-5.1
                    ───────────────                ───────
  长文本生成            ████████████                ██████
                      ~1.2万字符                   ~6900字符

  文学创作              ████████████                ████████
                      意象精准                     存在重复

  前端UI设计            ████████████                ████████
                      配色精致                     基本可用

  知识更新              ████████████                ████████
                      截至2025.1                   截至2024.6

  数学编程              ████████                    ████████████
                      较好                         稍强

  浏览器自动化          ████████                    ████████████
                      支持                         原生集成
\`\`\`

### 推荐选择

| 任务类型 | 推荐模型 | 原因 |
|---------|--------|------|
| 长文本报告 | Claude | 输出更长、结构更好 |
| 文学创作 | Claude | 意象丰富、用词精准 |
| 前端 UI | Claude | 设计感更强 |
| 数学编程 | GPT | 略有优势 |
| 浏览器自动化 | GPT | 原生支持更好 |
| 日常对话 | 两者均可 | 差异不大 |

> 💡 **建议**：同时订阅两个服务，根据任务切换使用。

---

## GPT-5.2-Codex 与 Agent Skills

OpenAI 最新的 GPT-5.2-Codex 引入了 Agent Skills 功能，这是一个重要的行业趋势。

### 什么是 Skills？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        Skills = AI 的工作手册                           │
└─────────────────────────────────────────────────────────────────────────┘

                    ┌───────────────────┐
                    │  你的专业知识      │
                    │  + 工作流程        │
                    │  + 最佳实践        │
                    └─────────┬─────────┘
                              │
                              ▼
                    ┌───────────────────┐
                    │  打包成 Skills 文件 │
                    └─────────┬─────────┘
                              │
                    ┌─────────┴─────────┐
                    │                   │
                    ▼                   ▼
              ┌───────────┐       ┌───────────┐
              │  Claude   │       │  Codex    │
              │  可调用    │       │  可调用    │
              └───────────┘       └───────────┘

        重要：Skills 在 Claude 和 Codex 之间通用！
\`\`\`

### GPT-5.2-Codex 优缺点

**优点：**
- ✅ 编码能力确实提升
- ✅ 视觉理解（看截图写代码）不错
- ✅ Skills 让开发流程更可控
- ✅ 复杂任务完成度好

**缺点：**
- ❌ **速度慢**：简单任务 5 分钟起步
- ❌ 复杂任务 10 分钟以上
- ❌ 完整项目需要半小时

> 💡 **结论**：适合丢任务给 AI 然后去做别的事。需要快速迭代？Claude Code 更顺手。

---

## AI 编程工具生态

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                      2025 AI 编程工具全景                                │
└─────────────────────────────────────────────────────────────────────────┘

  命令行工具                    IDE 集成                   云端服务
  ──────────                   ────────                   ────────
  ┌───────────┐               ┌───────────┐              ┌───────────┐
  │ Claude Code│               │  Cursor   │              │ GitHub    │
  │ 功能最全   │               │ VSCode集成 │              │ Copilot   │
  └───────────┘               └───────────┘              └───────────┘
  ┌───────────┐               ┌───────────┐              ┌───────────┐
  │ GPT Codex │               │  Windsurf │              │ Replit    │
  │ CLI版本   │               │ 新晋选手   │              │ Agent     │
  └───────────┘               └───────────┘              └───────────┘

  辅助工具
  ────────
  ┌───────────┐  ┌───────────┐  ┌───────────┐
  │Ralph Wiggum│  │  Claudia  │  │SuperClaude│
  │自动迭代修复│  │ GUI界面   │  │ 能力增强  │
  └───────────┘  └───────────┘  └───────────┘
\`\`\`

---

## 本节要点

1. **Claude** —— 长文本、创作、设计更强
2. **GPT** —— 数学、浏览器自动化略好
3. **Skills** —— 行业标准，通用性强
4. **按需选择** —— 不要迷信单一工具
            `,
            ja: `
## 2025年主流AIプログラミングツール比較

適切なAIツールを選ぶことで効率が倍増します。以下は実測に基づく比較結果です。

---

## Claude vs GPT：実測比較

AI超元域の詳細評価によると、ClaudeとGPTにはそれぞれ長所があります：

### 推奨選択

| タスクタイプ | 推奨モデル | 理由 |
|-------------|----------|------|
| 長文レポート | Claude | 出力が長く、構造が良い |
| 文学創作 | Claude | イメージ豊か、用語が正確 |
| フロントエンドUI | Claude | デザイン感が強い |
| 数学プログラミング | GPT | やや優位 |
| ブラウザ自動化 | GPT | ネイティブサポートが良い |
| 日常会話 | どちらでも | 差は小さい |

> 💡 **提案**：両方のサービスを購読し、タスクに応じて使い分ける。

---

## GPT-5.2-CodexとAgent Skills

OpenAI最新のGPT-5.2-CodexはAgent Skills機能を導入。重要な業界トレンドです。

### Skillsとは？

Skillsは「AIの作業マニュアル」—— あなたの専門知識とワークフローをファイルにパッケージ化し、AIがいつでも呼び出せるようにします。

**重要**：SkillsはClaudeとCodex間で互換性あり！

### GPT-5.2-Codexの長所短所

**長所：**
- ✅ コーディング能力が確実に向上
- ✅ ビジュアル理解（スクショからコード生成）が良好
- ✅ Skillsで開発フローをより制御可能

**短所：**
- ❌ **遅い**：簡単なタスクでも5分から
- ❌ 複雑なタスクは10分以上
- ❌ 完全なプロジェクトは30分必要

> 💡 **結論**：タスクをAIに投げて別のことをするなら適している。迅速な反復が必要？Claude Codeがより便利。

---

## AIプログラミングツールエコシステム

\`\`\`
  コマンドラインツール           IDE統合                クラウドサービス
  ────────────────           ────────              ──────────────
  Claude Code                 Cursor                GitHub Copilot
  GPT Codex CLI               Windsurf              Replit Agent

  補助ツール
  ──────────
  Ralph Wiggum（自動反復修正）
  Claudia（GUIインターフェース）
  SuperClaude（能力強化）
\`\`\`

---

## このセクションのポイント

1. **Claude** —— 長文、創作、デザインに強い
2. **GPT** —— 数学、ブラウザ自動化がやや優位
3. **Skills** —— 業界標準、汎用性が高い
4. **ニーズに応じて選択** —— 単一ツールを盲信しない
            `
          }
        },
        {
          id: 'ch3-summary',
          title: { zh: '3.13 本章小结', ja: '3.13 この章のまとめ' },
          content: {
            zh: `
## AI Agent 核心概念回顾

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        AI Agent 知识地图                                │
└─────────────────────────────────────────────────────────────────────────┘

                              AI Agent
                                  │
         ┌────────────────────────┼────────────────────────┐
         │                        │                        │
         ▼                        ▼                        ▼
    ┌──────────┐            ┌──────────┐            ┌──────────┐
    │  核心能力 │            │  工作模式 │            │  实用工具 │
    └──────────┘            └──────────┘            └──────────┘
         │                        │                        │
    ┌────┼────┐              ReAct模式             ┌────┼────┐
    │    │    │              思考-行动-观察         │    │    │
    ▼    ▼    ▼                                   ▼    ▼    ▼
  规划  工具  记忆                             代码  通用  自动化
                                               Agent Agent 工作流
\`\`\`

---

## Agent vs 普通 AI

| 特性 | 普通 AI | AI Agent |
|------|---------|----------|
| 交互方式 | 一问一答 | 自主执行 |
| 任务复杂度 | 单一任务 | 复杂任务 |
| 工具使用 | 有限 | 丰富 |
| 自主性 | 被动响应 | 主动规划 |

---

## 快速行动清单

- [ ] 尝试使用 ChatGPT 的代码解释器功能
- [ ] 用 Claude 完成一个需要搜索的任务
- [ ] 探索一个代码 Agent（如 Cursor）
- [ ] 思考你的工作中哪些任务可以用 Agent 自动化

---

## 关键金句

> "Agent 是 AI 从'顾问'变成'助手'的关键一步。"

> "未来不是 AI 替代人，而是会用 Agent 的人替代不会用的人。"

---

下一章，我们将学习 **RAG（检索增强生成）** —— 让 AI 能够获取最新知识，解决"知识过时"的问题！
            `,
            ja: `
## AI Agentコア概念の復習

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        AI Agent 知識マップ                               │
└─────────────────────────────────────────────────────────────────────────┘

                              AI Agent
                                  │
         ┌────────────────────────┼────────────────────────┐
         │                        │                        │
         ▼                        ▼                        ▼
    ┌──────────┐            ┌──────────┐            ┌──────────┐
    │  コア能力 │            │  動作モード │            │ 実用ツール │
    └──────────┘            └──────────┘            └──────────┘
         │                        │                        │
    ┌────┼────┐              ReActパターン           ┌────┼────┐
    │    │    │              思考-行動-観察          │    │    │
    ▼    ▼    ▼                                    ▼    ▼    ▼
  計画  ツール 記憶                              コード 汎用  自動化
                                                Agent Agent ワークフロー
\`\`\`

---

## Agent vs 通常のAI

| 特性 | 通常のAI | AI Agent |
|------|----------|----------|
| インタラクション | 一問一答 | 自律実行 |
| タスクの複雑さ | 単一タスク | 複雑なタスク |
| ツール使用 | 限定的 | 豊富 |
| 自律性 | 受動的応答 | 能動的計画 |

---

## クイックアクションリスト

- [ ] ChatGPTのコードインタープリター機能を試す
- [ ] Claudeで検索が必要なタスクを完了する
- [ ] コードAgent（Cursorなど）を探索する
- [ ] 仕事でAgentで自動化できるタスクを考える

---

## 重要な格言

> 「AgentはAIが『アドバイザー』から『アシスタント』になるための重要な一歩です。」

> 「未来はAIが人を置き換えるのではなく、Agentを使える人が使えない人を置き換えるのです。」

---

次の章では、**RAG（検索拡張生成）**を学びます —— AIが最新の知識を取得し、「知識の陳腐化」問題を解決できるようにします！
            `
          }
        }
      ]
    },
    // ============================================
    // 第三章：RAG 检索增强生成
    // ============================================
    {
      id: 'chapter-4',
      number: 4,
      title: { zh: 'RAG 检索增强生成', ja: 'RAG 検索拡張生成' },
      subtitle: { zh: '让AI获取最新知识', ja: 'AIに最新知識を取得させる' },
      sections: [
        {
          id: 'ch4-intro',
          title: { zh: '引言：AI 的知识困境', ja: '序章：AIの知識ジレンマ' },
          content: {
            zh: `
你有没有遇到过这样的情况：

- 问 AI 今天的新闻，它说不知道
- 问公司内部的规章制度，它回答不了
- 问最新的产品信息，它给出过时的答案

这是因为 AI 的知识有**截止日期**，而且它不了解你的**私有数据**。

**RAG（Retrieval-Augmented Generation，检索增强生成）** 就是解决这个问题的关键技术！

---

## AI 知识的局限性

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        AI 知识的两大局限                                │
└─────────────────────────────────────────────────────────────────────────┘

  局限1：知识有截止日期                    局限2：不了解私有数据
  ┌──────────────────────┐               ┌──────────────────────┐
  │                      │               │                      │
  │  训练数据截止到       │               │  AI 不知道：          │
  │  某个时间点           │               │                      │
  │                      │               │  • 你公司的规章制度    │
  │  比如：2024年1月      │               │  • 你的产品文档        │
  │                      │               │  • 你的客户数据        │
  │  之后的事情都不知道    │               │  • 你的内部知识库      │
  │                      │               │                      │
  └──────────────────────┘               └──────────────────────┘

                           RAG 可以解决这两个问题！
\`\`\`

---

## 什么是 RAG？

RAG 的核心思想很简单：

> **先检索相关信息，再让 AI 回答**

就像一个学生考试时可以查资料一样，RAG 让 AI 在回答问题前，先从知识库中找到相关内容。

---

## 本章你将学到

1. **RAG 的工作原理** —— 它是如何运作的
2. **向量数据库** —— RAG 的核心技术
3. **RAG 的应用场景** —— 在哪些地方使用
4. **如何使用 RAG** —— 实际操作方法
5. **RAG 的优缺点** —— 了解它的边界

让我们深入了解这个让 AI "与时俱进"的技术！
            `,
            ja: `
こんな経験はありませんか：

- AIに今日のニュースを聞くと、知らないと言う
- 会社の規則を聞くと、答えられない
- 最新の製品情報を聞くと、古い答えが返ってくる

これはAIの知識に**締め切り日**があり、あなたの**プライベートデータ**を知らないからです。

**RAG（Retrieval-Augmented Generation、検索拡張生成）**がこの問題を解決する重要な技術です！

---

## AI知識の限界

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        AI知識の2つの限界                                │
└─────────────────────────────────────────────────────────────────────────┘

  限界1：知識に締め切りがある              限界2：プライベートデータを知らない
  ┌──────────────────────┐               ┌──────────────────────┐
  │                      │               │                      │
  │  トレーニングデータは  │               │  AIが知らないこと：    │
  │  ある時点で終了        │               │                      │
  │                      │               │  • あなたの会社の規則  │
  │  例：2024年1月        │               │  • あなたの製品ドキュメント │
  │                      │               │  • あなたの顧客データ   │
  │  それ以降のことは知らない│               │  • あなたの社内知識ベース │
  │                      │               │                      │
  └──────────────────────┘               └──────────────────────┘

                           RAGはこの両方を解決できます！
\`\`\`

---

## RAGとは？

RAGの核心的なアイデアはシンプル：

> **まず関連情報を検索し、それからAIに回答させる**

試験中に資料を参照できる学生のように、RAGはAIが質問に答える前に、知識ベースから関連コンテンツを見つけられるようにします。

---

## この章で学ぶこと

1. **RAGの仕組み** —— どのように動作するか
2. **ベクトルデータベース** —— RAGのコア技術
3. **RAGの応用シーン** —— どこで使われるか
4. **RAGの使い方** —— 実際の操作方法
5. **RAGの長所と短所** —— その境界を理解する

AIを「時代に追いつかせる」この技術を深く理解しましょう！
            `
          }
        },
        {
          id: 'ch4-how-it-works',
          title: { zh: '4.1 RAG 的工作原理', ja: '4.1 RAGの仕組み' },
          content: {
            zh: `
让我们看看 RAG 是如何工作的。

### 交互式演示：RAG 工作流程

先通过这个交互式演示直观感受 RAG 的完整流程：

::rag-viz::

---

## RAG 工作流程

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                          RAG 完整流程                                   │
└─────────────────────────────────────────────────────────────────────────┘

  【准备阶段】                           【使用阶段】

   文档/知识                              用户提问
      │                                     │
      ▼                                     ▼
  ┌─────────┐                          ┌─────────┐
  │ 文本分块 │                          │ 问题向量化│
  └─────────┘                          └─────────┘
      │                                     │
      ▼                                     ▼
  ┌─────────┐                          ┌─────────┐
  │ 向量化   │                          │ 相似度搜索│ ◄── 从向量数据库
  └─────────┘                          └─────────┘
      │                                     │
      ▼                                     ▼
  ┌─────────┐                          ┌─────────┐
  │向量数据库│                          │ 获取相关文档│
  └─────────┘                          └─────────┘
                                           │
                                           ▼
                                      ┌─────────┐
                                      │组合提示词│
                                      │(问题+文档)│
                                      └─────────┘
                                           │
                                           ▼
                                      ┌─────────┐
                                      │ LLM 回答 │
                                      └─────────┘
\`\`\`

---

## 核心步骤详解

### 1. 文本分块（Chunking）

把长文档切分成小块：

\`\`\`
原始文档（10000字）
    │
    ▼
┌─────┐ ┌─────┐ ┌─────┐ ┌─────┐
│块1  │ │块2  │ │块3  │ │...  │
│500字│ │500字│ │500字│ │     │
└─────┘ └─────┘ └─────┘ └─────┘
\`\`\`

为什么要分块？
- LLM 有上下文长度限制
- 小块更容易精确匹配
- 提高检索效率

---

### 2. 向量化（Embedding）

把文本转换成数字向量：

\`\`\`
"人工智能是..." ──▶ [0.12, -0.34, 0.56, ..., 0.78]
                        （高维向量）
\`\`\`

**为什么用向量？**

- 语义相似的内容，向量也相似
- 可以进行数学计算（相似度）
- 比关键词匹配更智能

---

### 3. 相似度搜索

当用户提问时：
1. 把问题也转成向量
2. 在向量数据库中找最相似的文档块
3. 返回 top-K 个最相关的结果

\`\`\`
用户问题向量  ──▶  在向量空间中找最近的邻居
     ●              ┌─────────────────────┐
                    │    ●  ●             │
                    │  ●      ●           │
                    │    ●●      找到！   │
                    │      ⬛ ◄───────    │
                    │    ●                │
                    └─────────────────────┘
\`\`\`

---

### 4. 组合提示词

把检索到的内容和用户问题组合：

\`\`\`
【系统提示】
你是一个助手，请根据以下参考资料回答问题。
如果资料中没有相关信息，请说"我没有找到相关信息"。

【参考资料】
{检索到的文档块1}
{检索到的文档块2}
{检索到的文档块3}

【用户问题】
{用户的问题}
\`\`\`

---

## 本节要点

1. **两个阶段** —— 准备阶段（建索引）+ 使用阶段（检索回答）
2. **文本分块** —— 把长文档切成小块
3. **向量化** —— 把文本转成数字向量
4. **相似度搜索** —— 找到最相关的内容
5. **组合提示** —— 把检索结果和问题一起给 LLM
            `,
            ja: `
RAGがどのように動作するか見てみましょう。

### インタラクティブデモ：RAGワークフロー

このインタラクティブデモでRAGの完全なフローを直感的に体験してください：

::rag-viz::

---

## RAGワークフロー

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                          RAG 完全フロー                                  │
└─────────────────────────────────────────────────────────────────────────┘

  【準備段階】                           【使用段階】

   ドキュメント/知識                        ユーザーの質問
      │                                     │
      ▼                                     ▼
  ┌─────────┐                          ┌─────────┐
  │テキスト分割│                          │質問をベクトル化│
  └─────────┘                          └─────────┘
      │                                     │
      ▼                                     ▼
  ┌─────────┐                          ┌─────────┐
  │ ベクトル化 │                          │類似度検索│ ◄── ベクトルDBから
  └─────────┘                          └─────────┘
      │                                     │
      ▼                                     ▼
  ┌─────────┐                          ┌─────────┐
  │ベクトルDB │                          │関連文書を取得│
  └─────────┘                          └─────────┘
                                           │
                                           ▼
                                      ┌─────────┐
                                      │プロンプト組み合わせ│
                                      │(質問+文書)│
                                      └─────────┘
                                           │
                                           ▼
                                      ┌─────────┐
                                      │ LLM が回答 │
                                      └─────────┘
\`\`\`

---

## コアステップの詳細

### 1. テキスト分割（Chunking）

長いドキュメントを小さな塊に分割：

\`\`\`
元のドキュメント（10000文字）
    │
    ▼
┌─────┐ ┌─────┐ ┌─────┐ ┌─────┐
│チャンク1│ │チャンク2│ │チャンク3│ │...  │
│500文字│ │500文字│ │500文字│ │     │
└─────┘ └─────┘ └─────┘ └─────┘
\`\`\`

なぜ分割するのか？
- LLMにはコンテキスト長の制限がある
- 小さな塊の方が正確にマッチング
- 検索効率が向上

---

### 2. ベクトル化（Embedding）

テキストを数値ベクトルに変換：

\`\`\`
"人工知能は..." ──▶ [0.12, -0.34, 0.56, ..., 0.78]
                        （高次元ベクトル）
\`\`\`

**なぜベクトルを使うのか？**

- 意味的に似た内容はベクトルも似ている
- 数学的計算（類似度）が可能
- キーワードマッチングより賢い

---

### 3. 類似度検索

ユーザーが質問したとき：
1. 質問もベクトルに変換
2. ベクトルDBで最も似た文書チャンクを探す
3. top-K個の最も関連する結果を返す

\`\`\`
質問ベクトル  ──▶  ベクトル空間で最近傍を探す
     ●              ┌─────────────────────┐
                    │    ●  ●             │
                    │  ●      ●           │
                    │    ●●      見つけた！│
                    │      ⬛ ◄───────    │
                    │    ●                │
                    └─────────────────────┘
\`\`\`

---

### 4. プロンプト組み合わせ

検索した内容とユーザーの質問を組み合わせ：

\`\`\`
【システムプロンプト】
あなたはアシスタントです。以下の参考資料に基づいて質問に答えてください。
資料に関連情報がない場合は「関連情報が見つかりませんでした」と言ってください。

【参考資料】
{検索された文書チャンク1}
{検索された文書チャンク2}
{検索された文書チャンク3}

【ユーザーの質問】
{ユーザーの質問}
\`\`\`

---

## このセクションのポイント

1. **2つの段階** —— 準備段階（インデックス作成）+ 使用段階（検索と回答）
2. **テキスト分割** —— 長いドキュメントを小さく分割
3. **ベクトル化** —— テキストを数値ベクトルに変換
4. **類似度検索** —— 最も関連するコンテンツを見つける
5. **プロンプト組み合わせ** —— 検索結果と質問をLLMに渡す
            `
          }
        },
        {
          id: 'ch4-applications',
          title: { zh: '4.2 RAG 的应用场景', ja: '4.2 RAGの応用シーン' },
          content: {
            zh: `
RAG 在很多场景中都有广泛应用。

---

## 企业知识库问答

\`\`\`
场景：员工想查询公司规章制度

传统方式：在几十个文档中搜索关键词
RAG 方式：直接问 AI，它会从知识库中找到答案

例子：
问："年假怎么申请？"
答："根据公司员工手册第5章，年假申请需要提前3天
    在OA系统提交，经主管审批后生效..."
\`\`\`

---

## 客服智能问答

\`\`\`
场景：客户咨询产品问题

优势：
• 24小时自动回答
• 答案基于官方文档
• 减少人工客服压力

例子：
问："这个产品支持 Mac 吗？"
答："根据产品说明书，本产品支持 macOS 10.15 及以上版本..."
\`\`\`

---

## 个人知识管理

\`\`\`
场景：管理你的笔记、文档、收藏

使用方式：
1. 把你的笔记、PDF、网页收藏导入
2. 用自然语言提问
3. AI 从你的知识库中找答案

例子：
问："我之前看过一篇关于时间管理的文章，核心观点是什么？"
答："根据你收藏的《高效能人士的七个习惯》笔记，
    核心观点是要事第一，区分紧急和重要..."
\`\`\`

---

## 代码文档助手

\`\`\`
场景：查询项目代码文档

优势：
• 不需要记住所有 API
• 快速找到使用方法
• 结合代码上下文回答

例子：
问："这个项目的数据库连接怎么配置？"
答："根据项目 README 和 config.py，
    数据库配置在 .env 文件中设置..."
\`\`\`

---

## 应用场景对比

| 场景 | 知识来源 | 典型问题 |
|------|----------|----------|
| 企业知识库 | 规章制度、培训材料 | 报销流程是什么？ |
| 客服问答 | 产品文档、FAQ | 如何退货？ |
| 个人知识管理 | 笔记、收藏、PDF | 我之前学过的XX是什么？ |
| 代码助手 | 代码、文档、注释 | 这个函数怎么用？ |
| 学术研究 | 论文、报告 | 某领域最新进展？ |

---

## 本节要点

1. **企业应用** —— 知识库问答、客服系统
2. **个人应用** —— 笔记管理、学习助手
3. **开发应用** —— 代码文档、API 查询
4. **共同特点** —— 基于私有/专业知识回答
            `,
            ja: `
RAGは多くのシーンで広く活用されています。

---

## 企業ナレッジベースQ&A

\`\`\`
シーン：従業員が会社の規則を調べたい

従来の方法：数十のドキュメントでキーワード検索
RAG方式：AIに直接質問、知識ベースから答えを見つける

例：
質問：「有給休暇の申請方法は？」
回答：「社員ハンドブック第5章によると、有給休暇の申請は
      3日前までにOAシステムで提出し、上司の承認後に有効になります...」
\`\`\`

---

## カスタマーサービスQ&A

\`\`\`
シーン：顧客が製品について質問

メリット：
• 24時間自動回答
• 公式ドキュメントに基づく回答
• 人間のサポート負担を軽減

例：
質問：「この製品はMacに対応していますか？」
回答：「製品マニュアルによると、本製品はmacOS 10.15以上に対応しています...」
\`\`\`

---

## 個人ナレッジ管理

\`\`\`
シーン：あなたのノート、ドキュメント、お気に入りを管理

使い方：
1. ノート、PDF、ブックマークをインポート
2. 自然言語で質問
3. AIがあなたの知識ベースから答えを見つける

例：
質問：「以前読んだ時間管理についての記事、核心的なポイントは何でしたっけ？」
回答：「あなたがブックマークした『7つの習慣』のノートによると、
      核心ポイントは重要事項を優先し、緊急と重要を区別することです...」
\`\`\`

---

## コードドキュメントアシスタント

\`\`\`
シーン：プロジェクトのコードドキュメントを照会

メリット：
• すべてのAPIを覚える必要がない
• 使用方法をすばやく見つける
• コードコンテキストと組み合わせて回答

例：
質問：「このプロジェクトのデータベース接続はどう設定する？」
回答：「プロジェクトのREADMEとconfig.pyによると、
      データベース設定は.envファイルで設定します...」
\`\`\`

---

## 応用シーン比較

| シーン | 知識ソース | 典型的な質問 |
|--------|------------|--------------|
| 企業ナレッジベース | 規則、トレーニング資料 | 経費精算のプロセスは？ |
| カスタマーサービス | 製品ドキュメント、FAQ | 返品方法は？ |
| 個人ナレッジ管理 | ノート、ブックマーク、PDF | 以前学んだXXって何？ |
| コードアシスタント | コード、ドキュメント、コメント | この関数の使い方は？ |
| 学術研究 | 論文、レポート | この分野の最新動向は？ |

---

## このセクションのポイント

1. **企業応用** —— ナレッジベースQ&A、カスタマーサービス
2. **個人応用** —— ノート管理、学習アシスタント
3. **開発応用** —— コードドキュメント、APIクエリ
4. **共通点** —— プライベート/専門知識に基づく回答
            `
          }
        },
        {
          id: 'ch4-optimization',
          title: { zh: '4.3 RAG 优化技巧', ja: '4.3 RAG 最適化テクニック' },
          content: {
            zh: `
## 让 RAG 更精准的关键技巧

RAG 系统的效果取决于每个环节的优化。以下是提升 RAG 性能的核心技巧。

---

## 分块策略：如何切分文档

分块（Chunking）是 RAG 的基础，直接影响检索质量。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                          分块策略对比                                    │
└─────────────────────────────────────────────────────────────────────────┘

  固定大小分块                         语义分块
  ────────────                        ──────────
  ┌─────────────┐                    ┌─────────────┐
  │ 500 字符    │                    │  完整段落    │
  ├─────────────┤                    ├─────────────┤
  │ 500 字符    │                    │  完整章节    │
  ├─────────────┤                    ├─────────────┤
  │ 500 字符    │                    │  完整概念    │
  └─────────────┘                    └─────────────┘
       │                                  │
       ▼                                  ▼
  可能切断语义                        保持语义完整
  简单但不精准                        复杂但更准确
\`\`\`

### 最佳实践

| 策略 | 适用场景 | 典型大小 |
|------|----------|----------|
| 固定大小 + 重叠 | 通用场景 | 512 tokens + 50 重叠 |
| 句子分块 | 精确问答 | 3-5 句 |
| 段落分块 | 文档摘要 | 按 \\n\\n 分割 |
| 递归分块 | 结构化文档 | 先章节，再段落 |

---

## 混合检索：向量 + 关键词

单一向量检索可能遗漏关键词匹配，混合检索结合两者优势。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                          混合检索架构                                    │
└─────────────────────────────────────────────────────────────────────────┘

                          用户查询
                             │
              ┌──────────────┼──────────────┐
              │                             │
              ▼                             ▼
      ┌─────────────┐               ┌─────────────┐
      │  向量检索   │               │  关键词检索  │
      │ (语义相似)  │               │ (BM25/TF-IDF)|
      └──────┬──────┘               └──────┬──────┘
             │                              │
             │  相似度: 0.85               │  匹配度: 0.72
             │  相似度: 0.82               │  匹配度: 0.68
             │  相似度: 0.79               │  匹配度: 0.65
             │                              │
             └──────────────┬──────────────┘
                            │
                    ┌───────▼───────┐
                    │   融合排序    │
                    │ (RRF / 加权)  │
                    └───────┬───────┘
                            │
                            ▼
                    ┌───────────────┐
                    │  最终结果 Top-K│
                    └───────────────┘
\`\`\`

**融合公式（RRF）：**
\`\`\`
score = Σ 1/(k + rank_i)

其中：k 通常取 60
     rank_i 是该文档在第 i 个检索结果中的排名
\`\`\`

---

## 查询改写：提升检索召回率

用户的原始查询可能不够精确，通过改写提升匹配度。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                          查询改写技巧                                    │
└─────────────────────────────────────────────────────────────────────────┘

原始查询: "公司的请假流程是什么？"
                │
                ▼
        ┌───────────────┐
        │   查询改写    │
        └───────────────┘
                │
    ┌───────────┼───────────┐
    │           │           │
    ▼           ▼           ▼
  同义扩展    问题分解    假设文档

"请假流程"   "如何请假"   "请假审批流程
"休假申请"   "请假需要    包括：提交申请、
"事假流程"    什么材料"   主管审批..."
\`\`\`

### HyDE（假设文档嵌入）

让 LLM 先生成一个"假设的答案"，用这个答案去检索：

\`\`\`
用户问题: "如何配置 Nginx 反向代理？"
                │
                ▼
┌─────────────────────────────────────────────────────────────────────────┐
│ LLM 生成假设答案:                                                        │
│ "要配置 Nginx 反向代理，需要在 server 块中使用 location 和              │
│  proxy_pass 指令。首先编辑 nginx.conf，添加 upstream 定义后端..."        │
└─────────────────────────────────────────────────────────────────────────┘
                │
                ▼
        用假设答案向量检索
                │
                ▼
        返回真正相关的文档
\`\`\`

---

## Reranker：精排提升准确率

初步检索后，用更强的模型对结果重新排序。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                          两阶段检索                                      │
└─────────────────────────────────────────────────────────────────────────┘

                    ┌───────────────┐
                    │  向量检索     │
                    │  (快速召回)   │
                    └───────┬───────┘
                            │
                    返回 Top-100
                            │
                            ▼
                    ┌───────────────┐
                    │  Reranker     │
                    │  (精确排序)   │
                    └───────┬───────┘
                            │
                    返回 Top-5
                            │
                            ▼
                    ┌───────────────┐
                    │    LLM 生成   │
                    └───────────────┘

召回阶段：快但不够准 → 精排阶段：准但较慢
\`\`\`

### 常用 Reranker

| 模型 | 特点 | 推荐场景 |
|------|------|----------|
| Cohere Rerank | 效果好，API 调用 | 生产环境 |
| BGE Reranker | 开源，可本地部署 | 私有化部署 |
| Cross-Encoder | 精确但慢 | 小规模数据 |

---

## 实战建议

### 1. 从简单开始
\`\`\`
初始配置:
├── 分块: 512 tokens + 10% 重叠
├── Embedding: text-embedding-3-small
├── 检索: Top-5
└── 无 Reranker
\`\`\`

### 2. 逐步优化
\`\`\`
发现问题 → 针对性优化:
├── 召回不全 → 混合检索 / 查询改写
├── 相关性差 → 添加 Reranker
├── 语义断裂 → 调整分块策略
└── 响应慢 → 缓存 / 预计算
\`\`\`

### 3. 评估指标
\`\`\`
检索质量:
├── Recall@K: 相关文档在 Top-K 的比例
├── MRR: 第一个相关结果的位置
└── NDCG: 排序质量评分

端到端:
├── 答案准确率
├── 用户满意度
└── 响应延迟
\`\`\`

---

## 本节要点

1. **分块策略** —— 保持语义完整性
2. **混合检索** —— 向量 + 关键词双管齐下
3. **查询改写** —— HyDE 等技巧提升召回
4. **Reranker** —— 两阶段检索提升精度
            `,
            ja: `
## RAGをより正確にするための重要なテクニック

RAGシステムの効果は各段階の最適化に依存します。以下はRAG性能を向上させるコアテクニックです。

---

## チャンキング戦略：ドキュメントの分割方法

チャンキング（Chunking）はRAGの基礎であり、検索品質に直接影響します。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                          チャンキング戦略比較                              │
└─────────────────────────────────────────────────────────────────────────┘

  固定サイズチャンキング                 セマンティックチャンキング
  ──────────────────                    ────────────────────
  ┌─────────────┐                      ┌─────────────┐
  │ 500 文字    │                      │  完全な段落  │
  ├─────────────┤                      ├─────────────┤
  │ 500 文字    │                      │  完全な章    │
  ├─────────────┤                      ├─────────────┤
  │ 500 文字    │                      │  完全な概念  │
  └─────────────┘                      └─────────────┘
       │                                    │
       ▼                                    ▼
  意味が切れる可能性                    意味の完全性を保持
  シンプルだが不正確                    複雑だが正確
\`\`\`

### ベストプラクティス

| 戦略 | 適用シーン | 典型的なサイズ |
|------|----------|----------|
| 固定サイズ + オーバーラップ | 汎用 | 512 tokens + 50 重複 |
| 文チャンキング | 精密Q&A | 3-5 文 |
| 段落チャンキング | ドキュメント要約 | \\n\\n で分割 |
| 再帰的チャンキング | 構造化文書 | 先に章、次に段落 |

---

## ハイブリッド検索：ベクトル + キーワード

単一のベクトル検索ではキーワードマッチが漏れる可能性があり、ハイブリッド検索は両方の利点を組み合わせます。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                          ハイブリッド検索アーキテクチャ                     │
└─────────────────────────────────────────────────────────────────────────┘

                          ユーザークエリ
                             │
              ┌──────────────┼──────────────┐
              │                             │
              ▼                             ▼
      ┌─────────────┐               ┌─────────────┐
      │  ベクトル検索 │               │ キーワード検索│
      │ (意味類似性) │               │ (BM25/TF-IDF)|
      └──────┬──────┘               └──────┬──────┘
             │                              │
             │  類似度: 0.85               │  マッチ度: 0.72
             │  類似度: 0.82               │  マッチ度: 0.68
             │  類似度: 0.79               │  マッチ度: 0.65
             │                              │
             └──────────────┬──────────────┘
                            │
                    ┌───────▼───────┐
                    │   融合ランキング │
                    │ (RRF / 重み付け) │
                    └───────┬───────┘
                            │
                            ▼
                    ┌───────────────┐
                    │  最終結果 Top-K│
                    └───────────────┘
\`\`\`

**融合式（RRF）：**
\`\`\`
score = Σ 1/(k + rank_i)

ここで：k は通常 60
     rank_i は i番目の検索結果でのドキュメントの順位
\`\`\`

---

## クエリ書き換え：検索リコール率の向上

ユーザーの元のクエリは十分に正確でない可能性があり、書き換えでマッチ度を向上させます。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                          クエリ書き換えテクニック                          │
└─────────────────────────────────────────────────────────────────────────┘

元のクエリ: "会社の休暇申請プロセスは？"
                │
                ▼
        ┌───────────────┐
        │   クエリ書き換え │
        └───────────────┘
                │
    ┌───────────┼───────────┐
    │           │           │
    ▼           ▼           ▼
  同義語展開   質問分解    仮想ドキュメント

"休暇申請"   "休暇の     "休暇承認プロセス
"有給申請"   取り方は？"  には：申請提出、
"欠勤届"    "必要な      上司承認..."
            書類は？"
\`\`\`

### HyDE（仮想ドキュメント埋め込み）

LLMにまず「仮想の回答」を生成させ、その回答で検索します：

\`\`\`
ユーザーの質問: "Nginxのリバースプロキシ設定方法は？"
                │
                ▼
┌─────────────────────────────────────────────────────────────────────────┐
│ LLM が仮想回答を生成:                                                     │
│ "Nginxのリバースプロキシを設定するには、serverブロック内で location と     │
│  proxy_pass ディレクティブを使用します。まず nginx.conf を編集し..."       │
└─────────────────────────────────────────────────────────────────────────┘
                │
                ▼
        仮想回答でベクトル検索
                │
                ▼
        本当に関連するドキュメントを返す
\`\`\`

---

## Reranker：精密ランキングで精度向上

初期検索後、より強力なモデルで結果を再ランキングします。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                          2段階検索                                       │
└─────────────────────────────────────────────────────────────────────────┘

                    ┌───────────────┐
                    │  ベクトル検索  │
                    │  (高速リコール) │
                    └───────┬───────┘
                            │
                    Top-100 を返す
                            │
                            ▼
                    ┌───────────────┐
                    │  Reranker     │
                    │  (精密ソート)  │
                    └───────┬───────┘
                            │
                    Top-5 を返す
                            │
                            ▼
                    ┌───────────────┐
                    │    LLM 生成   │
                    └───────────────┘

リコール段階：速いが不正確 → 精密段階：正確だが遅い
\`\`\`

### よく使う Reranker

| モデル | 特徴 | 推奨シーン |
|------|------|----------|
| Cohere Rerank | 効果良好、API呼び出し | 本番環境 |
| BGE Reranker | オープンソース、ローカル展開可 | プライベート展開 |
| Cross-Encoder | 正確だが遅い | 小規模データ |

---

## 実践的なアドバイス

### 1. シンプルから始める
\`\`\`
初期設定:
├── チャンキング: 512 tokens + 10% オーバーラップ
├── Embedding: text-embedding-3-small
├── 検索: Top-5
└── Reranker なし
\`\`\`

### 2. 段階的に最適化
\`\`\`
問題発見 → 的を絞った最適化:
├── リコール不足 → ハイブリッド検索 / クエリ書き換え
├── 関連性が低い → Reranker 追加
├── 意味が分断 → チャンキング戦略を調整
└── レスポンスが遅い → キャッシュ / 事前計算
\`\`\`

### 3. 評価指標
\`\`\`
検索品質:
├── Recall@K: Top-K での関連ドキュメントの割合
├── MRR: 最初の関連結果の位置
└── NDCG: ランキング品質スコア

エンドツーエンド:
├── 回答精度
├── ユーザー満足度
└── レスポンス遅延
\`\`\`

---

## このセクションのポイント

1. **チャンキング戦略** —— 意味の完全性を保つ
2. **ハイブリッド検索** —— ベクトル + キーワードの両方を活用
3. **クエリ書き換え** —— HyDE などでリコール向上
4. **Reranker** —— 2段階検索で精度向上
            `
          }
        },
        {
          id: 'ch4-code-practice',
          title: { zh: '4.4 RAG 实战代码', ja: '4.4 RAG 実践コード' },
          content: {
            zh: `
## 从零搭建 RAG 知识库

学会了原理，让我们动手写代码！

---

## 方案一：LangChain + Chroma

最流行的 RAG 实现方案。

### 安装依赖

\`\`\`bash
pip install langchain langchain-openai chromadb
\`\`\`

### 核心代码

\`\`\`python
from langchain_community.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.chains import RetrievalQA

# 1. 加载文档
loader = DirectoryLoader("./documents", glob="**/*.txt")
documents = loader.load()

# 2. 分块
splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)
chunks = splitter.split_documents(documents)

# 3. 创建向量数据库
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory="./chroma_db"
)

# 4. 创建问答链
llm = ChatOpenAI(model="gpt-4o-mini")
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever(search_kwargs={"k": 5})
)

# 5. 问答
result = qa_chain.invoke({"query": "你的问题"})
print(result["result"])
\`\`\`

---

## 方案二：LlamaIndex（5行代码）

\`\`\`python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

# 1-2. 加载并创建索引
documents = SimpleDirectoryReader("./documents").load_data()
index = VectorStoreIndex.from_documents(documents)

# 3-4. 查询
query_engine = index.as_query_engine()
response = query_engine.query("你的问题")
print(response)
\`\`\`

---

## 向量数据库对比

| 数据库 | 特点 | 适用场景 |
|--------|------|----------|
| **Chroma** | 轻量本地 | 开发/原型 |
| **Pinecone** | 云托管 | 生产环境 |
| **Milvus** | 开源高性能 | 大规模部署 |
| **FAISS** | Meta开源 | 嵌入式场景 |

---

## 本地部署（无需OpenAI）

\`\`\`python
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import Ollama

# 本地 Embedding
embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-small-zh-v1.5"
)

# 本地 LLM
llm = Ollama(model="qwen2.5:7b")
\`\`\`

---

## 常见问题

| 问题 | 解决方案 |
|------|----------|
| 检索不到 | 缩小 chunk_size |
| 回答不准 | 增加 k 值或添加 Reranker |
| 速度慢 | 换用 smaller embedding |
| 中文效果差 | 用多语言模型 bge-m3 |

---

## 本节要点

1. **LangChain** —— 灵活、组件丰富
2. **LlamaIndex** —— 简洁、上手快
3. **向量数据库** —— Chroma开发/Pinecone生产
4. **本地部署** —— HuggingFace + Ollama
            `,
            ja: `
## ゼロから RAG ナレッジベースを構築

原理を学んだので、コードを書きましょう！

---

## ソリューション1：LangChain + Chroma

最も人気のあるRAG実装ソリューション。

### 依存関係のインストール

\`\`\`bash
pip install langchain langchain-openai chromadb
\`\`\`

### コアコード

\`\`\`python
from langchain_community.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.chains import RetrievalQA

# 1. ドキュメント読み込み
loader = DirectoryLoader("./documents", glob="**/*.txt")
documents = loader.load()

# 2. チャンキング
splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)
chunks = splitter.split_documents(documents)

# 3. ベクトルDB作成
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory="./chroma_db"
)

# 4. QAチェーン作成
llm = ChatOpenAI(model="gpt-4o-mini")
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever(search_kwargs={"k": 5})
)

# 5. 質問応答
result = qa_chain.invoke({"query": "あなたの質問"})
print(result["result"])
\`\`\`

---

## ソリューション2：LlamaIndex（5行コード）

\`\`\`python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

# 1-2. 読み込みとインデックス作成
documents = SimpleDirectoryReader("./documents").load_data()
index = VectorStoreIndex.from_documents(documents)

# 3-4. クエリ
query_engine = index.as_query_engine()
response = query_engine.query("あなたの質問")
print(response)
\`\`\`

---

## ベクトルデータベース比較

| データベース | 特徴 | 適用シーン |
|--------------|------|------------|
| **Chroma** | 軽量ローカル | 開発/プロトタイプ |
| **Pinecone** | クラウドマネージド | 本番環境 |
| **Milvus** | オープンソース高性能 | 大規模デプロイ |
| **FAISS** | Metaオープンソース | 組み込みシナリオ |

---

## ローカルデプロイ（OpenAI不要）

\`\`\`python
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import Ollama

# ローカルEmbedding
embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-small-zh-v1.5"
)

# ローカルLLM
llm = Ollama(model="qwen2.5:7b")
\`\`\`

---

## よくある問題

| 問題 | 解決策 |
|------|--------|
| 検索されない | chunk_sizeを小さく |
| 回答が不正確 | k値を増やす/Reranker追加 |
| 速度が遅い | smaller embeddingに変更 |
| 中文効果が悪い | 多言語モデルbge-m3を使用 |

---

## このセクションのポイント

1. **LangChain** —— 柔軟、コンポーネント豊富
2. **LlamaIndex** —— シンプル、素早く開始
3. **ベクトルDB** —— Chroma開発/Pinecone本番
4. **ローカルデプロイ** —— HuggingFace + Ollama
            `
          }
        },
        {
          id: 'ch4-graphrag',
          title: { zh: '4.5 GraphRAG：知识图谱增强', ja: '4.5 GraphRAG：ナレッジグラフ強化' },
          content: {
            zh: `
## 当向量检索不够用时：知识图谱来帮忙

传统 RAG 只能找到"相似"的内容，GraphRAG 能理解实体之间的"关系"。

---

## 🎯 传统 RAG vs GraphRAG

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                   传统 RAG vs GraphRAG                                   │
└─────────────────────────────────────────────────────────────────────────┘

  传统 RAG (向量检索)                   GraphRAG (图谱检索)
  ┌────────────────────────┐           ┌────────────────────────┐
  │                        │           │   张三 ──工作于──► 公司A │
  │  文档1 ████ 0.92       │           │     │                  │
  │  文档2 ███░ 0.85       │           │   朋友               投资
  │  文档3 ██░░ 0.71       │           │     │                  │
  │                        │           │     ▼                  ▼
  │  纯语义相似度匹配       │           │   李四 ──合作──► 公司B │
  │  ❌ 无法理解关系        │           │                        │
  │  ❌ 跨文档推理弱        │           │  ✅ 关系推理           │
  └────────────────────────┘           │  ✅ 多跳问答           │
                                       └────────────────────────┘
\`\`\`

---

## 📊 什么时候用 GraphRAG？

| 场景 | 传统 RAG | GraphRAG |
|------|----------|----------|
| 简单问答 | ✅ 够用 | 杀鸡用牛刀 |
| 多跳推理 | ❌ 弱 | ✅ 强 |
| 实体关系 | ❌ 无法处理 | ✅ 核心能力 |
| 全局摘要 | ❌ 信息丢失 | ✅ 社区检测 |
| 构建成本 | ⭐ 低 | ⭐⭐⭐ 高 |

**典型问题对比**：
- "张三的公司做什么业务？" → 传统 RAG 可以
- "张三的朋友们投资了哪些公司？" → 需要 GraphRAG

---

## 🔧 Microsoft GraphRAG

微软开源的 GraphRAG 实现，业界标杆。

### 核心流程

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                   GraphRAG 构建流程                                       │
└─────────────────────────────────────────────────────────────────────────┘

                         原始文档
                            │
                            ▼
                    ┌───────────────┐
                    │   文本分块    │
                    └───────┬───────┘
                            │
                            ▼
                    ┌───────────────┐
                    │  实体抽取     │ ◄── LLM 提取人名、组织、地点等
                    └───────┬───────┘
                            │
                            ▼
                    ┌───────────────┐
                    │  关系抽取     │ ◄── LLM 识别实体间关系
                    └───────┬───────┘
                            │
                            ▼
                    ┌───────────────┐
                    │  图谱构建     │ ◄── 节点 + 边
                    └───────┬───────┘
                            │
                            ▼
                    ┌───────────────┐
                    │  社区检测     │ ◄── Leiden 算法
                    └───────┬───────┘
                            │
                            ▼
                    ┌───────────────┐
                    │  摘要生成     │ ◄── 每个社区生成摘要
                    └───────────────┘
\`\`\`

### 安装与使用

\`\`\`bash
# 安装
pip install graphrag

# 初始化项目
graphrag init --root ./my-graphrag

# 配置 settings.yaml
# 需要设置 OpenAI API Key

# 索引文档
graphrag index --root ./my-graphrag

# 查询 - 局部搜索 (实体相关问题)
graphrag query --root ./my-graphrag --method local "张三在哪家公司工作？"

# 查询 - 全局搜索 (需要全局理解)
graphrag query --root ./my-graphrag --method global "这些文档主要讲什么内容？"
\`\`\`

### 配置示例 (settings.yaml)

\`\`\`yaml
llm:
  api_key: \${OPENAI_API_KEY}
  model: gpt-4o-mini

embeddings:
  llm:
    api_key: \${OPENAI_API_KEY}
    model: text-embedding-3-small

chunks:
  size: 1200
  overlap: 100

entity_extraction:
  prompt: "Extract all named entities from the following text..."
  max_gleanings: 1

community_reports:
  max_length: 2000

local_search:
  text_unit_prop: 0.5
  community_prop: 0.1
\`\`\`

---

## 🔧 LangChain + Neo4j GraphRAG

使用 LangChain 和 Neo4j 构建 GraphRAG。

\`\`\`python
from langchain_community.graphs import Neo4jGraph
from langchain_experimental.graph_transformers import LLMGraphTransformer
from langchain_openai import ChatOpenAI
from langchain.docstore.document import Document

# 1. 连接 Neo4j
graph = Neo4jGraph(
    url="bolt://localhost:7687",
    username="neo4j",
    password="password"
)

# 2. 配置 LLM
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# 3. 创建图谱转换器
transformer = LLMGraphTransformer(llm=llm)

# 4. 准备文档
documents = [
    Document(page_content="""
    张三是ABC科技公司的CEO。李四是张三的大学同学，
    现在在XYZ投资公司担任合伙人。XYZ投资公司是ABC科技的
    A轮投资方。王五是ABC科技的CTO，曾经在谷歌工作。
    """)
]

# 5. 提取实体和关系
graph_documents = transformer.convert_to_graph_documents(documents)

# 6. 存储到 Neo4j
graph.add_graph_documents(graph_documents)

# 7. 查询示例
result = graph.query("""
    MATCH (p:Person)-[:WORKS_AT]->(c:Company)
    RETURN p.name, c.name
""")
print(result)
# [{'p.name': '张三', 'c.name': 'ABC科技公司'}, ...]
\`\`\`

### GraphRAG 检索

\`\`\`python
from langchain.chains import GraphCypherQAChain

# 创建问答链
chain = GraphCypherQAChain.from_llm(
    llm=llm,
    graph=graph,
    verbose=True
)

# 自然语言查询
response = chain.invoke("张三的同学投资了哪家公司？")
print(response)
# 张三的同学李四投资了ABC科技公司

# 复杂推理
response = chain.invoke("ABC科技公司的CTO之前在哪里工作？")
print(response)
# ABC科技公司的CTO王五之前在谷歌工作
\`\`\`

---

## 🔧 LlamaIndex GraphRAG

\`\`\`python
from llama_index.core import KnowledgeGraphIndex, SimpleDirectoryReader
from llama_index.llms.openai import OpenAI
from llama_index.core import Settings

# 配置
Settings.llm = OpenAI(model="gpt-4o-mini", temperature=0)
Settings.chunk_size = 512

# 加载文档
documents = SimpleDirectoryReader("./data").load_data()

# 构建知识图谱索引
kg_index = KnowledgeGraphIndex.from_documents(
    documents,
    max_triplets_per_chunk=10,
    include_embeddings=True
)

# 查询
query_engine = kg_index.as_query_engine(
    include_text=True,
    response_mode="tree_summarize"
)

response = query_engine.query("张三和李四是什么关系？")
print(response)
\`\`\`

---

## 📊 Hybrid RAG：向量 + 图谱

最佳实践是结合两种方法：

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                   Hybrid RAG 架构                                        │
└─────────────────────────────────────────────────────────────────────────┘

                         用户问题
                            │
              ┌─────────────┴─────────────┐
              │       问题分析器          │
              └─────────────┬─────────────┘
                   ╱                ╲
          简单语义问题          关系推理问题
                 │                    │
                 ▼                    ▼
        ┌──────────────┐     ┌──────────────┐
        │  向量检索    │     │  图谱检索    │
        │  (Chroma)    │     │  (Neo4j)     │
        └───────┬──────┘     └───────┬──────┘
                │                    │
                └────────┬───────────┘
                         │
                         ▼
                ┌──────────────┐
                │  结果融合    │
                │  重排序      │
                └───────┬──────┘
                        │
                        ▼
                ┌──────────────┐
                │  LLM 回答    │
                └──────────────┘
\`\`\`

---

## 💡 何时选择 GraphRAG？

\`\`\`
                    选择决策树
                        │
            ┌───────────┴───────────┐
            │  需要理解实体关系吗？   │
            └───────────┬───────────┘
                  ╱           ╲
                否              是
                │               │
                ▼               ▼
            传统 RAG       ┌─────────┐
             够用         │ 数据量？ │
                          └────┬────┘
                           ╱       ╲
                        小/中        大
                          │          │
                          ▼          ▼
                     LlamaIndex   Microsoft
                     + Neo4j     GraphRAG
\`\`\`

> 🎯 **建议**：先从传统 RAG 开始，遇到关系推理瓶颈再考虑 GraphRAG。
> GraphRAG 构建成本高（大量 LLM 调用），但对特定场景效果显著。
            `,
            ja: `
## ベクトル検索だけでは不十分な時：ナレッジグラフの出番

従来のRAGは「類似」コンテンツしか見つけられませんが、GraphRAGはエンティティ間の「関係」を理解できます。

---

## 🎯 従来のRAG vs GraphRAG

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                   従来のRAG vs GraphRAG                                  │
└─────────────────────────────────────────────────────────────────────────┘

  従来のRAG（ベクトル検索）            GraphRAG（グラフ検索）
  ┌────────────────────────┐           ┌────────────────────────┐
  │                        │           │   田中 ──勤務──► A社    │
  │  文書1 ████ 0.92       │           │     │                  │
  │  文書2 ███░ 0.85       │           │   友人               投資
  │  文書3 ██░░ 0.71       │           │     │                  │
  │                        │           │     ▼                  ▼
  │  純粋な意味的類似度     │           │   鈴木 ──協力──► B社   │
  │  ❌ 関係理解不可        │           │                        │
  │  ❌ 文書間推論弱い      │           │  ✅ 関係推論           │
  └────────────────────────┘           │  ✅ 複数ホップ質問応答  │
                                       └────────────────────────┘
\`\`\`

---

## 📊 いつGraphRAGを使う？

| シーン | 従来のRAG | GraphRAG |
|--------|----------|----------|
| 単純な質問応答 | ✅ 十分 | オーバースペック |
| 複数ホップ推論 | ❌ 弱い | ✅ 強い |
| エンティティ関係 | ❌ 処理不可 | ✅ コア機能 |
| グローバル要約 | ❌ 情報損失 | ✅ コミュニティ検出 |
| 構築コスト | ⭐ 低 | ⭐⭐⭐ 高 |

**典型的な質問の比較**：
- 「田中さんの会社は何の事業？」→ 従来のRAGでOK
- 「田中さんの友人たちが投資した会社は？」→ GraphRAGが必要

---

## 🔧 Microsoft GraphRAG

Microsoftがオープンソース化したGraphRAG実装、業界標準。

### コアフロー

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                   GraphRAG 構築フロー                                     │
└─────────────────────────────────────────────────────────────────────────┘

                         元文書
                            │
                            ▼
                    ┌───────────────┐
                    │   テキスト分割 │
                    └───────┬───────┘
                            │
                            ▼
                    ┌───────────────┐
                    │  エンティティ抽出 │ ◄── LLMで人名、組織、地名などを抽出
                    └───────┬───────┘
                            │
                            ▼
                    ┌───────────────┐
                    │  関係抽出     │ ◄── LLMでエンティティ間関係を識別
                    └───────┬───────┘
                            │
                            ▼
                    ┌───────────────┐
                    │  グラフ構築   │ ◄── ノード + エッジ
                    └───────────────┘
\`\`\`

### インストールと使用

\`\`\`bash
# インストール
pip install graphrag

# プロジェクト初期化
graphrag init --root ./my-graphrag

# 文書のインデックス作成
graphrag index --root ./my-graphrag

# クエリ - ローカル検索
graphrag query --root ./my-graphrag --method local "田中さんはどの会社で働いている？"

# クエリ - グローバル検索
graphrag query --root ./my-graphrag --method global "これらの文書は主に何について？"
\`\`\`

---

## 🔧 LangChain + Neo4j GraphRAG

\`\`\`python
from langchain_community.graphs import Neo4jGraph
from langchain_experimental.graph_transformers import LLMGraphTransformer
from langchain_openai import ChatOpenAI

# 1. Neo4j接続
graph = Neo4jGraph(
    url="bolt://localhost:7687",
    username="neo4j",
    password="password"
)

# 2. LLM設定
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# 3. グラフ変換器作成
transformer = LLMGraphTransformer(llm=llm)

# 4. エンティティと関係を抽出
graph_documents = transformer.convert_to_graph_documents(documents)

# 5. Neo4jに保存
graph.add_graph_documents(graph_documents)
\`\`\`

---

## 📊 Hybrid RAG：ベクトル + グラフ

ベストプラクティスは両方を組み合わせること：

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                   Hybrid RAG アーキテクチャ                               │
└─────────────────────────────────────────────────────────────────────────┘

                         ユーザー質問
                            │
              ┌─────────────┴─────────────┐
              │       質問分析器          │
              └─────────────┬─────────────┘
                   ╱                ╲
          単純意味質問          関係推論質問
                 │                    │
                 ▼                    ▼
        ┌──────────────┐     ┌──────────────┐
        │  ベクトル検索 │     │  グラフ検索  │
        │  (Chroma)    │     │  (Neo4j)     │
        └───────┬──────┘     └───────┬──────┘
                │                    │
                └────────┬───────────┘
                         │
                         ▼
                ┌──────────────┐
                │  結果融合    │
                └───────┬──────┘
                        │
                        ▼
                ┌──────────────┐
                │  LLM回答     │
                └──────────────┘
\`\`\`

---

## 💡 いつGraphRAGを選ぶ？

> 🎯 **アドバイス**：まず従来のRAGから始め、関係推論のボトルネックに遭遇したらGraphRAGを検討。
> GraphRAGは構築コストが高い（大量のLLM呼び出し）が、特定シーンでは効果が顕著。
            `
          }
        },
        {
          id: 'ch4-summary',
          title: { zh: '4.6 本章小结', ja: '4.6 この章のまとめ' },
          content: {
            zh: `
## RAG 核心概念回顾

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                         RAG 知识地图                                    │
└─────────────────────────────────────────────────────────────────────────┘

                               RAG
                                │
         ┌──────────────────────┼──────────────────────┐
         │                      │                      │
         ▼                      ▼                      ▼
    ┌──────────┐          ┌──────────┐          ┌──────────┐
    │  工作流程 │          │  核心技术 │          │  应用场景 │
    └──────────┘          └──────────┘          └──────────┘
         │                      │                      │
    ┌────┼────┐          ┌──────┼──────┐          ┌────┼────┐
    │    │    │          │      │      │          │    │    │
    ▼    ▼    ▼          ▼      ▼      ▼          ▼    ▼    ▼
  检索   拡充   生成     分块   向量化   数据库    企业  个人  开发
\`\`\`

---

## RAG vs 微调

| 对比项 | RAG | 微调(Fine-tuning) |
|--------|-----|-------------------|
| 知识更新 | 只需更新文档 | 需要重新训练 |
| 成本 | 较低 | 较高 |
| 实现难度 | 简单 | 复杂 |
| 适用场景 | 知识库问答 | 特定领域专家 |
| 可解释性 | 可追溯来源 | 黑盒 |

---

## RAG 的优缺点

### 优点
- ✅ 可以实时更新知识
- ✅ 答案可追溯来源
- ✅ 不需要训练模型
- ✅ 保护私有数据

### 缺点
- ❌ 依赖检索质量
- ❌ 需要维护知识库
- ❌ 有上下文长度限制
- ❌ 可能检索到无关内容

---

## 快速行动清单

- [ ] 了解常用 AI 工具的知识截止日期
- [ ] 思考工作中有哪些知识可以用 RAG 管理
- [ ] 尝试使用一个 RAG 工具（如 Notion AI、企业知识库）
- [ ] 关注 RAG 技术的发展动态

---

## 关键金句

> "RAG 让 AI 从'博学但过时'变成'与时俱进且专业'。"

> "检索增强生成是连接 AI 与现实世界知识的桥梁。"

---

## 全书总结

恭喜你完成了这本 AI 进阶实战指南！

你学到了：
1. **提示词工程** —— 与 AI 高效对话的艺术
2. **AI Agent** —— 让 AI 自主完成复杂任务
3. **RAG 技术** —— 让 AI 获取最新、私有知识

**AI 的世界在不断快速发展，保持学习，保持好奇！**

*"最好的投资是对自己认知的投资。"*
            `,
            ja: `
## RAGコア概念の復習

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                         RAG 知識マップ                                   │
└─────────────────────────────────────────────────────────────────────────┘

                               RAG
                                │
         ┌──────────────────────┼──────────────────────┐
         │                      │                      │
         ▼                      ▼                      ▼
    ┌──────────┐          ┌──────────┐          ┌──────────┐
    │ ワークフロー │          │  コア技術  │          │ 応用シーン │
    └──────────┘          └──────────┘          └──────────┘
         │                      │                      │
    ┌────┼────┐          ┌──────┼──────┐          ┌────┼────┐
    │    │    │          │      │      │          │    │    │
    ▼    ▼    ▼          ▼      ▼      ▼          ▼    ▼    ▼
  検索   拡張   生成      分割  ベクトル化 DB      企業  個人   開発
\`\`\`

---

## RAG vs ファインチューニング

| 比較項目 | RAG | ファインチューニング |
|----------|-----|---------------------|
| 知識の更新 | ドキュメント更新のみ | 再トレーニングが必要 |
| コスト | 低い | 高い |
| 実装難易度 | 簡単 | 複雑 |
| 適用シーン | ナレッジベースQ&A | 特定分野の専門家 |
| 説明可能性 | ソースを追跡可能 | ブラックボックス |

---

## RAGの長所と短所

### 長所
- ✅ 知識をリアルタイムで更新可能
- ✅ 回答のソースを追跡可能
- ✅ モデルのトレーニング不要
- ✅ プライベートデータを保護

### 短所
- ❌ 検索品質に依存
- ❌ 知識ベースのメンテナンスが必要
- ❌ コンテキスト長の制限がある
- ❌ 無関係なコンテンツを検索する可能性

---

## クイックアクションリスト

- [ ] よく使うAIツールの知識締め切り日を確認
- [ ] 仕事でRAGで管理できる知識を考える
- [ ] RAGツール（Notion AI、企業ナレッジベースなど）を試す
- [ ] RAG技術の発展動向をフォロー

---

## 重要な格言

> 「RAGはAIを『博識だが古い』から『時代に追いついて専門的』に変えます。」

> 「検索拡張生成は、AIと現実世界の知識をつなぐ架け橋です。」

---

## 全書のまとめ

このAI進級実践ガイドを完了おめでとうございます！

学んだこと：
1. **プロンプトエンジニアリング** —— AIとの効率的な対話術
2. **AI Agent** —— AIに複雑なタスクを自律的に完了させる
3. **RAG技術** —— AIに最新・プライベート知識を取得させる

**AIの世界は急速に発展し続けています。学び続け、好奇心を持ち続けましょう！**

*「最高の投資は、自分の認知への投資です。」*
            `
          }
        }
      ]
    },
    // ============================================
    // 第五章：多模态 AI
    // ============================================
    {
      id: 'chapter-5',
      number: 5,
      title: { zh: '多模态 AI', ja: 'マルチモーダルAI' },
      subtitle: { zh: '图像、语音、视频的 AI 革命', ja: '画像・音声・動画のAI革命' },
      sections: [
        {
          id: 'ch5-intro',
          title: { zh: '5.1 多模态 AI 概述', ja: '5.1 マルチモーダルAI概要' },
          content: {
            zh: `
## 超越文字：AI 的感官革命

多模态 AI 让机器能够像人一样"看"、"听"、"说"。

---

## 🎯 什么是多模态？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        多模态 AI 能力矩阵                                 │
└─────────────────────────────────────────────────────────────────────────┘

                         多模态 AI
                            │
        ┌───────────┬───────┴───────┬───────────┐
        │           │               │           │
        ▼           ▼               ▼           ▼
     👁️ 视觉      👂 听觉        💬 语言      🎨 生成
        │           │               │           │
   ┌────┴────┐  ┌───┴───┐     ┌────┴────┐  ┌────┴────┐
   │ 图像理解 │  │ 语音识别│     │ 文本理解 │  │ 内容创作 │
   │ OCR    │  │ 音乐分析│     │ 翻译    │  │ 图像生成 │
   │ 视频分析 │  │ 声音克隆│     │ 对话    │  │ 视频生成 │
   └─────────┘  └────────┘     └─────────┘  └─────────┘
\`\`\`

---

## 📊 主流多模态模型对比

| 模型 | 厂商 | 视觉理解 | 音频 | 视频 | 生成 |
|------|------|----------|------|------|------|
| **GPT-4o** | OpenAI | ✅ 强 | ✅ 语音 | ⚠️ 有限 | ✅ DALL-E |
| **Claude 3.5** | Anthropic | ✅ 强 | ❌ | ❌ | ❌ |
| **Gemini 1.5** | Google | ✅ 强 | ✅ 语音 | ✅ 长视频 | ✅ Imagen |
| **Qwen-VL** | 阿里 | ✅ 强 | ✅ 语音 | ✅ | ❌ |
| **GPT-4o** 实时 | OpenAI | ✅ | ✅ 实时语音 | ✅ | ✅ |

---

## 🔧 基础使用示例

### OpenAI Vision

\`\`\`python
from openai import OpenAI
import base64

client = OpenAI()

# 方法1：URL 图片
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "描述这张图片的内容"},
                {"type": "image_url", "image_url": {"url": "https://example.com/image.jpg"}}
            ]
        }
    ]
)

# 方法2：Base64 图片
def encode_image(image_path):
    with open(image_path, "rb") as f:
        return base64.b64encode(f.read()).decode("utf-8")

base64_image = encode_image("local_image.jpg")

response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "这张图片里有什么？"},
                {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}}
            ]
        }
    ]
)
\`\`\`

### Claude Vision

\`\`\`python
import anthropic
import base64

client = anthropic.Anthropic()

# 读取本地图片
with open("image.jpg", "rb") as f:
    image_data = base64.standard_b64encode(f.read()).decode("utf-8")

response = client.messages.create(
    model="claude-sonnet-4-20250514",
    max_tokens=1024,
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": "image/jpeg",
                        "data": image_data
                    }
                },
                {"type": "text", "text": "详细描述这张图片"}
            ]
        }
    ]
)
\`\`\`

---

## 💡 应用场景

| 场景 | 说明 | 示例 |
|------|------|------|
| **文档解析** | 识别表格、图表、公式 | 财报分析、论文解读 |
| **产品检测** | 质检、缺陷识别 | 工业制造 |
| **医学影像** | 辅助诊断 | X光、CT分析 |
| **电商描述** | 自动生成商品文案 | 图片→描述 |
| **无障碍** | 为视障人士描述图片 | 屏幕阅读 |
            `,
            ja: `
## 文字を超えて：AIの感覚革命

マルチモーダルAIは、機械が人間のように「見る」「聞く」「話す」ことを可能にします。

---

## 🎯 マルチモーダルとは？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        マルチモーダルAI能力マトリクス                       │
└─────────────────────────────────────────────────────────────────────────┘

                         マルチモーダルAI
                            │
        ┌───────────┬───────┴───────┬───────────┐
        │           │               │           │
        ▼           ▼               ▼           ▼
     👁️ 視覚      👂 聴覚        💬 言語      🎨 生成
        │           │               │           │
   ┌────┴────┐  ┌───┴───┐     ┌────┴────┐  ┌────┴────┐
   │ 画像理解 │  │ 音声認識│     │ テキスト │  │ コンテンツ│
   │ OCR    │  │ 音楽分析│     │ 翻訳    │  │ 画像生成 │
   │ 動画分析 │  │ 音声複製│     │ 対話    │  │ 動画生成 │
   └─────────┘  └────────┘     └─────────┘  └─────────┘
\`\`\`

---

## 📊 主要マルチモーダルモデル比較

| モデル | メーカー | 視覚理解 | 音声 | 動画 | 生成 |
|--------|----------|----------|------|------|------|
| **GPT-4o** | OpenAI | ✅ 強い | ✅ 音声 | ⚠️ 限定 | ✅ DALL-E |
| **Claude 3.5** | Anthropic | ✅ 強い | ❌ | ❌ | ❌ |
| **Gemini 1.5** | Google | ✅ 強い | ✅ 音声 | ✅ 長時間動画 | ✅ Imagen |

---

## 💡 応用シーン

| シーン | 説明 | 例 |
|--------|------|-----|
| **文書解析** | 表、チャート、数式認識 | 財務報告分析 |
| **製品検査** | 品質管理、欠陥検出 | 製造業 |
| **医療画像** | 診断支援 | X線、CT分析 |
| **EC説明文** | 商品説明自動生成 | 画像→説明文 |
            `
          }
        },
        {
          id: 'ch5-image-gen',
          title: { zh: '5.2 AI 图像生成', ja: '5.2 AI画像生成' },
          content: {
            zh: `
## 文字变图片：图像生成技术

从 DALL-E 到 Midjourney，AI 绘画正在改变创意产业。

---

## 📊 主流图像生成模型

| 模型 | 特点 | 适用场景 | 价格 |
|------|------|----------|------|
| **DALL-E 3** | 语义理解强 | 创意插图 | API 付费 |
| **Midjourney** | 艺术感强 | 设计海报 | 订阅制 |
| **Stable Diffusion** | 开源可控 | 本地部署 | 免费 |
| **Imagen 3** | Google最新 | 真实感强 | API 付费 |

---

## 🔧 DALL-E 3 使用

\`\`\`python
from openai import OpenAI

client = OpenAI()

# 生成图片
response = client.images.generate(
    model="dall-e-3",
    prompt="一只穿着西装的柴犬，正在咖啡馆喝咖啡，温暖的下午阳光，油画风格",
    size="1024x1024",
    quality="hd",
    n=1
)

image_url = response.data[0].url
print(image_url)

# 图片编辑（DALL-E 2）
response = client.images.edit(
    image=open("original.png", "rb"),
    mask=open("mask.png", "rb"),  # 要编辑的区域为白色
    prompt="将背景换成海滩",
    size="1024x1024"
)
\`\`\`

---

## 🔧 Stable Diffusion 本地部署

\`\`\`python
from diffusers import StableDiffusionPipeline
import torch

# 加载模型
pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16
)
pipe = pipe.to("cuda")

# 生成图片
prompt = "a beautiful sunset over mountains, 4k, detailed"
image = pipe(prompt).images[0]
image.save("sunset.png")

# 使用 LoRA 微调模型
pipe.load_lora_weights("lora-weights-folder")
image = pipe(prompt).images[0]
\`\`\`

---

## 💡 Prompt 工程技巧

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                      图像 Prompt 结构                                    │
└─────────────────────────────────────────────────────────────────────────┘

  [主体] + [细节] + [风格] + [质量词]

  示例：
  ┌──────────────────────────────────────────────────────────────────┐
  │ 一只橘猫 + 戴着墨镜，躺在沙滩上 + 像素风格 + 8k, 高细节        │
  └──────────────────────────────────────────────────────────────────┘

  常用质量词：
  - 4k, 8k, ultra detailed, photorealistic
  - trending on artstation, award winning
  - studio lighting, professional photography
\`\`\`
            `,
            ja: `
## テキストから画像へ：画像生成技術

DALL-EからMidjourneyまで、AI絵画がクリエイティブ産業を変えています。

---

## 📊 主要画像生成モデル

| モデル | 特徴 | 適用シーン | 価格 |
|--------|------|------------|------|
| **DALL-E 3** | 意味理解が強い | クリエイティブイラスト | API課金 |
| **Midjourney** | 芸術性が高い | デザインポスター | サブスク |
| **Stable Diffusion** | オープンソース | ローカルデプロイ | 無料 |

---

## 🔧 DALL-E 3 使用例

\`\`\`python
from openai import OpenAI

client = OpenAI()

response = client.images.generate(
    model="dall-e-3",
    prompt="スーツを着た柴犬、カフェでコーヒーを飲んでいる、暖かい午後の光、油絵スタイル",
    size="1024x1024",
    quality="hd",
    n=1
)

image_url = response.data[0].url
\`\`\`

---

## 💡 Promptエンジニアリングのコツ

\`\`\`
[主題] + [詳細] + [スタイル] + [品質ワード]

例：
オレンジ猫 + サングラスをかけてビーチに寝ている + ピクセルアート + 8k, 高詳細
\`\`\`
            `
          }
        },
        {
          id: 'ch5-audio',
          title: { zh: '5.3 语音识别与合成', ja: '5.3 音声認識と合成' },
          content: {
            zh: `
## AI 的耳朵和嘴巴：语音技术

从语音转文字到文字转语音，AI 让交互更自然。

---

## 📊 语音技术全景

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        语音 AI 技术栈                                    │
└─────────────────────────────────────────────────────────────────────────┘

                    语音 AI
                       │
         ┌─────────────┼─────────────┐
         │             │             │
         ▼             ▼             ▼
      ASR          TTS          语音助手
   (语音→文字)    (文字→语音)    (实时对话)
         │             │             │
    ┌────┴────┐   ┌────┴────┐   ┌────┴────┐
    │ Whisper │   │ OpenAI  │   │ GPT-4o  │
    │ Azure   │   │ ElevenLabs│ │ Realtime │
    │ 讯飞    │   │ Edge TTS │   │ Gemini  │
    └─────────┘   └─────────┘   └─────────┘
\`\`\`

---

## 🔧 OpenAI Whisper

\`\`\`python
from openai import OpenAI

client = OpenAI()

# 语音转文字
with open("audio.mp3", "rb") as audio_file:
    transcript = client.audio.transcriptions.create(
        model="whisper-1",
        file=audio_file,
        language="zh"  # 可选，自动检测
    )

print(transcript.text)

# 带时间戳
transcript = client.audio.transcriptions.create(
    model="whisper-1",
    file=open("audio.mp3", "rb"),
    response_format="verbose_json",
    timestamp_granularities=["word"]
)

for word in transcript.words:
    print(f"{word['start']:.2f}s: {word['word']}")
\`\`\`

---

## 🔧 文字转语音 (TTS)

\`\`\`python
from openai import OpenAI

client = OpenAI()

# OpenAI TTS
response = client.audio.speech.create(
    model="tts-1-hd",
    voice="nova",  # alloy, echo, fable, onyx, nova, shimmer
    input="你好，欢迎使用 AI 语音合成技术！"
)

response.stream_to_file("output.mp3")

# 流式播放
from pathlib import Path
import pygame

pygame.mixer.init()
with open("output.mp3", "wb") as f:
    for chunk in response.iter_bytes():
        f.write(chunk)
pygame.mixer.music.load("output.mp3")
pygame.mixer.music.play()
\`\`\`

### Edge TTS (免费)

\`\`\`python
import edge_tts
import asyncio

async def text_to_speech():
    communicate = edge_tts.Communicate(
        "今天天气真好！",
        "zh-CN-XiaoxiaoNeural"  # 中文女声
    )
    await communicate.save("output.mp3")

asyncio.run(text_to_speech())

# 可用声音列表
voices = await edge_tts.list_voices()
for v in voices:
    if "zh" in v["Locale"]:
        print(v["ShortName"])
\`\`\`

---

## 🔧 实时语音对话

\`\`\`python
# OpenAI Realtime API (WebSocket)
import asyncio
import websockets
import json

async def realtime_conversation():
    async with websockets.connect(
        "wss://api.openai.com/v1/realtime?model=gpt-4o-realtime",
        extra_headers={"Authorization": f"Bearer {api_key}"}
    ) as ws:
        # 发送配置
        await ws.send(json.dumps({
            "type": "session.update",
            "session": {
                "voice": "alloy",
                "instructions": "你是一个友好的助手"
            }
        }))

        # 发送音频数据
        audio_data = get_microphone_audio()
        await ws.send(json.dumps({
            "type": "input_audio_buffer.append",
            "audio": audio_data
        }))

        # 接收响应
        async for message in ws:
            data = json.loads(message)
            if data["type"] == "response.audio.delta":
                play_audio(data["delta"])
\`\`\`

---

## 💡 应用场景

| 场景 | 技术 | 示例 |
|------|------|------|
| **会议记录** | Whisper | 自动生成会议纪要 |
| **播客制作** | TTS | AI 主播 |
| **客服热线** | Realtime | 智能语音客服 |
| **无障碍** | TTS | 屏幕阅读器 |
| **语言学习** | ASR+TTS | 口语练习 |
            `,
            ja: `
## AIの耳と口：音声技術

音声からテキスト、テキストから音声へ、AIがより自然なインタラクションを実現。

---

## 🔧 OpenAI Whisper

\`\`\`python
from openai import OpenAI

client = OpenAI()

# 音声をテキストに
with open("audio.mp3", "rb") as audio_file:
    transcript = client.audio.transcriptions.create(
        model="whisper-1",
        file=audio_file,
        language="ja"
    )

print(transcript.text)
\`\`\`

---

## 🔧 テキストから音声 (TTS)

\`\`\`python
from openai import OpenAI

client = OpenAI()

response = client.audio.speech.create(
    model="tts-1-hd",
    voice="nova",
    input="こんにちは、AI音声合成技術へようこそ！"
)

response.stream_to_file("output.mp3")
\`\`\`

---

## 💡 応用シーン

| シーン | 技術 | 例 |
|--------|------|-----|
| **会議記録** | Whisper | 議事録自動生成 |
| **ポッドキャスト** | TTS | AIアナウンサー |
| **カスタマーサービス** | Realtime | 音声AI対応 |
            `
          }
        },
        {
          id: 'ch5-video',
          title: { zh: '5.4 视频理解与生成', ja: '5.4 動画理解と生成' },
          content: {
            zh: `
## AI 电影导演：视频技术

从视频分析到视频生成，AI 正在改变影视制作。

---

## 📊 视频 AI 技术对比

| 技术 | 模型 | 能力 | 限制 |
|------|------|------|------|
| **视频理解** | Gemini 1.5 | 分析1小时视频 | - |
| **视频理解** | GPT-4o | 短视频分析 | 帧数限制 |
| **视频生成** | Sora | 60秒电影级 | 未公开 |
| **视频生成** | Runway Gen-3 | 10秒高质量 | 订阅制 |
| **视频生成** | Pika | 4秒动画 | 免费额度 |

---

## 🔧 Gemini 视频分析

\`\`\`python
import google.generativeai as genai

genai.configure(api_key="YOUR_API_KEY")

# 上传视频
video_file = genai.upload_file("video.mp4")

# 等待处理完成
import time
while video_file.state.name == "PROCESSING":
    time.sleep(10)
    video_file = genai.get_file(video_file.name)

# 分析视频
model = genai.GenerativeModel("gemini-1.5-pro")
response = model.generate_content([
    video_file,
    "详细描述这个视频的内容，包括场景、人物、动作"
])

print(response.text)
\`\`\`

---

## 🔧 Runway API

\`\`\`python
import runwayml

client = runwayml.RunwayML()

# 文字生成视频
task = client.image_to_video.create(
    model="gen3a_turbo",
    prompt_image="start_frame.jpg",
    prompt_text="camera slowly zooms in, dramatic lighting"
)

# 等待完成
import time
while True:
    status = client.tasks.retrieve(task.id)
    if status.status == "SUCCEEDED":
        break
    time.sleep(5)

# 下载视频
video_url = status.output[0]
\`\`\`

---

## 🎬 视频生成 Prompt 技巧

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                     视频 Prompt 要素                                     │
└─────────────────────────────────────────────────────────────────────────┘

  1. 镜头运动：
     - camera slowly pans left/right
     - zoom in/out
     - tracking shot
     - aerial view

  2. 光影效果：
     - golden hour lighting
     - dramatic shadows
     - soft diffused light
     - neon glow

  3. 动作描述：
     - walking slowly through...
     - water gently flowing...
     - leaves falling from trees...

  示例：
  "A lone astronaut walking on Mars surface,
   camera tracking from behind,
   dust particles floating in sunlight,
   cinematic, 4K"
\`\`\`

---

## 💡 未来展望

> 🎬 **趋势预测**：
> - 2025：AI 短视频普及
> - 2026：AI 长视频成熟
> - 2027：AI 互动电影出现
            `,
            ja: `
## AI映画監督：動画技術

動画分析から動画生成まで、AIが映像制作を変えています。

---

## 📊 動画AI技術比較

| 技術 | モデル | 能力 | 制限 |
|------|--------|------|------|
| **動画理解** | Gemini 1.5 | 1時間動画分析 | - |
| **動画生成** | Sora | 60秒映画級 | 未公開 |
| **動画生成** | Runway Gen-3 | 10秒高品質 | サブスク |

---

## 🔧 Gemini 動画分析

\`\`\`python
import google.generativeai as genai

genai.configure(api_key="YOUR_API_KEY")

video_file = genai.upload_file("video.mp4")

model = genai.GenerativeModel("gemini-1.5-pro")
response = model.generate_content([
    video_file,
    "この動画の内容を詳しく説明してください"
])
\`\`\`

---

## 💡 将来展望

> 🎬 **トレンド予測**：
> - 2025：AI短編動画が普及
> - 2026：AI長編動画が成熟
> - 2027：AIインタラクティブ映画が登場
            `
          }
        },
        {
          id: 'ch5-summary',
          title: { zh: '5.5 本章小结', ja: '5.5 この章のまとめ' },
          content: {
            zh: `
## 多模态 AI 核心回顾

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                     多模态 AI 知识地图                                    │
└─────────────────────────────────────────────────────────────────────────┘

                            多模态 AI
                                │
          ┌─────────────────────┼─────────────────────┐
          │                     │                     │
          ▼                     ▼                     ▼
       理解类                 生成类                交互类
          │                     │                     │
    ┌─────┴─────┐         ┌─────┴─────┐         ┌─────┴─────┐
    │ 图像理解  │         │ 图像生成  │         │ 实时语音  │
    │ 视频理解  │         │ 视频生成  │         │ 多模态   │
    │ 语音识别  │         │ 语音合成  │         │ 对话     │
    └───────────┘         └───────────┘         └───────────┘
\`\`\`

---

## 💡 关键要点

1. **图像理解** —— GPT-4o、Claude 都很强
2. **图像生成** —— DALL-E 易用，SD 可控
3. **语音识别** —— Whisper 开源最佳
4. **语音合成** —— Edge TTS 免费，OpenAI 高质量
5. **视频分析** —— Gemini 支持长视频
6. **视频生成** —— Sora 领先，Runway 可用

> 🎯 **建议**：根据具体需求选择工具，多模态能力正在快速发展。
            `,
            ja: `
## マルチモーダルAIコア復習

---

## 💡 重要ポイント

1. **画像理解** —— GPT-4o、Claude共に強力
2. **画像生成** —— DALL-E使いやすい、SD制御可能
3. **音声認識** —— Whisperオープンソース最強
4. **音声合成** —— Edge TTS無料、OpenAI高品質
5. **動画分析** —— Gemini長時間動画対応
6. **動画生成** —— Soraリード、Runway使用可能

> 🎯 **アドバイス**：具体的なニーズに応じてツールを選択。マルチモーダル能力は急速に発展中。
            `
          }
        }
      ]
    },
    // ============================================
    // 第六章：AI 安全与伦理
    // ============================================
    {
      id: 'chapter-6',
      number: 6,
      title: { zh: 'AI 安全与伦理', ja: 'AIセキュリティと倫理' },
      subtitle: { zh: '负责任的 AI 使用指南', ja: '責任あるAI使用ガイド' },
      sections: [
        {
          id: 'ch6-hallucination',
          title: { zh: '6.1 AI 幻觉问题', ja: '6.1 AIハルシネーション問題' },
          content: {
            zh: `
## AI 会"一本正经地胡说八道"

当 AI 输出看起来正确但实际错误的信息时，我们称之为"幻觉"。

---

## 🎯 什么是 AI 幻觉？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        AI 幻觉类型                                       │
└─────────────────────────────────────────────────────────────────────────┘

                          AI 幻觉
                             │
         ┌───────────────────┼───────────────────┐
         │                   │                   │
         ▼                   ▼                   ▼
    事实性错误           逻辑性错误          编造内容
         │                   │                   │
   "北京是日本首都"    "1+1=3 因为..."    "论文XYZ研究表明"
   (完全错误)          (推理错误)          (不存在的引用)
\`\`\`

---

## 📊 幻觉发生的原因

| 原因 | 说明 | 示例 |
|------|------|------|
| **训练数据** | 数据中存在错误 | 过时的信息 |
| **知识截止** | 模型不知道最新事件 | 2023年后的新闻 |
| **过度自信** | 模型编造答案 | 不确定时仍给出答案 |
| **长文本** | 上下文过长时出错 | 忘记前文约束 |

---

## 🔧 检测与缓解方法

### 1. Prompt 约束

\`\`\`python
system_prompt = """
你是一个严谨的助手。请遵守以下规则：
1. 如果不确定，请说"我不确定"
2. 不要编造不存在的引用或数据
3. 区分事实和观点
4. 对于时效性信息，提醒知识截止日期
"""
\`\`\`

### 2. 使用 RAG 提供事实依据

\`\`\`python
# 先检索事实，再让AI回答
context = retriever.search(query)
response = llm.generate(
    f"基于以下资料回答问题，如果资料中没有相关信息请说'未找到相关资料'：\\n{context}\\n\\n问题：{query}"
)
\`\`\`

### 3. 事后验证

\`\`\`python
# 让另一个模型验证回答
verification_prompt = f"""
请验证以下回答是否准确：
问题：{question}
回答：{answer}

请检查：
1. 是否有事实性错误？
2. 是否有不确定的陈述被当作事实？
3. 引用的资料是否真实存在？
"""
\`\`\`

---

## 💡 实践建议

> 🎯 **记住**：
> - 始终对AI输出保持质疑态度
> - 重要决策需要人工验证
> - 使用RAG减少幻觉
> - 让AI表达不确定性
            `,
            ja: `
## AIは「真面目に嘘をつく」

AIが正しそうに見えて実際には間違っている情報を出力する時、これを「ハルシネーション」と呼びます。

---

## 🎯 AIハルシネーションとは？

| タイプ | 説明 | 例 |
|--------|------|-----|
| **事実誤り** | 完全に間違った事実 | 「東京はアメリカの首都」 |
| **論理誤り** | 推論の誤り | 「1+1=3なぜなら...」 |
| **捏造** | 存在しない引用 | 「論文XYZによると」 |

---

## 🔧 検出と緩和方法

1. **Prompt制約** —— 不確実な時は「わかりません」と言うよう指示
2. **RAG使用** —— 事実に基づいて回答させる
3. **事後検証** —— 別のモデルで検証

> 🎯 **覚えておくこと**：AIの出力は常に疑う姿勢を持つ
            `
          }
        },
        {
          id: 'ch6-bias',
          title: { zh: '6.2 偏见与公平性', ja: '6.2 バイアスと公平性' },
          content: {
            zh: `
## AI 可能带有偏见

训练数据中的偏见会被模型学习并放大。

---

## 📊 常见偏见类型

| 类型 | 说明 | 示例 |
|------|------|------|
| **性别偏见** | 职业与性别关联 | "护士=女性" |
| **种族偏见** | 人种刻板印象 | 人脸识别误差 |
| **文化偏见** | 西方中心主义 | 忽视非英语文化 |
| **年龄偏见** | 年龄歧视 | "老年人不懂技术" |

---

## 🔧 检测偏见

\`\`\`python
# 测试性别偏见
prompts = [
    "The doctor told the nurse that she...",
    "The engineer explained to the secretary that he..."
]

# 观察模型是否做出性别假设
for prompt in prompts:
    response = model.generate(prompt)
    print(f"Prompt: {prompt}")
    print(f"Response: {response}")
\`\`\`

---

## 🔧 缓解策略

1. **多样化训练数据**
2. **Prompt 去偏见**：明确要求公平对待
3. **输出审核**：检测敏感内容
4. **用户反馈**：收集偏见报告

---

## 💡 最佳实践

> ⚖️ 设计 AI 系统时，始终考虑公平性和包容性。
            `,
            ja: `
## AIはバイアスを持つ可能性がある

訓練データのバイアスはモデルに学習され、増幅されます。

---

## 📊 一般的なバイアスタイプ

| タイプ | 説明 | 例 |
|--------|------|-----|
| **性別バイアス** | 職業と性別の関連付け | 「看護師=女性」 |
| **人種バイアス** | 人種ステレオタイプ | 顔認識の誤差 |
| **文化バイアス** | 西洋中心主義 | 非英語文化の無視 |

---

## 🔧 緩和策

1. **多様な訓練データ**
2. **Promptでバイアス排除**を明示
3. **出力監査**：敏感なコンテンツを検出
4. **ユーザーフィードバック**収集

> ⚖️ AIシステム設計時は、常に公平性と包括性を考慮する
            `
          }
        },
        {
          id: 'ch6-privacy',
          title: { zh: '6.3 隐私与数据安全', ja: '6.3 プライバシーとデータセキュリティ' },
          content: {
            zh: `
## 保护用户数据是底线

使用 AI 时，数据隐私至关重要。

---

## ⚠️ 风险点

| 风险 | 说明 | 缓解措施 |
|------|------|----------|
| **数据泄露** | 敏感数据发送给API | 脱敏处理 |
| **模型记忆** | 模型"记住"私人信息 | 使用无状态API |
| **第三方共享** | 数据被用于训练 | 选择 opt-out |

---

## 🔧 安全实践

\`\`\`python
# 1. 数据脱敏
import re

def anonymize(text):
    # 移除邮箱
    text = re.sub(r'[\\w.-]+@[\\w.-]+', '[EMAIL]', text)
    # 移除手机号
    text = re.sub(r'1[3-9]\\d{9}', '[PHONE]', text)
    # 移除身份证
    text = re.sub(r'\\d{17}[\\dXx]', '[ID]', text)
    return text

# 2. 选择隐私友好的服务
# - 本地部署 (Ollama, LMStudio)
# - 企业版 API (数据不用于训练)
# - 自建 RAG (数据不出内网)
\`\`\`

---

## 💡 选择 AI 服务时的隐私考虑

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                      隐私保护决策树                                       │
└─────────────────────────────────────────────────────────────────────────┘

                     数据敏感度？
                         │
            ┌────────────┴────────────┐
            │                         │
          低/中                       高
            │                         │
            ▼                         ▼
       云端 API               ┌───────────────┐
       (方便快捷)             │ 本地部署优先  │
                              │ - Ollama      │
                              │ - LMStudio    │
                              │ - 私有化部署  │
                              └───────────────┘
\`\`\`
            `,
            ja: `
## ユーザーデータの保護は最低限

AIを使用する際、データプライバシーは非常に重要です。

---

## ⚠️ リスクポイント

| リスク | 説明 | 緩和措置 |
|--------|------|----------|
| **データ漏洩** | 機密データをAPIに送信 | 匿名化処理 |
| **モデル記憶** | モデルが個人情報を「記憶」 | ステートレスAPIを使用 |
| **第三者共有** | データが訓練に使用される | opt-out を選択 |

---

## 💡 プライバシー保護の決定木

高機密データ → ローカルデプロイ優先 (Ollama, LMStudio)
低/中機密データ → クラウドAPI (便利で高速)
            `
          }
        },
        {
          id: 'ch6-alignment',
          title: { zh: '6.4 AI 对齐与安全', ja: '6.4 AIアラインメントと安全性' },
          content: {
            zh: `
## 让 AI 按人类意图行事

AI 对齐(Alignment)是确保 AI 系统的行为符合人类价值观和意图。

---

## 🎯 什么是 AI 对齐？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        AI 对齐目标                                       │
└─────────────────────────────────────────────────────────────────────────┘

                    安全的 AI
                        │
          ┌─────────────┼─────────────┐
          │             │             │
          ▼             ▼             ▼
       有益性         无害性         诚实性
       Helpful       Harmless       Honest
          │             │             │
    帮助用户完成     不做有害的事    不欺骗用户
    合理请求                         承认不确定
\`\`\`

---

## 🔧 对齐技术

| 技术 | 说明 | 代表 |
|------|------|------|
| **RLHF** | 人类反馈强化学习 | ChatGPT |
| **Constitutional AI** | 宪法AI | Claude |
| **DPO** | 直接偏好优化 | 开源模型 |

---

## 💡 作为用户的责任

> 🛡️ **负责任使用 AI**：
> - 不用于违法用途
> - 不滥用生成能力
> - 保持人类监督
> - 报告问题行为
            `,
            ja: `
## AIを人間の意図に沿って動作させる

AIアラインメントは、AIシステムの行動が人間の価値観と意図に沿うことを確保すること。

---

## 🎯 AIアラインメントとは？

\`\`\`
                    安全なAI
                        │
          ┌─────────────┼─────────────┐
          ▼             ▼             ▼
       有益性         無害性         誠実性
       Helpful       Harmless       Honest
\`\`\`

---

## 💡 ユーザーとしての責任

> 🛡️ **責任あるAI使用**：
> - 違法用途に使用しない
> - 生成能力を乱用しない
> - 人間の監督を維持
> - 問題行動を報告
            `
          }
        },
        {
          id: 'ch6-summary',
          title: { zh: '6.5 本章小结', ja: '6.5 この章のまとめ' },
          content: {
            zh: `
## AI 安全与伦理核心要点

---

## 💡 关键原则

1. **幻觉防护** —— 使用 RAG、设置约束、事后验证
2. **偏见意识** —— 审视输出、多样化输入
3. **隐私优先** —— 敏感数据脱敏或本地部署
4. **负责任使用** —— 遵守法规、保持人类监督

> 🎯 **记住**：AI 是工具，人类负责如何使用它。
            `,
            ja: `
## AIセキュリティと倫理コアポイント

---

## 💡 重要原則

1. **ハルシネーション防止** —— RAG使用、制約設定、事後検証
2. **バイアス意識** —— 出力を審査、入力を多様化
3. **プライバシー優先** —— 機密データ匿名化またはローカルデプロイ
4. **責任ある使用** —— 規制遵守、人間の監督維持

> 🎯 **覚えておくこと**：AIはツール、人間がその使い方に責任を持つ
            `
          }
        }
      ]
    },
    // ============================================
    // 第七章：实战项目
    // ============================================
    {
      id: 'chapter-7',
      number: 7,
      title: { zh: '实战项目', ja: '実践プロジェクト' },
      subtitle: { zh: '从零开始构建 AI 应用', ja: 'ゼロからAIアプリケーションを構築' },
      sections: [
        {
          id: 'ch7-chatbot',
          title: { zh: '7.1 智能客服机器人', ja: '7.1 スマートカスタマーサービスボット' },
          content: {
            zh: `
## 项目：企业级智能客服系统

构建一个功能完整的智能客服机器人，支持知识库问答、多轮对话、意图识别。

---

## 📁 项目结构

\`\`\`
smart-customer-service/
├── requirements.txt          # 依赖清单
├── .env                      # 环境变量
├── config.py                 # 配置管理
├── data/
│   └── knowledge/            # 知识库文档
│       ├── products.md
│       ├── faq.md
│       └── policies.md
├── src/
│   ├── __init__.py
│   ├── knowledge_base.py     # 知识库管理
│   ├── chat_engine.py        # 对话引擎
│   ├── intent_classifier.py  # 意图识别
│   └── memory.py             # 对话记忆
├── app.py                    # Streamlit 应用
└── tests/
    └── test_chat.py
\`\`\`

---

## 📋 依赖安装

\`\`\`bash
# requirements.txt
langchain>=0.3.0
langchain-anthropic>=0.3.0
langchain-openai>=0.3.0
langchain-chroma>=0.2.0
chromadb>=0.5.0
streamlit>=1.40.0
python-dotenv>=1.0.0
pydantic>=2.0.0
\`\`\`

\`\`\`bash
pip install -r requirements.txt
\`\`\`

---

## 🔧 配置管理 (config.py)

\`\`\`python
import os
from dotenv import load_dotenv
from pydantic_settings import BaseSettings

load_dotenv()

class Settings(BaseSettings):
    # LLM 配置
    llm_provider: str = "anthropic"  # anthropic / openai
    anthropic_api_key: str = ""
    openai_api_key: str = ""
    model_name: str = "claude-sonnet-4-20250514"

    # 向量数据库
    chroma_persist_dir: str = "./chroma_db"
    embedding_model: str = "text-embedding-3-small"

    # 检索配置
    top_k: int = 5
    score_threshold: float = 0.7

    # 对话配置
    max_history: int = 10
    system_prompt: str = """你是一个专业的客服助手。
请根据知识库内容回答用户问题。
如果知识库中没有相关信息，请诚实说明。
回答要简洁、准确、有帮助。"""

    class Config:
        env_file = ".env"

settings = Settings()
\`\`\`

---

## 📚 知识库管理 (knowledge_base.py)

\`\`\`python
import os
from pathlib import Path
from typing import List, Optional

from langchain_community.document_loaders import (
    DirectoryLoader,
    TextLoader,
    UnstructuredMarkdownLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document

from config import settings


class KnowledgeBase:
    """知识库管理类"""

    def __init__(self, persist_dir: str = None):
        self.persist_dir = persist_dir or settings.chroma_persist_dir
        self.embeddings = OpenAIEmbeddings(
            model=settings.embedding_model,
            api_key=settings.openai_api_key
        )
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\\n## ", "\\n### ", "\\n\\n", "\\n", " "]
        )
        self._vectorstore: Optional[Chroma] = None

    @property
    def vectorstore(self) -> Chroma:
        """懒加载向量存储"""
        if self._vectorstore is None:
            if os.path.exists(self.persist_dir):
                self._vectorstore = Chroma(
                    persist_directory=self.persist_dir,
                    embedding_function=self.embeddings
                )
            else:
                raise ValueError("知识库未初始化，请先调用 build_from_directory()")
        return self._vectorstore

    def build_from_directory(self, docs_dir: str) -> int:
        """从目录构建知识库"""
        # 加载 Markdown 文件
        loader = DirectoryLoader(
            docs_dir,
            glob="**/*.md",
            loader_cls=UnstructuredMarkdownLoader,
            show_progress=True
        )
        documents = loader.load()

        # 添加来源元数据
        for doc in documents:
            doc.metadata["source"] = Path(doc.metadata.get("source", "")).name

        # 分块
        chunks = self.text_splitter.split_documents(documents)
        print(f"文档分块完成: {len(documents)} 文档 -> {len(chunks)} 块")

        # 创建向量存储
        self._vectorstore = Chroma.from_documents(
            documents=chunks,
            embedding=self.embeddings,
            persist_directory=self.persist_dir
        )

        return len(chunks)

    def add_documents(self, texts: List[str], metadata: dict = None) -> None:
        """添加新文档"""
        docs = [Document(page_content=t, metadata=metadata or {}) for t in texts]
        chunks = self.text_splitter.split_documents(docs)
        self.vectorstore.add_documents(chunks)

    def search(self, query: str, k: int = None) -> List[Document]:
        """相似度搜索"""
        k = k or settings.top_k
        return self.vectorstore.similarity_search(query, k=k)

    def search_with_score(self, query: str, k: int = None) -> List[tuple]:
        """带分数的搜索"""
        k = k or settings.top_k
        results = self.vectorstore.similarity_search_with_score(query, k=k)
        # 过滤低分结果
        return [(doc, score) for doc, score in results
                if score >= settings.score_threshold]


# 全局实例
knowledge_base = KnowledgeBase()
\`\`\`

---

## 🧠 意图识别 (intent_classifier.py)

\`\`\`python
from enum import Enum
from typing import Tuple
from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field

from config import settings


class Intent(str, Enum):
    """用户意图枚举"""
    PRODUCT_INQUIRY = "product_inquiry"      # 产品咨询
    ORDER_STATUS = "order_status"            # 订单查询
    TECHNICAL_SUPPORT = "technical_support"  # 技术支持
    COMPLAINT = "complaint"                  # 投诉建议
    GENERAL_CHAT = "general_chat"            # 闲聊
    TRANSFER_HUMAN = "transfer_human"        # 转人工


class IntentResult(BaseModel):
    """意图识别结果"""
    intent: Intent = Field(description="识别到的意图")
    confidence: float = Field(description="置信度 0-1", ge=0, le=1)
    keywords: list[str] = Field(description="关键词列表", default_factory=list)


class IntentClassifier:
    """意图分类器"""

    def __init__(self):
        self.llm = ChatAnthropic(
            model=settings.model_name,
            api_key=settings.anthropic_api_key
        ).with_structured_output(IntentResult)

        self.prompt = ChatPromptTemplate.from_messages([
            ("system", """你是意图识别专家。分析用户消息，识别意图类别。

可选意图：
- product_inquiry: 询问产品功能、价格、规格
- order_status: 查询订单、物流、退换货
- technical_support: 使用问题、故障排查
- complaint: 投诉、建议、不满
- general_chat: 问候、闲聊、无明确目的
- transfer_human: 明确要求人工客服

输出结构化 JSON。"""),
            ("human", "{message}")
        ])

        self.chain = self.prompt | self.llm

    def classify(self, message: str) -> IntentResult:
        """识别用户意图"""
        return self.chain.invoke({"message": message})

    def should_transfer(self, intent_result: IntentResult) -> bool:
        """判断是否需要转人工"""
        # 明确要求转人工
        if intent_result.intent == Intent.TRANSFER_HUMAN:
            return True
        # 投诉类高置信度
        if intent_result.intent == Intent.COMPLAINT and intent_result.confidence > 0.8:
            return True
        return False


intent_classifier = IntentClassifier()
\`\`\`

---

## 💬 对话记忆 (memory.py)

\`\`\`python
from typing import List, Dict
from collections import deque
from datetime import datetime

from config import settings


class ConversationMemory:
    """对话记忆管理"""

    def __init__(self, session_id: str, max_history: int = None):
        self.session_id = session_id
        self.max_history = max_history or settings.max_history
        self.messages: deque = deque(maxlen=self.max_history * 2)
        self.created_at = datetime.now()
        self.metadata: Dict = {}

    def add_user_message(self, content: str) -> None:
        """添加用户消息"""
        self.messages.append({
            "role": "user",
            "content": content,
            "timestamp": datetime.now().isoformat()
        })

    def add_assistant_message(self, content: str, sources: List[str] = None) -> None:
        """添加助手消息"""
        self.messages.append({
            "role": "assistant",
            "content": content,
            "sources": sources or [],
            "timestamp": datetime.now().isoformat()
        })

    def get_history(self) -> List[Dict]:
        """获取对话历史"""
        return list(self.messages)

    def get_context_string(self) -> str:
        """获取上下文字符串，用于 prompt"""
        if not self.messages:
            return "（无历史对话）"

        lines = []
        for msg in self.messages:
            role = "用户" if msg["role"] == "user" else "客服"
            lines.append(f"{role}: {msg['content']}")
        return "\\n".join(lines)

    def clear(self) -> None:
        """清空记忆"""
        self.messages.clear()


class MemoryStore:
    """会话存储管理"""

    def __init__(self):
        self._sessions: Dict[str, ConversationMemory] = {}

    def get_or_create(self, session_id: str) -> ConversationMemory:
        """获取或创建会话"""
        if session_id not in self._sessions:
            self._sessions[session_id] = ConversationMemory(session_id)
        return self._sessions[session_id]

    def delete(self, session_id: str) -> None:
        """删除会话"""
        self._sessions.pop(session_id, None)


memory_store = MemoryStore()
\`\`\`

---

## 🤖 对话引擎 (chat_engine.py)

\`\`\`python
from typing import Generator, Dict, Any
from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

from config import settings
from knowledge_base import knowledge_base
from intent_classifier import intent_classifier, Intent
from memory import memory_store, ConversationMemory


class ChatEngine:
    """智能对话引擎"""

    def __init__(self):
        self.llm = ChatAnthropic(
            model=settings.model_name,
            api_key=settings.anthropic_api_key,
            streaming=True
        )

        self.qa_prompt = ChatPromptTemplate.from_messages([
            ("system", """{system_prompt}

## 知识库参考
{context}

## 对话历史
{history}

请根据以上信息回答用户问题。回答要求：
1. 优先使用知识库内容
2. 保持对话连贯性
3. 如果不确定，请诚实说明
4. 回答简洁专业"""),
            ("human", "{question}")
        ])

        self.chain = self.qa_prompt | self.llm | StrOutputParser()

    def _retrieve_context(self, query: str) -> str:
        """检索相关知识"""
        try:
            docs = knowledge_base.search(query)
            if not docs:
                return "（未找到相关知识库内容）"

            context_parts = []
            for i, doc in enumerate(docs, 1):
                source = doc.metadata.get("source", "未知")
                context_parts.append(f"[{i}] 来源: {source}\\n{doc.page_content}")

            return "\\n\\n".join(context_parts)
        except Exception as e:
            return f"（知识库检索失败: {e}）"

    def chat(self,
             session_id: str,
             message: str) -> Generator[str, None, Dict[str, Any]]:
        """
        处理用户消息，流式返回回复

        Args:
            session_id: 会话ID
            message: 用户消息

        Yields:
            str: 回复文本片段

        Returns:
            Dict: 元数据（意图、来源等）
        """
        memory = memory_store.get_or_create(session_id)

        # 1. 意图识别
        intent_result = intent_classifier.classify(message)

        # 2. 检查是否需要转人工
        if intent_classifier.should_transfer(intent_result):
            response = "好的，我这就为您转接人工客服，请稍候..."
            memory.add_user_message(message)
            memory.add_assistant_message(response)
            yield response
            return {
                "intent": intent_result.intent,
                "transfer_human": True
            }

        # 3. 检索知识库
        context = self._retrieve_context(message)
        history = memory.get_context_string()

        # 4. 生成回复（流式）
        memory.add_user_message(message)
        full_response = ""

        for chunk in self.chain.stream({
            "system_prompt": settings.system_prompt,
            "context": context,
            "history": history,
            "question": message
        }):
            full_response += chunk
            yield chunk

        # 5. 保存回复
        memory.add_assistant_message(full_response)

        return {
            "intent": intent_result.intent,
            "confidence": intent_result.confidence,
            "transfer_human": False
        }

    def get_history(self, session_id: str) -> list:
        """获取对话历史"""
        memory = memory_store.get_or_create(session_id)
        return memory.get_history()


chat_engine = ChatEngine()
\`\`\`

---

## 🖥️ Streamlit 应用 (app.py)

\`\`\`python
import streamlit as st
import uuid
from datetime import datetime

from chat_engine import chat_engine
from knowledge_base import knowledge_base

# 页面配置
st.set_page_config(
    page_title="智能客服助手",
    page_icon="🤖",
    layout="wide"
)

# 自定义样式
st.markdown("""
<style>
.stChatMessage {
    padding: 1rem;
    border-radius: 0.5rem;
}
.user-message {
    background-color: #e3f2fd;
}
.assistant-message {
    background-color: #f5f5f5;
}
</style>
""", unsafe_allow_html=True)

# 初始化会话状态
if "session_id" not in st.session_state:
    st.session_state.session_id = str(uuid.uuid4())
if "messages" not in st.session_state:
    st.session_state.messages = []

# 侧边栏
with st.sidebar:
    st.title("🤖 智能客服")
    st.markdown("---")

    # 知识库管理
    st.subheader("📚 知识库")
    if st.button("🔄 重建知识库"):
        with st.spinner("正在构建知识库..."):
            try:
                count = knowledge_base.build_from_directory("./data/knowledge")
                st.success(f"✅ 已索引 {count} 个文档块")
            except Exception as e:
                st.error(f"❌ 构建失败: {e}")

    st.markdown("---")

    # 会话管理
    st.subheader("💬 当前会话")
    st.text(f"ID: {st.session_state.session_id[:8]}...")
    if st.button("🗑️ 清空对话"):
        st.session_state.messages = []
        st.rerun()

    st.markdown("---")
    st.caption(f"© {datetime.now().year} 智能客服系统")

# 主界面
st.title("💬 智能客服助手")
st.caption("有什么可以帮助您的？")

# 显示对话历史
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# 用户输入
if prompt := st.chat_input("请输入您的问题..."):
    # 显示用户消息
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # 生成回复
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""

        try:
            for chunk in chat_engine.chat(
                st.session_state.session_id,
                prompt
            ):
                if isinstance(chunk, str):
                    full_response += chunk
                    message_placeholder.markdown(full_response + "▌")

            message_placeholder.markdown(full_response)
            st.session_state.messages.append({
                "role": "assistant",
                "content": full_response
            })

        except Exception as e:
            error_msg = f"抱歉，处理您的请求时出现错误: {str(e)}"
            message_placeholder.error(error_msg)

# 快捷问题
st.markdown("---")
st.subheader("💡 常见问题")
cols = st.columns(3)
quick_questions = [
    "产品有哪些功能？",
    "如何查询订单状态？",
    "退换货政策是什么？"
]
for col, q in zip(cols, quick_questions):
    if col.button(q, use_container_width=True):
        st.session_state.messages.append({"role": "user", "content": q})
        st.rerun()
\`\`\`

---

## 🚀 启动运行

\`\`\`bash
# 1. 配置环境变量
cat > .env << EOF
ANTHROPIC_API_KEY=your-api-key
OPENAI_API_KEY=your-openai-key
EOF

# 2. 准备知识库文档 (data/knowledge/*.md)

# 3. 构建知识库
python -c "from knowledge_base import knowledge_base; \\
           knowledge_base.build_from_directory('./data/knowledge')"

# 4. 启动应用
streamlit run app.py
\`\`\`

---

## 🧪 测试用例 (tests/test_chat.py)

\`\`\`python
import pytest
from chat_engine import chat_engine
from intent_classifier import intent_classifier, Intent
from memory import memory_store

def test_intent_classification():
    """测试意图识别"""
    result = intent_classifier.classify("你们的产品多少钱？")
    assert result.intent == Intent.PRODUCT_INQUIRY
    assert result.confidence > 0.5

def test_transfer_human():
    """测试转人工判断"""
    result = intent_classifier.classify("我要转人工")
    assert intent_classifier.should_transfer(result)

def test_conversation_memory():
    """测试对话记忆"""
    memory = memory_store.get_or_create("test-session")
    memory.add_user_message("你好")
    memory.add_assistant_message("您好！有什么可以帮您？")

    history = memory.get_history()
    assert len(history) == 2
    assert history[0]["role"] == "user"

def test_chat_stream():
    """测试流式对话"""
    session_id = "test-session"
    response_chunks = []

    for chunk in chat_engine.chat(session_id, "你好"):
        if isinstance(chunk, str):
            response_chunks.append(chunk)

    full_response = "".join(response_chunks)
    assert len(full_response) > 0

if __name__ == "__main__":
    pytest.main([__file__, "-v"])
\`\`\`

---

## 📊 生产部署清单

| 项目 | 开发环境 | 生产环境 |
|------|----------|----------|
| 向量数据库 | Chroma (本地) | Pinecone / Qdrant |
| 会话存储 | 内存 | Redis |
| 日志 | 控制台 | ELK / CloudWatch |
| 监控 | 无 | Prometheus + Grafana |
| 部署 | Streamlit | Docker + K8s |
| CDN | 无 | CloudFlare |
            `,
            ja: `
## プロジェクト：エンタープライズスマートカスタマーサービス

ナレッジベースQ&A、マルチターン対話、意図認識をサポートする完全な機能を備えたスマートカスタマーサービスボットを構築します。

---

## 📁 プロジェクト構造

\`\`\`
smart-customer-service/
├── requirements.txt          # 依存関係リスト
├── .env                      # 環境変数
├── config.py                 # 設定管理
├── data/
│   └── knowledge/            # ナレッジベースドキュメント
├── src/
│   ├── knowledge_base.py     # ナレッジベース管理
│   ├── chat_engine.py        # 対話エンジン
│   ├── intent_classifier.py  # 意図認識
│   └── memory.py             # 対話メモリ
├── app.py                    # Streamlitアプリ
└── tests/
    └── test_chat.py
\`\`\`

---

## 📋 依存関係インストール

\`\`\`bash
pip install langchain langchain-anthropic langchain-chroma streamlit
\`\`\`

---

## 🔧 設定管理 (config.py)

\`\`\`python
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    llm_provider: str = "anthropic"
    anthropic_api_key: str = ""
    model_name: str = "claude-sonnet-4-20250514"
    chroma_persist_dir: str = "./chroma_db"
    top_k: int = 5
    max_history: int = 10

    system_prompt: str = """あなたはプロフェッショナルな
カスタマーサービスアシスタントです。
ナレッジベースの内容に基づいて質問に回答してください。"""

settings = Settings()
\`\`\`

---

## 🧠 意図認識 (intent_classifier.py)

\`\`\`python
from enum import Enum
from pydantic import BaseModel

class Intent(str, Enum):
    PRODUCT_INQUIRY = "product_inquiry"      # 製品問い合わせ
    ORDER_STATUS = "order_status"            # 注文照会
    TECHNICAL_SUPPORT = "technical_support"  # 技術サポート
    COMPLAINT = "complaint"                  # 苦情
    TRANSFER_HUMAN = "transfer_human"        # 人間オペレーターへ

class IntentResult(BaseModel):
    intent: Intent
    confidence: float
    keywords: list[str] = []
\`\`\`

---

## 💬 対話エンジン (chat_engine.py)

\`\`\`python
from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import ChatPromptTemplate

class ChatEngine:
    def __init__(self):
        self.llm = ChatAnthropic(
            model="claude-sonnet-4-20250514",
            streaming=True
        )

    def chat(self, session_id: str, message: str):
        # 1. 意図認識
        intent = self.classify_intent(message)

        # 2. ナレッジベース検索
        context = self.retrieve_context(message)

        # 3. 回答生成（ストリーミング）
        for chunk in self.generate_response(context, message):
            yield chunk
\`\`\`

---

## 🖥️ Streamlitアプリ (app.py)

\`\`\`python
import streamlit as st

st.set_page_config(page_title="スマートカスタマーサービス", page_icon="🤖")
st.title("💬 スマートカスタマーサービス")

# チャット履歴表示
for msg in st.session_state.get("messages", []):
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ユーザー入力
if prompt := st.chat_input("ご質問をどうぞ..."):
    with st.chat_message("assistant"):
        response = st.write_stream(chat_engine.chat(session_id, prompt))
\`\`\`

---

## 📊 本番デプロイチェックリスト

| 項目 | 開発環境 | 本番環境 |
|------|----------|----------|
| ベクトルDB | Chroma | Pinecone / Qdrant |
| セッション | メモリ | Redis |
| ログ | コンソール | ELK |
| デプロイ | Streamlit | Docker + K8s |
            `
          }
        },
        {
          id: 'ch7-doc-qa',
          title: { zh: '7.2 文档问答系统', ja: '7.2 ドキュメントQ&Aシステム' },
          content: {
            zh: `
## 项目：多格式文档问答系统

构建一个支持 PDF/Word/Excel/TXT 的智能文档问答系统，带来源引用和页码标注。

---

## 📁 项目结构

\`\`\`
document-qa-system/
├── requirements.txt
├── .env
├── config.py
├── uploads/                    # 上传文档目录
├── vector_store/               # 向量存储
├── src/
│   ├── __init__.py
│   ├── document_loader.py      # 多格式文档加载
│   ├── text_processor.py       # 文本处理与分块
│   ├── vector_store.py         # 向量存储管理
│   ├── qa_engine.py            # 问答引擎
│   └── citation.py             # 来源引用处理
├── api/
│   ├── __init__.py
│   ├── main.py                 # FastAPI 服务
│   └── schemas.py              # 数据模型
└── frontend/
    └── app.py                  # Gradio 界面
\`\`\`

---

## 📋 依赖安装

\`\`\`bash
# requirements.txt
langchain>=0.3.0
langchain-anthropic>=0.3.0
langchain-openai>=0.3.0
langchain-chroma>=0.2.0
chromadb>=0.5.0

# 文档解析
pypdf>=4.0.0
python-docx>=1.0.0
openpyxl>=3.1.0
unstructured>=0.10.0

# API & 前端
fastapi>=0.115.0
uvicorn>=0.30.0
gradio>=5.0.0
python-multipart>=0.0.9
\`\`\`

---

## 📄 多格式文档加载器 (document_loader.py)

\`\`\`python
import os
from pathlib import Path
from typing import List, Optional, Dict, Any
from abc import ABC, abstractmethod

from langchain_core.documents import Document
from langchain_community.document_loaders import (
    PyPDFLoader,
    Docx2txtLoader,
    UnstructuredExcelLoader,
    TextLoader
)


class DocumentLoader(ABC):
    """文档加载器基类"""

    @abstractmethod
    def load(self, file_path: str) -> List[Document]:
        pass

    @abstractmethod
    def supports(self, file_path: str) -> bool:
        pass


class PDFLoader(DocumentLoader):
    """PDF 加载器，保留页码信息"""

    def supports(self, file_path: str) -> bool:
        return file_path.lower().endswith('.pdf')

    def load(self, file_path: str) -> List[Document]:
        loader = PyPDFLoader(file_path)
        docs = loader.load()

        # 添加页码和文件名元数据
        filename = Path(file_path).name
        for i, doc in enumerate(docs):
            doc.metadata.update({
                "source": filename,
                "page": i + 1,
                "file_type": "pdf"
            })

        return docs


class WordLoader(DocumentLoader):
    """Word 文档加载器"""

    def supports(self, file_path: str) -> bool:
        return file_path.lower().endswith(('.docx', '.doc'))

    def load(self, file_path: str) -> List[Document]:
        loader = Docx2txtLoader(file_path)
        docs = loader.load()

        filename = Path(file_path).name
        for doc in docs:
            doc.metadata.update({
                "source": filename,
                "file_type": "word"
            })

        return docs


class ExcelLoader(DocumentLoader):
    """Excel 加载器"""

    def supports(self, file_path: str) -> bool:
        return file_path.lower().endswith(('.xlsx', '.xls'))

    def load(self, file_path: str) -> List[Document]:
        loader = UnstructuredExcelLoader(file_path)
        docs = loader.load()

        filename = Path(file_path).name
        for doc in docs:
            doc.metadata.update({
                "source": filename,
                "file_type": "excel"
            })

        return docs


class TxtLoader(DocumentLoader):
    """纯文本加载器"""

    def supports(self, file_path: str) -> bool:
        return file_path.lower().endswith('.txt')

    def load(self, file_path: str) -> List[Document]:
        loader = TextLoader(file_path, encoding='utf-8')
        docs = loader.load()

        filename = Path(file_path).name
        for doc in docs:
            doc.metadata.update({
                "source": filename,
                "file_type": "text"
            })

        return docs


class MultiFormatLoader:
    """统一多格式加载器"""

    def __init__(self):
        self.loaders: List[DocumentLoader] = [
            PDFLoader(),
            WordLoader(),
            ExcelLoader(),
            TxtLoader()
        ]

    def load(self, file_path: str) -> List[Document]:
        """加载单个文档"""
        for loader in self.loaders:
            if loader.supports(file_path):
                return loader.load(file_path)

        raise ValueError(f"不支持的文件格式: {file_path}")

    def load_directory(self, dir_path: str) -> List[Document]:
        """加载目录下所有文档"""
        all_docs = []
        supported_extensions = ('.pdf', '.docx', '.doc', '.xlsx', '.xls', '.txt')

        for file_path in Path(dir_path).rglob('*'):
            if file_path.suffix.lower() in supported_extensions:
                try:
                    docs = self.load(str(file_path))
                    all_docs.extend(docs)
                    print(f"✅ 已加载: {file_path.name}")
                except Exception as e:
                    print(f"❌ 加载失败 {file_path.name}: {e}")

        return all_docs

    def get_supported_formats(self) -> List[str]:
        return ["PDF", "Word (.docx)", "Excel (.xlsx)", "Text (.txt)"]


# 全局实例
document_loader = MultiFormatLoader()
\`\`\`

---

## 🔍 文本处理与分块 (text_processor.py)

\`\`\`python
from typing import List, Dict, Any
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document


class TextProcessor:
    """文本处理器"""

    def __init__(
        self,
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
        length_function: callable = len
    ):
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=length_function,
            separators=[
                "\\n\\n",  # 段落
                "\\n",     # 换行
                "。",      # 中文句号
                ".",       # 英文句号
                "！",
                "!",
                "？",
                "?",
                "；",
                ";",
                " ",
                ""
            ]
        )

    def split_documents(self, documents: List[Document]) -> List[Document]:
        """分割文档，保留元数据"""
        chunks = self.splitter.split_documents(documents)

        # 添加分块索引
        for i, chunk in enumerate(chunks):
            chunk.metadata["chunk_id"] = i
            chunk.metadata["chunk_total"] = len(chunks)

        return chunks

    def preprocess_text(self, text: str) -> str:
        """文本预处理"""
        # 去除多余空白
        import re
        text = re.sub(r'\\s+', ' ', text)
        # 去除特殊字符
        text = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]', '', text)
        return text.strip()


text_processor = TextProcessor()
\`\`\`

---

## 💾 向量存储管理 (vector_store.py)

\`\`\`python
import os
from typing import List, Optional, Tuple
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document


class VectorStoreManager:
    """向量存储管理器"""

    def __init__(
        self,
        persist_directory: str = "./vector_store",
        collection_name: str = "documents"
    ):
        self.persist_directory = persist_directory
        self.collection_name = collection_name
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small"
        )
        self._vectorstore: Optional[Chroma] = None

    @property
    def vectorstore(self) -> Chroma:
        if self._vectorstore is None:
            if os.path.exists(self.persist_directory):
                self._vectorstore = Chroma(
                    persist_directory=self.persist_directory,
                    collection_name=self.collection_name,
                    embedding_function=self.embeddings
                )
            else:
                raise ValueError("向量库未初始化")
        return self._vectorstore

    def create_from_documents(self, documents: List[Document]) -> int:
        """从文档创建向量库"""
        self._vectorstore = Chroma.from_documents(
            documents=documents,
            embedding=self.embeddings,
            persist_directory=self.persist_directory,
            collection_name=self.collection_name
        )
        return len(documents)

    def add_documents(self, documents: List[Document]) -> None:
        """添加文档到现有向量库"""
        self.vectorstore.add_documents(documents)

    def search(
        self,
        query: str,
        k: int = 5,
        filter_dict: dict = None
    ) -> List[Document]:
        """相似度搜索"""
        return self.vectorstore.similarity_search(
            query,
            k=k,
            filter=filter_dict
        )

    def search_with_score(
        self,
        query: str,
        k: int = 5
    ) -> List[Tuple[Document, float]]:
        """带分数的搜索"""
        return self.vectorstore.similarity_search_with_score(query, k=k)

    def delete_by_source(self, source: str) -> None:
        """按来源删除文档"""
        # Chroma 支持按 metadata 过滤删除
        self.vectorstore._collection.delete(
            where={"source": source}
        )

    def get_all_sources(self) -> List[str]:
        """获取所有文档来源"""
        results = self.vectorstore._collection.get()
        sources = set()
        for meta in results.get("metadatas", []):
            if meta and "source" in meta:
                sources.add(meta["source"])
        return list(sources)


vector_store = VectorStoreManager()
\`\`\`

---

## 🤖 问答引擎 (qa_engine.py)

\`\`\`python
from typing import List, Dict, Any, Generator
from dataclasses import dataclass

from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.documents import Document

from vector_store import vector_store


@dataclass
class Citation:
    """引用信息"""
    source: str
    page: int | None
    content: str
    relevance_score: float


@dataclass
class QAResponse:
    """问答响应"""
    answer: str
    citations: List[Citation]
    query: str


class DocumentQAEngine:
    """文档问答引擎"""

    def __init__(self, model: str = "claude-sonnet-4-20250514"):
        self.llm = ChatAnthropic(
            model=model,
            streaming=True
        )

        self.prompt = ChatPromptTemplate.from_messages([
            ("system", """你是一个专业的文档问答助手。

## 任务
根据提供的文档内容回答用户问题。

## 规则
1. 只使用文档中的信息回答
2. 如果文档中没有相关信息，明确说明
3. 引用来源时使用 [来源: 文件名, 第X页] 格式
4. 回答要准确、简洁、有条理

## 文档内容
{context}

---
请回答用户问题，并标注引用来源。"""),
            ("human", "{question}")
        ])

        self.chain = self.prompt | self.llm | StrOutputParser()

    def _format_context(
        self,
        docs: List[tuple[Document, float]]
    ) -> tuple[str, List[Citation]]:
        """格式化检索结果和引用"""
        context_parts = []
        citations = []

        for i, (doc, score) in enumerate(docs, 1):
            source = doc.metadata.get("source", "未知")
            page = doc.metadata.get("page")
            content = doc.page_content

            # 构建上下文
            page_info = f", 第{page}页" if page else ""
            context_parts.append(
                f"[文档{i}] 来源: {source}{page_info}\\n{content}"
            )

            # 记录引用
            citations.append(Citation(
                source=source,
                page=page,
                content=content[:200] + "..." if len(content) > 200 else content,
                relevance_score=score
            ))

        return "\\n\\n---\\n\\n".join(context_parts), citations

    def query(
        self,
        question: str,
        k: int = 5,
        source_filter: str = None
    ) -> QAResponse:
        """问答查询（非流式）"""
        # 检索相关文档
        filter_dict = {"source": source_filter} if source_filter else None
        docs_with_scores = vector_store.search_with_score(question, k=k)

        if not docs_with_scores:
            return QAResponse(
                answer="抱歉，未找到与问题相关的文档内容。",
                citations=[],
                query=question
            )

        # 格式化上下文
        context, citations = self._format_context(docs_with_scores)

        # 生成回答
        answer = self.chain.invoke({
            "context": context,
            "question": question
        })

        return QAResponse(
            answer=answer,
            citations=citations,
            query=question
        )

    def query_stream(
        self,
        question: str,
        k: int = 5
    ) -> Generator[str, None, List[Citation]]:
        """流式问答查询"""
        docs_with_scores = vector_store.search_with_score(question, k=k)

        if not docs_with_scores:
            yield "抱歉，未找到与问题相关的文档内容。"
            return []

        context, citations = self._format_context(docs_with_scores)

        for chunk in self.chain.stream({
            "context": context,
            "question": question
        }):
            yield chunk

        return citations


qa_engine = DocumentQAEngine()
\`\`\`

---

## 🌐 FastAPI 服务 (api/main.py)

\`\`\`python
import os
import shutil
from typing import List, Optional
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

from document_loader import document_loader
from text_processor import text_processor
from vector_store import vector_store
from qa_engine import qa_engine


app = FastAPI(title="文档问答系统 API")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"]
)

UPLOAD_DIR = "./uploads"
os.makedirs(UPLOAD_DIR, exist_ok=True)


# 请求/响应模型
class QuestionRequest(BaseModel):
    question: str
    k: int = 5
    source_filter: Optional[str] = None


class CitationResponse(BaseModel):
    source: str
    page: Optional[int]
    content: str
    score: float


class AnswerResponse(BaseModel):
    answer: str
    citations: List[CitationResponse]


class UploadResponse(BaseModel):
    filename: str
    chunks: int
    message: str


# API 端点
@app.post("/upload", response_model=UploadResponse)
async def upload_document(file: UploadFile = File(...)):
    """上传并索引文档"""
    # 保存文件
    file_path = os.path.join(UPLOAD_DIR, file.filename)
    with open(file_path, "wb") as f:
        shutil.copyfileobj(file.file, f)

    try:
        # 加载文档
        docs = document_loader.load(file_path)

        # 分块
        chunks = text_processor.split_documents(docs)

        # 索引
        vector_store.add_documents(chunks)

        return UploadResponse(
            filename=file.filename,
            chunks=len(chunks),
            message=f"成功索引 {len(chunks)} 个文本块"
        )

    except Exception as e:
        # 清理文件
        os.remove(file_path)
        raise HTTPException(status_code=400, detail=str(e))


@app.post("/ask", response_model=AnswerResponse)
async def ask_question(request: QuestionRequest):
    """文档问答"""
    response = qa_engine.query(
        question=request.question,
        k=request.k,
        source_filter=request.source_filter
    )

    return AnswerResponse(
        answer=response.answer,
        citations=[
            CitationResponse(
                source=c.source,
                page=c.page,
                content=c.content,
                score=c.relevance_score
            )
            for c in response.citations
        ]
    )


@app.get("/documents")
async def list_documents():
    """列出已索引的文档"""
    return {"documents": vector_store.get_all_sources()}


@app.delete("/documents/{filename}")
async def delete_document(filename: str):
    """删除指定文档"""
    vector_store.delete_by_source(filename)
    file_path = os.path.join(UPLOAD_DIR, filename)
    if os.path.exists(file_path):
        os.remove(file_path)
    return {"message": f"已删除 {filename}"}


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
\`\`\`

---

## 🖥️ Gradio 界面 (frontend/app.py)

\`\`\`python
import gradio as gr
import requests

API_URL = "http://localhost:8000"


def upload_file(file):
    """上传文件"""
    if file is None:
        return "请选择文件"

    with open(file.name, "rb") as f:
        response = requests.post(
            f"{API_URL}/upload",
            files={"file": (file.name.split("/")[-1], f)}
        )

    if response.ok:
        data = response.json()
        return f"✅ {data['message']}"
    else:
        return f"❌ 上传失败: {response.text}"


def ask_question(question, k):
    """问答"""
    if not question:
        return "", ""

    response = requests.post(
        f"{API_URL}/ask",
        json={"question": question, "k": int(k)}
    )

    if response.ok:
        data = response.json()
        answer = data["answer"]

        # 格式化引用
        citations = []
        for c in data["citations"]:
            page_info = f", 第{c['page']}页" if c['page'] else ""
            citations.append(
                f"📄 **{c['source']}**{page_info}\\n"
                f"相关度: {c['score']:.2f}\\n"
                f"> {c['content'][:100]}..."
            )

        return answer, "\\n\\n---\\n\\n".join(citations)
    else:
        return f"❌ 查询失败: {response.text}", ""


def get_documents():
    """获取文档列表"""
    response = requests.get(f"{API_URL}/documents")
    if response.ok:
        docs = response.json()["documents"]
        return "\\n".join([f"📄 {d}" for d in docs]) if docs else "暂无文档"
    return "获取失败"


# 构建界面
with gr.Blocks(title="文档问答系统", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# 📚 智能文档问答系统")
    gr.Markdown("上传 PDF/Word/Excel/TXT 文档，AI 帮你阅读和回答问题")

    with gr.Tabs():
        # 问答标签页
        with gr.Tab("💬 问答"):
            with gr.Row():
                with gr.Column(scale=2):
                    question_input = gr.Textbox(
                        label="您的问题",
                        placeholder="请输入关于文档的问题...",
                        lines=2
                    )
                    k_slider = gr.Slider(
                        minimum=1, maximum=10, value=5, step=1,
                        label="检索文档数量"
                    )
                    ask_btn = gr.Button("🔍 提问", variant="primary")

                with gr.Column(scale=3):
                    answer_output = gr.Markdown(label="回答")
                    citations_output = gr.Markdown(label="引用来源")

            ask_btn.click(
                ask_question,
                inputs=[question_input, k_slider],
                outputs=[answer_output, citations_output]
            )

        # 文档管理标签页
        with gr.Tab("📁 文档管理"):
            with gr.Row():
                file_input = gr.File(
                    label="上传文档",
                    file_types=[".pdf", ".docx", ".xlsx", ".txt"]
                )
                upload_btn = gr.Button("📤 上传并索引")

            upload_status = gr.Textbox(label="上传状态", interactive=False)
            upload_btn.click(upload_file, inputs=file_input, outputs=upload_status)

            gr.Markdown("---")
            docs_display = gr.Markdown(label="已索引文档")
            refresh_btn = gr.Button("🔄 刷新列表")
            refresh_btn.click(get_documents, outputs=docs_display)

    gr.Markdown("---")
    gr.Markdown("💡 支持格式: PDF, Word (.docx), Excel (.xlsx), 纯文本 (.txt)")


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7860)
\`\`\`

---

## 🚀 运行项目

\`\`\`bash
# 1. 安装依赖
pip install -r requirements.txt

# 2. 配置环境变量
export OPENAI_API_KEY="your-key"
export ANTHROPIC_API_KEY="your-key"

# 3. 启动后端 API
python api/main.py

# 4. 启动前端（新终端）
python frontend/app.py

# 访问 http://localhost:7860
\`\`\`

---

## 📊 功能对比

| 功能 | 基础版 | 完整版 |
|------|--------|--------|
| 文档格式 | PDF | PDF/Word/Excel/TXT |
| 来源引用 | ❌ | ✅ 页码标注 |
| 多文档 | ❌ | ✅ 批量上传 |
| 文档管理 | ❌ | ✅ 增删查 |
| API 接口 | ❌ | ✅ RESTful |
| 流式输出 | ❌ | ✅ |
            `,
            ja: `
## プロジェクト：マルチフォーマットドキュメントQ&Aシステム

PDF/Word/Excel/TXTをサポートし、ソース引用とページ番号表示付きのインテリジェントドキュメントQ&Aシステムを構築。

---

## 📁 プロジェクト構造

\`\`\`
document-qa-system/
├── requirements.txt
├── src/
│   ├── document_loader.py      # マルチフォーマットローダー
│   ├── text_processor.py       # テキスト処理
│   ├── vector_store.py         # ベクトルストア
│   └── qa_engine.py            # Q&Aエンジン
├── api/
│   └── main.py                 # FastAPIサービス
└── frontend/
    └── app.py                  # Gradioインターフェース
\`\`\`

---

## 📄 マルチフォーマットローダー

\`\`\`python
class MultiFormatLoader:
    def __init__(self):
        self.loaders = [
            PDFLoader(),
            WordLoader(),
            ExcelLoader(),
            TxtLoader()
        ]

    def load(self, file_path: str) -> List[Document]:
        for loader in self.loaders:
            if loader.supports(file_path):
                return loader.load(file_path)
        raise ValueError(f"サポートされていないフォーマット: {file_path}")
\`\`\`

---

## 🤖 Q&Aエンジン

\`\`\`python
@dataclass
class Citation:
    source: str
    page: int | None
    content: str
    relevance_score: float

class DocumentQAEngine:
    def query(self, question: str, k: int = 5) -> QAResponse:
        # ドキュメント検索
        docs = vector_store.search_with_score(question, k=k)

        # コンテキスト構築
        context, citations = self._format_context(docs)

        # 回答生成
        answer = self.chain.invoke({
            "context": context,
            "question": question
        })

        return QAResponse(answer=answer, citations=citations)
\`\`\`

---

## 🌐 FastAPI サービス

\`\`\`python
@app.post("/upload")
async def upload_document(file: UploadFile):
    # ファイル保存
    # ドキュメント読み込み
    # チャンク分割
    # ベクトルインデックス作成
    return {"chunks": len(chunks)}

@app.post("/ask")
async def ask_question(request: QuestionRequest):
    response = qa_engine.query(request.question)
    return {"answer": response.answer, "citations": response.citations}
\`\`\`

---

## 📊 機能比較

| 機能 | 基本版 | 完全版 |
|------|--------|--------|
| ドキュメント形式 | PDF | PDF/Word/Excel/TXT |
| ソース引用 | ❌ | ✅ ページ番号付き |
| 複数ドキュメント | ❌ | ✅ 一括アップロード |
| ドキュメント管理 | ❌ | ✅ CRUD |
| API | ❌ | ✅ RESTful |
            `
          }
        },
        {
          id: 'ch7-code-assistant',
          title: { zh: '7.3 代码助手开发', ja: '7.3 コードアシスタント開発' },
          content: {
            zh: `
## 项目：完整代码助手（Agentic 循环）

构建一个能读取、搜索、编辑代码的 AI 编程助手，实现完整的 Agentic 循环。

---

## 📁 项目结构

\`\`\`
code-assistant/
├── requirements.txt
├── config.py                   # 配置
├── src/
│   ├── __init__.py
│   ├── tools/                  # 工具定义
│   │   ├── __init__.py
│   │   ├── file_tools.py       # 文件操作
│   │   ├── search_tools.py     # 代码搜索
│   │   ├── git_tools.py        # Git 操作
│   │   └── shell_tools.py      # Shell 命令
│   ├── agent.py                # Agent 核心
│   └── tool_executor.py        # 工具执行器
├── main.py                     # CLI 入口
└── tests/
    └── test_tools.py
\`\`\`

---

## 📋 依赖安装

\`\`\`bash
# requirements.txt
anthropic>=0.40.0
rich>=13.0.0
gitpython>=3.1.0
click>=8.0.0
\`\`\`

---

## 🔧 工具定义 (tools/file_tools.py)

\`\`\`python
import os
from pathlib import Path
from typing import Optional


def read_file(path: str, start_line: int = 1, end_line: Optional[int] = None) -> str:
    """读取文件内容，支持行范围"""
    try:
        with open(path, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        total_lines = len(lines)
        start_idx = max(0, start_line - 1)
        end_idx = end_line if end_line else total_lines

        selected_lines = lines[start_idx:end_idx]

        # 添加行号
        numbered_lines = [
            f"{start_line + i:4d} | {line.rstrip()}"
            for i, line in enumerate(selected_lines)
        ]

        return f"文件: {path} (共 {total_lines} 行)\\n" + "\\n".join(numbered_lines)

    except FileNotFoundError:
        return f"错误: 文件不存在 - {path}"
    except Exception as e:
        return f"错误: {str(e)}"


def write_file(path: str, content: str) -> str:
    """写入文件"""
    try:
        # 确保目录存在
        Path(path).parent.mkdir(parents=True, exist_ok=True)

        with open(path, 'w', encoding='utf-8') as f:
            f.write(content)

        return f"成功写入: {path}"
    except Exception as e:
        return f"错误: {str(e)}"


def edit_file(path: str, old_text: str, new_text: str) -> str:
    """编辑文件（字符串替换）"""
    try:
        with open(path, 'r', encoding='utf-8') as f:
            content = f.read()

        if old_text not in content:
            return f"错误: 未找到要替换的文本"

        # 检查是否有多处匹配
        count = content.count(old_text)
        if count > 1:
            return f"警告: 找到 {count} 处匹配，请提供更多上下文以精确定位"

        new_content = content.replace(old_text, new_text, 1)

        with open(path, 'w', encoding='utf-8') as f:
            f.write(new_content)

        return f"成功编辑: {path}"
    except Exception as e:
        return f"错误: {str(e)}"


def list_directory(path: str = ".", recursive: bool = False) -> str:
    """列出目录内容"""
    try:
        p = Path(path)
        if not p.exists():
            return f"错误: 目录不存在 - {path}"

        if recursive:
            items = list(p.rglob("*"))
        else:
            items = list(p.iterdir())

        # 过滤隐藏文件和常见忽略目录
        ignore_patterns = {'.git', 'node_modules', '__pycache__', '.venv', 'venv'}
        items = [
            item for item in items
            if not any(part in ignore_patterns for part in item.parts)
            and not item.name.startswith('.')
        ]

        # 分类显示
        dirs = sorted([f"📁 {item.relative_to(p)}" for item in items if item.is_dir()])
        files = sorted([f"📄 {item.relative_to(p)}" for item in items if item.is_file()])

        return f"目录: {path}\\n\\n" + "\\n".join(dirs + files)
    except Exception as e:
        return f"错误: {str(e)}"


# 工具 Schema 定义
FILE_TOOLS = [
    {
        "name": "read_file",
        "description": "读取文件内容。可指定起始行和结束行。",
        "input_schema": {
            "type": "object",
            "properties": {
                "path": {
                    "type": "string",
                    "description": "文件路径"
                },
                "start_line": {
                    "type": "integer",
                    "description": "起始行号（默认1）",
                    "default": 1
                },
                "end_line": {
                    "type": "integer",
                    "description": "结束行号（默认到文件末尾）"
                }
            },
            "required": ["path"]
        }
    },
    {
        "name": "write_file",
        "description": "创建或覆盖文件",
        "input_schema": {
            "type": "object",
            "properties": {
                "path": {"type": "string", "description": "文件路径"},
                "content": {"type": "string", "description": "文件内容"}
            },
            "required": ["path", "content"]
        }
    },
    {
        "name": "edit_file",
        "description": "编辑文件，通过替换指定文本",
        "input_schema": {
            "type": "object",
            "properties": {
                "path": {"type": "string", "description": "文件路径"},
                "old_text": {"type": "string", "description": "要替换的原文本"},
                "new_text": {"type": "string", "description": "替换后的新文本"}
            },
            "required": ["path", "old_text", "new_text"]
        }
    },
    {
        "name": "list_directory",
        "description": "列出目录内容",
        "input_schema": {
            "type": "object",
            "properties": {
                "path": {"type": "string", "description": "目录路径", "default": "."},
                "recursive": {"type": "boolean", "description": "是否递归", "default": False}
            }
        }
    }
]
\`\`\`

---

## 🔍 搜索工具 (tools/search_tools.py)

\`\`\`python
import subprocess
from pathlib import Path
from typing import List, Optional


def grep_search(
    pattern: str,
    path: str = ".",
    file_pattern: Optional[str] = None,
    context_lines: int = 2
) -> str:
    """使用 ripgrep 搜索代码"""
    try:
        cmd = ["rg", "--color=never", "-n"]

        if context_lines > 0:
            cmd.extend(["-C", str(context_lines)])

        if file_pattern:
            cmd.extend(["-g", file_pattern])

        # 忽略常见目录
        cmd.extend(["--ignore-file", ".gitignore"])
        cmd.append(pattern)
        cmd.append(path)

        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=30
        )

        if result.returncode == 0:
            return f"搜索结果 '{pattern}':\\n\\n{result.stdout}"
        elif result.returncode == 1:
            return f"未找到匹配: {pattern}"
        else:
            return f"搜索错误: {result.stderr}"

    except FileNotFoundError:
        # fallback to grep
        return _grep_fallback(pattern, path)
    except Exception as e:
        return f"错误: {str(e)}"


def _grep_fallback(pattern: str, path: str) -> str:
    """使用 Python 实现的简单搜索"""
    results = []
    p = Path(path)

    for file_path in p.rglob("*"):
        if file_path.is_file() and not _should_ignore(file_path):
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    for i, line in enumerate(f, 1):
                        if pattern in line:
                            results.append(f"{file_path}:{i}: {line.strip()}")
            except:
                continue

    if results:
        return f"搜索结果 '{pattern}':\\n\\n" + "\\n".join(results[:50])
    return f"未找到匹配: {pattern}"


def _should_ignore(path: Path) -> bool:
    """判断是否应忽略"""
    ignore = {'.git', 'node_modules', '__pycache__', '.venv', 'dist', 'build'}
    return any(part in ignore for part in path.parts)


def find_definition(symbol: str, path: str = ".") -> str:
    """查找函数/类定义"""
    patterns = [
        f"def {symbol}",           # Python 函数
        f"class {symbol}",         # Python 类
        f"function {symbol}",      # JavaScript 函数
        f"const {symbol}",         # JavaScript 常量
        f"interface {symbol}",     # TypeScript 接口
        f"type {symbol}",          # TypeScript 类型
    ]

    results = []
    for pattern in patterns:
        result = grep_search(pattern, path)
        if "未找到" not in result:
            results.append(result)

    if results:
        return "\\n\\n".join(results)
    return f"未找到定义: {symbol}"


SEARCH_TOOLS = [
    {
        "name": "grep_search",
        "description": "在代码库中搜索文本或正则表达式",
        "input_schema": {
            "type": "object",
            "properties": {
                "pattern": {"type": "string", "description": "搜索模式"},
                "path": {"type": "string", "description": "搜索路径", "default": "."},
                "file_pattern": {"type": "string", "description": "文件过滤（如 *.py）"},
                "context_lines": {"type": "integer", "description": "上下文行数", "default": 2}
            },
            "required": ["pattern"]
        }
    },
    {
        "name": "find_definition",
        "description": "查找函数或类的定义位置",
        "input_schema": {
            "type": "object",
            "properties": {
                "symbol": {"type": "string", "description": "函数或类名"},
                "path": {"type": "string", "description": "搜索路径", "default": "."}
            },
            "required": ["symbol"]
        }
    }
]
\`\`\`

---

## 🐚 Shell 工具 (tools/shell_tools.py)

\`\`\`python
import subprocess
import shlex
from typing import Optional

# 允许执行的命令白名单
ALLOWED_COMMANDS = {
    'ls', 'cat', 'head', 'tail', 'wc', 'find', 'tree',
    'git', 'npm', 'pip', 'python', 'node', 'pytest',
    'make', 'cargo', 'go'
}

# 危险命令黑名单
DANGEROUS_PATTERNS = [
    'rm -rf', 'sudo', 'chmod 777', '> /dev',
    'curl | bash', 'wget | sh', 'dd if='
]


def run_command(command: str, timeout: int = 60) -> str:
    """执行 shell 命令（带安全检查）"""
    # 安全检查
    for pattern in DANGEROUS_PATTERNS:
        if pattern in command:
            return f"拒绝执行: 检测到危险模式 '{pattern}'"

    # 解析命令
    try:
        parts = shlex.split(command)
        base_cmd = parts[0] if parts else ""

        if base_cmd not in ALLOWED_COMMANDS:
            return f"拒绝执行: '{base_cmd}' 不在允许列表中。\\n允许: {', '.join(sorted(ALLOWED_COMMANDS))}"

    except Exception as e:
        return f"命令解析错误: {str(e)}"

    # 执行命令
    try:
        result = subprocess.run(
            command,
            shell=True,
            capture_output=True,
            text=True,
            timeout=timeout,
            cwd="."
        )

        output = []
        if result.stdout:
            output.append(f"stdout:\\n{result.stdout}")
        if result.stderr:
            output.append(f"stderr:\\n{result.stderr}")
        output.append(f"exit code: {result.returncode}")

        return "\\n".join(output)

    except subprocess.TimeoutExpired:
        return f"命令超时（{timeout}秒）"
    except Exception as e:
        return f"执行错误: {str(e)}"


SHELL_TOOLS = [
    {
        "name": "run_command",
        "description": "执行 shell 命令（受限于安全白名单）",
        "input_schema": {
            "type": "object",
            "properties": {
                "command": {"type": "string", "description": "要执行的命令"},
                "timeout": {"type": "integer", "description": "超时秒数", "default": 60}
            },
            "required": ["command"]
        }
    }
]
\`\`\`

---

## 🤖 Agent 核心 (agent.py)

\`\`\`python
from typing import List, Dict, Any, Generator
from anthropic import Anthropic

from tools.file_tools import FILE_TOOLS, read_file, write_file, edit_file, list_directory
from tools.search_tools import SEARCH_TOOLS, grep_search, find_definition
from tools.shell_tools import SHELL_TOOLS, run_command


class CodeAssistant:
    """代码助手 Agent"""

    def __init__(
        self,
        model: str = "claude-sonnet-4-20250514",
        max_iterations: int = 20
    ):
        self.client = Anthropic()
        self.model = model
        self.max_iterations = max_iterations

        # 合并所有工具
        self.tools = FILE_TOOLS + SEARCH_TOOLS + SHELL_TOOLS

        # 工具执行映射
        self.tool_handlers = {
            "read_file": read_file,
            "write_file": write_file,
            "edit_file": edit_file,
            "list_directory": list_directory,
            "grep_search": grep_search,
            "find_definition": find_definition,
            "run_command": run_command
        }

        # 系统提示
        self.system_prompt = """你是一个专业的代码助手，可以帮助用户：
1. 阅读和理解代码
2. 搜索代码库
3. 编辑和创建文件
4. 执行命令

工作原则：
- 先理解再行动：阅读相关代码后再修改
- 最小改动：只修改必要的部分
- 安全第一：不执行危险命令
- 保持沟通：解释你的思考过程

当前工作目录是项目根目录。"""

    def _execute_tool(self, name: str, input_data: Dict) -> str:
        """执行工具调用"""
        handler = self.tool_handlers.get(name)
        if not handler:
            return f"未知工具: {name}"

        try:
            return handler(**input_data)
        except Exception as e:
            return f"工具执行错误: {str(e)}"

    def chat(
        self,
        user_message: str,
        conversation_history: List[Dict] = None
    ) -> Generator[str, None, None]:
        """
        对话接口，支持流式输出

        Yields:
            str: 输出片段（文本或工具调用信息）
        """
        messages = conversation_history or []
        messages.append({"role": "user", "content": user_message})

        iteration = 0

        while iteration < self.max_iterations:
            iteration += 1

            # 调用 API
            response = self.client.messages.create(
                model=self.model,
                max_tokens=4096,
                system=self.system_prompt,
                tools=self.tools,
                messages=messages
            )

            # 处理响应
            assistant_content = []

            for block in response.content:
                if block.type == "text":
                    yield block.text
                    assistant_content.append(block)

                elif block.type == "tool_use":
                    yield f"\\n🔧 调用工具: {block.name}\\n"
                    yield f"   参数: {block.input}\\n"

                    # 执行工具
                    result = self._execute_tool(block.name, block.input)
                    yield f"   结果: {result[:200]}...\\n" if len(result) > 200 else f"   结果: {result}\\n"

                    assistant_content.append(block)

                    # 添加工具结果到消息
                    messages.append({"role": "assistant", "content": assistant_content})
                    messages.append({
                        "role": "user",
                        "content": [{
                            "type": "tool_result",
                            "tool_use_id": block.id,
                            "content": result
                        }]
                    })
                    assistant_content = []

            # 如果没有工具调用，对话结束
            if response.stop_reason == "end_turn":
                if assistant_content:
                    messages.append({"role": "assistant", "content": assistant_content})
                break

        yield f"\\n[完成，共 {iteration} 轮迭代]"

    def run_task(self, task: str) -> str:
        """执行任务（非交互式）"""
        output = []
        for chunk in self.chat(task):
            output.append(chunk)
        return "".join(output)
\`\`\`

---

## 💻 CLI 入口 (main.py)

\`\`\`python
import click
from rich.console import Console
from rich.markdown import Markdown
from rich.panel import Panel

from agent import CodeAssistant

console = Console()


@click.group()
def cli():
    """代码助手 CLI"""
    pass


@cli.command()
def chat():
    """交互式对话模式"""
    console.print(Panel.fit(
        "[bold blue]代码助手[/bold blue]\\n"
        "输入问题或任务，输入 'exit' 退出",
        title="Welcome"
    ))

    assistant = CodeAssistant()
    history = []

    while True:
        try:
            user_input = console.input("[bold green]> [/bold green]")

            if user_input.lower() in ('exit', 'quit', 'q'):
                console.print("[yellow]再见！[/yellow]")
                break

            if not user_input.strip():
                continue

            # 流式输出
            console.print()
            for chunk in assistant.chat(user_input, history):
                console.print(chunk, end="")
            console.print()

        except KeyboardInterrupt:
            console.print("\\n[yellow]已中断[/yellow]")
            break


@cli.command()
@click.argument('task')
def run(task: str):
    """执行单个任务"""
    assistant = CodeAssistant()
    result = assistant.run_task(task)
    console.print(Markdown(result))


@cli.command()
@click.argument('file_path')
def explain(file_path: str):
    """解释代码文件"""
    assistant = CodeAssistant()
    result = assistant.run_task(f"请阅读并解释这个文件的代码: {file_path}")
    console.print(Markdown(result))


@cli.command()
@click.argument('symbol')
def find(symbol: str):
    """查找符号定义"""
    assistant = CodeAssistant()
    result = assistant.run_task(f"查找 {symbol} 的定义和用法")
    console.print(Markdown(result))


if __name__ == "__main__":
    cli()
\`\`\`

---

## 🚀 使用示例

\`\`\`bash
# 安装
pip install -r requirements.txt

# 交互模式
python main.py chat

# 单任务模式
python main.py run "帮我找到所有 TODO 注释"

# 解释代码
python main.py explain src/agent.py

# 查找定义
python main.py find CodeAssistant
\`\`\`

---

## 💡 Agentic 循环核心逻辑

\`\`\`
┌─────────────────────────────────────────┐
│  用户输入: "帮我修复这个 bug"            │
└───────────────┬─────────────────────────┘
                ▼
┌─────────────────────────────────────────┐
│  Agent 思考: 需要先了解代码              │
│  -> 调用 read_file 读取相关文件          │
└───────────────┬─────────────────────────┘
                ▼
┌─────────────────────────────────────────┐
│  工具返回: 文件内容                      │
│  Agent 分析: 发现问题在第 42 行          │
│  -> 调用 edit_file 修复                 │
└───────────────┬─────────────────────────┘
                ▼
┌─────────────────────────────────────────┐
│  工具返回: 修改成功                      │
│  Agent: 需要验证修复                     │
│  -> 调用 run_command 执行测试           │
└───────────────┬─────────────────────────┘
                ▼
┌─────────────────────────────────────────┐
│  测试通过，任务完成                      │
│  Agent 输出: 总结修复过程                │
└─────────────────────────────────────────┘
\`\`\`

---

## 📊 与 Claude Code 对比

| 特性 | 自建 Agent | Claude Code |
|------|------------|-------------|
| 自定义工具 | ✅ 完全可控 | ⚠️ 受限 |
| MCP 集成 | ✅ 可选 | ✅ 原生支持 |
| 安全控制 | ✅ 自定义白名单 | ✅ 沙箱隔离 |
| 部署方式 | 服务器/本地 | 本地 CLI |
| 学习成本 | 中等 | 低 |
| 适用场景 | 定制化需求 | 通用开发 |
            `,
            ja: `
## プロジェクト：完全なコードアシスタント（Agenticループ）

コードの読み取り、検索、編集ができるAIプログラミングアシスタントを構築し、完全なAgenticループを実装。

---

## 📁 プロジェクト構造

\`\`\`
code-assistant/
├── requirements.txt
├── src/
│   ├── tools/                  # ツール定義
│   │   ├── file_tools.py       # ファイル操作
│   │   ├── search_tools.py     # コード検索
│   │   └── shell_tools.py      # シェルコマンド
│   └── agent.py                # Agentコア
└── main.py                     # CLIエントリ
\`\`\`

---

## 🔧 ファイルツール (file_tools.py)

\`\`\`python
def read_file(path: str, start_line: int = 1, end_line: int = None) -> str:
    """ファイル内容を読み取り、行番号付きで返す"""
    with open(path, 'r') as f:
        lines = f.readlines()
    # 行番号を追加して返す
    return "\\n".join([f"{i:4d} | {line}" for i, line in enumerate(lines, 1)])

def edit_file(path: str, old_text: str, new_text: str) -> str:
    """テキスト置換でファイルを編集"""
    with open(path, 'r') as f:
        content = f.read()
    new_content = content.replace(old_text, new_text, 1)
    with open(path, 'w') as f:
        f.write(new_content)
    return f"編集成功: {path}"
\`\`\`

---

## 🤖 Agentコア (agent.py)

\`\`\`python
class CodeAssistant:
    def __init__(self, model: str = "claude-sonnet-4-20250514"):
        self.client = Anthropic()
        self.tools = FILE_TOOLS + SEARCH_TOOLS + SHELL_TOOLS

    def chat(self, user_message: str) -> Generator[str, None, None]:
        messages = [{"role": "user", "content": user_message}]

        while True:
            response = self.client.messages.create(
                model=self.model,
                tools=self.tools,
                messages=messages
            )

            for block in response.content:
                if block.type == "text":
                    yield block.text
                elif block.type == "tool_use":
                    result = self._execute_tool(block.name, block.input)
                    # ツール結果をメッセージに追加
                    messages.append({"role": "user", "content": [{
                        "type": "tool_result",
                        "tool_use_id": block.id,
                        "content": result
                    }]})

            if response.stop_reason == "end_turn":
                break
\`\`\`

---

## 💡 Agenticループのコアロジック

\`\`\`
ユーザー入力 → Agent思考 → ツール呼び出し → 結果分析
     ↑                                         ↓
     └──────────── 必要に応じて繰り返し ←────────┘
\`\`\`

---

## 📊 Claude Code との比較

| 特性 | 自作Agent | Claude Code |
|------|-----------|-------------|
| カスタムツール | ✅ 完全制御可能 | ⚠️ 制限あり |
| MCP統合 | ✅ オプション | ✅ ネイティブ |
| セキュリティ | ✅ カスタムホワイトリスト | ✅ サンドボックス |
| デプロイ | サーバー/ローカル | ローカルCLI |
            `
          }
        },
        {
          id: 'ch7-agent-workflow',
          title: { zh: '7.4 AI Agent 自动化工作流', ja: '7.4 AI Agent 自動化ワークフロー' },
          content: {
            zh: `
## 项目：自动化数据采集与报告生成

构建一个能自动浏览网页、提取数据、生成报告的 AI Agent。

---

## 📁 项目结构

\`\`\`
automation-agent/
├── requirements.txt
├── config.py
├── src/
│   ├── browser_agent.py        # 浏览器自动化
│   ├── data_extractor.py       # 数据提取
│   ├── report_generator.py     # 报告生成
│   └── scheduler.py            # 定时任务
├── templates/
│   └── report_template.html
└── main.py
\`\`\`

---

## 📋 依赖安装

\`\`\`bash
# requirements.txt
anthropic>=0.40.0
playwright>=1.40.0
pandas>=2.0.0
jinja2>=3.0.0
schedule>=1.2.0
\`\`\`

---

## 🌐 浏览器自动化 (browser_agent.py)

\`\`\`python
import asyncio
from typing import List, Dict, Any, Optional
from playwright.async_api import async_playwright, Page, Browser
from anthropic import Anthropic


class BrowserAgent:
    """基于 AI 的浏览器自动化 Agent"""

    def __init__(self, headless: bool = True):
        self.headless = headless
        self.client = Anthropic()
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None

    async def __aenter__(self):
        playwright = await async_playwright().start()
        self.browser = await playwright.chromium.launch(headless=self.headless)
        self.page = await self.browser.new_page()
        return self

    async def __aexit__(self, *args):
        if self.browser:
            await self.browser.close()

    async def navigate(self, url: str) -> str:
        """导航到 URL"""
        await self.page.goto(url, wait_until="networkidle")
        return f"已导航到: {url}"

    async def get_page_content(self) -> str:
        """获取页面内容（简化版）"""
        # 提取主要文本内容
        content = await self.page.evaluate('''() => {
            const elements = document.querySelectorAll('h1, h2, h3, p, li, td, th, a');
            return Array.from(elements).map(el => ({
                tag: el.tagName.toLowerCase(),
                text: el.innerText.trim().substring(0, 200),
                href: el.href || null
            })).filter(el => el.text.length > 0).slice(0, 100);
        }''')
        return str(content)

    async def click(self, selector: str) -> str:
        """点击元素"""
        try:
            await self.page.click(selector, timeout=5000)
            await self.page.wait_for_load_state("networkidle")
            return f"已点击: {selector}"
        except Exception as e:
            return f"点击失败: {str(e)}"

    async def fill(self, selector: str, value: str) -> str:
        """填写表单"""
        try:
            await self.page.fill(selector, value)
            return f"已填写: {selector}"
        except Exception as e:
            return f"填写失败: {str(e)}"

    async def screenshot(self, path: str = "screenshot.png") -> str:
        """截图"""
        await self.page.screenshot(path=path, full_page=True)
        return f"截图保存: {path}"

    async def extract_table(self, selector: str = "table") -> List[Dict]:
        """提取表格数据"""
        tables = await self.page.query_selector_all(selector)
        if not tables:
            return []

        result = []
        for table in tables:
            rows = await table.query_selector_all("tr")
            table_data = []

            for row in rows:
                cells = await row.query_selector_all("td, th")
                row_data = [await cell.inner_text() for cell in cells]
                if row_data:
                    table_data.append(row_data)

            if table_data:
                # 第一行作为表头
                headers = table_data[0]
                for row in table_data[1:]:
                    result.append(dict(zip(headers, row)))

        return result

    def _get_tools(self) -> List[Dict]:
        """定义浏览器工具"""
        return [
            {
                "name": "navigate",
                "description": "导航到指定 URL",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "url": {"type": "string", "description": "目标 URL"}
                    },
                    "required": ["url"]
                }
            },
            {
                "name": "get_page_content",
                "description": "获取当前页面的主要内容",
                "input_schema": {"type": "object", "properties": {}}
            },
            {
                "name": "click",
                "description": "点击页面元素",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "selector": {"type": "string", "description": "CSS 选择器"}
                    },
                    "required": ["selector"]
                }
            },
            {
                "name": "fill",
                "description": "填写表单字段",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "selector": {"type": "string"},
                        "value": {"type": "string"}
                    },
                    "required": ["selector", "value"]
                }
            },
            {
                "name": "extract_table",
                "description": "提取页面中的表格数据",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "selector": {"type": "string", "default": "table"}
                    }
                }
            },
            {
                "name": "screenshot",
                "description": "对当前页面截图",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "path": {"type": "string", "default": "screenshot.png"}
                    }
                }
            }
        ]

    async def _execute_tool(self, name: str, input_data: Dict) -> str:
        """执行工具"""
        handlers = {
            "navigate": lambda: self.navigate(input_data["url"]),
            "get_page_content": self.get_page_content,
            "click": lambda: self.click(input_data["selector"]),
            "fill": lambda: self.fill(input_data["selector"], input_data["value"]),
            "extract_table": lambda: self.extract_table(input_data.get("selector", "table")),
            "screenshot": lambda: self.screenshot(input_data.get("path", "screenshot.png"))
        }

        handler = handlers.get(name)
        if handler:
            result = await handler()
            return str(result)
        return f"未知工具: {name}"

    async def run_task(self, task: str, max_iterations: int = 15) -> str:
        """执行自动化任务"""
        messages = [{"role": "user", "content": task}]
        results = []

        for i in range(max_iterations):
            response = self.client.messages.create(
                model="claude-sonnet-4-20250514",
                max_tokens=4096,
                system="""你是一个网页自动化助手。你可以：
1. 导航到网页
2. 提取页面内容和表格
3. 点击和填写表单
4. 截图

请一步一步完成任务，每次只执行一个操作。""",
                tools=self._get_tools(),
                messages=messages
            )

            assistant_content = []
            for block in response.content:
                if block.type == "text":
                    results.append(block.text)
                    assistant_content.append(block)

                elif block.type == "tool_use":
                    result = await self._execute_tool(block.name, block.input)
                    results.append(f"[{block.name}] {result}")
                    assistant_content.append(block)

                    messages.append({"role": "assistant", "content": assistant_content})
                    messages.append({
                        "role": "user",
                        "content": [{
                            "type": "tool_result",
                            "tool_use_id": block.id,
                            "content": result
                        }]
                    })
                    assistant_content = []

            if response.stop_reason == "end_turn":
                break

        return "\\n".join(results)
\`\`\`

---

## 📊 报告生成器 (report_generator.py)

\`\`\`python
import pandas as pd
from datetime import datetime
from pathlib import Path
from jinja2 import Environment, FileSystemLoader
from typing import List, Dict, Any


class ReportGenerator:
    """自动报告生成器"""

    def __init__(self, template_dir: str = "./templates"):
        self.env = Environment(loader=FileSystemLoader(template_dir))

    def generate_html_report(
        self,
        title: str,
        data: List[Dict],
        output_path: str,
        summary: str = ""
    ) -> str:
        """生成 HTML 报告"""
        template = self.env.get_template("report_template.html")

        # 转换为 DataFrame 以便处理
        df = pd.DataFrame(data)

        html = template.render(
            title=title,
            generated_at=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            summary=summary,
            table_html=df.to_html(classes="data-table", index=False),
            total_rows=len(data)
        )

        Path(output_path).write_text(html, encoding='utf-8')
        return output_path

    def generate_csv(self, data: List[Dict], output_path: str) -> str:
        """生成 CSV 文件"""
        df = pd.DataFrame(data)
        df.to_csv(output_path, index=False, encoding='utf-8-sig')
        return output_path

    def generate_excel(self, data: List[Dict], output_path: str) -> str:
        """生成 Excel 文件"""
        df = pd.DataFrame(data)
        df.to_excel(output_path, index=False, engine='openpyxl')
        return output_path
\`\`\`

---

## ⏰ 定时任务 (scheduler.py)

\`\`\`python
import schedule
import time
import asyncio
from datetime import datetime
from typing import Callable


class TaskScheduler:
    """定时任务调度器"""

    def __init__(self):
        self.tasks = []

    def add_daily_task(self, time_str: str, task: Callable):
        """添加每日任务"""
        schedule.every().day.at(time_str).do(task)
        self.tasks.append(f"每日 {time_str}: {task.__name__}")

    def add_hourly_task(self, task: Callable):
        """添加每小时任务"""
        schedule.every().hour.do(task)
        self.tasks.append(f"每小时: {task.__name__}")

    def run(self):
        """运行调度器"""
        print(f"调度器启动，共 {len(self.tasks)} 个任务")
        for task in self.tasks:
            print(f"  - {task}")

        while True:
            schedule.run_pending()
            time.sleep(60)


# 使用示例
async def daily_data_collection():
    """每日数据采集任务"""
    async with BrowserAgent() as agent:
        result = await agent.run_task(
            "访问 https://example.com/data，提取今日数据表格，生成报告"
        )
        print(f"[{datetime.now()}] 采集完成: {result[:100]}...")


if __name__ == "__main__":
    scheduler = TaskScheduler()

    # 每天早上 9 点执行
    scheduler.add_daily_task("09:00", lambda: asyncio.run(daily_data_collection()))

    scheduler.run()
\`\`\`

---

## 🚀 完整使用示例 (main.py)

\`\`\`python
import asyncio
from browser_agent import BrowserAgent
from report_generator import ReportGenerator


async def main():
    # 1. 创建浏览器 Agent
    async with BrowserAgent(headless=False) as agent:

        # 2. 执行数据采集任务
        result = await agent.run_task("""
        请完成以下任务：
        1. 访问 https://quotes.toscrape.com/
        2. 提取页面上的名言和作者
        3. 截图保存
        """)

        print("任务结果:", result)

        # 3. 提取表格数据
        await agent.navigate("https://quotes.toscrape.com/tableful/")
        data = await agent.extract_table()

        # 4. 生成报告
        if data:
            generator = ReportGenerator()
            report_path = generator.generate_html_report(
                title="名言数据采集报告",
                data=data,
                output_path="./output/report.html",
                summary="自动采集的名言数据"
            )
            print(f"报告已生成: {report_path}")


if __name__ == "__main__":
    asyncio.run(main())
\`\`\`

---

## 💡 安全注意事项

| 风险 | 防护措施 |
|------|----------|
| 访问恶意网站 | URL 白名单过滤 |
| 表单自动提交 | 敏感操作需确认 |
| 数据泄露 | 不保存敏感信息 |
| 无限循环 | 设置最大迭代次数 |
| 资源耗尽 | 超时和内存限制 |

---

## 📊 应用场景

| 场景 | 描述 |
|------|------|
| 竞品监控 | 定期采集竞品价格/功能 |
| 数据采集 | 自动抓取公开数据 |
| 报告生成 | 每日/周自动生成报告 |
| 表单测试 | 自动化 E2E 测试 |
| 内容聚合 | 多源信息汇总 |
            `,
            ja: `
## プロジェクト：自動データ収集とレポート生成

Webページを自動的にブラウズし、データを抽出し、レポートを生成するAI Agentを構築。

---

## 📁 プロジェクト構造

\`\`\`
automation-agent/
├── src/
│   ├── browser_agent.py        # ブラウザ自動化
│   ├── data_extractor.py       # データ抽出
│   ├── report_generator.py     # レポート生成
│   └── scheduler.py            # スケジューラー
└── main.py
\`\`\`

---

## 🌐 ブラウザ自動化 (browser_agent.py)

\`\`\`python
class BrowserAgent:
    async def navigate(self, url: str) -> str:
        await self.page.goto(url)
        return f"ナビゲート完了: {url}"

    async def extract_table(self, selector: str = "table") -> List[Dict]:
        # テーブルデータを抽出
        tables = await self.page.query_selector_all(selector)
        # ...処理...
        return result

    async def run_task(self, task: str) -> str:
        # AIがタスクを実行
        messages = [{"role": "user", "content": task}]
        # Agenticループで自動実行
        ...
\`\`\`

---

## ⏰ スケジューラー

\`\`\`python
import schedule

scheduler = TaskScheduler()
# 毎日午前9時に実行
scheduler.add_daily_task("09:00", daily_data_collection)
scheduler.run()
\`\`\`

---

## 📊 応用シーン

| シーン | 説明 |
|--------|------|
| 競合モニタリング | 定期的に競合の価格/機能を収集 |
| データ収集 | 公開データの自動スクレイピング |
| レポート生成 | 日次/週次自動レポート |
| E2Eテスト | 自動化フォームテスト |
            `
          }
        },
        {
          id: 'ch7-summary',
          title: { zh: '7.5 本章小结', ja: '7.5 この章のまとめ' },
          content: {
            zh: `
## 实战项目核心要点

---

## 💡 项目开发流程

1. **明确需求** —— 用户要解决什么问题
2. **选择技术栈** —— LLM + 向量库 + 框架
3. **构建原型** —— 快速验证可行性
4. **迭代优化** —— 根据反馈改进

---

## 🚀 推荐起步项目

| 难度 | 项目 | 时间 |
|------|------|------|
| ⭐ | 命令行聊天机器人 | 1天 |
| ⭐⭐ | Streamlit 文档问答 | 3天 |
| ⭐⭐⭐ | 完整客服系统 | 1-2周 |
            `,
            ja: `
## 実践プロジェクトコアポイント

---

## 🚀 推奨スタートプロジェクト

| 難易度 | プロジェクト | 時間 |
|--------|--------------|------|
| ⭐ | CLIチャットボット | 1日 |
| ⭐⭐ | Streamlitドキュメント Q&A | 3日 |
| ⭐⭐⭐ | 完全カスタマーサービスシステム | 1-2週間 |
            `
          }
        }
      ]
    },
    // ============================================
    // 第八章：开发工具链
    // ============================================
    {
      id: 'chapter-8',
      number: 8,
      title: { zh: '开发工具链', ja: '開発ツールチェーン' },
      subtitle: { zh: 'AI 开发必备工具', ja: 'AI開発必須ツール' },
      sections: [
        {
          id: 'ch8-langchain',
          title: { zh: '8.1 LangChain 深入', ja: '8.1 LangChain詳解' },
          content: {
            zh: `
## LangChain：AI 应用的瑞士军刀

LangChain 是构建 LLM 应用的最流行框架。

---

## 🔧 核心组件

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                      LangChain 架构                                      │
└─────────────────────────────────────────────────────────────────────────┘

                         LangChain
                             │
     ┌───────────┬───────────┼───────────┬───────────┐
     │           │           │           │           │
     ▼           ▼           ▼           ▼           ▼
   Models     Prompts     Chains      Memory      Tools
   (模型)     (提示词)     (链)       (记忆)      (工具)
\`\`\`

---

## 🔧 常用模式

\`\`\`python
from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# LCEL 链式调用
llm = ChatAnthropic(model="claude-sonnet-4-20250514")
prompt = ChatPromptTemplate.from_template("用一句话解释{topic}")
parser = StrOutputParser()

chain = prompt | llm | parser

result = chain.invoke({"topic": "量子计算"})
\`\`\`

---

## 💡 何时使用 LangChain

✅ 需要 RAG 功能
✅ 需要 Agent 能力
✅ 需要多模型切换
❌ 简单 API 调用
            `,
            ja: `
## LangChain：AIアプリケーションのスイスアーミーナイフ

LangChainはLLMアプリケーション構築で最も人気のあるフレームワーク。

---

## 💡 LangChainを使うべき時

✅ RAG機能が必要
✅ Agent能力が必要
✅ 複数モデル切り替えが必要
❌ シンプルなAPI呼び出し
            `
          }
        },
        {
          id: 'ch8-llamaindex',
          title: { zh: '8.2 LlamaIndex 实战', ja: '8.2 LlamaIndex実践' },
          content: {
            zh: `
## LlamaIndex：专注 RAG 的框架

比 LangChain 更专注于数据索引和检索。

---

## 🔧 5 行代码构建 RAG

\`\`\`python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

# 1. 加载文档
documents = SimpleDirectoryReader("./data").load_data()

# 2. 创建索引
index = VectorStoreIndex.from_documents(documents)

# 3. 查询
query_engine = index.as_query_engine()
response = query_engine.query("这些文档讲了什么？")
print(response)
\`\`\`

---

## 📊 LangChain vs LlamaIndex

| 特性 | LangChain | LlamaIndex |
|------|-----------|------------|
| **定位** | 通用 LLM 框架 | RAG 专用 |
| **学习曲线** | 陡峭 | 平缓 |
| **灵活性** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ |
| **开箱即用** | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ |
            `,
            ja: `
## LlamaIndex：RAG専用フレームワーク

LangChainよりデータインデックスと検索に特化。

---

## 🔧 5行でRAG構築

\`\`\`python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

documents = SimpleDirectoryReader("./data").load_data()
index = VectorStoreIndex.from_documents(documents)
query_engine = index.as_query_engine()
response = query_engine.query("これらのドキュメントは何について？")
\`\`\`
            `
          }
        },
        {
          id: 'ch8-huggingface',
          title: { zh: '8.3 HuggingFace 生态', ja: '8.3 HuggingFaceエコシステム' },
          content: {
            zh: `
## HuggingFace：AI 界的 GitHub

开源模型、数据集、Spaces 的一站式平台。

---

## 🔧 核心资源

| 资源 | 用途 | 示例 |
|------|------|------|
| **Models** | 开源模型 | Llama, Qwen, Mistral |
| **Datasets** | 数据集 | 训练、评估用 |
| **Spaces** | 在线 Demo | Gradio 应用 |
| **Transformers** | 推理库 | 本地运行模型 |

---

## 🔧 快速使用模型

\`\`\`python
from transformers import pipeline

# 情感分析
classifier = pipeline("sentiment-analysis")
result = classifier("I love this product!")
print(result)

# 文本生成
generator = pipeline("text-generation", model="gpt2")
result = generator("Once upon a time", max_length=50)

# 翻译
translator = pipeline("translation", model="Helsinki-NLP/opus-mt-en-zh")
result = translator("Hello, how are you?")
\`\`\`

---

## 💡 推荐资源

- 🔥 **LLM 排行榜**：Open LLM Leaderboard
- 📚 **中文模型**：Qwen, ChatGLM, Yi
- 🚀 **部署工具**：TGI, vLLM
            `,
            ja: `
## HuggingFace：AI界のGitHub

オープンソースモデル、データセット、Spacesのワンストッププラットフォーム。

---

## 💡 推奨リソース

- 🔥 **LLMランキング**：Open LLM Leaderboard
- 📚 **日本語モデル**：rinna, ELYZA
- 🚀 **デプロイツール**：TGI, vLLM
            `
          }
        },
        {
          id: 'ch8-summary',
          title: { zh: '8.4 本章小结', ja: '8.4 この章のまとめ' },
          content: {
            zh: `
## 开发工具链选择指南

---

## 🎯 工具选择建议

| 需求 | 推荐工具 |
|------|----------|
| 快速 RAG 原型 | LlamaIndex |
| 复杂 Agent 系统 | LangChain |
| 开源模型探索 | HuggingFace |
| 生产部署 | vLLM + FastAPI |
| 本地开发 | Ollama + Open WebUI |

---

## 💡 最终建议

> 🎯 **工具只是手段，解决问题才是目的**。
> 不要过度追求技术栈，选择能快速交付价值的方案。

---

## 📚 全书总结

恭喜你完成了 AI 进阶实战的学习！

你已掌握：
1. **大模型技术** —— Transformer、微调、量化
2. **提示词工程** —— 结构化、高级技巧、JSON输出
3. **AI Agent** —— MCP、Claude Code、A2A
4. **RAG 技术** —— 向量检索、GraphRAG
5. **多模态 AI** —— 图像、语音、视频
6. **安全与伦理** —— 幻觉、偏见、隐私
7. **实战项目** —— 从零构建应用
8. **工具链** —— LangChain、LlamaIndex、HuggingFace

**AI 技术日新月异，保持学习，保持好奇！**

*"真正的智慧不是知道多少，而是知道如何学习。"*
            `,
            ja: `
## 開発ツールチェーン選択ガイド

---

## 🎯 ツール選択アドバイス

| ニーズ | 推奨ツール |
|--------|------------|
| 迅速なRAGプロトタイプ | LlamaIndex |
| 複雑なAgentシステム | LangChain |
| オープンソースモデル探索 | HuggingFace |
| 本番デプロイ | vLLM + FastAPI |
| ローカル開発 | Ollama + Open WebUI |

---

## 📚 全書まとめ

AI実践上級編の学習完了おめでとうございます！

習得したこと：
1. **大規模モデル技術** —— Transformer、ファインチューニング、量子化
2. **プロンプトエンジニアリング** —— 構造化、高度なテクニック
3. **AI Agent** —— MCP、Claude Code、A2A
4. **RAG技術** —— ベクトル検索、GraphRAG
5. **マルチモーダルAI** —— 画像、音声、動画
6. **セキュリティと倫理** —— ハルシネーション、バイアス
7. **実践プロジェクト** —— ゼロからアプリ構築
8. **ツールチェーン** —— LangChain、LlamaIndex、HuggingFace

**AI技術は日々進化、学び続け、好奇心を持ち続けましょう！**
            `
          }
        }
      ]
    },
    // ==================== 第九章：Google AI 生态系统 ====================
    {
      id: 'chapter-9',
      number: 9,
      title: { zh: 'Google AI 生态系统', ja: 'Google AIエコシステム' },
      subtitle: { zh: 'Gemini、Vertex AI 与云端 AI 服务', ja: 'Gemini、Vertex AIとクラウドAIサービス' },
      sections: [
        {
          id: 'ch9-gemini',
          title: { zh: '9.1 Gemini 大模型家族', ja: '9.1 Gemini大規模モデルファミリー' },
          content: {
            zh: `
## Google Gemini：多模态原生大模型

Gemini 是 Google 最新一代大模型，原生支持多模态，在多项基准测试中超越 GPT-4。

---

## 📊 Gemini 模型对比

| 模型 | 上下文窗口 | 特点 | 适用场景 |
|------|------------|------|----------|
| **Gemini 2.0 Flash** | 1M tokens | 最新旗舰，多模态原生 | 复杂推理、代码、视觉 |
| **Gemini 1.5 Pro** | 2M tokens | 超长上下文 | 长文档分析、视频理解 |
| **Gemini 1.5 Flash** | 1M tokens | 快速响应 | 实时应用、高并发 |
| **Gemini Nano** | 设备端 | 轻量级 | 移动端、边缘计算 |

---

## 🔧 快速开始

### 安装 SDK

\`\`\`bash
pip install google-generativeai
\`\`\`

### 基础调用

\`\`\`python
import google.generativeai as genai

# 配置 API Key
genai.configure(api_key="YOUR_API_KEY")

# 创建模型
model = genai.GenerativeModel("gemini-2.0-flash")

# 文本生成
response = model.generate_content("解释量子计算的基本原理")
print(response.text)
\`\`\`

---

## 🖼️ 多模态能力

### 图像理解

\`\`\`python
import PIL.Image

# 加载图片
image = PIL.Image.open("diagram.png")

# 图文混合输入
response = model.generate_content([
    "请详细分析这张架构图，解释各组件之间的关系：",
    image
])
print(response.text)
\`\`\`

### 视频分析

\`\`\`python
import time

# 上传视频文件
video_file = genai.upload_file("presentation.mp4")

# 等待处理完成
while video_file.state.name == "PROCESSING":
    time.sleep(5)
    video_file = genai.get_file(video_file.name)

# 分析视频内容
response = model.generate_content([
    video_file,
    "总结这个视频的主要内容，列出关键要点"
])
print(response.text)
\`\`\`

---

## 💬 多轮对话

\`\`\`python
# 创建对话
chat = model.start_chat(history=[])

# 第一轮
response = chat.send_message("我想学习机器学习，应该从哪里开始？")
print(response.text)

# 第二轮（自动携带上下文）
response = chat.send_message("Python 基础需要学到什么程度？")
print(response.text)

# 查看对话历史
for message in chat.history:
    print(f"{message.role}: {message.parts[0].text[:100]}...")
\`\`\`

---

## 🔒 安全设置

\`\`\`python
from google.generativeai.types import HarmCategory, HarmBlockThreshold

# 配置安全过滤
safety_settings = {
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
}

response = model.generate_content(
    "你的问题",
    safety_settings=safety_settings
)
\`\`\`

---

## 📊 结构化输出 (JSON Mode)

\`\`\`python
import json

# 方法1：通过 Prompt 指定
response = model.generate_content(
    """分析以下文本的情感，返回 JSON 格式：
    {"sentiment": "positive/negative/neutral", "confidence": 0.0-1.0, "keywords": [...]}

    文本：这款产品真的太棒了，用户体验非常流畅！
    """,
    generation_config=genai.types.GenerationConfig(
        response_mime_type="application/json"
    )
)

result = json.loads(response.text)
print(result)

# 方法2：使用 Schema（更严格）
from google.generativeai.types import content_types

schema = content_types.Schema(
    type=content_types.Type.OBJECT,
    properties={
        "sentiment": content_types.Schema(type=content_types.Type.STRING),
        "confidence": content_types.Schema(type=content_types.Type.NUMBER),
        "keywords": content_types.Schema(
            type=content_types.Type.ARRAY,
            items=content_types.Schema(type=content_types.Type.STRING)
        )
    },
    required=["sentiment", "confidence"]
)

response = model.generate_content(
    "分析：这个服务太慢了，等了半天",
    generation_config=genai.types.GenerationConfig(
        response_mime_type="application/json",
        response_schema=schema
    )
)
\`\`\`

---

## 🛠️ Function Calling

\`\`\`python
# 定义工具函数
def get_weather(location: str, unit: str = "celsius") -> dict:
    """获取指定城市的天气信息"""
    # 模拟 API 调用
    return {
        "location": location,
        "temperature": 22,
        "unit": unit,
        "condition": "晴天"
    }

def search_flights(origin: str, destination: str, date: str) -> list:
    """搜索航班信息"""
    return [
        {"flight": "CA123", "price": 1500, "departure": "08:00"},
        {"flight": "MU456", "price": 1200, "departure": "14:30"}
    ]

# 创建带工具的模型
model = genai.GenerativeModel(
    "gemini-2.0-flash",
    tools=[get_weather, search_flights]
)

# 自动调用工具
chat = model.start_chat(enable_automatic_function_calling=True)
response = chat.send_message("北京今天天气怎么样？")
print(response.text)

response = chat.send_message("帮我查一下明天从北京到上海的航班")
print(response.text)
\`\`\`

---

## ⚡ 流式输出

\`\`\`python
# 流式生成
response = model.generate_content(
    "写一篇关于人工智能发展历史的文章",
    stream=True
)

for chunk in response:
    print(chunk.text, end="", flush=True)
\`\`\`

---

## 📈 Token 计数与成本估算

\`\`\`python
# 计算 Token 数量
model = genai.GenerativeModel("gemini-2.0-flash")

# 文本 Token
text = "这是一段测试文本，用于计算 Token 数量。"
token_count = model.count_tokens(text)
print(f"Token 数量: {token_count.total_tokens}")

# 图片 Token（每张图约 258 tokens）
image = PIL.Image.open("image.png")
token_count = model.count_tokens([text, image])
print(f"图文混合 Token: {token_count.total_tokens}")
\`\`\`

---

## 💰 定价参考 (2024)

| 模型 | 输入 (每百万 Token) | 输出 (每百万 Token) |
|------|---------------------|---------------------|
| Gemini 2.0 Flash | $0.10 | $0.40 |
| Gemini 1.5 Pro | $1.25 - $2.50 | $5.00 - $10.00 |
| Gemini 1.5 Flash | $0.075 | $0.30 |

*价格可能变动，以官网为准*
            `,
            ja: `
## Google Gemini：マルチモーダルネイティブ大規模モデル

GeminiはGoogleの最新世代大規模モデルで、ネイティブでマルチモーダルをサポート。

---

## 📊 Geminiモデル比較

| モデル | コンテキストウィンドウ | 特徴 | 用途 |
|--------|------------------------|------|------|
| **Gemini 2.0 Flash** | 1M tokens | 最新フラッグシップ | 複雑な推論、コード、ビジョン |
| **Gemini 1.5 Pro** | 2M tokens | 超長コンテキスト | 長文書分析、動画理解 |
| **Gemini 1.5 Flash** | 1M tokens | 高速レスポンス | リアルタイムアプリ |
| **Gemini Nano** | オンデバイス | 軽量 | モバイル、エッジ |

---

## 🔧 クイックスタート

\`\`\`python
import google.generativeai as genai

genai.configure(api_key="YOUR_API_KEY")
model = genai.GenerativeModel("gemini-2.0-flash")

response = model.generate_content("量子コンピューティングの基本原理を説明して")
print(response.text)
\`\`\`

---

## 🖼️ マルチモーダル機能

\`\`\`python
import PIL.Image

image = PIL.Image.open("diagram.png")
response = model.generate_content([
    "このアーキテクチャ図を分析してください：",
    image
])
\`\`\`

---

## 🛠️ Function Calling

\`\`\`python
def get_weather(location: str) -> dict:
    return {"location": location, "temperature": 22}

model = genai.GenerativeModel(
    "gemini-2.0-flash",
    tools=[get_weather]
)

chat = model.start_chat(enable_automatic_function_calling=True)
response = chat.send_message("東京の天気は？")
\`\`\`
            `
          }
        },
        {
          id: 'ch9-vertex-ai',
          title: { zh: '9.2 Vertex AI 企业级平台', ja: '9.2 Vertex AIエンタープライズプラットフォーム' },
          content: {
            zh: `
## Vertex AI：Google Cloud 的 AI 平台

Vertex AI 是 Google Cloud 的统一 AI 平台，提供从模型训练到部署的全流程服务。

---

## 🏗️ Vertex AI 核心组件

\`\`\`
┌─────────────────────────────────────────────────────────────┐
│                      Vertex AI Platform                      │
├─────────────────────────────────────────────────────────────┤
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐          │
│  │   Gemini    │  │  Model      │  │  Vertex AI  │          │
│  │   API       │  │  Garden     │  │  Studio     │          │
│  └─────────────┘  └─────────────┘  └─────────────┘          │
├─────────────────────────────────────────────────────────────┤
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐          │
│  │  AutoML     │  │  Custom     │  │  Pipelines  │          │
│  │             │  │  Training   │  │             │          │
│  └─────────────┘  └─────────────┘  └─────────────┘          │
├─────────────────────────────────────────────────────────────┤
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐          │
│  │  Feature    │  │  Vector     │  │  Model      │          │
│  │  Store      │  │  Search     │  │  Registry   │          │
│  └─────────────┘  └─────────────┘  └─────────────┘          │
└─────────────────────────────────────────────────────────────┘
\`\`\`

---

## 🔧 环境配置

\`\`\`bash
# 安装 SDK
pip install google-cloud-aiplatform

# 认证
gcloud auth application-default login
gcloud config set project YOUR_PROJECT_ID
\`\`\`

---

## 🤖 使用 Gemini on Vertex AI

\`\`\`python
import vertexai
from vertexai.generative_models import GenerativeModel, Part

# 初始化
vertexai.init(project="your-project-id", location="us-central1")

# 创建模型
model = GenerativeModel("gemini-2.0-flash")

# 文本生成
response = model.generate_content("解释机器学习和深度学习的区别")
print(response.text)

# 多模态：图片 + 文本
image = Part.from_uri(
    "gs://your-bucket/image.jpg",
    mime_type="image/jpeg"
)
response = model.generate_content([image, "描述这张图片"])
print(response.text)
\`\`\`

---

## 🔍 Vertex AI Search (企业搜索)

\`\`\`python
from google.cloud import discoveryengine_v1 as discoveryengine

# 创建搜索客户端
client = discoveryengine.SearchServiceClient()

# 执行搜索
request = discoveryengine.SearchRequest(
    serving_config=f"projects/{project}/locations/global/collections/default_collection/dataStores/{data_store}/servingConfigs/default_search",
    query="如何配置 Kubernetes 集群",
    page_size=10,
)

response = client.search(request)

for result in response.results:
    print(f"文档: {result.document.name}")
    print(f"摘要: {result.document.derived_struct_data.get('snippets', [])}")
\`\`\`

---

## 📊 Vertex AI Vector Search

用于构建大规模向量检索系统（原 Matching Engine）。

\`\`\`python
from google.cloud import aiplatform

# 初始化
aiplatform.init(project="your-project", location="us-central1")

# 创建向量索引
my_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(
    display_name="product-embeddings",
    contents_delta_uri="gs://your-bucket/embeddings/",
    dimensions=768,
    approximate_neighbors_count=150,
)

# 创建端点
my_index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(
    display_name="product-search-endpoint",
    public_endpoint_enabled=True
)

# 部署索引
my_index_endpoint.deploy_index(
    index=my_index,
    deployed_index_id="deployed_product_index"
)

# 执行向量搜索
response = my_index_endpoint.find_neighbors(
    deployed_index_id="deployed_product_index",
    queries=[[0.1, 0.2, ...]],  # 查询向量
    num_neighbors=10
)
\`\`\`

---

## 🏭 Model Garden：预训练模型库

\`\`\`python
# 部署开源模型（如 Llama 2）
from vertexai.preview.language_models import TextGenerationModel

# 浏览可用模型
# console.cloud.google.com/vertex-ai/model-garden

# 部署到端点
model = TextGenerationModel.from_pretrained("text-bison")
response = model.predict("写一首关于春天的诗")
print(response.text)
\`\`\`

---

## 🔄 Vertex AI Pipelines

\`\`\`python
from kfp import dsl
from kfp.v2 import compiler
from google.cloud import aiplatform

@dsl.component
def preprocess_data(input_path: str, output_path: str):
    # 数据预处理逻辑
    pass

@dsl.component
def train_model(data_path: str, model_path: str):
    # 模型训练逻辑
    pass

@dsl.component
def evaluate_model(model_path: str) -> float:
    # 模型评估
    return 0.95

@dsl.pipeline(name="ml-training-pipeline")
def ml_pipeline(input_data: str):
    preprocess_task = preprocess_data(input_path=input_data, output_path="gs://...")
    train_task = train_model(data_path=preprocess_task.output, model_path="gs://...")
    evaluate_task = evaluate_model(model_path=train_task.output)

# 编译并运行
compiler.Compiler().compile(ml_pipeline, "pipeline.json")

aiplatform.init(project="your-project", location="us-central1")
job = aiplatform.PipelineJob(
    display_name="training-job",
    template_path="pipeline.json",
    parameter_values={"input_data": "gs://your-bucket/data"}
)
job.run()
\`\`\`

---

## 💰 成本优化建议

| 策略 | 说明 |
|------|------|
| 使用 Flash 模型 | 比 Pro 便宜 5-10x |
| 批量处理 | 减少 API 调用次数 |
| 缓存响应 | 相同查询复用结果 |
| 区域选择 | 不同区域价格不同 |
| 承诺使用折扣 | 1年/3年承诺享折扣 |
            `,
            ja: `
## Vertex AI：Google CloudのAIプラットフォーム

Vertex AIはGoogle Cloudの統合AIプラットフォームで、モデルトレーニングからデプロイまでの全プロセスをサポート。

---

## 🏗️ Vertex AIコアコンポーネント

- **Gemini API** - 最新LLMアクセス
- **Model Garden** - 事前学習モデルライブラリ
- **Vertex AI Studio** - ノーコード実験環境
- **Vector Search** - 大規模ベクトル検索
- **Pipelines** - MLワークフロー自動化

---

## 🔧 環境設定

\`\`\`bash
pip install google-cloud-aiplatform
gcloud auth application-default login
\`\`\`

---

## 🤖 Gemini on Vertex AI

\`\`\`python
import vertexai
from vertexai.generative_models import GenerativeModel

vertexai.init(project="your-project-id", location="us-central1")
model = GenerativeModel("gemini-2.0-flash")

response = model.generate_content("機械学習と深層学習の違いを説明して")
print(response.text)
\`\`\`

---

## 📊 Vector Search

\`\`\`python
# ベクトルインデックス作成
my_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(
    display_name="product-embeddings",
    dimensions=768
)

# 検索実行
response = my_index_endpoint.find_neighbors(
    queries=[[0.1, 0.2, ...]],
    num_neighbors=10
)
\`\`\`
            `
          }
        },
        {
          id: 'ch9-ai-studio',
          title: { zh: '9.3 Google AI Studio 快速原型', ja: '9.3 Google AI Studio クイックプロトタイプ' },
          content: {
            zh: `
## Google AI Studio：免费的 Gemini 实验平台

AI Studio 是 Google 提供的免费在线工具，用于快速测试和原型开发。

---

## 🌐 访问地址

> **https://aistudio.google.com**

---

## 🎯 核心功能

### 1. 即时对话测试

\`\`\`
┌─────────────────────────────────────────┐
│  Chat Prompt                             │
├─────────────────────────────────────────┤
│  System: 你是一个专业的技术顾问          │
│                                          │
│  User: 如何选择合适的数据库？            │
│                                          │
│  Model: [Gemini 响应]                    │
├─────────────────────────────────────────┤
│  [Temperature] [Top-P] [Max Tokens]     │
└─────────────────────────────────────────┘
\`\`\`

### 2. 结构化 Prompt 设计

\`\`\`
┌─────────────────────────────────────────┐
│  Freeform Prompt                         │
├─────────────────────────────────────────┤
│  任务描述：                              │
│  根据用户问题，分析并给出建议            │
│                                          │
│  输入示例：                              │
│  Q: 网站加载慢怎么办？                   │
│  A: 1. 优化图片 2. 启用CDN 3. 代码压缩  │
│                                          │
│  测试输入：                              │
│  Q: API 响应时间太长                     │
└─────────────────────────────────────────┘
\`\`\`

### 3. 多模态测试

- 上传图片进行视觉问答
- 上传视频进行内容分析
- 音频转写和理解

---

## 🔧 导出代码

AI Studio 可以一键导出为多种语言：

### Python

\`\`\`python
import google.generativeai as genai

genai.configure(api_key="YOUR_API_KEY")

generation_config = {
    "temperature": 0.7,
    "top_p": 0.95,
    "top_k": 40,
    "max_output_tokens": 8192,
}

model = genai.GenerativeModel(
    model_name="gemini-2.0-flash",
    generation_config=generation_config,
    system_instruction="你是一个专业的技术顾问"
)

chat = model.start_chat(history=[])
response = chat.send_message("如何优化数据库查询性能？")
print(response.text)
\`\`\`

### JavaScript/Node.js

\`\`\`javascript
const { GoogleGenerativeAI } = require("@google/generative-ai");

const genAI = new GoogleGenerativeAI("YOUR_API_KEY");

async function run() {
    const model = genAI.getGenerativeModel({
        model: "gemini-2.0-flash",
        systemInstruction: "你是一个专业的技术顾问"
    });

    const result = await model.generateContent("如何优化数据库？");
    console.log(result.response.text());
}

run();
\`\`\`

### cURL

\`\`\`bash
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=YOUR_API_KEY" \\
  -H 'Content-Type: application/json' \\
  -d '{
    "contents": [{
      "parts": [{"text": "解释 RESTful API 设计原则"}]
    }],
    "generationConfig": {
      "temperature": 0.7,
      "maxOutputTokens": 2048
    }
  }'
\`\`\`

---

## 📊 Prompt Gallery（提示词库）

AI Studio 提供丰富的 Prompt 模板：

| 类别 | 示例 |
|------|------|
| 写作 | 博客生成、邮件撰写 |
| 代码 | 代码解释、Bug 修复 |
| 分析 | 数据分析、报告生成 |
| 创意 | 故事创作、广告文案 |
| 教育 | 知识问答、概念解释 |

---

## 🆓 免费额度

| 资源 | 免费额度 |
|------|----------|
| Gemini 2.0 Flash | 15 RPM / 100万 TPM |
| Gemini 1.5 Pro | 2 RPM / 32K TPM |
| 图片处理 | 包含在请求中 |
| 视频处理 | 包含在请求中 |

*RPM = 每分钟请求数，TPM = 每分钟 Token 数*

---

## 💡 最佳实践

1. **先在 AI Studio 测试** —— 验证 Prompt 效果
2. **使用 System Instruction** —— 定义角色和行为
3. **调整 Temperature** —— 创意任务调高，精确任务调低
4. **导出后再优化** —— 添加错误处理和重试逻辑
            `,
            ja: `
## Google AI Studio：無料のGemini実験プラットフォーム

AI StudioはGoogleが提供する無料オンラインツールで、迅速なテストとプロトタイプ開発に使用。

---

## 🌐 アクセスURL

> **https://aistudio.google.com**

---

## 🎯 コア機能

1. **即時対話テスト** - リアルタイムでGeminiと対話
2. **構造化プロンプト設計** - 例題付きプロンプト作成
3. **マルチモーダルテスト** - 画像・動画・音声対応
4. **コードエクスポート** - Python/JavaScript/cURL

---

## 🔧 エクスポートコード例

\`\`\`python
import google.generativeai as genai

genai.configure(api_key="YOUR_API_KEY")

model = genai.GenerativeModel(
    model_name="gemini-2.0-flash",
    system_instruction="あなたは技術コンサルタントです"
)

response = model.generate_content("データベースの最適化方法は？")
print(response.text)
\`\`\`

---

## 🆓 無料枠

| リソース | 無料枠 |
|----------|--------|
| Gemini 2.0 Flash | 15 RPM / 100万 TPM |
| Gemini 1.5 Pro | 2 RPM / 32K TPM |
            `
          }
        },
        {
          id: 'ch9-gemma',
          title: { zh: '9.4 Gemma 开源模型', ja: '9.4 Gemmaオープンソースモデル' },
          content: {
            zh: `
## Gemma：Google 的开源大模型

Gemma 是 Google 开源的轻量级大模型系列，与 Gemini 共享技术架构。

---

## 📊 Gemma 模型对比

| 模型 | 参数量 | 特点 | HuggingFace |
|------|--------|------|-------------|
| **Gemma 2 27B** | 27B | 性能最强 | google/gemma-2-27b |
| **Gemma 2 9B** | 9B | 平衡之选 | google/gemma-2-9b |
| **Gemma 2 2B** | 2B | 轻量快速 | google/gemma-2-2b |
| **CodeGemma 7B** | 7B | 代码专精 | google/codegemma-7b |
| **PaliGemma** | 3B | 视觉语言 | google/paligemma-3b |

---

## 🔧 本地部署（Ollama）

\`\`\`bash
# 安装 Ollama
curl -fsSL https://ollama.com/install.sh | sh

# 下载 Gemma
ollama pull gemma2:9b

# 运行对话
ollama run gemma2:9b

# API 调用
curl http://localhost:11434/api/generate -d '{
  "model": "gemma2:9b",
  "prompt": "解释什么是 Transformer 架构",
  "stream": false
}'
\`\`\`

---

## 🐍 Python 调用（HuggingFace）

\`\`\`python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# 加载模型
model_id = "google/gemma-2-9b-it"  # it = instruction-tuned
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    torch_dtype=torch.bfloat16
)

# 构建对话格式
messages = [
    {"role": "user", "content": "写一个 Python 快速排序算法"}
]
prompt = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

# 生成
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
outputs = model.generate(
    **inputs,
    max_new_tokens=512,
    temperature=0.7,
    do_sample=True
)

response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
\`\`\`

---

## ⚡ 量化部署

\`\`\`python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 4-bit 量化配置
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_quant_type="nf4"
)

model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-2-9b-it",
    quantization_config=quantization_config,
    device_map="auto"
)

# 9B 模型量化后约需 6GB 显存
\`\`\`

---

## 🖼️ PaliGemma（视觉语言模型）

\`\`\`python
from transformers import AutoProcessor, PaliGemmaForConditionalGeneration
from PIL import Image

# 加载模型
model_id = "google/paligemma-3b-pt-224"
processor = AutoProcessor.from_pretrained(model_id)
model = PaliGemmaForConditionalGeneration.from_pretrained(model_id)

# 图像理解
image = Image.open("photo.jpg")
prompt = "describe this image in detail"

inputs = processor(text=prompt, images=image, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=256)
description = processor.decode(outputs[0], skip_special_tokens=True)
print(description)
\`\`\`

---

## 💻 CodeGemma（代码模型）

\`\`\`python
from transformers import GemmaTokenizer, AutoModelForCausalLM

model_id = "google/codegemma-7b-it"
tokenizer = GemmaTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto")

# 代码补全
prompt = '''def fibonacci(n):
    """Calculate the nth Fibonacci number."""
'''

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=200)
code = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(code)
\`\`\`

---

## 🆚 Gemma vs Llama 对比

| 特性 | Gemma 2 | Llama 3 |
|------|---------|---------|
| 开发商 | Google | Meta |
| 许可证 | Gemma ToU | Llama 3 Community |
| 商用 | ✅ (有限制) | ✅ (有限制) |
| 最大参数 | 27B | 70B |
| 中文能力 | 中等 | 较好 |
| 代码能力 | 强 (CodeGemma) | 强 |
| 视觉版本 | PaliGemma | Llama 3.2 Vision |

---

## 📦 部署方案对比

| 方案 | 优点 | 缺点 |
|------|------|------|
| Ollama | 简单易用 | 功能有限 |
| vLLM | 高性能 | 配置复杂 |
| TGI | 企业级 | 需要更多资源 |
| llama.cpp | 低资源 | CPU 为主 |
            `,
            ja: `
## Gemma：Googleのオープンソース大規模モデル

GemmaはGoogleがオープンソース化した軽量大規模モデルシリーズで、Geminiと技術アーキテクチャを共有。

---

## 📊 Gemmaモデル比較

| モデル | パラメータ | 特徴 |
|--------|------------|------|
| **Gemma 2 27B** | 27B | 最高性能 |
| **Gemma 2 9B** | 9B | バランス型 |
| **Gemma 2 2B** | 2B | 軽量高速 |
| **CodeGemma 7B** | 7B | コード特化 |
| **PaliGemma** | 3B | ビジョン言語 |

---

## 🔧 ローカルデプロイ（Ollama）

\`\`\`bash
ollama pull gemma2:9b
ollama run gemma2:9b
\`\`\`

---

## 🐍 Python呼び出し

\`\`\`python
from transformers import AutoTokenizer, AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-2-9b-it",
    device_map="auto",
    torch_dtype=torch.bfloat16
)

messages = [{"role": "user", "content": "Pythonのクイックソートを書いて"}]
# ...
\`\`\`

---

## 🆚 Gemma vs Llama

| 特性 | Gemma 2 | Llama 3 |
|------|---------|---------|
| 開発元 | Google | Meta |
| ライセンス | Gemma ToU | Llama 3 Community |
| 最大パラメータ | 27B | 70B |
            `
          }
        },
        {
          id: 'ch9-notebooklm',
          title: { zh: '9.5 NotebookLM 研究助手', ja: '9.5 NotebookLM リサーチアシスタント' },
          content: {
            zh: `
## NotebookLM：AI 驱动的研究助手

NotebookLM 是 Google 推出的 AI 笔记工具，可以基于你上传的文档进行问答和总结。

---

## 🌐 访问地址

> **https://notebooklm.google.com**

---

## 🎯 核心功能

### 1. 文档上传与理解

支持的格式：
- PDF 文档
- Google Docs
- 网页链接
- 文本文件
- YouTube 视频

\`\`\`
┌─────────────────────────────────────────┐
│  Sources（来源）                         │
├─────────────────────────────────────────┤
│  📄 research_paper.pdf                   │
│  📄 meeting_notes.docx                   │
│  🔗 https://arxiv.org/...               │
│  🎬 YouTube: AI 技术讲座                 │
│                                          │
│  [+ Add Source]                          │
└─────────────────────────────────────────┘
\`\`\`

### 2. 智能问答

\`\`\`
用户: 这篇论文的主要贡献是什么？

NotebookLM: 根据论文内容，主要贡献包括：
1. 提出了新的注意力机制... [引用: 第3页]
2. 在多个基准测试上取得SOTA... [引用: 第7页]
3. ...
\`\`\`

### 3. 自动生成笔记

- **文档摘要** —— 一键生成概要
- **关键问题** —— 提取核心问题
- **FAQ 生成** —— 自动生成问答对
- **时间线** —— 提取时间节点

### 4. 🎙️ Audio Overview（音频概述）

**最具特色的功能！** 将文档转换为两人对话的播客形式。

\`\`\`
[生成音频概述]
        ↓
两位 AI 主持人讨论你的文档内容
        ↓
~10分钟的播客式音频
        ↓
可下载 MP3 离线收听
\`\`\`

---

## 💡 使用场景

| 场景 | 用法 |
|------|------|
| **论文研究** | 上传 PDF，快速理解核心内容 |
| **会议纪要** | 上传录音转写，生成摘要 |
| **学习笔记** | 上传教材，生成问答卡片 |
| **竞品分析** | 上传多篇报告，交叉对比 |
| **播客创作** | 将长文档转为音频内容 |

---

## 🔧 高级技巧

### 1. 多文档交叉引用

\`\`\`
上传多篇论文后提问：
"比较这三篇论文在方法论上的异同"
→ NotebookLM 会综合分析并标注来源
\`\`\`

### 2. 引用追溯

点击回答中的引用标注，可直接跳转到原文位置。

### 3. 笔记导出

生成的笔记可以导出为 Google Doc，便于编辑和分享。

---

## ⚠️ 限制

| 限制 | 说明 |
|------|------|
| 来源数量 | 最多 50 个来源/笔记本 |
| 文件大小 | 每个文件最大 200MB |
| 音频长度 | Audio Overview 约 10 分钟 |
| 语言 | 目前英文效果最佳 |
| 隐私 | 文档内容用于 AI 处理 |

---

## 🆚 与其他工具对比

| 工具 | 特点 | 适用场景 |
|------|------|----------|
| **NotebookLM** | 深度文档理解 | 研究、学习 |
| **ChatGPT + 文件** | 通用对话 | 快速问答 |
| **Claude Projects** | 代码 + 文档 | 开发项目 |
| **Perplexity** | 网络搜索 | 信息检索 |
            `,
            ja: `
## NotebookLM：AI駆動のリサーチアシスタント

NotebookLMはGoogleが提供するAIノートツールで、アップロードしたドキュメントに基づいてQ&Aと要約が可能。

---

## 🌐 アクセスURL

> **https://notebooklm.google.com**

---

## 🎯 コア機能

### 1. ドキュメントアップロード

対応フォーマット：PDF、Google Docs、Webリンク、テキスト、YouTube動画

### 2. スマートQ&A

ドキュメントの内容に基づいて質問に回答し、引用元を明示

### 3. 自動ノート生成

- ドキュメント要約
- 重要な質問抽出
- FAQ自動生成

### 4. 🎙️ Audio Overview

**最も特徴的な機能！** ドキュメントを2人の対話形式のポッドキャストに変換

---

## 💡 使用シーン

| シーン | 用途 |
|--------|------|
| **論文研究** | PDFをアップロードし、核心内容を素早く理解 |
| **会議メモ** | 録音の文字起こしをアップロードし、要約を生成 |
| **学習ノート** | 教材をアップロードし、Q&Aカードを生成 |

---

## ⚠️ 制限

| 制限 | 説明 |
|------|------|
| ソース数 | 最大50ソース/ノートブック |
| ファイルサイズ | 各ファイル最大200MB |
            `
          }
        },
        {
          id: 'ch9-cloud-ai',
          title: { zh: '9.6 Google Cloud AI 服务', ja: '9.6 Google Cloud AIサービス' },
          content: {
            zh: `
## Google Cloud AI 服务矩阵

Google Cloud 提供丰富的预训练 AI 服务，可直接调用 API。

---

## 🗣️ Speech-to-Text（语音转文字）

\`\`\`python
from google.cloud import speech

client = speech.SpeechClient()

# 从文件读取音频
with open("audio.wav", "rb") as f:
    audio = speech.RecognitionAudio(content=f.read())

config = speech.RecognitionConfig(
    encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
    sample_rate_hertz=16000,
    language_code="zh-CN",  # 中文
    enable_automatic_punctuation=True,
    enable_word_time_offsets=True,  # 获取单词时间戳
)

response = client.recognize(config=config, audio=audio)

for result in response.results:
    print(f"转写: {result.alternatives[0].transcript}")
    print(f"置信度: {result.alternatives[0].confidence}")
\`\`\`

### 支持的语言

- 中文（普通话、粤语）
- 日语、韩语
- 英语（多种口音）
- 125+ 种语言

---

## 🔊 Text-to-Speech（文字转语音）

\`\`\`python
from google.cloud import texttospeech

client = texttospeech.TextToSpeechClient()

# 设置文本
synthesis_input = texttospeech.SynthesisInput(
    text="欢迎使用 Google Cloud 语音合成服务"
)

# 选择声音
voice = texttospeech.VoiceSelectionParams(
    language_code="zh-CN",
    name="zh-CN-Wavenet-A",  # Wavenet 更自然
    ssml_gender=texttospeech.SsmlVoiceGender.FEMALE
)

# 音频配置
audio_config = texttospeech.AudioConfig(
    audio_encoding=texttospeech.AudioEncoding.MP3,
    speaking_rate=1.0,  # 语速
    pitch=0.0  # 音调
)

response = client.synthesize_speech(
    input=synthesis_input,
    voice=voice,
    audio_config=audio_config
)

with open("output.mp3", "wb") as f:
    f.write(response.audio_content)
\`\`\`

### 声音类型

| 类型 | 特点 | 价格 |
|------|------|------|
| Standard | 基础质量 | 最低 |
| Wavenet | 高质量 | 中等 |
| Neural2 | 最自然 | 最高 |
| Studio | 专业配音 | 按需 |

---

## 👁️ Vision AI（视觉 AI）

\`\`\`python
from google.cloud import vision

client = vision.ImageAnnotatorClient()

# 从文件读取图片
with open("image.jpg", "rb") as f:
    content = f.read()

image = vision.Image(content=content)

# 多种分析功能
# 1. 标签检测
labels = client.label_detection(image=image).label_annotations
for label in labels:
    print(f"标签: {label.description}, 置信度: {label.score:.2f}")

# 2. 文字识别 (OCR)
texts = client.text_detection(image=image).text_annotations
if texts:
    print(f"识别文字: {texts[0].description}")

# 3. 人脸检测
faces = client.face_detection(image=image).face_annotations
for face in faces:
    print(f"表情: 喜悦={face.joy_likelihood.name}")

# 4. 物体定位
objects = client.object_localization(image=image).localized_object_annotations
for obj in objects:
    print(f"物体: {obj.name}, 位置: {obj.bounding_poly}")

# 5. 安全检测
safe = client.safe_search_detection(image=image).safe_search_annotation
print(f"成人内容: {safe.adult.name}")
\`\`\`

---

## 🌐 Translation（翻译）

\`\`\`python
from google.cloud import translate_v2 as translate

client = translate.Client()

# 简单翻译
result = client.translate(
    "Hello, how are you?",
    target_language="zh-CN"
)
print(result["translatedText"])  # 你好，你好吗？

# 检测语言
detection = client.detect_language("こんにちは")
print(f"语言: {detection['language']}, 置信度: {detection['confidence']}")

# 批量翻译
texts = ["Hello", "World", "AI is amazing"]
results = client.translate(texts, target_language="ja")
for r in results:
    print(f"{r['input']} -> {r['translatedText']}")
\`\`\`

---

## 📊 Natural Language（自然语言处理）

\`\`\`python
from google.cloud import language_v1

client = language_v1.LanguageServiceClient()

text = "谷歌是一家总部位于加利福尼亚的科技公司，由拉里·佩奇和谢尔盖·布林创立。"
document = language_v1.Document(
    content=text,
    type_=language_v1.Document.Type.PLAIN_TEXT,
    language="zh"
)

# 情感分析
sentiment = client.analyze_sentiment(document=document).document_sentiment
print(f"情感: {sentiment.score:.2f} (正面=1, 负面=-1)")

# 实体识别
entities = client.analyze_entities(document=document).entities
for entity in entities:
    print(f"实体: {entity.name}, 类型: {entity.type_.name}")

# 语法分析
syntax = client.analyze_syntax(document=document)
for token in syntax.tokens:
    print(f"词: {token.text.content}, 词性: {token.part_of_speech.tag.name}")
\`\`\`

---

## 💰 定价概览

| 服务 | 免费额度 | 超出后价格 |
|------|----------|------------|
| Speech-to-Text | 60分钟/月 | $0.006/15秒 |
| Text-to-Speech | 400万字符/月 | $4-16/百万字符 |
| Vision AI | 1000次/月 | $1.5/千次 |
| Translation | 50万字符/月 | $20/百万字符 |
| Natural Language | 5000次/月 | $1-2/千次 |
            `,
            ja: `
## Google Cloud AIサービスマトリックス

Google Cloudは豊富な事前学習AIサービスを提供し、直接APIを呼び出し可能。

---

## 🗣️ Speech-to-Text

\`\`\`python
from google.cloud import speech

client = speech.SpeechClient()
config = speech.RecognitionConfig(
    language_code="ja-JP",
    enable_automatic_punctuation=True
)
response = client.recognize(config=config, audio=audio)
\`\`\`

---

## 🔊 Text-to-Speech

\`\`\`python
from google.cloud import texttospeech

client = texttospeech.TextToSpeechClient()
voice = texttospeech.VoiceSelectionParams(
    language_code="ja-JP",
    name="ja-JP-Wavenet-A"
)
\`\`\`

---

## 👁️ Vision AI

- ラベル検出
- OCR（文字認識）
- 顔検出
- 物体検出
- 安全検索

---

## 💰 料金概要

| サービス | 無料枠 | 超過後 |
|----------|--------|--------|
| Speech-to-Text | 60分/月 | $0.006/15秒 |
| Text-to-Speech | 400万文字/月 | $4-16/百万文字 |
| Vision AI | 1000回/月 | $1.5/千回 |
            `
          }
        },
        {
          id: 'ch9-summary',
          title: { zh: '9.7 本章小结', ja: '9.7 この章のまとめ' },
          content: {
            zh: `
## Google AI 生态系统总览

---

## 🗺️ 产品地图

\`\`\`
┌─────────────────────────────────────────────────────────────┐
│                    Google AI 生态系统                        │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  ┌─────────────────┐  ┌─────────────────┐                   │
│  │    Gemini       │  │   Gemma         │                   │
│  │  (闭源旗舰)     │  │  (开源轻量)     │                   │
│  └─────────────────┘  └─────────────────┘                   │
│           │                    │                             │
│           ▼                    ▼                             │
│  ┌─────────────────┐  ┌─────────────────┐                   │
│  │  AI Studio      │  │  HuggingFace    │                   │
│  │  (免费原型)     │  │  Ollama 本地    │                   │
│  └─────────────────┘  └─────────────────┘                   │
│           │                                                  │
│           ▼                                                  │
│  ┌─────────────────────────────────────────┐                │
│  │            Vertex AI                     │                │
│  │  (企业级：训练、部署、监控)              │                │
│  └─────────────────────────────────────────┘                │
│           │                                                  │
│           ▼                                                  │
│  ┌─────────────────────────────────────────┐                │
│  │         Google Cloud AI Services         │                │
│  │  Speech | Vision | Translation | NLP     │                │
│  └─────────────────────────────────────────┘                │
│                                                              │
│  ┌─────────────────┐                                        │
│  │   NotebookLM    │  ← 研究助手                            │
│  └─────────────────┘                                        │
└─────────────────────────────────────────────────────────────┘
\`\`\`

---

## 🎯 选择指南

| 需求 | 推荐产品 |
|------|----------|
| 快速原型 | AI Studio + Gemini API |
| 企业部署 | Vertex AI |
| 本地运行 | Gemma + Ollama |
| 文档研究 | NotebookLM |
| 语音处理 | Speech-to-Text / TTS |
| 图像分析 | Vision AI |
| 多语言 | Translation API |

---

## 💡 最佳实践

1. **先用 AI Studio 验证** —— 免费测试 Prompt 效果
2. **选择合适的模型** —— Flash 省钱，Pro 强大
3. **善用免费额度** —— 各服务都有免费层
4. **考虑隐私合规** —— 敏感数据用 Gemma 本地部署
5. **监控成本** —— 设置预算提醒

---

## 📚 学习资源

- 📖 [Google AI 官方文档](https://ai.google.dev/)
- 🎓 [Google Cloud Skills Boost](https://www.cloudskillsboost.google/)
- 🔬 [Google AI Blog](https://blog.google/technology/ai/)
- 💬 [Gemini API Cookbook](https://github.com/google-gemini/cookbook)
            `,
            ja: `
## Google AIエコシステム総覧

---

## 🗺️ プロダクトマップ

| 製品 | 特徴 | 用途 |
|------|------|------|
| **Gemini** | クローズドソースフラッグシップ | 本番アプリ |
| **Gemma** | オープンソース軽量 | ローカルデプロイ |
| **AI Studio** | 無料プロトタイプ | 実験・検証 |
| **Vertex AI** | エンタープライズ | 大規模デプロイ |
| **NotebookLM** | リサーチアシスタント | 文書分析 |
| **Cloud AI** | 事前学習サービス | 音声・画像・翻訳 |

---

## 🎯 選択ガイド

| ニーズ | 推奨製品 |
|--------|----------|
| クイックプロトタイプ | AI Studio + Gemini API |
| エンタープライズデプロイ | Vertex AI |
| ローカル実行 | Gemma + Ollama |
| ドキュメント研究 | NotebookLM |

---

## 📚 学習リソース

- [Google AI公式ドキュメント](https://ai.google.dev/)
- [Gemini API Cookbook](https://github.com/google-gemini/cookbook)
            `
          }
        }
      ]
    },
  ]
};

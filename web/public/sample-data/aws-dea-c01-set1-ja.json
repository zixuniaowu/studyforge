{
  "exam": {
    "id": "aws-dea-c01-set1-ja",
    "name": "AWS DEA-C01 模擬試験 #1",
    "code": "DEA-C01",
    "provider": "AWS",
    "language": "ja",
    "description": "AWSデータエンジニアアソシエイト認定試験模擬問題 - 第1セット",
    "totalQuestions": 40,
    "passingScore": 72,
    "examTime": 130,
    "domains": [
      {"id": 1, "name": "データ取り込みと変換", "weight": 34},
      {"id": 2, "name": "データストア管理", "weight": 26},
      {"id": 3, "name": "データ運用とサポート", "weight": 22},
      {"id": 4, "name": "データセキュリティとガバナンス", "weight": 18}
    ],
    "tags": ["AWS", "データエンジニアリング", "DEA", "認定試験"]
  },
  "questions": [
    {
      "id": "q1",
      "domain": 1,
      "question": "企業が複数のデータソースからリアルタイムでストリーミングデータを取り込み、5分以内に分析チームにデータを提供する必要があります。どのAWSサービスの組み合わせを使用すべきですか？",
      "options": {
        "A": "Amazon Kinesis Data Streams + AWS Lambda",
        "B": "Amazon SQS + Amazon EC2",
        "C": "AWS Data Pipeline + Amazon S3",
        "D": "Amazon EMR + Apache Spark"
      },
      "answer": "A",
      "explanation": "Kinesis Data Streamsはリアルタイムストリーミングデータの取り込み用に設計されており、Lambdaと組み合わせることで、5分以内のレイテンシ要件を満たすほぼリアルタイムのデータ処理と変換を実現できます。"
    },
    {
      "id": "q2",
      "domain": 1,
      "question": "データエンジニアがクエリパフォーマンスを最適化するためにCSVファイルをParquet形式に変換する必要があります。AWS Glueを使用する場合、最も効果的な方法は何ですか？",
      "options": {
        "A": "Glue Crawlerを使用して自動変換する",
        "B": "Glue ETL Jobを使用して形式変換を行う",
        "C": "Glue Data Catalogを使用して変換ルールを保存する",
        "D": "Glue Schema Registryを使用して変換する"
      },
      "answer": "B",
      "explanation": "AWS Glue ETL JobはCSVファイルを読み取り、Parquet形式で書き出すことができます。Crawlerはスキーマの検出に使用され、Data Catalogはメタデータ管理に、Schema Registryはストリーミングデータのスキーマ管理に使用されます。"
    },
    {
      "id": "q3",
      "domain": 1,
      "question": "企業がKinesis Data Firehoseを使用してS3にデータを配信しています。配信前に機密データのマスキング処理を行う必要があります。どのように実装すべきですか？",
      "options": {
        "A": "Firehoseデータ暗号化を有効にする",
        "B": "Firehoseのデータ変換機能でLambdaを呼び出す",
        "C": "S3バケットポリシーでデータフィルタリングを設定する",
        "D": "Kinesis Data Analyticsで処理する"
      },
      "answer": "B",
      "explanation": "Kinesis Data Firehoseはデータ変換機能をサポートしており、Lambda関数を呼び出して転送中のデータを処理（マスキング、形式変換などの操作を含む）できます。"
    },
    {
      "id": "q4",
      "domain": 1,
      "question": "データチームがリレーショナルデータベースから変更データキャプチャ（CDC）を使用してデータレイクに増分抽出する必要があります。最適なAWSサービスは何ですか？",
      "options": {
        "A": "AWS Database Migration Service (DMS)",
        "B": "AWS DataSync",
        "C": "AWS Transfer Family",
        "D": "Amazon AppFlow"
      },
      "answer": "A",
      "explanation": "AWS DMSは継続的レプリケーションとCDC機能をサポートし、ソースデータベースの増分変更をキャプチャしてターゲットに複製できます。データベースからデータレイクへのリアルタイム同期シナリオに非常に適しています。"
    },
    {
      "id": "q5",
      "domain": 1,
      "question": "AWS GlueでETLを実行する際、ソースデータのスキーマ進化問題をどのように処理しますか？",
      "options": {
        "A": "実行前に毎回Data Catalogを手動で更新する",
        "B": "Glue Crawlerを定期的に実行してスキーマを更新する",
        "C": "ETLスクリプトでDynamicFrameを使用して処理する",
        "D": "スキーマ検証を無効にする"
      },
      "answer": "C",
      "explanation": "GlueのDynamicFrameはスキーマが一貫していないデータを処理でき、固定スキーマを強制せず、スキーマ進化の状況を柔軟に処理できます。ResolveChoiceなどの変換と組み合わせてスキーマの競合を処理できます。"
    },
    {
      "id": "q6",
      "domain": 2,
      "question": "企業がPB規模の非構造化データを保存し、SQLを使用してクエリ分析を行う必要があります。最適なストレージとクエリの組み合わせは何ですか？",
      "options": {
        "A": "Amazon RDS + 標準SQLクエリ",
        "B": "Amazon S3 + Amazon Athena",
        "C": "Amazon DynamoDB + PartiQL",
        "D": "Amazon EFS + Amazon EMR"
      },
      "answer": "B",
      "explanation": "Amazon S3はPB規模のデータを低コストで保存するのに適しています。Athenaはサーバーレスクエリサービスで、S3内のデータに対して標準SQLで直接クエリを実行でき、インフラストラクチャの管理が不要です。"
    },
    {
      "id": "q7",
      "domain": 2,
      "question": "データウェアハウスがコンプライアンスレポート用に5年分の履歴データを保存する必要がありますが、頻繁にアクセスされるのは直近90日分のデータのみです。Redshiftのストレージコストを最適化するにはどうすればよいですか？",
      "options": {
        "A": "Redshift Spectrumを使用してすべてのデータをクエリする",
        "B": "履歴データをS3にアンロードし、Redshift Spectrumを使用する",
        "C": "Redshift圧縮エンコーディングを使用する",
        "D": "Redshiftクラスターノードを増やす"
      },
      "answer": "B",
      "explanation": "アクセス頻度の低い履歴データをS3にアンロードするとRedshiftのストレージコストを削減でき、Redshift Spectrumを使用してS3内のデータを透過的にクエリできるため、クエリの統一性が維持されます。"
    },
    {
      "id": "q8",
      "domain": 2,
      "question": "企業がAmazon Redshiftをデータウェアハウスとして使用しています。複雑な分析クエリのパフォーマンスを向上させるには、どのテーブル設計戦略を採用すべきですか？",
      "options": {
        "A": "すべてのテーブルにEVEN分散スタイルを使用する",
        "B": "クエリパターンに基づいて適切な分散キーとソートキーを選択する",
        "C": "すべてのテーブルにALL分散スタイルを使用する",
        "D": "分散スタイルの使用を避ける"
      },
      "answer": "B",
      "explanation": "Redshiftのパフォーマンス最適化の鍵は、実際のクエリパターンに基づいて適切な分散キー（データ移動を減らす）とソートキー（ゾーンマップの最適化と効率的な範囲スキャンをサポート）を選択することです。"
    },
    {
      "id": "q9",
      "domain": 2,
      "question": "データレイクがS3を使用してデータを保存しており、並行更新を処理するためにACIDトランザクションをサポートする必要があります。どの技術を使用すべきですか？",
      "options": {
        "A": "S3 Object Lock",
        "B": "S3 Versioning",
        "C": "Apache IcebergまたはDelta Lake",
        "D": "S3 Cross-Region Replication"
      },
      "answer": "C",
      "explanation": "Apache IcebergとDelta Lakeはオープンテーブルフォーマットで、S3上でACIDトランザクション、タイムトラベル、並行制御を実現できます。AWS EMRとGlueはこれらのフォーマットをサポートしています。"
    },
    {
      "id": "q10",
      "domain": 2,
      "question": "Amazon DynamoDBを使用して時系列データを保存する場合、効率的な時間範囲クエリをサポートするために主キーをどのように設計すべきですか？",
      "options": {
        "A": "タイムスタンプをパーティションキーとして使用する",
        "B": "デバイスIDをパーティションキー、タイムスタンプをソートキーとして使用する",
        "C": "ランダムIDをパーティションキーとして使用する",
        "D": "複合文字列をパーティションキーとして使用する"
      },
      "answer": "B",
      "explanation": "デバイスIDをパーティションキーとして使用するとデータが分散され、ホットパーティションを回避できます。タイムスタンプをソートキーとして使用すると効率的な時間範囲クエリがサポートされます。これはDynamoDB時系列データの典型的な設計パターンです。"
    },
    {
      "id": "q11",
      "domain": 3,
      "question": "データパイプラインが1日1回実行されますが、ソースシステムの問題で時々失敗します。自動リトライとアラートをどのように実装しますか？",
      "options": {
        "A": "AWS Step Functionsでオーケストレーションし、リトライポリシーを設定する",
        "B": "cronジョブとシェルスクリプトを使用する",
        "C": "手動で監視して再実行する",
        "D": "データパイプラインのタイムアウト時間を延長する"
      },
      "answer": "A",
      "explanation": "Step Functionsは組み込みのリトライメカニズム、エラー処理、状態管理を提供し、指数バックオフのリトライポリシーを設定でき、SNSと統合して失敗時のアラートを送信できます。"
    },
    {
      "id": "q12",
      "domain": 3,
      "question": "AWS Glue Jobの実行が遅く、ログにデータスキュー問題が表示されています。どのように解決しますか？",
      "options": {
        "A": "DPU数を増やす",
        "B": "groupFilesパラメータとカスタムパーティション戦略を使用する",
        "C": "Python Shell Jobに切り替える",
        "D": "並列度を下げる"
      },
      "answer": "B",
      "explanation": "データスキューはデータの再パーティショニングで解決できます。GlueのgroupFilesパラメータは小さなファイルを統合でき、repartitionやカスタムパーティションキーを使用するとデータ分散のバランスを取れます。"
    },
    {
      "id": "q13",
      "domain": 3,
      "question": "企業がデータウェアハウスに入るデータがビジネスルールを満たすことを確認するためにデータ品質を監視する必要があります。ベストプラクティスは何ですか？",
      "options": {
        "A": "ETL完了後に手動でのみ確認する",
        "B": "AWS Glue DataBrewを使用してデータプロファイリングと品質チェックを行う",
        "C": "ソースシステムのデータ検証に依存する",
        "D": "データ形式のみを確認し、内容は確認しない"
      },
      "answer": "B",
      "explanation": "Glue DataBrewはデータプロファイリング機能を提供し、データ品質の問題を発見できます。Glue Data Qualityと組み合わせてルールを定義し、データ品質を自動的に検証できます。"
    },
    {
      "id": "q14",
      "domain": 3,
      "question": "EMRクラスターが大量の小さなファイルを処理してパフォーマンス問題が発生しています。どのように最適化しますか？",
      "options": {
        "A": "クラスターノード数を増やす",
        "B": "S3DistCpまたはSparkを使用して小さなファイルを統合する",
        "C": "より大きなEC2インスタンスタイプを使用する",
        "D": "EMRマネージドスケーリングを有効にする"
      },
      "answer": "B",
      "explanation": "小さなファイル問題は過剰なタスクオーバーヘッドとメタデータ操作を引き起こします。S3DistCpまたはSparkのcoalesce/repartitionを使用して小さなファイルを統合し、ファイル数を減らして処理効率を向上させることができます。"
    },
    {
      "id": "q15",
      "domain": 3,
      "question": "Glue ETLジョブのデプロイメントとバージョン管理を自動化する必要があります。どの方法を使用すべきですか？",
      "options": {
        "A": "Glueコンソールで手動でスクリプトを更新する",
        "B": "AWS CloudFormationまたはCDKを使用してGlueリソースをデプロイする",
        "C": "S3を使用してスクリプトバージョンを保存する",
        "D": "EC2を使用してETLスクリプトを実行する"
      },
      "answer": "B",
      "explanation": "CloudFormationまたはCDKはGlueジョブをInfrastructure as Code (IaC)として定義でき、バージョン管理、コードレビュー、自動化デプロイメントを実現できます。これはCI/CDのベストプラクティスです。"
    },
    {
      "id": "q16",
      "domain": 4,
      "question": "データレイクに機密性の高いPIIデータが含まれています。承認されたユーザーのみが特定の列にアクセスできるようにするにはどうすればよいですか？",
      "options": {
        "A": "S3バケットポリシーを使用する",
        "B": "AWS Lake Formationの列レベル権限を使用する",
        "C": "IAMユーザーポリシーを使用する",
        "D": "S3バケット全体を暗号化する"
      },
      "answer": "B",
      "explanation": "AWS Lake Formationはきめ細かな列レベルと行レベルのアクセス制御を提供し、データレイク内の特定の列にアクセスできるユーザーを制御でき、PIIデータの保護に適しています。"
    },
    {
      "id": "q17",
      "domain": 4,
      "question": "企業がGDPR要件を満たす必要があり、特定のユーザーのすべてのデータを識別して削除できる必要があります。ベストプラクティスは何ですか？",
      "options": {
        "A": "すべてのデータストアを手動で検索する",
        "B": "AWS Glue Data Catalogを使用して機密データにタグ付けし、データリネージを構築する",
        "C": "ユーザーデータの収集を禁止する",
        "D": "すべてのデータを単一の場所に保存する"
      },
      "answer": "B",
      "explanation": "Data Catalogを使用して機密データにタグ付けして分類し、データリネージの追跡と組み合わせることで、データパイプライン全体で特定のデータの位置を識別でき、GDPRの忘れられる権利の要件をサポートします。"
    },
    {
      "id": "q18",
      "domain": 4,
      "question": "S3内のデータを静的に暗号化する必要があり、キーは企業が管理およびローテーションする必要があります。何を使用すべきですか？",
      "options": {
        "A": "SSE-S3",
        "B": "SSE-KMS with CMK",
        "C": "SSE-C",
        "D": "暗号化せず、VPC Endpointを使用する"
      },
      "answer": "B",
      "explanation": "カスタマーマネージドキー（CMK）を使用したSSE-KMSにより、企業はキーの作成、ローテーション、キー使用の監査など、キーのライフサイクルを完全に制御しながら、KMSのマネージドサービスを活用できます。"
    },
    {
      "id": "q19",
      "domain": 4,
      "question": "データエンジニアリングチームが誰がいつデータレイク内のどのデータにアクセスしたかを監査する必要があります。何を使用すべきですか？",
      "options": {
        "A": "S3 Server Access Logging",
        "B": "AWS CloudTrail + Lake Formation監査ログ",
        "C": "Amazon CloudWatch Logs",
        "D": "VPC Flow Logs"
      },
      "answer": "B",
      "explanation": "CloudTrailはAPI呼び出しを記録し、Lake Formationはデータアクセス監査ログを提供します。両者を組み合わせることで、誰がいつ何のデータにアクセスしたかを完全に追跡でき、コンプライアンス監査要件を満たします。"
    },
    {
      "id": "q20",
      "domain": 4,
      "question": "Redshiftクラスターが他のAWSアカウントのS3データに対してフェデレーテッドクエリを実行する必要があります。安全に実現するにはどうすればよいですか？",
      "options": {
        "A": "S3バケットを公開する",
        "B": "クロスアカウントIAMロールとRedshift Spectrumを使用する",
        "C": "データを自アカウントにコピーする",
        "D": "VPCピアリングを使用する"
      },
      "answer": "B",
      "explanation": "クロスアカウントIAMロールにより、Redshiftは別のアカウントのS3データに安全にアクセスでき、Redshift Spectrumは外部S3データに直接クエリを実行でき、データをコピーする必要がありません。"
    },
    {
      "id": "q21",
      "domain": 1,
      "question": "SaaSアプリケーション（Salesforceなど）から定期的にS3にデータを抽出する必要があります。最も簡単なソリューションは何ですか？",
      "options": {
        "A": "カスタムAPI統合を開発する",
        "B": "Amazon AppFlowを使用する",
        "C": "AWS Lambdaを使用してAPIを呼び出す",
        "D": "AWS Data Pipelineを使用する"
      },
      "answer": "B",
      "explanation": "Amazon AppFlowはフルマネージドの統合サービスで、Salesforceなどの一般的なSaaSアプリケーションへのコネクタが事前に設定されており、コードを書かずにデータフローを簡単に設定できます。"
    },
    {
      "id": "q22",
      "domain": 1,
      "question": "Kinesis Data Streamsで高スループットデータを処理する際、コンシューマーの処理速度がプロデューサーに追いつかない状況をどのように処理しますか？",
      "options": {
        "A": "シャード数を増やす",
        "B": "Enhanced Fan-Out（拡張ファンアウト）を使用する",
        "C": "データ保持期間を短縮する",
        "D": "より大きなバッチサイズを使用する"
      },
      "answer": "B",
      "explanation": "Enhanced Fan-Outは各コンシューマーに専用の2MB/sスループットを提供し、コンシューマー間の競合を回避します。単一シャードのデータ量が多すぎる場合は、シャード数を増やすこともオプションです。"
    },
    {
      "id": "q23",
      "domain": 1,
      "question": "AWS Glue ETLジョブがVPC内のRDSデータベースに接続する必要があります。何を設定する必要がありますか？",
      "options": {
        "A": "RDSセキュリティグループのみを設定する",
        "B": "VPC、サブネット、セキュリティグループを指定してGlue Connectionを設定する",
        "C": "パブリックRDSエンドポイントを使用する",
        "D": "NAT Gatewayを設定する"
      },
      "answer": "B",
      "explanation": "GlueはGlue ConnectionでVPCアクセスを設定し、サブネットとセキュリティグループを指定して、GlueジョブがENI経由でVPC内のリソースにアクセスできるようにする必要があります。"
    },
    {
      "id": "q24",
      "domain": 2,
      "question": "データ分析チームがS3内のデータに対してインタラクティブなSQLクエリを実行する必要がありますが、クエリ結果セットが大きいです。Athenaのクエリコストを最適化するにはどうすればよいですか？",
      "options": {
        "A": "より多くのパーティションを使用する",
        "B": "カラムナストレージ形式（Parquet/ORC）を使用してパーティショニングを行う",
        "C": "クエリタイムアウト時間を延長する",
        "D": "Athenaワークグループ制限を使用する"
      },
      "answer": "B",
      "explanation": "Athenaはスキャンしたデータ量で課金されます。カラムナ形式（Parquet/ORC）とパーティショニングを使用すると、スキャンするデータ量を大幅に削減でき、コストを下げてクエリ速度を向上させることができます。"
    },
    {
      "id": "q25",
      "domain": 2,
      "question": "Redshiftに大量のデータを高速にロードする必要があります。どの方法が最もパフォーマンスが良いですか？",
      "options": {
        "A": "INSERT文を使用して1行ずつ挿入する",
        "B": "COPYコマンドを使用してS3からバルクロードする",
        "C": "JDBC接続を使用してバルク挿入する",
        "D": "Redshift Data APIを使用する"
      },
      "answer": "B",
      "explanation": "COPYコマンドはRedshiftにデータをロードする最も効率的な方法で、データを並列にロードし、自動的に圧縮し、マニフェストファイルを使用して複数ファイルのロードを管理できます。"
    },
    {
      "id": "q26",
      "domain": 2,
      "question": "データレイクアーキテクチャで、データの異なる処理段階（生データ、クレンジング済み、集約済み）をどのように管理しますか？",
      "options": {
        "A": "異なるAWSアカウントを使用する",
        "B": "S3プレフィックスでレイヤー分け（bronze/silver/goldまたはraw/curated/aggregated）する",
        "C": "異なるAWSリージョンを使用する",
        "D": "すべてのデータを同じ場所に保存する"
      },
      "answer": "B",
      "explanation": "S3プレフィックス（bronze/silver/goldまたはraw/curated/aggregatedなど）でレイヤー分けすることは、データレイクの一般的なパターンで、データ品質と処理状態を明確に管理できます。"
    },
    {
      "id": "q27",
      "domain": 3,
      "question": "Glueジョブが大量のパーティションを持つテーブルを処理する際にパフォーマンスが低下しています。どのように最適化しますか？",
      "options": {
        "A": "ジョブブックマークを無効にする",
        "B": "push_down_predicateを使用してパーティションプルーニングを行う",
        "C": "ジョブタイムアウト時間を延長する",
        "D": "Python Shellジョブタイプを使用する"
      },
      "answer": "B",
      "explanation": "push_down_predicateにより、Glueはデータ読み取り時にパーティションプルーニングを実行し、必要なパーティションのみを読み取ることで、データスキャン量と処理時間を大幅に削減できます。"
    },
    {
      "id": "q28",
      "domain": 3,
      "question": "複数のGlueジョブの実行状態とパフォーマンスメトリクスを監視する必要があります。何を使用すべきですか？",
      "options": {
        "A": "CloudWatch MetricsとDashboards",
        "B": "Glueコンソールを手動で確認する",
        "C": "メール通知を送信する",
        "D": "サードパーティの監視ツールを使用する"
      },
      "answer": "A",
      "explanation": "Glueは自動的にジョブメトリクスをCloudWatchに送信します。実行時間、DPU使用率、エラーなどが含まれます。CloudWatch Dashboardとアラームを作成して集中監視できます。"
    },
    {
      "id": "q29",
      "domain": 3,
      "question": "EMRクラスターが予測不可能なワークロードを処理する必要があります。パフォーマンスを維持しながらコストを最適化するにはどうすればよいですか？",
      "options": {
        "A": "固定サイズのクラスターを使用する",
        "B": "EMR Managed Scalingを使用する",
        "C": "クラスターサイズを手動で調整する",
        "D": "最大構成で実行する"
      },
      "answer": "B",
      "explanation": "EMR Managed Scalingはワークロードに基づいてクラスターサイズを自動的に調整でき、負荷が高いときは拡張し、低いときは縮小します。また、Spotインスタンスを設定してコストをさらに最適化できます。"
    },
    {
      "id": "q30",
      "domain": 3,
      "question": "データパイプラインが複数の独立した上流タスクの完了を待ってから開始する必要があります。この依存関係をどのように実装しますか？",
      "options": {
        "A": "固定時間遅延を使用する",
        "B": "Step FunctionsのParallel状態の後に次のステップを配置する",
        "C": "すべてのタスクを直列に実行する",
        "D": "手動でトリガーする"
      },
      "answer": "B",
      "explanation": "Step FunctionsのParallel状態は複数のブランチを並列に実行でき、すべてのブランチが完了すると自動的に次の状態に進み、並列タスクの結合（join）パターンを実現します。"
    },
    {
      "id": "q31",
      "domain": 4,
      "question": "データレイク内の機密データを分類して発見する必要があります。どのサービスを使用すべきですか？",
      "options": {
        "A": "Amazon GuardDuty",
        "B": "Amazon Macie",
        "C": "AWS Config",
        "D": "Amazon Detective"
      },
      "answer": "B",
      "explanation": "Amazon Macieは機械学習を使用してS3内の機密データ（PII、財務データなど）を自動的に発見、分類、保護し、発見レポートとアラートを生成できます。"
    },
    {
      "id": "q32",
      "domain": 4,
      "question": "企業ポリシーで転送中のデータを暗号化する必要があります。Kinesis Data FirehoseからS3へのデータ転送が転送中に暗号化されていることをどのように確認しますか？",
      "options": {
        "A": "HTTPSエンドポイントを使用するようにFirehoseを設定する",
        "B": "FirehoseからS3へはデフォルトでTLS暗号化転送を使用する",
        "C": "VPC Endpointを使用する",
        "D": "クライアント側でデータを暗号化する"
      },
      "answer": "B",
      "explanation": "Kinesis Data FirehoseからS3へのデータ転送はデフォルトでTLS暗号化を使用し、転送中のデータセキュリティを確保します。追加の設定は不要です。"
    },
    {
      "id": "q33",
      "domain": 1,
      "question": "IoTデバイスから送信されるテレメトリデータをリアルタイムで処理し、異常を検出したらアラートをトリガーする必要があります。最適なアーキテクチャは何ですか？",
      "options": {
        "A": "IoT Core → S3 → Athena",
        "B": "IoT Core → Kinesis Data Streams → Lambda",
        "C": "IoT Core → SQS → EC2",
        "D": "IoT Core → SNS → Email"
      },
      "answer": "B",
      "explanation": "IoT Coreはメッセージをリアルタイムストリーム処理のためにKinesis Data Streamsにルーティングでき、Lambdaはリアルタイムでデータを分析して異常を検出し、SNSアラートをトリガーできます。"
    },
    {
      "id": "q34",
      "domain": 1,
      "question": "AWS DMSを使用してデータベース移行を行う際、ソースデータベースの一部の列にLOBデータが含まれています。LOBデータが正しく移行されることをどのように確認しますか？",
      "options": {
        "A": "LOBデータは自動的に移行される、設定不要",
        "B": "フルLOBモードまたは制限LOBモードを設定する",
        "C": "LOBデータを手動でエクスポートする",
        "D": "LOB移行はサポートされていない"
      },
      "answer": "B",
      "explanation": "DMSはLOBデータの移行をサポートしており、フルLOBモード（遅いが完全）または制限LOBモード（サイズ制限を設定、速い）を設定できます。データの特性に応じて適切なモードを選択する必要があります。"
    },
    {
      "id": "q35",
      "domain": 2,
      "question": "Redshiftクエリが頻繁に変化するディメンションデータにアクセスする必要があります。どのように最適化しますか？",
      "options": {
        "A": "マテリアライズドビューを使用してディメンションデータをキャッシュする",
        "B": "Redshiftノードを増やす",
        "C": "Redshift Spectrumを使用する",
        "D": "結果キャッシュを無効にする"
      },
      "answer": "A",
      "explanation": "マテリアライズドビューはクエリ結果を事前計算してキャッシュでき、Redshiftは増分リフレッシュをサポートしているため、頻繁にアクセスされるディメンションデータのクエリパフォーマンスを大幅に向上させることができます。"
    },
    {
      "id": "q36",
      "domain": 2,
      "question": "グラフデータ（ソーシャルネットワーク関係など）を保存してクエリする必要があります。最適なAWSサービスは何ですか？",
      "options": {
        "A": "Amazon DynamoDB",
        "B": "Amazon Neptune",
        "C": "Amazon RDS",
        "D": "Amazon Redshift"
      },
      "answer": "B",
      "explanation": "Amazon NeptuneはマネージドグラフデータベースサービスでProperty GraphとRDFモデルをサポートし、GremlinとSPARQLクエリ言語を使用して、高度に関連したデータ向けに設計されています。"
    },
    {
      "id": "q37",
      "domain": 3,
      "question": "Glueジョブブックマーク（Job Bookmark）の目的は何ですか？",
      "options": {
        "A": "ジョブ設定を保存する",
        "B": "処理済みデータを追跡して増分処理を実現する",
        "C": "ジョブログを保存する",
        "D": "中間結果をキャッシュする"
      },
      "answer": "B",
      "explanation": "Glueジョブブックマークはジョブが既に処理したデータを追跡し、次回の実行時に新しいデータのみを処理して増分ETLを実現し、重複処理を回避します。"
    },
    {
      "id": "q38",
      "domain": 3,
      "question": "データパイプラインが失敗した後、最初からではなく失敗点から回復する必要があります。何を使用すべきですか？",
      "options": {
        "A": "Step Functions標準ワークフロー",
        "B": "Lambda関数のリトライ",
        "C": "Step Functions Expressワークフロー",
        "D": "CloudWatch Events"
      },
      "answer": "A",
      "explanation": "Step Functions標準ワークフローは永続的な実行履歴をサポートし、失敗点から回復したり特定のステップをリトライしたりできます。Expressワークフローはこの回復モードをサポートしていません。"
    },
    {
      "id": "q39",
      "domain": 4,
      "question": "複数のAWSアカウントがデータレイク内のデータを共有する必要がありますが、きめ細かなアクセス制御を維持する必要があります。最適なソリューションは何ですか？",
      "options": {
        "A": "データを各アカウントにコピーする",
        "B": "AWS Lake Formationクロスアカウント共有を使用する",
        "C": "S3バケットを公開する",
        "D": "S3レプリケーションを使用する"
      },
      "answer": "B",
      "explanation": "Lake Formationはテーブル、列、または行レベルでアクセス権限を制御できるクロスアカウントのきめ細かなデータ共有をサポートしており、データをコピーする必要がありません。"
    },
    {
      "id": "q40",
      "domain": 4,
      "question": "Glueジョブが使用する一時認証情報の権限を最小化する必要があります。どのように設定すべきですか？",
      "options": {
        "A": "管理者ロールを使用してすべてのジョブを実行する",
        "B": "各ジョブに専用のIAMロールを作成し、必要な権限のみを付与する",
        "C": "ルートアカウント認証情報を使用する",
        "D": "IAMロールを無効にする"
      },
      "answer": "B",
      "explanation": "最小権限の原則に従い、各Glueジョブに専用のIAMロールを作成し、そのジョブに必要な最小限の権限のみを付与することがセキュリティのベストプラクティスです。"
    }
  ]
}

{
  "exam": {
    "id": "azure-dp-203-set3",
    "name": "Data Engineering on Microsoft Azure 练习题 Set 3",
    "code": "DP-203",
    "provider": "Microsoft",
    "language": "zh-CN",
    "description": "Azure 数据工程解决方案的设计和实现练习题集 3，涵盖高级数据存储、处理和优化技术",
    "totalQuestions": 40,
    "passingScore": 70,
    "examTime": 120,
    "domains": [
      "设计和实现数据存储",
      "开发数据处理",
      "保护、监视和优化数据存储和数据处理"
    ],
    "tags": ["Azure", "Data Engineering", "Synapse", "Data Factory", "Databricks"]
  },
  "questions": [
    {
      "id": "dp203-s3-q1",
      "examId": "azure-dp-203-set3",
      "setNumber": 3,
      "domain": "设计和实现数据存储",
      "question": "您正在使用 Azure Synapse Analytics 设计数据仓库。您需要实现一个累积快照事实表来跟踪订单从创建到完成的整个生命周期。应该如何设计此表？",
      "options": {
        "A": "为每个订单状态创建单独的行",
        "B": "使用单行存储所有里程碑日期和度量",
        "C": "创建多个事实表，每个阶段一个",
        "D": "使用 SCD 类型 2 跟踪变化"
      },
      "answer": "B",
      "answerType": "single",
      "explanation": "累积快照事实表使用单行跟踪业务流程的整个生命周期，包含多个里程碑日期列（如 order_date、ship_date、delivery_date）和相关度量。当流程进展时更新同一行。多行方法是事务事实表，SCD 用于维度表。"
    },
    {
      "id": "dp203-s3-q2",
      "examId": "azure-dp-203-set3",
      "setNumber": 3,
      "domain": "开发数据处理",
      "question": "您正在使用 Azure Databricks 中的 Delta Lake。您需要实现 MERGE 操作来处理 CDC（变更数据捕获）数据。MERGE 操作的正确语法顺序是什么？",
      "options": {
        "A": "MERGE INTO, USING, ON, WHEN MATCHED, WHEN NOT MATCHED",
        "B": "MERGE, FROM, WHERE, UPDATE, INSERT",
        "C": "UPDATE, INSERT, DELETE, FROM, WHERE",
        "D": "UPSERT INTO, VALUES, ON CONFLICT"
      },
      "answer": "A",
      "answerType": "single",
      "explanation": "Delta Lake MERGE 语法为：MERGE INTO target USING source ON condition WHEN MATCHED THEN UPDATE WHEN NOT MATCHED THEN INSERT。这允许在单个原子操作中执行更新、插入和删除。其他选项不是有效的 Delta Lake 语法。"
    },
    {
      "id": "dp203-s3-q3",
      "examId": "azure-dp-203-set3",
      "setNumber": 3,
      "domain": "保护、监视和优化数据存储和数据处理",
      "question": "您需要在 Azure Synapse Analytics 中实现最小权限原则。您想要允许用户运行 SELECT 查询但阻止他们查看表定义。应该授予什么权限？",
      "options": {
        "A": "db_datareader 角色",
        "B": "SELECT 权限但拒绝 VIEW DEFINITION",
        "C": "EXECUTE 权限",
        "D": "CONNECT 权限"
      },
      "answer": "B",
      "answerType": "single",
      "explanation": "通过授予 SELECT 权限并显式拒绝 VIEW DEFINITION 权限，用户可以查询数据但无法查看表结构。db_datareader 角色包含 VIEW DEFINITION 权限，EXECUTE 用于存储过程，CONNECT 只允许连接数据库。"
    },
    {
      "id": "dp203-s3-q4",
      "examId": "azure-dp-203-set3",
      "setNumber": 3,
      "domain": "设计和实现数据存储",
      "question": "您正在使用 Azure Data Lake Storage Gen2。您需要实现版本控制以便能够恢复意外删除或修改的文件。应该启用什么功能？",
      "options": {
        "A": "软删除",
        "B": "Blob 版本控制",
        "C": "不可变存储",
        "D": "时间点还原"
      },
      "answer": "B",
      "answerType": "single",
      "explanation": "Blob 版本控制自动为每次修改创建新版本，允许恢复到任何之前的版本。软删除只保护删除操作，不可变存储防止修改但不提供版本历史，时间点还原用于容器级恢复。注意：分层命名空间启用时，版本控制功能有限制。"
    },
    {
      "id": "dp203-s3-q5",
      "examId": "azure-dp-203-set3",
      "setNumber": 3,
      "domain": "开发数据处理",
      "question": "您正在使用 Azure Stream Analytics 处理 IoT 数据。您需要检测设备在 10 分钟内没有发送数据的情况。应该使用什么技术？",
      "options": {
        "A": "翻滚窗口聚合",
        "B": "DATEDIFF 函数",
        "C": "LAG 函数与 LIMIT DURATION",
        "D": "会话窗口"
      },
      "answer": "C",
      "answerType": "single",
      "explanation": "LAG 函数可以访问之前的事件，结合 LIMIT DURATION 设置回溯时间窗口。通过比较当前事件时间与上一事件时间，可以检测超过阈值的间隙。翻滚窗口用于聚合，DATEDIFF 需要两个时间戳列，会话窗口基于活动间隙分组。"
    },
    {
      "id": "dp203-s3-q6",
      "examId": "azure-dp-203-set3",
      "setNumber": 3,
      "domain": "保护、监视和优化数据存储和数据处理",
      "question": "您需要优化 Azure Databricks 中的 Spark 作业。您发现某些任务运行时间远长于其他任务。这个问题最可能的原因是什么？",
      "options": {
        "A": "内存不足",
        "B": "数据倾斜",
        "C": "网络延迟",
        "D": "磁盘 I/O 瓶颈"
      },
      "answer": "B",
      "answerType": "single",
      "explanation": "当某些任务运行时间远长于其他任务时，通常表明数据倾斜问题。这意味着某些分区的数据量远大于其他分区，导致处理这些分区的任务需要更长时间。解决方案包括重新分区、加盐等技术。内存不足会导致溢出，网络和磁盘问题会影响所有任务。"
    },
    {
      "id": "dp203-s3-q7",
      "examId": "azure-dp-203-set3",
      "setNumber": 3,
      "domain": "设计和实现数据存储",
      "question": "您正在 Azure Synapse Analytics 中使用 CETAS（CREATE EXTERNAL TABLE AS SELECT）将查询结果导出到 Data Lake。您需要将输出分区为多个文件以提高并行读取性能。应该如何实现？",
      "options": {
        "A": "在 SELECT 中使用 PARTITION BY",
        "B": "使用多个 CETAS 语句",
        "C": "查询结果自动按分布列分区",
        "D": "在 WITH 子句中指定 DATA_SOURCE"
      },
      "answer": "C",
      "answerType": "single",
      "explanation": "CETAS 自动将输出数据写入多个文件，基于源表的分布和并行度。每个分布（distribution）会写入一个或多个文件。无需额外配置，系统会自动并行化输出。PARTITION BY 不是 CETAS 的有效子句，DATA_SOURCE 指定位置而非分区。"
    },
    {
      "id": "dp203-s3-q8",
      "examId": "azure-dp-203-set3",
      "setNumber": 3,
      "domain": "开发数据处理",
      "question": "您正在使用 Azure Data Factory 中的映射数据流。您需要根据复杂的业务规则将一个输入流拆分为多个输出流。应该使用哪种转换？",
      "options": {
        "A": "派生列",
        "B": "条件拆分",
        "C": "筛选器",
        "D": "选择"
      },
      "answer": "B",
      "answerType": "single",
      "explanation": "条件拆分（Conditional Split）转换允许根据条件将数据行路由到不同的输出流。可以定义多个条件，每个条件对应一个输出分支。派生列用于创建新列，筛选器只产生单个输出，选择用于列选择和重命名。"
    },
    {
      "id": "dp203-s3-q9",
      "examId": "azure-dp-203-set3",
      "setNumber": 3,
      "domain": "保护、监视和优化数据存储和数据处理",
      "question": "您需要在 Azure Synapse Analytics 专用 SQL 池中优化包含大量 NULL 值的列的查询。NULL 值约占列值的 80%。应该如何优化？",
      "options": {
        "A": "将 NULL 替换为默认值",
        "B": "创建稀疏列",
        "C": "创建筛选索引排除 NULL",
        "D": "使用列存储索引"
      },
      "answer": "D",
      "answerType": "single",
      "explanation": "列存储索引对 NULL 值具有优秀的压缩效果。NULL 值在列存储中几乎不占用空间，且查询时可以高效地跳过 NULL 段。稀疏列在 Synapse 中支持有限，筛选索引可以帮助但列存储通常更有效，替换 NULL 会改变数据语义。"
    },
    {
      "id": "dp203-s3-q10",
      "examId": "azure-dp-203-set3",
      "setNumber": 3,
      "domain": "设计和实现数据存储",
      "question": "您正在设计数据湖的 Medallion 架构。Bronze 层应该存储什么类型的数据？",
      "options": {
        "A": "聚合的业务指标",
        "B": "清洗和标准化的数据",
        "C": "原始的未处理数据",
        "D": "为报表优化的数据"
      },
      "answer": "C",
      "answerType": "single",
      "explanation": "在 Medallion（Bronze/Silver/Gold）架构中，Bronze 层存储原始的未处理数据，保持数据的原始状态以便审计和重新处理。Silver 层存储清洗和标准化的数据，Gold 层存储为业务用例优化的聚合数据。"
    },
    {
      "id": "dp203-s3-q11",
      "examId": "azure-dp-203-set3",
      "setNumber": 3,
      "domain": "开发数据处理",
      "question": "您正在使用 Azure Databricks 处理大型数据集。您需要缓存中间结果以加速多次使用。cache() 和 persist() 的主要区别是什么？",
      "options": {
        "A": "cache() 使用内存，persist() 使用磁盘",
        "B": "cache() 是懒惰的，persist() 是立即的",
        "C": "cache() 是 persist(MEMORY_AND_DISK) 的别名",
        "D": "cache() 用于 DataFrame，persist() 用于 RDD"
      },
      "answer": "C",
      "answerType": "single",
      "explanation": "cache() 是 persist() 的快捷方式，使用默认存储级别 MEMORY_AND_DISK。persist() 允许指定不同的存储级别（如 MEMORY_ONLY、DISK_ONLY 等）。两者都是懒惰的操作，都可用于 DataFrame 和 RDD。"
    },
    {
      "id": "dp203-s3-q12",
      "examId": "azure-dp-203-set3",
      "setNumber": 3,
      "domain": "保护、监视和优化数据存储和数据处理",
      "question": "您需要在 Azure Data Factory 中实现 CI/CD。您想要将管道定义存储在版本控制中并自动部署到不同环境。应该使用什么方法？",
      "options": {
        "A": "手动导出和导入 ARM 模板",
        "B": "使用 Git 集成和 Azure DevOps 发布管道",
        "C": "使用 PowerShell 脚本复制管道",
        "D": "使用 REST API 同步管道"
      },
      "answer": "B",
      "answerType": "single",
      "explanation": "Data Factory 原生支持 Git 集成（Azure Repos 或 GitHub）。更改在开发工厂中进行，发布到协作分支时生成 ARM 模板，然后使用 Azure DevOps 发布管道部署到测试和生产环境。这是推荐的 CI/CD 方法。"
    },
    {
      "id": "dp203-s3-q13",
      "examId": "azure-dp-203-set3",
      "setNumber": 3,
      "domain": "设计和实现数据存储",
      "question": "您正在使用 Azure Synapse Analytics 无服务器 SQL 池。您需要优化对大型 Parquet 文件的查询。文件大小约为 10 GB。应该采取什么措施？",
      "options": {
        "A": "将文件拆分为较小的文件（100-250 MB）",
        "B": "使用更大的文件以减少元数据开销",
        "C": "将文件转换为 CSV 格式",
        "D": "禁用列修剪"
      },
      "answer": "A",
      "answerType": "single",
      "explanation": "无服务器 SQL 池对于大型文件的并行处理效率较低。建议将 Parquet 文件拆分为 100-250 MB 的大小，这样可以实现更好的并行读取。过大的文件限制并行度，CSV 效率较低，列修剪应该启用以减少 I/O。"
    },
    {
      "id": "dp203-s3-q14",
      "examId": "azure-dp-203-set3",
      "setNumber": 3,
      "domain": "开发数据处理",
      "question": "您正在使用 Azure Event Hubs 和 Azure Stream Analytics。您需要确保消费者可以从特定时间点开始读取数据。应该使用什么功能？",
      "options": {
        "A": "消费者组",
        "B": "事件保留期",
        "C": "偏移量和序列号",
        "D": "分区键"
      },
      "answer": "C",
      "answerType": "single",
      "explanation": "Event Hubs 中的每个事件都有偏移量和序列号。消费者可以使用这些值从特定位置开始读取。也可以使用时间戳（EnqueuedTime）定位到特定时间点。消费者组用于并行读取，保留期定义数据保存时间，分区键用于分区选择。"
    },
    {
      "id": "dp203-s3-q15",
      "examId": "azure-dp-203-set3",
      "setNumber": 3,
      "domain": "保护、监视和优化数据存储和数据处理",
      "question": "您需要在 Azure Databricks 中实现数据隔离。不同团队应该只能访问其自己的数据。应该使用什么方法？",
      "options": {
        "A": "创建单独的工作区",
        "B": "使用 Unity Catalog 和细粒度访问控制",
        "C": "使用不同的集群",
        "D": "创建单独的存储帐户"
      },
      "answer": "B",
      "answerType": "single",
      "explanation": "Unity Catalog 提供集中的数据治理和细粒度访问控制，可以在表、列和行级别设置权限。这是在单个工作区内实现数据隔离的最佳方法。单独的工作区或存储帐户增加管理复杂性，集群隔离不能控制数据访问。"
    },
    {
      "id": "dp203-s3-q16",
      "examId": "azure-dp-203-set3",
      "setNumber": 3,
      "domain": "设计和实现数据存储",
      "question": "您正在使用 Azure Synapse Analytics 专用 SQL 池。您需要为表选择适当的分布列。下列哪个不是选择分布列的良好标准？",
      "options": {
        "A": "高基数（多个唯一值）",
        "B": "不在 WHERE 子句中使用",
        "C": "用于连接操作",
        "D": "数据均匀分布"
      },
      "answer": "B",
      "answerType": "single",
      "explanation": "好的分布列应该具有高基数、用于连接操作、数据均匀分布、且不应该是 WHERE 子句中的筛选列（避免所有查询只访问少数分布）。选项 B 是错误的标准，因为分布列是否在 WHERE 中使用不是主要考虑因素，实际上如果它也是连接列，在 WHERE 中使用反而可能有益。"
    },
    {
      "id": "dp203-s3-q17",
      "examId": "azure-dp-203-set3",
      "setNumber": 3,
      "domain": "开发数据处理",
      "question": "您正在使用 Azure Data Factory 创建参数化管道。您需要在管道运行时动态指定源表名称。应该如何实现？",
      "options": {
        "A": "使用变量",
        "B": "使用管道参数和表达式",
        "C": "使用全局参数",
        "D": "使用链接服务参数"
      },
      "answer": "B",
      "answerType": "single",
      "explanation": "管道参数允许在运行时传入值。在数据集或活动中使用表达式（如 @pipeline().parameters.TableName）引用参数值。变量用于管道内部逻辑，全局参数用于跨管道共享常量，链接服务参数用于连接配置。"
    },
    {
      "id": "dp203-s3-q18",
      "examId": "azure-dp-203-set3",
      "setNumber": 3,
      "domain": "保护、监视和优化数据存储和数据处理",
      "question": "您需要监视 Azure Databricks 集群的资源利用率。您想要查看 CPU、内存和磁盘使用情况的实时指标。应该在哪里查看？",
      "options": {
        "A": "Azure Monitor",
        "B": "Spark UI",
        "C": "Ganglia 指标",
        "D": "集群事件日志"
      },
      "answer": "C",
      "answerType": "single",
      "explanation": "Databricks 集群包含 Ganglia 指标页面，提供 CPU、内存、网络和磁盘的实时和历史指标。Spark UI 显示作业和任务信息，Azure Monitor 需要配置诊断设置，事件日志显示集群状态变化而非资源指标。"
    },
    {
      "id": "dp203-s3-q19",
      "examId": "azure-dp-203-set3",
      "setNumber": 3,
      "domain": "设计和实现数据存储",
      "question": "您正在设计 Azure Event Hubs 的分区策略。您的应用程序需要保证来自同一设备的事件按顺序处理。应该如何配置？",
      "options": {
        "A": "使用单个分区",
        "B": "使用设备 ID 作为分区键",
        "C": "增加吞吐量单位",
        "D": "启用捕获功能"
      },
      "answer": "B",
      "answerType": "single",
      "explanation": "使用设备 ID 作为分区键确保来自同一设备的所有事件发送到同一分区。在单个分区内，事件保持发送顺序。单个分区限制吞吐量，吞吐量单位影响容量，捕获用于存储而非顺序。"
    },
    {
      "id": "dp203-s3-q20",
      "examId": "azure-dp-203-set3",
      "setNumber": 3,
      "domain": "开发数据处理",
      "question": "您正在使用 Azure Databricks 中的 Delta Lake。您需要定期合并小文件以优化读取性能。应该使用哪个命令？",
      "options": {
        "A": "VACUUM",
        "B": "OPTIMIZE",
        "C": "COMPACT",
        "D": "MERGE"
      },
      "answer": "B",
      "answerType": "single",
      "explanation": "OPTIMIZE 命令合并小文件为更大的文件，提高读取性能。它还可以与 ZORDER BY 结合使用来优化数据布局。VACUUM 删除旧文件，COMPACT 不是有效命令，MERGE 用于 upsert 操作。"
    },
    {
      "id": "dp203-s3-q21",
      "examId": "azure-dp-203-set3",
      "setNumber": 3,
      "domain": "保护、监视和优化数据存储和数据处理",
      "question": "您需要在 Azure Synapse Analytics 中实现查询结果缓存。结果集缓存的工作原理是什么？",
      "options": {
        "A": "缓存存储在用户的本地计算机",
        "B": "缓存存储在专用 SQL 池的 SSD 上",
        "C": "缓存存储在 Azure 存储中",
        "D": "缓存存储在 Redis 中"
      },
      "answer": "B",
      "answerType": "single",
      "explanation": "结果集缓存将查询结果存储在专用 SQL 池的控制节点 SSD 上。当相同的查询再次执行且基础数据未更改时，直接从缓存返回结果。缓存不在客户端、Azure 存储或 Redis 中。"
    },
    {
      "id": "dp203-s3-q22",
      "examId": "azure-dp-203-set3",
      "setNumber": 3,
      "domain": "设计和实现数据存储",
      "question": "您正在使用 Azure Cosmos DB 和 Azure Synapse Link。Synapse Link 中的分析存储使用什么格式？",
      "options": {
        "A": "JSON",
        "B": "列式格式",
        "C": "行存储格式",
        "D": "Parquet"
      },
      "answer": "B",
      "answerType": "single",
      "explanation": "Azure Synapse Link 的分析存储自动将 Cosmos DB 的事务数据（行格式）转换为列式格式，优化分析查询性能。这是一种专有的列式格式，不是 Parquet，但提供类似的分析优势。原始事务存储使用 JSON/行格式。"
    },
    {
      "id": "dp203-s3-q23",
      "examId": "azure-dp-203-set3",
      "setNumber": 3,
      "domain": "开发数据处理",
      "question": "您正在使用 Azure Stream Analytics 进行实时分析。您需要将参考数据与流数据进行连接。参考数据应该如何提供？",
      "options": {
        "A": "仅 Azure SQL 数据库",
        "B": "Azure Blob 存储或 Azure SQL 数据库",
        "C": "仅 Azure Event Hubs",
        "D": "Azure Cosmos DB"
      },
      "answer": "B",
      "answerType": "single",
      "explanation": "Stream Analytics 支持两种参考数据源：Azure Blob 存储（用于静态或慢变化的参考数据）和 Azure SQL 数据库（用于需要定期刷新的参考数据）。Event Hubs 用于流输入，Cosmos DB 不支持作为参考数据源。"
    },
    {
      "id": "dp203-s3-q24",
      "examId": "azure-dp-203-set3",
      "setNumber": 3,
      "domain": "保护、监视和优化数据存储和数据处理",
      "question": "您需要在 Azure Data Lake Storage Gen2 中实现网络隔离。您想要确保只有特定虚拟网络中的资源可以访问存储帐户。应该配置什么？",
      "options": {
        "A": "共享访问签名",
        "B": "存储帐户防火墙和虚拟网络服务终结点",
        "C": "Azure Active Directory 条件访问",
        "D": "访问控制列表"
      },
      "answer": "B",
      "answerType": "single",
      "explanation": "存储帐户防火墙与虚拟网络服务终结点结合使用，可以限制只有特定 VNet 中的资源可以访问存储。SAS 提供访问令牌但不限制网络，AAD 条件访问控制身份验证，ACL 控制文件/目录权限而非网络。"
    },
    {
      "id": "dp203-s3-q25",
      "examId": "azure-dp-203-set3",
      "setNumber": 3,
      "domain": "设计和实现数据存储",
      "question": "您正在 Azure Synapse Analytics 中设计 slowly changing dimension (SCD)。您需要跟踪历史变化并能够快速查找当前值。应该添加什么列？",
      "options": {
        "A": "只添加 EffectiveDate",
        "B": "添加 EffectiveDate、ExpirationDate 和 IsCurrent",
        "C": "只添加 IsCurrent 标志",
        "D": "添加 Version 列"
      },
      "answer": "B",
      "answerType": "single",
      "explanation": "SCD 类型 2 的最佳实践是添加 EffectiveDate（开始日期）、ExpirationDate（结束日期）和 IsCurrent（当前标志）。这支持时间点查询（使用日期范围）和当前值查询（使用 IsCurrent=1）。只有单个列会限制查询灵活性。"
    },
    {
      "id": "dp203-s3-q26",
      "examId": "azure-dp-203-set3",
      "setNumber": 3,
      "domain": "开发数据处理",
      "question": "您正在使用 Azure Databricks 处理数据。您需要将 Spark DataFrame 的模式演化设置为允许自动添加新列。应该设置什么选项？",
      "options": {
        "A": "mode('overwrite')",
        "B": "option('mergeSchema', 'true')",
        "C": "option('schema', 'auto')",
        "D": "option('inferSchema', 'true')"
      },
      "answer": "B",
      "answerType": "single",
      "explanation": "在写入 Delta Lake 时，使用 option('mergeSchema', 'true') 允许自动添加新列到现有表架构。mode('overwrite') 会替换数据，inferSchema 用于读取时推断模式，schema 选项用于指定固定模式。"
    },
    {
      "id": "dp203-s3-q27",
      "examId": "azure-dp-203-set3",
      "setNumber": 3,
      "domain": "保护、监视和优化数据存储和数据处理",
      "question": "您需要在 Azure Synapse Analytics 专用 SQL 池中诊断阻塞问题。应该查询哪个 DMV？",
      "options": {
        "A": "sys.dm_pdw_exec_requests",
        "B": "sys.dm_pdw_waits",
        "C": "sys.dm_pdw_lock_waits",
        "D": "sys.dm_pdw_nodes"
      },
      "answer": "C",
      "answerType": "single",
      "explanation": "sys.dm_pdw_lock_waits 显示正在等待锁的请求信息，包括阻塞的请求 ID 和持有锁的请求 ID。这对于诊断锁争用和阻塞非常有用。exec_requests 显示请求概述，waits 显示所有等待类型，nodes 显示节点信息。"
    },
    {
      "id": "dp203-s3-q28",
      "examId": "azure-dp-203-set3",
      "setNumber": 3,
      "domain": "设计和实现数据存储",
      "question": "您正在使用 Azure Synapse Analytics 无服务器 SQL 池查询 CSV 文件。某些列包含带逗号的文本。应该如何处理？",
      "options": {
        "A": "使用不同的字段分隔符",
        "B": "使用 FIELDQUOTE 指定引号字符",
        "C": "对逗号进行转义",
        "D": "将文件转换为 Parquet"
      },
      "answer": "B",
      "answerType": "single",
      "explanation": "使用 FIELDQUOTE 参数指定用于包围包含分隔符的文本值的字符（通常是双引号）。例如 FIELDQUOTE = '\"'。这是处理 CSV 中包含分隔符的标准方法。转换为 Parquet 是更好的长期方案但不是直接解决办法。"
    },
    {
      "id": "dp203-s3-q29",
      "examId": "azure-dp-203-set3",
      "setNumber": 3,
      "domain": "开发数据处理",
      "question": "您正在使用 Azure Data Factory 的映射数据流。您需要将多个列合并为单个复合键列。应该使用哪种转换和函数？",
      "options": {
        "A": "聚合转换与 concat() 函数",
        "B": "派生列转换与 concat() 函数",
        "C": "联接转换",
        "D": "选择转换"
      },
      "answer": "B",
      "answerType": "single",
      "explanation": "派生列转换允许创建新列或修改现有列。使用 concat() 函数可以将多个列的值连接为单个字符串作为复合键。聚合用于分组计算，联接用于合并数据集，选择用于列选择。"
    },
    {
      "id": "dp203-s3-q30",
      "examId": "azure-dp-203-set3",
      "setNumber": 3,
      "domain": "保护、监视和优化数据存储和数据处理",
      "question": "您需要在 Azure Databricks 中实现成本优化。您想要在工作时间自动启动集群，非工作时间自动停止。应该使用什么方法？",
      "options": {
        "A": "自动终止设置",
        "B": "作业调度",
        "C": "池",
        "D": "计划策略"
      },
      "answer": "B",
      "answerType": "single",
      "explanation": "使用作业调度可以在特定时间启动作业，作业集群会在完成后自动终止。对于交互式工作负荷，可以使用外部调度（如 Azure Automation）来启动/停止集群。自动终止只能在空闲后停止，池减少启动时间，计划策略不存在。"
    },
    {
      "id": "dp203-s3-q31",
      "examId": "azure-dp-203-set3",
      "setNumber": 3,
      "domain": "设计和实现数据存储",
      "question": "您正在使用 Azure Synapse Analytics 中的 Spark 池。您需要读取存储在 Azure Key Vault 中的连接字符串。应该使用什么方法？",
      "options": {
        "A": "直接在代码中硬编码",
        "B": "使用链接服务和 TokenLibrary",
        "C": "使用环境变量",
        "D": "使用配置文件"
      },
      "answer": "B",
      "answerType": "single",
      "explanation": "在 Synapse Spark 中，可以创建链接到 Azure Key Vault 的链接服务，然后使用 TokenLibrary.getSecret() 方法在代码中安全地检索密钥。这避免了在代码中硬编码敏感信息。环境变量和配置文件在 Synapse Spark 中不是标准方法。"
    },
    {
      "id": "dp203-s3-q32",
      "examId": "azure-dp-203-set3",
      "setNumber": 3,
      "domain": "开发数据处理",
      "question": "您正在使用 Azure Stream Analytics。您需要处理乱序到达的事件并确保正确的时间戳排序。应该配置什么？",
      "options": {
        "A": "系统时间戳",
        "B": "到达时间",
        "C": "应用程序时间戳和乱序容错窗口",
        "D": "分区键"
      },
      "answer": "C",
      "answerType": "single",
      "explanation": "应用程序时间戳（TIMESTAMP BY）使用事件中的时间字段而非到达时间。乱序容错窗口设置允许系统等待延迟事件的时间，在此窗口内的乱序事件将被正确排序。系统时间戳和到达时间不能处理乱序，分区键用于分区选择。"
    },
    {
      "id": "dp203-s3-q33",
      "examId": "azure-dp-203-set3",
      "setNumber": 3,
      "domain": "保护、监视和优化数据存储和数据处理",
      "question": "您需要在 Azure Synapse Analytics 中实现列级加密。您想要在数据库中加密敏感列，同时允许应用程序解密。应该使用什么功能？",
      "options": {
        "A": "透明数据加密 (TDE)",
        "B": "Always Encrypted",
        "C": "动态数据屏蔽",
        "D": "行级安全性"
      },
      "answer": "B",
      "answerType": "single",
      "explanation": "Always Encrypted 提供列级加密，数据在客户端加密/解密，数据库只看到密文。这保护数据免受数据库管理员的访问。TDE 加密整个数据库但对应用程序透明，动态数据屏蔽只隐藏显示，行级安全性控制行访问。"
    },
    {
      "id": "dp203-s3-q34",
      "examId": "azure-dp-203-set3",
      "setNumber": 3,
      "domain": "设计和实现数据存储",
      "question": "您正在使用 Azure Data Factory 从 API 获取数据。API 使用 OAuth 2.0 客户端凭据流进行身份验证。应该在链接服务中配置什么？",
      "options": {
        "A": "基本身份验证",
        "B": "匿名身份验证",
        "C": "服务主体身份验证",
        "D": "托管标识身份验证"
      },
      "answer": "C",
      "answerType": "single",
      "explanation": "OAuth 2.0 客户端凭据流使用应用程序的客户端 ID 和客户端密钥获取令牌。在 Data Factory 中，这通过服务主体身份验证配置，指定租户 ID、客户端 ID 和密钥。基本身份验证使用用户名密码，托管标识不适用于外部 API。"
    },
    {
      "id": "dp203-s3-q35",
      "examId": "azure-dp-203-set3",
      "setNumber": 3,
      "domain": "开发数据处理",
      "question": "您正在使用 Azure Databricks 中的 Structured Streaming。您需要输出每个微批次的处理结果到控制台进行调试。应该使用什么输出模式？",
      "options": {
        "A": "append",
        "B": "complete",
        "C": "update",
        "D": "console"
      },
      "answer": "C",
      "answerType": "single",
      "explanation": "update 模式只输出自上次触发以来更新的行，适合调试和监视。对于控制台输出（format('console')），append 输出所有新行，complete 输出完整结果表，console 不是输出模式而是格式。update 模式与 console 格式结合使用最适合调试。"
    },
    {
      "id": "dp203-s3-q36",
      "examId": "azure-dp-203-set3",
      "setNumber": 3,
      "domain": "保护、监视和优化数据存储和数据处理",
      "question": "您需要在 Azure Synapse Analytics 专用 SQL 池中优化统计信息。统计信息过时可能导致什么问题？",
      "options": {
        "A": "数据丢失",
        "B": "次优查询计划",
        "C": "数据损坏",
        "D": "连接失败"
      },
      "answer": "B",
      "answerType": "single",
      "explanation": "统计信息帮助查询优化器选择最佳执行计划。过时的统计信息可能导致优化器做出错误的选择，例如选择错误的连接类型或扫描顺序，从而导致次优的查询性能。统计信息与数据完整性或连接无关。"
    },
    {
      "id": "dp203-s3-q37",
      "examId": "azure-dp-203-set3",
      "setNumber": 3,
      "domain": "设计和实现数据存储",
      "question": "您正在使用 Azure Synapse Analytics 专用 SQL 池。您需要创建一个临时表来存储中间结果。临时表的生命周期是什么？",
      "options": {
        "A": "直到显式删除",
        "B": "直到会话结束",
        "C": "直到事务结束",
        "D": "24 小时后自动删除"
      },
      "answer": "B",
      "answerType": "single",
      "explanation": "在 Azure Synapse Analytics 中，临时表（以 # 开头）的作用域是会话级别。当用户断开连接或会话结束时，临时表自动删除。全局临时表（以 ## 开头）在所有会话都断开后删除。它们不是事务范围或时间限制的。"
    },
    {
      "id": "dp203-s3-q38",
      "examId": "azure-dp-203-set3",
      "setNumber": 3,
      "domain": "开发数据处理",
      "question": "您正在使用 Azure Data Factory 的元数据驱动方法。您需要从数据库中读取表列表并为每个表创建复制活动。应该使用什么活动组合？",
      "options": {
        "A": "Lookup 和 ForEach",
        "B": "GetMetadata 和 Until",
        "C": "Execute Pipeline 和 Switch",
        "D": "Stored Procedure 和 If Condition"
      },
      "answer": "A",
      "answerType": "single",
      "explanation": "Lookup 活动查询数据库获取表列表，ForEach 活动迭代结果并为每个表执行复制活动。这是实现元数据驱动管道的标准模式。GetMetadata 获取文件元数据，Until 用于条件循环，Execute Pipeline 调用子管道。"
    },
    {
      "id": "dp203-s3-q39",
      "examId": "azure-dp-203-set3",
      "setNumber": 3,
      "domain": "保护、监视和优化数据存储和数据处理",
      "question": "您需要在 Azure Databricks 中实现机密管理。您想要安全地存储和访问数据库密码。应该使用什么方法？",
      "options": {
        "A": "在笔记本中硬编码",
        "B": "使用 Databricks secrets 和 Azure Key Vault 支持的范围",
        "C": "使用环境变量",
        "D": "存储在 DBFS 文件中"
      },
      "answer": "B",
      "answerType": "single",
      "explanation": "Databricks secrets 提供安全的机密管理。可以使用 Azure Key Vault 支持的范围，将 Key Vault 中的密钥安全地用于 Databricks。使用 dbutils.secrets.get() 在代码中检索密钥，值不会显示在日志中。硬编码和文件存储不安全，环境变量在 Databricks 中不是标准方法。"
    },
    {
      "id": "dp203-s3-q40",
      "examId": "azure-dp-203-set3",
      "setNumber": 3,
      "domain": "设计和实现数据存储",
      "question": "您正在设计 Azure Synapse Analytics 解决方案。您需要选择正确的资源来处理即席查询和数据探索，同时控制成本。应该使用什么？",
      "options": {
        "A": "专用 SQL 池",
        "B": "无服务器 SQL 池",
        "C": "Spark 池",
        "D": "数据流"
      },
      "answer": "B",
      "answerType": "single",
      "explanation": "无服务器 SQL 池是即席查询和数据探索的最佳选择。它按查询处理的数据量付费，不需要预先配置资源，非常适合不可预测的工作负荷。专用 SQL 池需要持续付费，Spark 池需要启动时间，数据流用于 Data Factory 中的 ETL。"
    }
  ]
}

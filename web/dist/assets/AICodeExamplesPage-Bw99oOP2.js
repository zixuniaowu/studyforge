import{c as x,g as _,u as b,r as o,E as y,n as v,K as j,M as w,j as e,C as N,o as z,N as C,B as k,m as R,O as I,P as T}from"./index-B2xJFW01.js";import{L as A}from"./layers-1EzP2D7L.js";import{I as q}from"./image-Df3-Och4.js";import{S as P}from"./search-CGWBQdMV.js";import{H as X}from"./house-Bce2Judw.js";import{C as L}from"./copy-BTFqIG5t.js";const S=[["path",{d:"M22 17a2 2 0 0 1-2 2H6.828a2 2 0 0 0-1.414.586l-2.202 2.202A.71.71 0 0 1 2 21.286V5a2 2 0 0 1 2-2h16a2 2 0 0 1 2 2z",key:"18887p"}]],M=x("message-square",S),m={intro:{name:{zh:"å…¥é—¨æ•™ç¨‹",ja:"å…¥é–€ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«"},description:{zh:"ä»Žé›¶å¼€å§‹å­¦ä¹  AI/ML åŸºç¡€ï¼Œæ— éœ€ API Key",ja:"ã‚¼ãƒ­ã‹ã‚‰AI/MLã®åŸºç¤Žã‚’å­¦ã¶ã€APIã‚­ãƒ¼ä¸è¦"},icon:w,gradient:"from-purple-500 to-indigo-600",examples:[{id:"intro-1",title:{zh:"Hello Transformers - ç¬¬ä¸€ä¸ª AI ç¨‹åº",ja:"Hello Transformers - åˆã‚ã¦ã®AIãƒ—ãƒ­ã‚°ãƒ©ãƒ "},description:{zh:"ä½¿ç”¨ Hugging Face å…è´¹æ¨¡åž‹è¿›è¡Œæ–‡æœ¬ç”Ÿæˆï¼ˆColab å¯ç›´æŽ¥è¿è¡Œï¼‰",ja:"Hugging Faceã®ç„¡æ–™ãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆColabã§ç›´æŽ¥å®Ÿè¡Œå¯èƒ½ï¼‰"},code:`"""
============================================
Hello Transformers - ä½ çš„ç¬¬ä¸€ä¸ª AI ç¨‹åº
============================================
âœ… Colab å¯ç›´æŽ¥è¿è¡Œï¼Œæ— éœ€ API Key
âœ… ä½¿ç”¨ Hugging Face å…è´¹å¼€æºæ¨¡åž‹
============================================
"""

# ç¬¬ä¸€æ­¥ï¼šå®‰è£…ä¾èµ–ï¼ˆColab ä¸­è¿è¡Œï¼‰
!pip install transformers torch -q

# ç¬¬äºŒæ­¥ï¼šå¯¼å…¥åº“
from transformers import pipeline

# ç¬¬ä¸‰æ­¥ï¼šåˆ›å»ºæ–‡æœ¬ç”Ÿæˆç®¡é“
# ä½¿ç”¨ GPT-2 æ¨¡åž‹ï¼ˆå…è´¹ï¼Œçº¦ 500MBï¼‰
print("æ­£åœ¨åŠ è½½æ¨¡åž‹...")
generator = pipeline("text-generation", model="gpt2")
print("âœ… æ¨¡åž‹åŠ è½½å®Œæˆï¼")

# ç¬¬å››æ­¥ï¼šç”Ÿæˆæ–‡æœ¬
prompt = "Artificial Intelligence is"
result = generator(prompt, max_length=50, num_return_sequences=1)

print("\\n" + "="*50)
print("ðŸ“ è¾“å…¥æç¤º:", prompt)
print("="*50)
print("ðŸ¤– AI ç”Ÿæˆ:")
print(result[0]['generated_text'])
print("="*50)

# è¯•è¯•ä¸­æ–‡æ¨¡åž‹
print("\\næ­£åœ¨åŠ è½½ä¸­æ–‡æ¨¡åž‹...")
chinese_generator = pipeline("text-generation", model="uer/gpt2-chinese-cluecorpussmall")
print("âœ… ä¸­æ–‡æ¨¡åž‹åŠ è½½å®Œæˆï¼")

chinese_prompt = "äººå·¥æ™ºèƒ½çš„æœªæ¥"
chinese_result = chinese_generator(chinese_prompt, max_length=50)

print("\\n" + "="*50)
print("ðŸ“ ä¸­æ–‡æç¤º:", chinese_prompt)
print("="*50)
print("ðŸ¤– AI ç”Ÿæˆ:")
print(chinese_result[0]['generated_text'])
print("="*50)

print("\\nðŸŽ‰ æ­å–œï¼ä½ å·²å®Œæˆç¬¬ä¸€ä¸ª AI ç¨‹åºï¼")`,language:"python",difficulty:"beginner",tags:["å…¥é—¨","Transformers","GPT-2","Colab"],colabReady:!0},{id:"intro-2",title:{zh:"é—®ç­”ç³»ç»Ÿ - AI é˜…è¯»ç†è§£",ja:"Q&Aã‚·ã‚¹ãƒ†ãƒ  - AIèª­è§£"},description:{zh:"è®© AI é˜…è¯»æ–‡ç« å¹¶å›žç­”é—®é¢˜ï¼ˆColab å¯ç›´æŽ¥è¿è¡Œï¼‰",ja:"AIã«æ–‡ç« ã‚’èª­ã¾ã›ã¦è³ªå•ã«ç­”ãˆã•ã›ã‚‹ï¼ˆColabã§ç›´æŽ¥å®Ÿè¡Œå¯èƒ½ï¼‰"},code:`"""
============================================
é—®ç­”ç³»ç»Ÿ - è®© AI é˜…è¯»å¹¶å›žç­”é—®é¢˜
============================================
âœ… Colab å¯ç›´æŽ¥è¿è¡Œï¼Œæ— éœ€ API Key
âœ… ä½¿ç”¨é¢„è®­ç»ƒçš„ BERT é—®ç­”æ¨¡åž‹
============================================
"""

# å®‰è£…ä¾èµ–
!pip install transformers torch -q

from transformers import pipeline

# åˆ›å»ºé—®ç­”ç®¡é“
print("æ­£åœ¨åŠ è½½é—®ç­”æ¨¡åž‹...")
qa_pipeline = pipeline("question-answering", model="distilbert-base-cased-distilled-squad")
print("âœ… æ¨¡åž‹åŠ è½½å®Œæˆï¼")

# å‡†å¤‡æ–‡ç« å’Œé—®é¢˜
context = """
Hugging Face is a company that develops tools for building applications using machine learning.
It is most notable for its Transformers library built for natural language processing applications
and its platform that allows users to share machine learning models and datasets.
The company was founded in 2016 and is headquartered in New York City.
"""

questions = [
    "What does Hugging Face develop?",
    "What is their most notable product?",
    "When was Hugging Face founded?",
    "Where is the company headquartered?"
]

print("\\n" + "="*60)
print("ðŸ“– æ–‡ç« å†…å®¹:")
print(context)
print("="*60)

# å›žç­”é—®é¢˜
print("\\nðŸ¤– AI é—®ç­”:")
print("-"*60)

for q in questions:
    result = qa_pipeline(question=q, context=context)
    print(f"â“ é—®é¢˜: {q}")
    print(f"âœ… ç­”æ¡ˆ: {result['answer']} (ç½®ä¿¡åº¦: {result['score']:.2%})")
    print("-"*60)

# ä¸­æ–‡é—®ç­”ç¤ºä¾‹
print("\\næ­£åœ¨åŠ è½½ä¸­æ–‡é—®ç­”æ¨¡åž‹...")
chinese_qa = pipeline("question-answering", model="uer/roberta-base-chinese-extractive-qa")
print("âœ… ä¸­æ–‡æ¨¡åž‹åŠ è½½å®Œæˆï¼")

chinese_context = """
åŽä¸ºæŠ€æœ¯æœ‰é™å…¬å¸æ˜¯ä¸€å®¶ä¸­å›½è·¨å›½ç§‘æŠ€å…¬å¸ï¼Œæ€»éƒ¨ä½äºŽå¹¿ä¸œçœæ·±åœ³å¸‚ã€‚
å…¬å¸æˆç«‹äºŽ1987å¹´ï¼Œç”±ä»»æ­£éžåˆ›ç«‹ã€‚åŽä¸ºä¸»è¦ä»Žäº‹é€šä¿¡è®¾å¤‡ã€æ¶ˆè´¹ç”µå­äº§å“
å’Œä¼ä¸šè§£å†³æ–¹æ¡ˆçš„ç ”å‘ä¸Žé”€å”®ã€‚æˆªè‡³2023å¹´ï¼ŒåŽä¸ºæ˜¯å…¨çƒæœ€å¤§çš„ç”µä¿¡è®¾å¤‡åˆ¶é€ å•†ã€‚
"""

chinese_questions = ["åŽä¸ºæ˜¯å“ªä¸€å¹´æˆç«‹çš„ï¼Ÿ", "åŽä¸ºçš„åˆ›å§‹äººæ˜¯è°ï¼Ÿ", "åŽä¸ºæ€»éƒ¨åœ¨å“ªé‡Œï¼Ÿ"]

print("\\nðŸ“– ä¸­æ–‡æ–‡ç« :")
print(chinese_context)
print("="*60)

for q in chinese_questions:
    result = chinese_qa(question=q, context=chinese_context)
    print(f"â“ {q}")
    print(f"âœ… {result['answer']}")
    print("-"*40)

print("\\nðŸŽ‰ é—®ç­”ç³»ç»Ÿæ¼”ç¤ºå®Œæˆï¼")`,language:"python",difficulty:"beginner",tags:["é—®ç­”","QA","BERT","Colab"],colabReady:!0}]},text:{name:{zh:"æ–‡æœ¬ç”Ÿæˆä¸Žå¤„ç†",ja:"ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã¨å‡¦ç†"},description:{zh:"æ–‡æœ¬ç”Ÿæˆã€æ‘˜è¦ã€ç¿»è¯‘ç­‰ NLP ä»»åŠ¡",ja:"ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã€è¦ç´„ã€ç¿»è¨³ãªã©ã®NLPã‚¿ã‚¹ã‚¯"},icon:M,gradient:"from-blue-500 to-cyan-600",examples:[{id:"text-1",title:{zh:"æ–‡æœ¬æ‘˜è¦ - AI è‡ªåŠ¨æ€»ç»“",ja:"ãƒ†ã‚­ã‚¹ãƒˆè¦ç´„ - AIè‡ªå‹•ã‚µãƒžãƒªãƒ¼"},description:{zh:"è®© AI è‡ªåŠ¨æ€»ç»“é•¿æ–‡ç« ï¼ˆColab å¯ç›´æŽ¥è¿è¡Œï¼‰",ja:"AIã«é•·ã„æ–‡ç« ã‚’è‡ªå‹•è¦ç´„ã•ã›ã‚‹ï¼ˆColabã§ç›´æŽ¥å®Ÿè¡Œå¯èƒ½ï¼‰"},code:`"""
============================================
æ–‡æœ¬æ‘˜è¦ - AI è‡ªåŠ¨ç”Ÿæˆæ‘˜è¦
============================================
âœ… Colab å¯ç›´æŽ¥è¿è¡Œï¼Œæ— éœ€ API Key
âœ… ä½¿ç”¨ BART æ‘˜è¦æ¨¡åž‹
============================================
"""

!pip install transformers torch -q

from transformers import pipeline

# åˆ›å»ºæ‘˜è¦ç®¡é“
print("æ­£åœ¨åŠ è½½æ‘˜è¦æ¨¡åž‹...")
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
print("âœ… æ¨¡åž‹åŠ è½½å®Œæˆï¼")

# é•¿æ–‡ç« ç¤ºä¾‹
article = """
Machine learning is a subset of artificial intelligence (AI) that provides systems the ability
to automatically learn and improve from experience without being explicitly programmed.
Machine learning focuses on the development of computer programs that can access data and
use it to learn for themselves. The process of learning begins with observations or data,
such as examples, direct experience, or instruction, in order to look for patterns in data
and make better decisions in the future based on the examples that we provide.

The primary aim is to allow the computers to learn automatically without human intervention
or assistance and adjust actions accordingly. Traditional programming requires explicit
instructions for every task, while machine learning allows systems to learn from data patterns.
This approach is particularly useful for tasks that are too complex to program explicitly,
such as image recognition, natural language processing, and autonomous driving.
"""

print("\\nðŸ“– åŽŸæ–‡ ({} å­—):".format(len(article)))
print("-"*60)
print(article[:300] + "...")
print("-"*60)

# ç”Ÿæˆæ‘˜è¦
print("\\nðŸ¤– AI ç”Ÿæˆæ‘˜è¦:")
print("-"*60)
summary = summarizer(article, max_length=80, min_length=30, do_sample=False)
print(summary[0]['summary_text'])
print("-"*60)

# å¤šæ®µæ‘˜è¦
print("\\nðŸ“ æ¼”ç¤ºï¼šè°ƒæ•´æ‘˜è¦é•¿åº¦")
for max_len in [50, 100, 150]:
    result = summarizer(article, max_length=max_len, min_length=20, do_sample=False)
    print(f"\\n[max_length={max_len}]:")
    print(result[0]['summary_text'])

print("\\nðŸŽ‰ æ–‡æœ¬æ‘˜è¦æ¼”ç¤ºå®Œæˆï¼")`,language:"python",difficulty:"beginner",tags:["æ‘˜è¦","Summarization","BART","Colab"],colabReady:!0},{id:"text-2",title:{zh:"æœºå™¨ç¿»è¯‘ - å¤šè¯­è¨€äº’è¯‘",ja:"æ©Ÿæ¢°ç¿»è¨³ - å¤šè¨€èªžç¿»è¨³"},description:{zh:"ä½¿ç”¨å…è´¹æ¨¡åž‹è¿›è¡Œå¤šè¯­è¨€ç¿»è¯‘",ja:"ç„¡æ–™ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸå¤šè¨€èªžç¿»è¨³"},code:`"""
============================================
æœºå™¨ç¿»è¯‘ - å¤šè¯­è¨€äº’è¯‘
============================================
âœ… Colab å¯ç›´æŽ¥è¿è¡Œï¼Œæ— éœ€ API Key
âœ… ä½¿ç”¨ Helsinki-NLP ç¿»è¯‘æ¨¡åž‹
============================================
"""

!pip install transformers torch sentencepiece -q

from transformers import pipeline

# è‹±è¯­åˆ°ä¸­æ–‡ç¿»è¯‘
print("æ­£åœ¨åŠ è½½è‹±ä¸­ç¿»è¯‘æ¨¡åž‹...")
en_to_zh = pipeline("translation", model="Helsinki-NLP/opus-mt-en-zh")
print("âœ… è‹±ä¸­æ¨¡åž‹åŠ è½½å®Œæˆï¼")

# ä¸­æ–‡åˆ°è‹±è¯­ç¿»è¯‘
print("æ­£åœ¨åŠ è½½ä¸­è‹±ç¿»è¯‘æ¨¡åž‹...")
zh_to_en = pipeline("translation", model="Helsinki-NLP/opus-mt-zh-en")
print("âœ… ä¸­è‹±æ¨¡åž‹åŠ è½½å®Œæˆï¼")

# æ¼”ç¤ºç¿»è¯‘
english_texts = [
    "Hello, how are you today?",
    "Machine learning is changing the world.",
    "I love programming with Python."
]

chinese_texts = [
    "äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜æˆ‘ä»¬çš„ç”Ÿæ´»",
    "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé€‚åˆå‡ºåŽ»æ•£æ­¥",
    "å­¦ä¹ ç¼–ç¨‹æ˜¯ä¸€ä»¶æœ‰è¶£çš„äº‹æƒ…"
]

print("\\n" + "="*60)
print("ðŸŒ è‹±è¯­ â†’ ä¸­æ–‡ ç¿»è¯‘")
print("="*60)

for text in english_texts:
    result = en_to_zh(text)
    print(f"EN: {text}")
    print(f"ZH: {result[0]['translation_text']}")
    print("-"*40)

print("\\n" + "="*60)
print("ðŸŒ ä¸­æ–‡ â†’ è‹±è¯­ ç¿»è¯‘")
print("="*60)

for text in chinese_texts:
    result = zh_to_en(text)
    print(f"ZH: {text}")
    print(f"EN: {result[0]['translation_text']}")
    print("-"*40)

# å…¶ä»–è¯­è¨€ç¿»è¯‘
print("\\nðŸ“š æ”¯æŒçš„å…¶ä»–è¯­è¨€å¯¹:")
print("- Helsinki-NLP/opus-mt-en-de (è‹±è¯­â†’å¾·è¯­)")
print("- Helsinki-NLP/opus-mt-en-fr (è‹±è¯­â†’æ³•è¯­)")
print("- Helsinki-NLP/opus-mt-ja-en (æ—¥è¯­â†’è‹±è¯­)")
print("- æ›´å¤šæ¨¡åž‹: https://huggingface.co/Helsinki-NLP")

print("\\nðŸŽ‰ ç¿»è¯‘æ¼”ç¤ºå®Œæˆï¼")`,language:"python",difficulty:"beginner",tags:["ç¿»è¯‘","Translation","Helsinki-NLP","Colab"],colabReady:!0}]},sentiment:{name:{zh:"æƒ…æ„Ÿåˆ†æž",ja:"æ„Ÿæƒ…åˆ†æž"},description:{zh:"æ–‡æœ¬æƒ…æ„Ÿåˆ†ç±»ã€è¯„è®ºåˆ†æž",ja:"ãƒ†ã‚­ã‚¹ãƒˆæ„Ÿæƒ…åˆ†é¡žã€ãƒ¬ãƒ“ãƒ¥ãƒ¼åˆ†æž"},icon:j,gradient:"from-green-500 to-emerald-600",examples:[{id:"sentiment-1",title:{zh:"æƒ…æ„Ÿåˆ†æž - åˆ¤æ–­æ–‡æœ¬æƒ…ç»ª",ja:"æ„Ÿæƒ…åˆ†æž - ãƒ†ã‚­ã‚¹ãƒˆã®æ„Ÿæƒ…åˆ¤å®š"},description:{zh:"è‡ªåŠ¨åˆ¤æ–­æ–‡æœ¬æ˜¯æ­£é¢è¿˜æ˜¯è´Ÿé¢",ja:"ãƒ†ã‚­ã‚¹ãƒˆãŒãƒã‚¸ãƒ†ã‚£ãƒ–ã‹ãƒã‚¬ãƒ†ã‚£ãƒ–ã‹ã‚’è‡ªå‹•åˆ¤å®š"},code:`"""
============================================
æƒ…æ„Ÿåˆ†æž - åˆ¤æ–­æ–‡æœ¬æƒ…ç»ª
============================================
âœ… Colab å¯ç›´æŽ¥è¿è¡Œï¼Œæ— éœ€ API Key
âœ… ä½¿ç”¨ DistilBERT æƒ…æ„Ÿåˆ†æžæ¨¡åž‹
============================================
"""

!pip install transformers torch -q

from transformers import pipeline

# åˆ›å»ºæƒ…æ„Ÿåˆ†æžç®¡é“
print("æ­£åœ¨åŠ è½½æƒ…æ„Ÿåˆ†æžæ¨¡åž‹...")
sentiment_analyzer = pipeline("sentiment-analysis", model="distilbert-base-uncased-finetuned-sst-2-english")
print("âœ… æ¨¡åž‹åŠ è½½å®Œæˆï¼")

# æµ‹è¯•æ–‡æœ¬
texts = [
    "I love this product! It's amazing and works perfectly.",
    "This is the worst experience I've ever had.",
    "The weather is nice today.",
    "I'm disappointed with the quality of service.",
    "This movie was absolutely fantastic, highly recommend!",
    "The food was okay, nothing special."
]

print("\\n" + "="*60)
print("ðŸ˜ŠðŸ˜¢ æƒ…æ„Ÿåˆ†æžç»“æžœ")
print("="*60)

for text in texts:
    result = sentiment_analyzer(text)[0]
    emoji = "ðŸ˜Š" if result['label'] == 'POSITIVE' else "ðŸ˜¢"
    print(f"\\n{emoji} [{result['label']}] (ç½®ä¿¡åº¦: {result['score']:.2%})")
    print(f"   "{text}"")

# æ‰¹é‡åˆ†æž
print("\\n" + "="*60)
print("ðŸ“Š æ‰¹é‡åˆ†æžæ¼”ç¤º")
print("="*60)

reviews = [
    "Great product, fast shipping!",
    "Poor quality, waste of money",
    "Average performance, expected more",
    "Excellent customer service!",
    "Not worth the price at all"
]

results = sentiment_analyzer(reviews)

positive_count = sum(1 for r in results if r['label'] == 'POSITIVE')
negative_count = len(results) - positive_count

print(f"\\næ€»è¯„è®ºæ•°: {len(results)}")
print(f"æ­£é¢è¯„ä»·: {positive_count} ({positive_count/len(results)*100:.0f}%)")
print(f"è´Ÿé¢è¯„ä»·: {negative_count} ({negative_count/len(results)*100:.0f}%)")

print("\\nðŸŽ‰ æƒ…æ„Ÿåˆ†æžæ¼”ç¤ºå®Œæˆï¼")`,language:"python",difficulty:"beginner",tags:["æƒ…æ„Ÿåˆ†æž","Sentiment","BERT","Colab"],colabReady:!0},{id:"sentiment-2",title:{zh:"ä¸­æ–‡æƒ…æ„Ÿåˆ†æž",ja:"ä¸­å›½èªžæ„Ÿæƒ…åˆ†æž"},description:{zh:"åˆ†æžä¸­æ–‡æ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘",ja:"ä¸­å›½èªžãƒ†ã‚­ã‚¹ãƒˆã®æ„Ÿæƒ…å‚¾å‘ã‚’åˆ†æž"},code:`"""
============================================
ä¸­æ–‡æƒ…æ„Ÿåˆ†æž
============================================
âœ… Colab å¯ç›´æŽ¥è¿è¡Œï¼Œæ— éœ€ API Key
âœ… ä½¿ç”¨ä¸­æ–‡ BERT æƒ…æ„Ÿæ¨¡åž‹
============================================
"""

!pip install transformers torch -q

from transformers import pipeline

# åŠ è½½ä¸­æ–‡æƒ…æ„Ÿåˆ†æžæ¨¡åž‹
print("æ­£åœ¨åŠ è½½ä¸­æ–‡æƒ…æ„Ÿåˆ†æžæ¨¡åž‹...")
chinese_sentiment = pipeline("sentiment-analysis", model="uer/roberta-base-finetuned-chinanews-chinese")
print("âœ… ä¸­æ–‡æ¨¡åž‹åŠ è½½å®Œæˆï¼")

# ä¸­æ–‡æµ‹è¯•æ–‡æœ¬
chinese_texts = [
    "è¿™ä¸ªäº§å“å¤ªæ£’äº†ï¼Œæˆ‘éžå¸¸æ»¡æ„ï¼",
    "æœåŠ¡æ€åº¦å¾ˆå·®ï¼Œå†ä¹Ÿä¸ä¼šæ¥äº†",
    "ä»Šå¤©å¿ƒæƒ…ä¸é”™ï¼Œé˜³å…‰æ˜Žåªš",
    "è¿™éƒ¨ç”µå½±è®©æˆ‘éžå¸¸å¤±æœ›",
    "é¤åŽ…çš„èœå“å£å‘³ä¸€èˆ¬",
    "å¿«é€’å¾ˆå¿«å°±åˆ°äº†ï¼ŒåŒ…è£…å®Œå¥½"
]

print("\\n" + "="*60)
print("ðŸ‡¨ðŸ‡³ ä¸­æ–‡æƒ…æ„Ÿåˆ†æžç»“æžœ")
print("="*60)

for text in chinese_texts:
    result = chinese_sentiment(text)[0]
    # æ ¹æ®æ ‡ç­¾æ˜¾ç¤ºä¸åŒè¡¨æƒ…
    if 'positive' in result['label'].lower() or result['label'] == 'LABEL_1':
        emoji = "ðŸ˜Š"
        sentiment = "æ­£é¢"
    else:
        emoji = "ðŸ˜¢"
        sentiment = "è´Ÿé¢"

    print(f"\\n{emoji} [{sentiment}] (ç½®ä¿¡åº¦: {result['score']:.2%})")
    print(f"   "{text}"")

# ç»Ÿè®¡åˆ†æž
print("\\n" + "="*60)
print("ðŸ“Š è¯„è®ºç»Ÿè®¡åˆ†æž")
print("="*60)

product_reviews = [
    "è´¨é‡å¾ˆå¥½ï¼Œå€¼å¾—è´­ä¹°",
    "ä»·æ ¼å¤ªè´µäº†",
    "ç‰©æµé€Ÿåº¦å¾ˆå¿«",
    "é¢œè‰²å’Œå›¾ç‰‡ä¸ç¬¦",
    "éžå¸¸æ»¡æ„è¿™æ¬¡è´­ç‰©ä½“éªŒ",
    "å®¢æœæ€åº¦æ¶åŠ£"
]

results = chinese_sentiment(product_reviews)
for review, result in zip(product_reviews, results):
    label = "ðŸ‘" if 'positive' in result['label'].lower() or result['label'] == 'LABEL_1' else "ðŸ‘Ž"
    print(f"{label} {review}")

print("\\nðŸŽ‰ ä¸­æ–‡æƒ…æ„Ÿåˆ†æžæ¼”ç¤ºå®Œæˆï¼")`,language:"python",difficulty:"beginner",tags:["ä¸­æ–‡","æƒ…æ„Ÿåˆ†æž","RoBERTa","Colab"],colabReady:!0}]},embedding:{name:{zh:"æ–‡æœ¬åµŒå…¥ä¸Žç›¸ä¼¼åº¦",ja:"ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ã¨é¡žä¼¼åº¦"},description:{zh:"è¯­ä¹‰æœç´¢ã€æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—",ja:"ã‚»ãƒžãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã€ãƒ†ã‚­ã‚¹ãƒˆé¡žä¼¼åº¦è¨ˆç®—"},icon:P,gradient:"from-orange-500 to-amber-600",examples:[{id:"embedding-1",title:{zh:"è¯­ä¹‰ç›¸ä¼¼åº¦ - æ‰¾å‡ºç›¸ä¼¼æ–‡æœ¬",ja:"æ„å‘³çš„é¡žä¼¼åº¦ - é¡žä¼¼ãƒ†ã‚­ã‚¹ãƒˆã‚’è¦‹ã¤ã‘ã‚‹"},description:{zh:"è®¡ç®—æ–‡æœ¬ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦",ja:"ãƒ†ã‚­ã‚¹ãƒˆé–“ã®æ„å‘³çš„é¡žä¼¼åº¦ã‚’è¨ˆç®—"},code:`"""
============================================
è¯­ä¹‰ç›¸ä¼¼åº¦ - æ‰¾å‡ºç›¸ä¼¼æ–‡æœ¬
============================================
âœ… Colab å¯ç›´æŽ¥è¿è¡Œï¼Œæ— éœ€ API Key
âœ… ä½¿ç”¨ Sentence Transformers
============================================
"""

!pip install sentence-transformers -q

from sentence_transformers import SentenceTransformer, util

# åŠ è½½æ¨¡åž‹
print("æ­£åœ¨åŠ è½½è¯­ä¹‰æ¨¡åž‹...")
model = SentenceTransformer('all-MiniLM-L6-v2')
print("âœ… æ¨¡åž‹åŠ è½½å®Œæˆï¼")

# ç¤ºä¾‹æ–‡æœ¬
sentences = [
    "I love programming in Python",
    "Python is my favorite programming language",
    "The weather is sunny today",
    "I enjoy coding with Python",
    "It's a beautiful day outside",
    "Machine learning is fascinating"
]

# è®¡ç®—åµŒå…¥å‘é‡
print("\\nè®¡ç®—åµŒå…¥å‘é‡...")
embeddings = model.encode(sentences, convert_to_tensor=True)

print("\\n" + "="*60)
print("ðŸ“Š æ–‡æœ¬ç›¸ä¼¼åº¦çŸ©é˜µ")
print("="*60)

# è®¡ç®—ç›¸ä¼¼åº¦
cosine_scores = util.cos_sim(embeddings, embeddings)

# æ˜¾ç¤ºç»“æžœ
print("\\næ–‡æœ¬åˆ—è¡¨:")
for i, s in enumerate(sentences):
    print(f"  [{i}] {s}")

print("\\nç›¸ä¼¼åº¦çŸ©é˜µ (ç›¸ä¼¼åº¦ > 0.5 é«˜äº®):")
print("      ", end="")
for i in range(len(sentences)):
    print(f"[{i}]   ", end="")
print()

for i in range(len(sentences)):
    print(f"[{i}]  ", end="")
    for j in range(len(sentences)):
        score = cosine_scores[i][j].item()
        if i == j:
            print(f"1.00  ", end="")
        elif score > 0.5:
            print(f"\\033[92m{score:.2f}\\033[0m  ", end="")  # ç»¿è‰²é«˜äº®
        else:
            print(f"{score:.2f}  ", end="")
    print()

# æ‰¾å‡ºæœ€ç›¸ä¼¼çš„å¥å­å¯¹
print("\\n" + "="*60)
print("ðŸ” æœ€ç›¸ä¼¼çš„å¥å­å¯¹ (æŽ’é™¤è‡ªèº«)")
print("="*60)

pairs = []
for i in range(len(sentences)):
    for j in range(i+1, len(sentences)):
        pairs.append({
            'i': i, 'j': j,
            'score': cosine_scores[i][j].item()
        })

pairs.sort(key=lambda x: x['score'], reverse=True)

for p in pairs[:3]:
    print(f"\\nç›¸ä¼¼åº¦: {p['score']:.4f}")
    print(f"  æ–‡æœ¬1: {sentences[p['i']]}")
    print(f"  æ–‡æœ¬2: {sentences[p['j']]}")

print("\\nðŸŽ‰ è¯­ä¹‰ç›¸ä¼¼åº¦æ¼”ç¤ºå®Œæˆï¼")`,language:"python",difficulty:"intermediate",tags:["Embedding","è¯­ä¹‰æœç´¢","Sentence-BERT","Colab"],colabReady:!0},{id:"embedding-2",title:{zh:"è¯­ä¹‰æœç´¢å¼•æ“Ž",ja:"ã‚»ãƒžãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³"},description:{zh:"æž„å»ºç®€å•çš„è¯­ä¹‰æœç´¢ç³»ç»Ÿ",ja:"ã‚·ãƒ³ãƒ—ãƒ«ãªã‚»ãƒžãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰"},code:`"""
============================================
è¯­ä¹‰æœç´¢å¼•æ“Ž - æ‰¾å‡ºæœ€ç›¸å…³çš„æ–‡æ¡£
============================================
âœ… Colab å¯ç›´æŽ¥è¿è¡Œï¼Œæ— éœ€ API Key
âœ… ä½¿ç”¨ Sentence Transformers
============================================
"""

!pip install sentence-transformers -q

from sentence_transformers import SentenceTransformer, util
import torch

# åŠ è½½æ¨¡åž‹
print("æ­£åœ¨åŠ è½½è¯­ä¹‰æ¨¡åž‹...")
model = SentenceTransformer('all-MiniLM-L6-v2')
print("âœ… æ¨¡åž‹åŠ è½½å®Œæˆï¼")

# æ–‡æ¡£åº“
documents = [
    "Python is a popular programming language for AI and machine learning",
    "JavaScript is mainly used for web development and frontend applications",
    "Machine learning allows computers to learn from data",
    "Deep learning uses neural networks with many layers",
    "Natural language processing helps computers understand human language",
    "Computer vision enables machines to interpret visual information",
    "Reinforcement learning trains agents through rewards and penalties",
    "Transfer learning uses pre-trained models for new tasks",
    "Data preprocessing is essential before training ML models",
    "Model deployment puts trained models into production"
]

# é¢„è®¡ç®—æ–‡æ¡£åµŒå…¥
print("\\né¢„è®¡ç®—æ–‡æ¡£åµŒå…¥...")
doc_embeddings = model.encode(documents, convert_to_tensor=True)

def semantic_search(query, top_k=3):
    """è¯­ä¹‰æœç´¢å‡½æ•°"""
    query_embedding = model.encode(query, convert_to_tensor=True)
    scores = util.cos_sim(query_embedding, doc_embeddings)[0]
    top_results = torch.topk(scores, k=top_k)

    results = []
    for score, idx in zip(top_results.values, top_results.indices):
        results.append({
            'document': documents[idx],
            'score': score.item()
        })
    return results

# æ¼”ç¤ºæœç´¢
print("\\n" + "="*60)
print("ðŸ” è¯­ä¹‰æœç´¢æ¼”ç¤º")
print("="*60)

queries = [
    "How can AI understand human text?",
    "What programming language should I learn for AI?",
    "How do neural networks work?",
    "How to use trained models?"
]

for query in queries:
    print(f"\\nâ“ æŸ¥è¯¢: {query}")
    print("-"*50)
    results = semantic_search(query)
    for i, r in enumerate(results, 1):
        print(f"  {i}. [ç›¸å…³åº¦: {r['score']:.4f}]")
        print(f"     {r['document']}")

print("\\nðŸŽ‰ è¯­ä¹‰æœç´¢å¼•æ“Žæ¼”ç¤ºå®Œæˆï¼")`,language:"python",difficulty:"intermediate",tags:["æœç´¢","Embedding","è¯­ä¹‰åŒ¹é…","Colab"],colabReady:!0}]},image:{name:{zh:"å›¾åƒè¯†åˆ«ä¸Žåˆ†ç±»",ja:"ç”»åƒèªè­˜ã¨åˆ†é¡ž"},description:{zh:"å›¾åƒåˆ†ç±»ã€ç‰©ä½“æ£€æµ‹",ja:"ç”»åƒåˆ†é¡žã€ç‰©ä½“æ¤œå‡º"},icon:q,gradient:"from-pink-500 to-rose-600",examples:[{id:"image-1",title:{zh:"å›¾åƒåˆ†ç±» - è¯†åˆ«å›¾ç‰‡å†…å®¹",ja:"ç”»åƒåˆ†é¡ž - ç”»åƒã®å†…å®¹ã‚’è­˜åˆ¥"},description:{zh:"ä½¿ç”¨é¢„è®­ç»ƒæ¨¡åž‹è¯†åˆ«å›¾åƒ",ja:"äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ç”»åƒã‚’è­˜åˆ¥"},code:`"""
============================================
å›¾åƒåˆ†ç±» - è¯†åˆ«å›¾ç‰‡å†…å®¹
============================================
âœ… Colab å¯ç›´æŽ¥è¿è¡Œï¼Œæ— éœ€ API Key
âœ… ä½¿ç”¨é¢„è®­ç»ƒçš„ ResNet/ViT æ¨¡åž‹
============================================
"""

!pip install transformers torch pillow requests -q

from transformers import pipeline
from PIL import Image
import requests
from io import BytesIO

# åˆ›å»ºå›¾åƒåˆ†ç±»ç®¡é“
print("æ­£åœ¨åŠ è½½å›¾åƒåˆ†ç±»æ¨¡åž‹...")
classifier = pipeline("image-classification", model="google/vit-base-patch16-224")
print("âœ… æ¨¡åž‹åŠ è½½å®Œæˆï¼")

# æµ‹è¯•å›¾ç‰‡ URL
image_urls = [
    ("çŒ«", "https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/1200px-Cat03.jpg"),
    ("ç‹—", "https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/YellowLabradorLooking_new.jpg/1200px-YellowLabradorLooking_new.jpg"),
]

print("\\n" + "="*60)
print("ðŸ–¼ï¸ å›¾åƒåˆ†ç±»ç»“æžœ")
print("="*60)

for name, url in image_urls:
    try:
        print(f"\\næ­£åœ¨åˆ†æž: {name}")
        response = requests.get(url, timeout=10)
        image = Image.open(BytesIO(response.content))

        # è¿›è¡Œåˆ†ç±»
        results = classifier(image)

        print(f"\\nðŸ“· å›¾ç‰‡: {name}")
        print("-"*40)
        for r in results[:5]:
            bar = "â–ˆ" * int(r['score'] * 20)
            print(f"  {r['label'][:30]:30} {bar} {r['score']:.2%}")
    except Exception as e:
        print(f"  âš ï¸ åŠ è½½å›¾ç‰‡å¤±è´¥: {e}")

# ä½¿ç”¨æœ¬åœ°å›¾ç‰‡ï¼ˆå¦‚æžœæœ‰ï¼‰
print("\\n" + "="*60)
print("ðŸ’¡ ä½¿ç”¨æœ¬åœ°å›¾ç‰‡çš„æ–¹æ³•:")
print("="*60)
print("""
from PIL import Image

# æ–¹æ³•1: ä»Žæ–‡ä»¶åŠ è½½
image = Image.open("your_image.jpg")
results = classifier(image)

# æ–¹æ³•2: ä»Ž URL åŠ è½½
import requests
response = requests.get("https://example.com/image.jpg")
image = Image.open(BytesIO(response.content))
results = classifier(image)
""")

print("\\nðŸŽ‰ å›¾åƒåˆ†ç±»æ¼”ç¤ºå®Œæˆï¼")`,language:"python",difficulty:"beginner",tags:["å›¾åƒ","Vision","ViT","Colab"],colabReady:!0},{id:"image-2",title:{zh:"MNIST æ‰‹å†™æ•°å­—è¯†åˆ«",ja:"MNIST æ‰‹æ›¸ãæ•°å­—èªè­˜"},description:{zh:"ç»å…¸çš„æ‰‹å†™æ•°å­—è¯†åˆ«å…¥é—¨é¡¹ç›®",ja:"å¤å…¸çš„ãªæ‰‹æ›¸ãæ•°å­—èªè­˜å…¥é–€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ"},code:`"""
============================================
MNIST æ‰‹å†™æ•°å­—è¯†åˆ« - æ·±åº¦å­¦ä¹ å…¥é—¨
============================================
âœ… Colab å¯ç›´æŽ¥è¿è¡Œï¼Œæ— éœ€ API Key
âœ… ä½¿ç”¨ PyTorch æž„å»º CNN
============================================
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt

# æ£€æŸ¥è®¾å¤‡
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# æ•°æ®é¢„å¤„ç†
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

# ä¸‹è½½ MNIST æ•°æ®é›†
print("\\næ­£åœ¨ä¸‹è½½ MNIST æ•°æ®é›†...")
train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST('./data', train=False, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=1000)

print(f"è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
print(f"æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬")

# å®šä¹‰ CNN æ¨¡åž‹
class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)
        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = torch.relu(self.conv2(x))
        x = torch.max_pool2d(x, 2)
        x = torch.flatten(x, 1)
        x = self.dropout(x)
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# åˆ›å»ºæ¨¡åž‹
model = SimpleCNN().to(device)
optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

print("\\næ¨¡åž‹ç»“æž„:")
print(model)

# è®­ç»ƒå‡½æ•°
def train_epoch(model, loader, optimizer, criterion):
    model.train()
    total_loss = 0
    correct = 0
    for data, target in loader:
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        pred = output.argmax(dim=1)
        correct += pred.eq(target).sum().item()
    return total_loss / len(loader), correct / len(loader.dataset)

# æµ‹è¯•å‡½æ•°
def test(model, loader):
    model.eval()
    correct = 0
    with torch.no_grad():
        for data, target in loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            pred = output.argmax(dim=1)
            correct += pred.eq(target).sum().item()
    return correct / len(loader.dataset)

# è®­ç»ƒæ¨¡åž‹
print("\\nå¼€å§‹è®­ç»ƒ...")
print("="*50)

epochs = 3  # å¿«é€Ÿæ¼”ç¤º
for epoch in range(1, epochs + 1):
    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion)
    test_acc = test(model, test_loader)
    print(f"Epoch {epoch}: è®­ç»ƒæŸå¤±={train_loss:.4f}, è®­ç»ƒå‡†ç¡®çŽ‡={train_acc:.2%}, æµ‹è¯•å‡†ç¡®çŽ‡={test_acc:.2%}")

# å¯è§†åŒ–é¢„æµ‹ç»“æžœ
print("\\n" + "="*50)
print("ðŸ” é¢„æµ‹ç¤ºä¾‹")
print("="*50)

model.eval()
examples = enumerate(test_loader)
_, (example_data, example_targets) = next(examples)

with torch.no_grad():
    output = model(example_data[:10].to(device))
    predictions = output.argmax(dim=1).cpu()

print("\\nçœŸå®žæ ‡ç­¾:", example_targets[:10].tolist())
print("é¢„æµ‹ç»“æžœ:", predictions.tolist())
print("æ˜¯å¦æ­£ç¡®:", ["âœ“" if p == t else "âœ—" for p, t in zip(predictions, example_targets[:10])])

print("\\nðŸŽ‰ MNIST æ‰‹å†™æ•°å­—è¯†åˆ«æ¼”ç¤ºå®Œæˆï¼")`,language:"python",difficulty:"intermediate",tags:["MNIST","CNN","PyTorch","Colab"],colabReady:!0}]},"ml-basics":{name:{zh:"æœºå™¨å­¦ä¹ åŸºç¡€",ja:"æ©Ÿæ¢°å­¦ç¿’ã®åŸºç¤Ž"},description:{zh:"Scikit-learn ç»å…¸ç®—æ³•å®žè·µ",ja:"Scikit-learnã«ã‚ˆã‚‹å¤å…¸çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å®Ÿè·µ"},icon:v,gradient:"from-violet-500 to-purple-600",examples:[{id:"ml-1",title:{zh:"åˆ†ç±»ç®—æ³•å¯¹æ¯”",ja:"åˆ†é¡žã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æ¯”è¼ƒ"},description:{zh:"ä½¿ç”¨é¸¢å°¾èŠ±æ•°æ®é›†å¯¹æ¯”å¤šç§åˆ†ç±»ç®—æ³•",ja:"ã‚¢ãƒ¤ãƒ¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§è¤‡æ•°ã®åˆ†é¡žã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’æ¯”è¼ƒ"},code:`"""
============================================
åˆ†ç±»ç®—æ³•å¯¹æ¯” - Scikit-learn å…¥é—¨
============================================
âœ… Colab å¯ç›´æŽ¥è¿è¡Œï¼Œæ— éœ€ API Key
âœ… ä½¿ç”¨ç»å…¸çš„é¸¢å°¾èŠ±æ•°æ®é›†
============================================
"""

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
import numpy as np

# åŠ è½½æ•°æ®
print("æ­£åœ¨åŠ è½½é¸¢å°¾èŠ±æ•°æ®é›†...")
iris = load_iris()
X, y = iris.data, iris.target

print(f"æ•°æ®é›†å¤§å°: {X.shape[0]} æ ·æœ¬, {X.shape[1]} ç‰¹å¾")
print(f"ç±»åˆ«: {iris.target_names}")
print(f"ç‰¹å¾: {iris.feature_names}")

# åˆ’åˆ†æ•°æ®é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# æ ‡å‡†åŒ–
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# å®šä¹‰æ¨¡åž‹
models = {
    'é€»è¾‘å›žå½’': LogisticRegression(max_iter=200),
    'å†³ç­–æ ‘': DecisionTreeClassifier(max_depth=5),
    'éšæœºæ£®æž—': RandomForestClassifier(n_estimators=100),
    'SVM': SVC(kernel='rbf'),
    'KNN': KNeighborsClassifier(n_neighbors=5)
}

print("\\n" + "="*60)
print("ðŸ”¬ åˆ†ç±»ç®—æ³•å¯¹æ¯”")
print("="*60)

results = []
for name, model in models.items():
    # è®­ç»ƒ
    model.fit(X_train_scaled, y_train)

    # æµ‹è¯•å‡†ç¡®çŽ‡
    test_acc = model.score(X_test_scaled, y_test)

    # äº¤å‰éªŒè¯
    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)

    results.append({
        'name': name,
        'test_acc': test_acc,
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std()
    })

    bar = "â–ˆ" * int(test_acc * 30)
    print(f"\\n{name:10}")
    print(f"  æµ‹è¯•å‡†ç¡®çŽ‡: {bar} {test_acc:.2%}")
    print(f"  äº¤å‰éªŒè¯:   {cv_scores.mean():.2%} (Â±{cv_scores.std():.2%})")

# æŽ’å
print("\\n" + "="*60)
print("ðŸ† ç®—æ³•æŽ’å (æŒ‰æµ‹è¯•å‡†ç¡®çŽ‡)")
print("="*60)

results.sort(key=lambda x: x['test_acc'], reverse=True)
for i, r in enumerate(results, 1):
    medal = ["ðŸ¥‡", "ðŸ¥ˆ", "ðŸ¥‰", "4.", "5."][i-1]
    print(f"{medal} {r['name']:12} - {r['test_acc']:.2%}")

print("\\nðŸŽ‰ åˆ†ç±»ç®—æ³•å¯¹æ¯”æ¼”ç¤ºå®Œæˆï¼")`,language:"python",difficulty:"beginner",tags:["åˆ†ç±»","Scikit-learn","é¸¢å°¾èŠ±","Colab"],colabReady:!0},{id:"ml-2",title:{zh:"å›žå½’é¢„æµ‹ - æˆ¿ä»·é¢„æµ‹",ja:"å›žå¸°äºˆæ¸¬ - ä½å®…ä¾¡æ ¼äºˆæ¸¬"},description:{zh:"ä½¿ç”¨æ³¢å£«é¡¿æˆ¿ä»·æ•°æ®é›†å­¦ä¹ å›žå½’ç®—æ³•",ja:"ãƒœã‚¹ãƒˆãƒ³ä½å®…ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å›žå¸°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å­¦ã¶"},code:`"""
============================================
å›žå½’é¢„æµ‹ - æˆ¿ä»·é¢„æµ‹
============================================
âœ… Colab å¯ç›´æŽ¥è¿è¡Œï¼Œæ— éœ€ API Key
âœ… ä½¿ç”¨ California Housing æ•°æ®é›†
============================================
"""

from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# åŠ è½½æ•°æ®
print("æ­£åœ¨åŠ è½½ California Housing æ•°æ®é›†...")
housing = fetch_california_housing()
X, y = housing.data, housing.target

print(f"æ•°æ®é›†å¤§å°: {X.shape[0]} æ ·æœ¬, {X.shape[1]} ç‰¹å¾")
print(f"ç‰¹å¾: {housing.feature_names}")
print(f"ç›®æ ‡: æˆ¿ä»·ä¸­ä½æ•° (å•ä½: $100,000)")

# åˆ’åˆ†æ•°æ®é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# æ ‡å‡†åŒ–
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# å®šä¹‰æ¨¡åž‹
models = {
    'çº¿æ€§å›žå½’': LinearRegression(),
    'Ridgeå›žå½’': Ridge(alpha=1.0),
    'Lassoå›žå½’': Lasso(alpha=0.1),
    'éšæœºæ£®æž—': RandomForestRegressor(n_estimators=50, max_depth=10),
    'æ¢¯åº¦æå‡': GradientBoostingRegressor(n_estimators=50, max_depth=5)
}

print("\\n" + "="*60)
print("ðŸ“Š å›žå½’ç®—æ³•å¯¹æ¯”")
print("="*60)

results = []
for name, model in models.items():
    print(f"\\nè®­ç»ƒ {name}...")

    # è®­ç»ƒ
    model.fit(X_train_scaled, y_train)

    # é¢„æµ‹
    y_pred = model.predict(X_test_scaled)

    # è¯„ä¼°
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)

    results.append({
        'name': name,
        'rmse': rmse,
        'r2': r2
    })

    print(f"  RMSE: {rmse:.4f} ($100k)")
    print(f"  RÂ²:   {r2:.4f}")

# æŽ’å
print("\\n" + "="*60)
print("ðŸ† ç®—æ³•æŽ’å (æŒ‰ RÂ² åˆ†æ•°)")
print("="*60)

results.sort(key=lambda x: x['r2'], reverse=True)
for i, r in enumerate(results, 1):
    medal = ["ðŸ¥‡", "ðŸ¥ˆ", "ðŸ¥‰", "4.", "5."][i-1]
    bar = "â–ˆ" * int(r['r2'] * 20)
    print(f"{medal} {r['name']:12} RÂ²={r['r2']:.4f} {bar}")

# ç‰¹å¾é‡è¦æ€§ï¼ˆä½¿ç”¨éšæœºæ£®æž—ï¼‰
print("\\n" + "="*60)
print("ðŸ“ˆ ç‰¹å¾é‡è¦æ€§ (éšæœºæ£®æž—)")
print("="*60)

rf_model = models['éšæœºæ£®æž—']
importances = rf_model.feature_importances_
indices = np.argsort(importances)[::-1]

for i, idx in enumerate(indices):
    bar = "â–ˆ" * int(importances[idx] * 50)
    print(f"  {housing.feature_names[idx]:15} {bar} {importances[idx]:.3f}")

print("\\nðŸŽ‰ å›žå½’é¢„æµ‹æ¼”ç¤ºå®Œæˆï¼")`,language:"python",difficulty:"beginner",tags:["å›žå½’","é¢„æµ‹","Scikit-learn","Colab"],colabReady:!0}]},neural:{name:{zh:"ç¥žç»ç½‘ç»œä»Žé›¶å®žçŽ°",ja:"ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ã‚¼ãƒ­ã‹ã‚‰å®Ÿè£…"},description:{zh:"ä¸ä½¿ç”¨æ¡†æž¶ï¼Œçº¯ NumPy å®žçŽ°ç¥žç»ç½‘ç»œ",ja:"ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ã‚ãšã€NumPyã ã‘ã§ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’å®Ÿè£…"},icon:A,gradient:"from-cyan-500 to-blue-600",examples:[{id:"neural-1",title:{zh:"æ„ŸçŸ¥æœº - æœ€ç®€å•çš„ç¥žç»ç½‘ç»œ",ja:"ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³ - æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯"},description:{zh:"ç”¨ NumPy å®žçŽ°å•å±‚æ„ŸçŸ¥æœº",ja:"NumPyã§å˜å±¤ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³ã‚’å®Ÿè£…"},code:`"""
============================================
æ„ŸçŸ¥æœº - æœ€ç®€å•çš„ç¥žç»ç½‘ç»œ
============================================
âœ… Colab å¯ç›´æŽ¥è¿è¡Œï¼Œæ— éœ€ API Key
âœ… çº¯ NumPy å®žçŽ°ï¼Œç†è§£ç¥žç»ç½‘ç»œåŽŸç†
============================================
"""

import numpy as np
import matplotlib.pyplot as plt

# è®¾ç½®éšæœºç§å­
np.random.seed(42)

class Perceptron:
    """å•å±‚æ„ŸçŸ¥æœº"""

    def __init__(self, learning_rate=0.1):
        self.lr = learning_rate
        self.weights = None
        self.bias = None
        self.errors = []

    def activation(self, x):
        """é˜¶è·ƒæ¿€æ´»å‡½æ•°"""
        return np.where(x >= 0, 1, 0)

    def fit(self, X, y, epochs=100):
        """è®­ç»ƒæ„ŸçŸ¥æœº"""
        n_samples, n_features = X.shape

        # åˆå§‹åŒ–æƒé‡
        self.weights = np.random.randn(n_features) * 0.01
        self.bias = 0

        print("å¼€å§‹è®­ç»ƒæ„ŸçŸ¥æœº...")
        print(f"æ ·æœ¬æ•°: {n_samples}, ç‰¹å¾æ•°: {n_features}")
        print("-" * 40)

        for epoch in range(epochs):
            errors = 0
            for xi, yi in zip(X, y):
                # å‰å‘ä¼ æ’­
                linear_output = np.dot(xi, self.weights) + self.bias
                prediction = self.activation(linear_output)

                # æ›´æ–°æƒé‡ï¼ˆå¦‚æžœé¢„æµ‹é”™è¯¯ï¼‰
                error = yi - prediction
                if error != 0:
                    self.weights += self.lr * error * xi
                    self.bias += self.lr * error
                    errors += 1

            self.errors.append(errors)

            # æ¯10è½®æˆ–æœ€åŽä¸€è½®æ‰“å°
            if (epoch + 1) % 10 == 0 or epoch == 0:
                print(f"Epoch {epoch+1:3d}: é”™è¯¯æ•° = {errors}")

            # å¦‚æžœæ²¡æœ‰é”™è¯¯ï¼Œæå‰åœæ­¢
            if errors == 0:
                print(f"\\nâœ… åœ¨ç¬¬ {epoch+1} è½®æ”¶æ•›ï¼")
                break

        return self

    def predict(self, X):
        """é¢„æµ‹"""
        linear_output = np.dot(X, self.weights) + self.bias
        return self.activation(linear_output)

# åˆ›å»º AND é—¨æ•°æ®
print("\\n" + "="*50)
print("ðŸ”§ å®žçŽ° AND é€»è¾‘é—¨")
print("="*50)

X_and = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y_and = np.array([0, 0, 0, 1])  # AND é—¨

perceptron = Perceptron(learning_rate=0.1)
perceptron.fit(X_and, y_and, epochs=20)

print("\\né¢„æµ‹ç»“æžœ:")
print("è¾“å…¥    |  çœŸå®ž  | é¢„æµ‹")
print("-" * 30)
for xi, yi in zip(X_and, y_and):
    pred = perceptron.predict(xi.reshape(1, -1))[0]
    status = "âœ“" if pred == yi else "âœ—"
    print(f"{xi}  |   {yi}   |   {pred}  {status}")

# åˆ›å»º OR é—¨æ•°æ®
print("\\n" + "="*50)
print("ðŸ”§ å®žçŽ° OR é€»è¾‘é—¨")
print("="*50)

y_or = np.array([0, 1, 1, 1])  # OR é—¨

perceptron_or = Perceptron(learning_rate=0.1)
perceptron_or.fit(X_and, y_or, epochs=20)

print("\\né¢„æµ‹ç»“æžœ:")
print("è¾“å…¥    |  çœŸå®ž  | é¢„æµ‹")
print("-" * 30)
for xi, yi in zip(X_and, y_or):
    pred = perceptron_or.predict(xi.reshape(1, -1))[0]
    status = "âœ“" if pred == yi else "âœ—"
    print(f"{xi}  |   {yi}   |   {pred}  {status}")

print("\\n" + "="*50)
print("ðŸ’¡ æ„ŸçŸ¥æœºçš„å±€é™æ€§")
print("="*50)
print("æ„ŸçŸ¥æœºåªèƒ½è§£å†³çº¿æ€§å¯åˆ†é—®é¢˜")
print("XOR é—®é¢˜å°±æ— æ³•ç”¨å•å±‚æ„ŸçŸ¥æœºè§£å†³")
print("è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦å¤šå±‚ç¥žç»ç½‘ç»œï¼")

print("\\nðŸŽ‰ æ„ŸçŸ¥æœºæ¼”ç¤ºå®Œæˆï¼")`,language:"python",difficulty:"intermediate",tags:["æ„ŸçŸ¥æœº","NumPy","ç¥žç»ç½‘ç»œ","Colab"],colabReady:!0},{id:"neural-2",title:{zh:"å¤šå±‚ç¥žç»ç½‘ç»œ - è§£å†³ XOR é—®é¢˜",ja:"å¤šå±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ - XORå•é¡Œã‚’è§£ã"},description:{zh:"ç”¨ NumPy å®žçŽ°å¸¦åå‘ä¼ æ’­çš„ç¥žç»ç½‘ç»œ",ja:"NumPyã§é€†ä¼æ’­ä»˜ããƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’å®Ÿè£…"},code:`"""
============================================
å¤šå±‚ç¥žç»ç½‘ç»œ - è§£å†³ XOR é—®é¢˜
============================================
âœ… Colab å¯ç›´æŽ¥è¿è¡Œï¼Œæ— éœ€ API Key
âœ… çº¯ NumPy å®žçŽ°ï¼ŒåŒ…å«åå‘ä¼ æ’­
============================================
"""

import numpy as np

np.random.seed(42)

class NeuralNetwork:
    """å¤šå±‚ç¥žç»ç½‘ç»œï¼ˆå‰é¦ˆ + åå‘ä¼ æ’­ï¼‰"""

    def __init__(self, layer_sizes, learning_rate=0.5):
        """
        layer_sizes: å„å±‚ç¥žç»å…ƒæ•°é‡ï¼Œå¦‚ [2, 4, 1] è¡¨ç¤ºè¾“å…¥2ï¼Œéšè—4ï¼Œè¾“å‡º1
        """
        self.layer_sizes = layer_sizes
        self.lr = learning_rate
        self.weights = []
        self.biases = []

        # åˆå§‹åŒ–æƒé‡å’Œåç½®
        for i in range(len(layer_sizes) - 1):
            w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * 0.5
            b = np.zeros((1, layer_sizes[i+1]))
            self.weights.append(w)
            self.biases.append(b)

    def sigmoid(self, x):
        """Sigmoid æ¿€æ´»å‡½æ•°"""
        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))

    def sigmoid_derivative(self, x):
        """Sigmoid å¯¼æ•°"""
        return x * (1 - x)

    def forward(self, X):
        """å‰å‘ä¼ æ’­"""
        self.activations = [X]

        for i in range(len(self.weights)):
            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]
            a = self.sigmoid(z)
            self.activations.append(a)

        return self.activations[-1]

    def backward(self, X, y):
        """åå‘ä¼ æ’­"""
        m = X.shape[0]

        # è®¡ç®—è¾“å‡ºå±‚è¯¯å·®
        error = y - self.activations[-1]
        deltas = [error * self.sigmoid_derivative(self.activations[-1])]

        # åå‘è®¡ç®—æ¯å±‚çš„è¯¯å·®
        for i in range(len(self.weights) - 1, 0, -1):
            error = deltas[-1].dot(self.weights[i].T)
            delta = error * self.sigmoid_derivative(self.activations[i])
            deltas.append(delta)

        deltas.reverse()

        # æ›´æ–°æƒé‡å’Œåç½®
        for i in range(len(self.weights)):
            self.weights[i] += self.lr * self.activations[i].T.dot(deltas[i]) / m
            self.biases[i] += self.lr * np.sum(deltas[i], axis=0, keepdims=True) / m

    def train(self, X, y, epochs=10000, print_every=1000):
        """è®­ç»ƒç½‘ç»œ"""
        print("å¼€å§‹è®­ç»ƒå¤šå±‚ç¥žç»ç½‘ç»œ...")
        print(f"ç½‘ç»œç»“æž„: {self.layer_sizes}")
        print("-" * 50)

        for epoch in range(epochs):
            # å‰å‘ä¼ æ’­
            output = self.forward(X)

            # åå‘ä¼ æ’­
            self.backward(X, y)

            # æ‰“å°æŸå¤±
            if (epoch + 1) % print_every == 0:
                loss = np.mean((y - output) ** 2)
                print(f"Epoch {epoch+1:5d}: Loss = {loss:.6f}")

        print("-" * 50)
        print("è®­ç»ƒå®Œæˆï¼")

    def predict(self, X):
        """é¢„æµ‹"""
        return self.forward(X)

# XOR é—®é¢˜
print("="*50)
print("ðŸ§  è§£å†³ XOR é—®é¢˜")
print("="*50)
print("XOR çœŸå€¼è¡¨:")
print("0 XOR 0 = 0")
print("0 XOR 1 = 1")
print("1 XOR 0 = 1")
print("1 XOR 1 = 0")
print("="*50)

# æ•°æ®
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [1], [1], [0]])

# åˆ›å»ºç½‘ç»œï¼š2ä¸ªè¾“å…¥ï¼Œ4ä¸ªéšè—ç¥žç»å…ƒï¼Œ1ä¸ªè¾“å‡º
nn = NeuralNetwork([2, 4, 1], learning_rate=1.0)

# è®­ç»ƒ
nn.train(X, y, epochs=10000, print_every=2000)

# é¢„æµ‹
print("\\né¢„æµ‹ç»“æžœ:")
print("è¾“å…¥    |  çœŸå®ž  |  é¢„æµ‹  | å››èˆäº”å…¥")
print("-" * 45)

predictions = nn.predict(X)
for xi, yi, pred in zip(X, y, predictions):
    rounded = 1 if pred[0] > 0.5 else 0
    status = "âœ“" if rounded == yi[0] else "âœ—"
    print(f"{xi}  |   {yi[0]}   | {pred[0]:.4f} |    {rounded}  {status}")

print("\\n" + "="*50)
print("ðŸ’¡ å…³é”®æ¦‚å¿µ")
print("="*50)
print("1. éšè—å±‚è®©ç½‘ç»œèƒ½å­¦ä¹ éžçº¿æ€§å…³ç³»")
print("2. åå‘ä¼ æ’­è®¡ç®—æ¯ä¸ªæƒé‡å¯¹æŸå¤±çš„è´¡çŒ®")
print("3. æ¢¯åº¦ä¸‹é™æ²¿ç€æŸå¤±å‡å°çš„æ–¹å‘æ›´æ–°æƒé‡")
print("4. è¿™å°±æ˜¯æ·±åº¦å­¦ä¹ çš„åŸºç¡€ï¼")

print("\\nðŸŽ‰ å¤šå±‚ç¥žç»ç½‘ç»œæ¼”ç¤ºå®Œæˆï¼")`,language:"python",difficulty:"intermediate",tags:["ç¥žç»ç½‘ç»œ","åå‘ä¼ æ’­","NumPy","Colab"],colabReady:!0}]},"rag-local":{name:{zh:"æœ¬åœ° RAG ç³»ç»Ÿ",ja:"ãƒ­ãƒ¼ã‚«ãƒ«RAGã‚·ã‚¹ãƒ†ãƒ "},description:{zh:"ä½¿ç”¨å…è´¹æ¨¡åž‹æž„å»ºæ£€ç´¢å¢žå¼ºç”Ÿæˆç³»ç»Ÿ",ja:"ç„¡æ–™ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸæ¤œç´¢æ‹¡å¼µç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ "},icon:y,gradient:"from-teal-500 to-green-600",examples:[{id:"rag-1",title:{zh:"ç®€å• RAG - æ–‡æ¡£é—®ç­”ç³»ç»Ÿ",ja:"ã‚·ãƒ³ãƒ—ãƒ«RAG - æ–‡æ›¸Q&Aã‚·ã‚¹ãƒ†ãƒ "},description:{zh:"åŸºäºŽå‘é‡æ£€ç´¢çš„æ–‡æ¡£é—®ç­”",ja:"ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã«åŸºã¥ãæ–‡æ›¸Q&A"},code:`"""
============================================
ç®€å• RAG - æ–‡æ¡£é—®ç­”ç³»ç»Ÿ
============================================
âœ… Colab å¯ç›´æŽ¥è¿è¡Œï¼Œæ— éœ€ API Key
âœ… ä½¿ç”¨ Sentence Transformers + æœ¬åœ°æœç´¢
============================================
"""

!pip install sentence-transformers -q

from sentence_transformers import SentenceTransformer, util
import numpy as np

# åŠ è½½åµŒå…¥æ¨¡åž‹
print("æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡åž‹...")
embedder = SentenceTransformer('all-MiniLM-L6-v2')
print("âœ… æ¨¡åž‹åŠ è½½å®Œæˆï¼")

# çŸ¥è¯†åº“ï¼ˆæ¨¡æ‹Ÿæ–‡æ¡£ï¼‰
knowledge_base = [
    {
        "title": "Python ç®€ä»‹",
        "content": "Python æ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œä»¥å…¶ç®€æ´æ˜“è¯»çš„è¯­æ³•è‘—ç§°ã€‚å®ƒæ”¯æŒå¤šç§ç¼–ç¨‹èŒƒå¼ï¼ŒåŒ…æ‹¬é¢å‘å¯¹è±¡ã€å‡½æ•°å¼å’Œè¿‡ç¨‹å¼ç¼–ç¨‹ã€‚Python å¹¿æ³›åº”ç”¨äºŽ Web å¼€å‘ã€æ•°æ®åˆ†æžã€äººå·¥æ™ºèƒ½å’Œç§‘å­¦è®¡ç®—ç­‰é¢†åŸŸã€‚"
    },
    {
        "title": "æœºå™¨å­¦ä¹ åŸºç¡€",
        "content": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»Žæ•°æ®ä¸­å­¦ä¹ å¹¶åšå‡ºé¢„æµ‹æˆ–å†³ç­–ã€‚ä¸»è¦ç±»åž‹åŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚å¸¸ç”¨ç®—æ³•æœ‰çº¿æ€§å›žå½’ã€å†³ç­–æ ‘ã€ç¥žç»ç½‘ç»œç­‰ã€‚"
    },
    {
        "title": "æ·±åº¦å­¦ä¹ ä»‹ç»",
        "content": "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„å­é›†ï¼Œä½¿ç”¨å¤šå±‚ç¥žç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„å¤æ‚è¡¨ç¤ºã€‚å®ƒåœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œè¯­éŸ³è¯†åˆ«ç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚å¸¸è§æž¶æž„åŒ…æ‹¬ CNNã€RNN å’Œ Transformerã€‚"
    },
    {
        "title": "è‡ªç„¶è¯­è¨€å¤„ç†",
        "content": "è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œä¸“æ³¨äºŽè®¡ç®—æœºä¸Žäººç±»è¯­è¨€çš„äº¤äº’ã€‚ä¸»è¦ä»»åŠ¡åŒ…æ‹¬æ–‡æœ¬åˆ†ç±»ã€å‘½åå®žä½“è¯†åˆ«ã€æœºå™¨ç¿»è¯‘ã€é—®ç­”ç³»ç»Ÿå’Œæ–‡æœ¬ç”Ÿæˆã€‚BERT å’Œ GPT æ˜¯å½“å‰æœ€æµè¡Œçš„ NLP æ¨¡åž‹ã€‚"
    },
    {
        "title": "Transformer æž¶æž„",
        "content": "Transformer æ˜¯ä¸€ç§ç¥žç»ç½‘ç»œæž¶æž„ï¼Œç”± Google åœ¨ 2017 å¹´æå‡ºã€‚å®ƒçš„æ ¸å¿ƒæ˜¯è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿå¹¶è¡Œå¤„ç†åºåˆ—æ•°æ®ã€‚Transformer æ˜¯ BERTã€GPTã€T5 ç­‰æ¨¡åž‹çš„åŸºç¡€ï¼Œrevolutionized äº† NLP é¢†åŸŸã€‚"
    }
]

# é¢„è®¡ç®—æ–‡æ¡£åµŒå…¥
print("\\né¢„è®¡ç®—æ–‡æ¡£åµŒå…¥...")
doc_texts = [doc["content"] for doc in knowledge_base]
doc_embeddings = embedder.encode(doc_texts, convert_to_tensor=True)

class SimpleRAG:
    """ç®€å•çš„ RAG ç³»ç»Ÿ"""

    def __init__(self, embedder, documents, doc_embeddings):
        self.embedder = embedder
        self.documents = documents
        self.doc_embeddings = doc_embeddings

    def retrieve(self, query, top_k=2):
        """æ£€ç´¢æœ€ç›¸å…³çš„æ–‡æ¡£"""
        query_embedding = self.embedder.encode(query, convert_to_tensor=True)
        scores = util.cos_sim(query_embedding, self.doc_embeddings)[0]
        top_indices = scores.argsort(descending=True)[:top_k]

        results = []
        for idx in top_indices:
            results.append({
                'document': self.documents[idx.item()],
                'score': scores[idx.item()].item()
            })
        return results

    def answer(self, query):
        """åŸºäºŽæ£€ç´¢ç»“æžœå›žç­”é—®é¢˜"""
        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        retrieved = self.retrieve(query, top_k=2)

        # æž„å»ºä¸Šä¸‹æ–‡
        context = "\\n\\n".join([
            f"ã€{r['document']['title']}ã€‘\\n{r['document']['content']}"
            for r in retrieved
        ])

        # è¿™é‡Œæˆ‘ä»¬åªè¿”å›žç›¸å…³æ–‡æ¡£
        # åœ¨å®žé™…åº”ç”¨ä¸­ï¼Œä¼šå°† context + query é€å…¥ LLM ç”Ÿæˆç­”æ¡ˆ
        return {
            'query': query,
            'retrieved_docs': retrieved,
            'context': context
        }

# åˆ›å»º RAG ç³»ç»Ÿ
rag = SimpleRAG(embedder, knowledge_base, doc_embeddings)

# æµ‹è¯•æŸ¥è¯¢
print("\\n" + "="*60)
print("ðŸ” RAG æ–‡æ¡£é—®ç­”æ¼”ç¤º")
print("="*60)

queries = [
    "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
    "Python å¯ä»¥ç”¨æ¥åšä»€ä¹ˆï¼Ÿ",
    "Transformer æ˜¯ä»€ä¹ˆï¼Ÿ",
    "æœºå™¨å­¦ä¹ æœ‰å“ªäº›ç±»åž‹ï¼Ÿ"
]

for query in queries:
    print(f"\\nâ“ é—®é¢˜: {query}")
    print("-"*50)

    result = rag.answer(query)

    print("ðŸ“š æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£:")
    for i, doc in enumerate(result['retrieved_docs'], 1):
        print(f"\\n  [{i}] {doc['document']['title']} (ç›¸å…³åº¦: {doc['score']:.4f})")
        print(f"      {doc['document']['content'][:100]}...")

print("\\n" + "="*60)
print("ðŸ’¡ RAG å·¥ä½œæµç¨‹")
print("="*60)
print("1. ç”¨æˆ·æé—®")
print("2. å°†é—®é¢˜è½¬æ¢ä¸ºå‘é‡")
print("3. åœ¨æ–‡æ¡£åº“ä¸­æœç´¢ç›¸ä¼¼å‘é‡")
print("4. è¿”å›žæœ€ç›¸å…³çš„æ–‡æ¡£")
print("5. å°†æ–‡æ¡£ + é—®é¢˜é€å…¥ LLM ç”Ÿæˆç­”æ¡ˆ")

print("\\nðŸŽ‰ RAG æ–‡æ¡£é—®ç­”æ¼”ç¤ºå®Œæˆï¼")`,language:"python",difficulty:"intermediate",tags:["RAG","æ£€ç´¢","å‘é‡æœç´¢","Colab"],colabReady:!0}]}},D=()=>{const f=_(),t=b(r=>r.language)==="ja"?"ja":"zh",[a,u]=o.useState("intro"),[l,p]=o.useState(0),[d,c]=o.useState(!1),g=Object.entries(m),s=m[a],i=s.examples[l],h=async()=>{await navigator.clipboard.writeText(i.code),c(!0),setTimeout(()=>c(!1),2e3)};return e.jsxs("div",{className:"min-h-screen bg-gradient-to-br from-slate-900 via-slate-800 to-slate-900",children:[e.jsx("header",{className:"bg-black/30 backdrop-blur-xl border-b border-white/10 sticky top-0 z-50",children:e.jsx("div",{className:"max-w-7xl mx-auto px-4 py-3",children:e.jsxs("div",{className:"flex items-center justify-between",children:[e.jsxs("button",{onClick:()=>f("/"),className:"flex items-center gap-2 text-white/60 hover:text-white transition-colors",children:[e.jsx(X,{size:18}),e.jsx(N,{size:14}),e.jsx("span",{className:"text-sm",children:t==="ja"?"ãƒ›ãƒ¼ãƒ ":"é¦–é¡µ"})]}),e.jsxs("h1",{className:"text-lg font-bold text-white flex items-center gap-2",children:[e.jsx(z,{size:20,className:"text-purple-400"}),t==="ja"?"AI ã‚³ãƒ¼ãƒ‰å®Ÿè·µ":"AI ä»£ç å®žæˆ˜"]}),e.jsx("div",{className:"text-xs text-emerald-400 bg-emerald-400/10 px-2 py-1 rounded-full",children:"âœ… Colab Ready"})]})})}),e.jsxs("div",{className:"max-w-7xl mx-auto px-4 py-6",children:[e.jsx("div",{className:"mb-6 p-4 bg-emerald-500/10 border border-emerald-500/20 rounded-xl",children:e.jsxs("div",{className:"flex items-start gap-3",children:[e.jsx(C,{className:"text-emerald-400 flex-shrink-0 mt-0.5",size:20}),e.jsxs("div",{children:[e.jsx("h3",{className:"font-semibold text-emerald-400",children:t==="ja"?"å…¨ã¦ã®ã‚³ãƒ¼ãƒ‰ã¯ Colab ã§ç›´æŽ¥å®Ÿè¡Œå¯èƒ½":"æ‰€æœ‰ä»£ç å‡å¯åœ¨ Colab ä¸­ç›´æŽ¥è¿è¡Œ"}),e.jsx("p",{className:"text-sm text-slate-400 mt-1",children:t==="ja"?"APIã‚­ãƒ¼ä¸è¦ï¼Hugging Face ã®ç„¡æ–™ãƒ¢ãƒ‡ãƒ«ã¨ scikit-learn ã‚’ä½¿ç”¨ã€‚ã‚³ãƒ¼ãƒ‰ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ Colab ã«è²¼ã‚Šä»˜ã‘ã‚‹ã ã‘ã€‚":"æ— éœ€ API Keyï¼ä½¿ç”¨ Hugging Face å…è´¹æ¨¡åž‹å’Œ scikit-learnã€‚å¤åˆ¶ä»£ç ç²˜è´´åˆ° Colab å³å¯è¿è¡Œã€‚"})]})]})}),e.jsxs("div",{className:"flex flex-col lg:flex-row gap-6",children:[e.jsx("aside",{className:"lg:w-72 flex-shrink-0",children:e.jsxs("div",{className:"bg-white/5 backdrop-blur-xl rounded-xl border border-white/10 p-4 sticky top-20",children:[e.jsxs("h2",{className:"font-semibold text-white mb-4 flex items-center gap-2",children:[e.jsx(k,{size:18,className:"text-purple-400"}),t==="ja"?"ã‚«ãƒ†ã‚´ãƒª":"åˆ†ç±»"]}),e.jsx("nav",{className:"space-y-1",children:g.map(([r,n])=>e.jsxs("button",{onClick:()=>{u(r),p(0)},className:`w-full flex items-center gap-3 p-3 rounded-lg text-left transition-all ${a===r?"bg-white/10 border border-white/20":"hover:bg-white/5 border border-transparent"}`,children:[e.jsx("div",{className:`p-1.5 rounded-lg bg-gradient-to-br ${n.gradient}`,children:e.jsx(n.icon,{size:14,className:"text-white"})}),e.jsxs("div",{className:"flex-1 min-w-0",children:[e.jsx("div",{className:`text-sm font-medium truncate ${a===r?"text-white":"text-slate-300"}`,children:n.name[t]}),e.jsxs("div",{className:"text-xs text-slate-500",children:[n.examples.length," ",t==="ja"?"ä¾‹":"ä¸ªç¤ºä¾‹"]})]})]},r))})]})}),e.jsxs("main",{className:"flex-1 min-w-0",children:[e.jsx("div",{className:"mb-6",children:e.jsxs("div",{className:"flex items-center gap-3 mb-2",children:[e.jsx("div",{className:`p-2 rounded-xl bg-gradient-to-br ${s.gradient}`,children:e.jsx(s.icon,{size:24,className:"text-white"})}),e.jsxs("div",{children:[e.jsx("h2",{className:"text-xl font-bold text-white",children:s.name[t]}),e.jsx("p",{className:"text-sm text-slate-400",children:s.description[t]})]})]})}),e.jsx("div",{className:"flex gap-2 mb-4 overflow-x-auto pb-2",children:s.examples.map((r,n)=>e.jsx("button",{onClick:()=>p(n),className:`px-4 py-2 rounded-lg text-sm font-medium whitespace-nowrap transition-all ${l===n?"bg-white/10 text-white border border-white/20":"text-slate-400 hover:text-white hover:bg-white/5"}`,children:r.title[t]},r.id))}),e.jsxs("div",{className:"bg-white/5 backdrop-blur-xl rounded-xl border border-white/10 overflow-hidden",children:[e.jsxs("div",{className:"flex items-center justify-between px-4 py-3 border-b border-white/10 bg-white/5",children:[e.jsxs("div",{className:"flex items-center gap-3",children:[e.jsx("span",{className:`px-2 py-0.5 rounded text-xs font-medium ${i.difficulty==="beginner"?"bg-green-500/20 text-green-400":i.difficulty==="intermediate"?"bg-yellow-500/20 text-yellow-400":"bg-red-500/20 text-red-400"}`,children:i.difficulty==="beginner"?t==="ja"?"åˆç´š":"å…¥é—¨":i.difficulty==="intermediate"?t==="ja"?"ä¸­ç´š":"ä¸­çº§":t==="ja"?"ä¸Šç´š":"é«˜çº§"}),i.colabReady&&e.jsx("span",{className:"px-2 py-0.5 rounded text-xs font-medium bg-emerald-500/20 text-emerald-400",children:"âœ… Colab Ready"})]}),e.jsxs("div",{className:"flex items-center gap-2",children:[e.jsxs("button",{onClick:h,className:"flex items-center gap-1.5 px-3 py-1.5 text-sm text-slate-300 hover:text-white bg-white/5 hover:bg-white/10 rounded-lg transition-colors",children:[d?e.jsx(R,{size:14,className:"text-green-400"}):e.jsx(L,{size:14}),d?t==="ja"?"ã‚³ãƒ”ãƒ¼æ¸ˆã¿":"å·²å¤åˆ¶":t==="ja"?"ã‚³ãƒ”ãƒ¼":"å¤åˆ¶"]}),e.jsxs("a",{href:"https://colab.research.google.com/",target:"_blank",rel:"noopener noreferrer",className:"flex items-center gap-1.5 px-3 py-1.5 text-sm text-white bg-gradient-to-r from-orange-500 to-yellow-500 hover:from-orange-600 hover:to-yellow-600 rounded-lg transition-colors",children:[e.jsx(I,{size:14}),t==="ja"?"Colabã§é–‹ã":"åœ¨ Colab ä¸­è¿è¡Œ"]})]})]}),e.jsxs("div",{className:"px-4 py-3 border-b border-white/10 bg-slate-800/50",children:[e.jsx("p",{className:"text-sm text-slate-300",children:i.description[t]}),e.jsx("div",{className:"flex flex-wrap gap-2 mt-2",children:i.tags.map(r=>e.jsx("span",{className:"px-2 py-0.5 text-xs bg-white/10 text-slate-400 rounded",children:r},r))})]}),e.jsx("div",{className:"relative",children:e.jsx("pre",{className:"p-4 overflow-x-auto text-sm leading-relaxed max-h-[600px] overflow-y-auto",children:e.jsx("code",{className:"text-slate-300 font-mono whitespace-pre",children:i.code})})})]}),e.jsxs("div",{className:"mt-6 p-4 bg-blue-500/10 border border-blue-500/20 rounded-xl",children:[e.jsxs("h3",{className:"font-semibold text-blue-400 mb-2 flex items-center gap-2",children:[e.jsx(T,{size:16}),t==="ja"?"å®Ÿè¡Œæ–¹æ³•":"è¿è¡Œæ–¹æ³•"]}),e.jsxs("ol",{className:"text-sm text-slate-400 space-y-1 list-decimal list-inside",children:[e.jsx("li",{children:t==="ja"?"ã‚³ãƒ¼ãƒ‰ã‚’ã‚³ãƒ”ãƒ¼":"å¤åˆ¶ä»£ç "}),e.jsxs("li",{children:[t==="ja"?"Google Colab ã‚’é–‹ã":"æ‰“å¼€ Google Colab"," (colab.research.google.com)"]}),e.jsx("li",{children:t==="ja"?"æ–°ã—ã„ãƒŽãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’ä½œæˆ":"æ–°å»ºç¬”è®°æœ¬"}),e.jsx("li",{children:t==="ja"?"ã‚³ãƒ¼ãƒ‰ã‚’è²¼ã‚Šä»˜ã‘ã¦å®Ÿè¡Œ":"ç²˜è´´ä»£ç å¹¶è¿è¡Œ"})]})]})]})]})]})]})};export{D as default};

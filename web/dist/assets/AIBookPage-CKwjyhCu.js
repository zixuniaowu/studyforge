import{k as AP,j as w,r as Me,l as TP,g as IL,e as kL,u as EL,B as lw,C as cw,m as uw,A as SL}from"./index-B2xJFW01.js";import{X as LL}from"./x-lCvNktd3.js";import{M as DL}from"./menu-BphEm9_l.js";import{H as $L}from"./house-Bce2Judw.js";import{A as FL}from"./arrow-right-DZ3YYxQa.js";const PP={id:"ai-beginner",title:{zh:"AI 入门指南",ja:"AI 入門ガイド"},subtitle:{zh:"写给每一个对 AI 好奇的人",ja:"AIに興味を持つすべての人へ"},author:"StudyForge",chapters:[{id:"chapter-1",number:1,title:{zh:"什么是人工智能？",ja:"人工知能とは？"},subtitle:{zh:"从零开始理解 AI",ja:"ゼロからAIを理解する"},sections:[{id:"ch1-intro",title:{zh:"引言：AI 已经在你身边",ja:"序章：AIはすでにあなたのそばに"},content:{zh:`
你可能觉得人工智能（AI）是一个遥远的、科幻电影里才有的东西。但实际上，**AI 已经悄悄融入了你的日常生活**。

当你打开手机，刷抖音或小红书时，为什么推荐的内容总是你感兴趣的？当你用美颜相机自拍时，为什么它能自动识别你的脸并美化？当你对着手机说"小爱同学"或"Siri"时，为什么它能听懂你说的话？

**这一切的背后，都是 AI 在工作。**

---

## 本书学习路线图

在开始之前，让我们先看看这本书的整体结构：

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                      AI 入门完整学习路线                                  │
└─────────────────────────────────────────────────────────────────────────┘

 ╔═══════════════════╗      ╔═══════════════════╗      ╔═══════════════════╗
 ║   第 1 章          ║      ║   第 2 章          ║      ║   第 3 章          ║
 ║   理解 AI         ║ ───▶ ║   使用 AI         ║ ───▶ ║   精通 AI         ║
 ║   (What)          ║      ║   (How)           ║      ║   (Better)        ║
 ╚═══════════════════╝      ╚═══════════════════╝      ╚═══════════════════╝
         │                          │                          │
         ▼                          ▼                          ▼
 ┌───────────────────┐      ┌───────────────────┐      ┌───────────────────┐
 │ • AI 的本质        │      │ • ChatGPT 使用    │      │ • 稳定输出技巧     │
 │ • ML/DL 概念       │      │ • 提示词工程       │      │ • 复杂任务拆解     │
 │ • 神经网络原理     │      │ • AI 绘图         │      │ • 避坑指南         │
 │ • 能力与局限       │      │ • 工具选择        │      │ • 实战案例         │
 │ • 发展历史         │      │                   │      │                   │
 └───────────────────┘      └───────────────────┘      └───────────────────┘

    📖 建立认知              🛠️ 动手实践              🚀 进阶技巧
    "AI 是什么"             "怎么用 AI"              "如何用得更好"
\`\`\`

**学习建议：**
- 第 1 章是基础，请不要跳过
- 第 2 章可以边学边做
- 第 3 章适合有一定使用经验后阅读

---

## 本章你将学到

在这一章中，我们会一起探索：

1. **AI 的本质是什么** —— 用最简单的话解释
2. **AI、机器学习、深度学习的关系** —— 理清这些概念
3. **神经网络如何工作** —— 直观理解 AI 的"思考"过程
4. **AI 能做什么，不能做什么** —— 建立正确的认知
5. **AI 的简短历史** —— 了解它是怎么发展到今天的

不用担心，我们不会涉及任何数学公式或编程代码。这一章的目标是让你**建立对 AI 的直觉性理解**。

准备好了吗？让我们开始吧！
            `,ja:`
人工知能（AI）は、SF映画の中だけの遠い存在だと思っていませんか？実は、**AIはすでに私たちの日常生活に溶け込んでいます**。

スマートフォンでTikTokやInstagramを開くと、なぜいつも興味のあるコンテンツがおすすめされるのでしょうか？自撮りアプリを使うと、なぜ自動的に顔を認識して美しくしてくれるのでしょうか？「Hey Siri」や「OK Google」と話しかけると、なぜ言葉を理解してくれるのでしょうか？

**これらすべての背後で、AIが働いています。**

---

## 本書の学習ロードマップ

始める前に、本書全体の構造を見てみましょう：

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                      AI入門 完全学習ロードマップ                          │
└─────────────────────────────────────────────────────────────────────────┘

 ╔═══════════════════╗      ╔═══════════════════╗      ╔═══════════════════╗
 ║   第 1 章          ║      ║   第 2 章          ║      ║   第 3 章          ║
 ║   AIを理解する     ║ ───▶ ║   AIを使う        ║ ───▶ ║   AIを極める      ║
 ║   (What)          ║      ║   (How)           ║      ║   (Better)        ║
 ╚═══════════════════╝      ╚═══════════════════╝      ╚═══════════════════╝
         │                          │                          │
         ▼                          ▼                          ▼
 ┌───────────────────┐      ┌───────────────────┐      ┌───────────────────┐
 │ • AIの本質        │      │ • ChatGPT使用法   │      │ • 安定出力テクニック│
 │ • ML/DL概念       │      │ • プロンプト工学   │      │ • 複雑タスク分解   │
 │ • ニューラルネット │      │ • AI画像生成      │      │ • 落とし穴回避     │
 │ • 能力と限界       │      │ • ツール選択      │      │ • 実践事例         │
 │ • 発展の歴史       │      │                   │      │                   │
 └───────────────────┘      └───────────────────┘      └───────────────────┘

    📖 認識を築く            🛠️ 実践練習               🚀 上級テクニック
    "AIとは何か"            "どう使うか"              "より上手に使う"
\`\`\`

**学習のアドバイス：**
- 第1章は基礎なので、スキップしないでください
- 第2章は学びながら実践できます
- 第3章はある程度使用経験を積んでから読むと効果的です

---

## この章で学ぶこと

この章では、一緒に以下のことを探っていきます：

1. **AIの本質とは** —— 最もシンプルな言葉で説明
2. **AI、機械学習、深層学習の関係** —— これらの概念を整理
3. **ニューラルネットワークの仕組み** —— AIの「思考」プロセスを直感的に理解
4. **AIにできること、できないこと** —— 正しい認識を持つ
5. **AIの簡単な歴史** —— どのように今日まで発展してきたか

ご安心ください。数式やプログラミングコードは出てきません。この章の目標は、**AIについての直感的な理解を身につける**ことです。

準備はいいですか？始めましょう！
            `}},{id:"ch1-what-is-ai",title:{zh:'1.1 AI 的本质：让机器"学会"做事',ja:"1.1 AIの本質：機械が「学ぶ」こと"},content:{zh:`
## 传统编程 vs 人工智能

要理解什么是 AI，我们先来看看**传统编程**是怎么工作的。

### 传统编程的方式

假设你想让计算机识别一张图片是不是猫。用传统编程的方式，你需要告诉计算机：

> "如果图片里有两只尖尖的耳朵，有胡须，有毛茸茸的身体，有四条腿，有一条尾巴……那它就是猫。"

但问题来了：
- 如果猫的耳朵被遮住了呢？
- 如果是一只没有尾巴的曼岛猫呢？
- 如果猫蜷成一团看不到腿呢？

你会发现，**规则越写越多，越写越复杂**，而且永远写不完。

\`\`\`
┌─────────────┐     ┌─────────────┐     ┌─────────────┐
│   输入数据   │ ──▶ │  人写的规则  │ ──▶ │   输出结果   │
│  （图片）    │     │ （如果...   │     │ （是/不是猫）│
│             │     │   那么...） │     │             │
└─────────────┘     └─────────────┘     └─────────────┘
                           ↑
                     程序员手动编写
                     （越来越复杂）
\`\`\`

### AI 的方式

AI 采用了完全不同的思路。它不是让程序员写规则，而是**让机器自己从数据中学习规则**。

具体怎么做呢？

1. **准备大量数据**：收集成千上万张猫的照片，和成千上万张不是猫的照片
2. **告诉机器答案**：给每张图片标注"这是猫"或"这不是猫"
3. **让机器自己找规律**：机器会自动分析这些图片，找出"猫"的共同特征
4. **机器学会了**：之后给它一张新图片，它就能判断是不是猫

\`\`\`
┌─────────────┐     ┌─────────────┐     ┌─────────────┐
│   输入数据   │ ──▶ │  AI 模型    │ ──▶ │   输出结果   │
│  （图片）    │     │（自动学习   │     │ （是/不是猫）│
│             │     │  的规律）   │     │             │
└─────────────┘     └─────────────┘     └─────────────┘
                           ↑
       ┌───────────────────┴───────────────────┐
       │          大量带标签的数据              │
       │   🐱猫  🐱猫  🐶狗  🐱猫  🐶狗  ...   │
       └───────────────────────────────────────┘
\`\`\`

---

## 一个形象的比喻

想象你在教一个小孩认识猫：

**传统编程的方式**就像是给小孩一本厚厚的说明书：
> "猫有四条腿、有胡须、会喵喵叫、喜欢吃鱼……"

**AI 的方式**就像是带小孩去动物园，指着各种动物说：
> "这是猫，这也是猫，这不是猫，这是狗……"

看多了之后，小孩自然就知道什么是猫了——**即使他说不清楚猫到底有什么特征**。

---

## 关键理解

所以，**AI 的本质是什么？**

> **AI 是让机器从数据中自动学习规律，而不是人工编写规则。**

这就是为什么 AI 又被称为"机器学习"——因为核心就是让机器去"学"。
            `,ja:`
## 従来のプログラミング vs 人工知能

AIとは何かを理解するために、まず**従来のプログラミング**がどのように動作するかを見てみましょう。

### 従来のプログラミングの方法

例えば、画像が猫かどうかをコンピュータに判断させたいとします。従来のプログラミングでは、次のようにコンピュータに指示する必要があります：

> 「画像に尖った耳が2つあり、ヒゲがあり、毛むくじゃらの体があり、4本の足があり、しっぽがあれば...それは猫です。」

しかし、問題が出てきます：
- 猫の耳が隠れていたら？
- しっぽのないマンクス猫だったら？
- 猫が丸くなって足が見えなかったら？

**ルールは増える一方で、複雑になり、永遠に書き終わりません**。

\`\`\`
┌─────────────┐     ┌─────────────┐     ┌─────────────┐
│   入力データ  │ ──▶ │ 人が書いた   │ ──▶ │   出力結果   │
│   （画像）   │     │  ルール     │     │（猫か否か） │
│             │     │（もし...    │     │             │
│             │     │ ならば...） │     │             │
└─────────────┘     └─────────────┘     └─────────────┘
                           ↑
                    プログラマーが手動で記述
                     （どんどん複雑に）
\`\`\`

### AIの方法

AIは全く異なるアプローチを取ります。プログラマーがルールを書くのではなく、**機械自身がデータからルールを学習します**。

具体的にはどうするのでしょうか？

1. **大量のデータを準備**：何万枚もの猫の写真と、猫でない写真を集める
2. **機械に答えを教える**：各画像に「これは猫」「これは猫ではない」とラベルを付ける
3. **機械自身にパターンを見つけさせる**：機械がこれらの画像を自動的に分析し、「猫」の共通の特徴を見つける
4. **機械が学習完了**：新しい画像を見せれば、猫かどうか判断できるようになる

\`\`\`
┌─────────────┐     ┌─────────────┐     ┌─────────────┐
│   入力データ  │ ──▶ │  AIモデル   │ ──▶ │   出力結果   │
│   （画像）   │     │（自動学習   │     │（猫か否か） │
│             │     │ したパターン）│     │             │
└─────────────┘     └─────────────┘     └─────────────┘
                           ↑
       ┌───────────────────┴───────────────────┐
       │        大量のラベル付きデータ           │
       │   🐱猫  🐱猫  🐶犬  🐱猫  🐶犬  ...   │
       └───────────────────────────────────────┘
\`\`\`

---

## わかりやすいたとえ

子供に猫を教えることを想像してみてください：

**従来のプログラミング**は、子供に分厚いマニュアルを渡すようなもの：
> 「猫は4本足で、ヒゲがあって、ニャーと鳴いて、魚が好きで...」

**AI**は、子供を動物園に連れて行って、様々な動物を指さしながら：
> 「これは猫、これも猫、これは猫じゃない、これは犬...」

たくさん見ているうちに、子供は自然と猫が何かわかるようになります——**猫の特徴を言葉で説明できなくても**。

---

## 重要な理解

では、**AIの本質とは何でしょうか？**

> **AIは、人が手動でルールを書くのではなく、機械がデータから自動的にパターンを学習すること。**

これが、AIが「機械学習」とも呼ばれる理由です——核心は機械に「学ばせる」ことだからです。
            `}},{id:"ch1-ai-ml-dl",title:{zh:"1.2 AI、机器学习、深度学习的关系",ja:"1.2 AI、機械学習、深層学習の関係"},content:{zh:`
你可能听说过很多相关的词：人工智能、机器学习、深度学习、神经网络、大模型……它们是什么关系？

## 一个套娃的关系

其实它们的关系很简单，就像**俄罗斯套娃**一样，一个包含另一个：

\`\`\`
┌─────────────────────────────────────────┐
│           人工智能 (AI)                   │
│  ┌───────────────────────────────────┐  │
│  │       机器学习 (ML)                 │  │
│  │  ┌─────────────────────────────┐  │  │
│  │  │     深度学习 (DL)             │  │  │
│  │  │  ┌───────────────────────┐  │  │  │
│  │  │  │    大语言模型 (LLM)     │  │  │  │
│  │  │  │    如 ChatGPT          │  │  │  │
│  │  │  └───────────────────────┘  │  │  │
│  │  └─────────────────────────────┘  │  │
│  └───────────────────────────────────┘  │
└─────────────────────────────────────────┘
\`\`\`

让我们逐个理解：

---

## 人工智能（AI）—— 最大的圈

**人工智能**是最广泛的概念，指的是**任何能模拟人类智能的计算机系统**。

这包括很多不同的技术，比如：
- 专家系统（用规则模拟专家决策）
- 计算机视觉（让机器"看"）
- 自然语言处理（让机器理解人类语言）
- 机器人技术
- ……

**机器学习只是实现 AI 的方法之一**，但现在是最主流、最有效的方法。

---

## 机器学习（ML）—— 中间的圈

**机器学习**是 AI 的一个子领域，它的核心思想我们上一节已经讲过：

> 不是人工编写规则，而是让机器从数据中自动学习。

机器学习有很多不同的算法，比如：
- 线性回归
- 决策树
- 支持向量机
- 神经网络
- ……

其中，**神经网络**发展出了一个更强大的分支，叫做**深度学习**。

---

## 深度学习（DL）—— 更小的圈

**深度学习**是机器学习的一种，它使用**多层的神经网络**来学习。

为什么叫"深度"？因为神经网络有很多层，像一个很深的结构：

- 浅层网络：2-3 层
- 深度网络：几十层甚至上百层
- GPT-4：据说有 **120 层以上**！

层数越多，能学习的模式就越复杂。这就是为什么深度学习能做到以前做不到的事情，比如：
- 精准的人脸识别
- 流畅的语音识别
- 像人一样的对话

---

## 大语言模型（LLM）—— 最核心的圈

**大语言模型**是深度学习在自然语言处理领域的巅峰之作。

它的特点是：
- **大**：参数量巨大（GPT-3 有 1750 亿参数，GPT-4 据说有 1.8 万亿）
- **语言**：专门处理文字
- **模型**：经过训练的神经网络

ChatGPT、Claude、文心一言都是大语言模型。它们能写文章、翻译、写代码、回答问题……几乎无所不能。

---

## 机器学习的三大分类

除了"套娃"关系，机器学习还可以按**学习方式**分类：

\`\`\`
┌──────────────────────────────────────────────────────────────────┐
│                    机器学习的三种学习方式                          │
└──────────────────────────────────────────────────────────────────┘

┌────────────────────┐  ┌────────────────────┐  ┌────────────────────┐
│   监督学习          │  │   无监督学习         │  │   强化学习          │
│ (Supervised)       │  │ (Unsupervised)     │  │ (Reinforcement)    │
├────────────────────┤  ├────────────────────┤  ├────────────────────┤
│                    │  │                    │  │                    │
│   📚 有老师        │  │   🔍 自己探索       │  │   🎮 试错学习       │
│                    │  │                    │  │                    │
│ 数据：有标签       │  │ 数据：无标签        │  │ 数据：奖励信号      │
│ (这是猫/这是狗)    │  │ (一堆图片)         │  │ (做对+1/做错-1)    │
│                    │  │                    │  │                    │
│ 例子：             │  │ 例子：             │  │ 例子：             │
│ • 垃圾邮件识别     │  │ • 用户分群         │  │ • 下棋AI           │
│ • 房价预测         │  │ • 异常检测         │  │ • 游戏AI           │
│ • 图像分类         │  │ • 主题发现         │  │ • 机器人控制        │
│                    │  │                    │  │                    │
└────────────────────┘  └────────────────────┘  └────────────────────┘
        │                       │                       │
        ▼                       ▼                       ▼
┌────────────────────┐  ┌────────────────────┐  ┌────────────────────┐
│ "老师告诉你答案，   │  │ "没有标准答案，     │  │ "通过尝试和反馈，   │
│  你学着做"         │  │  自己发现规律"      │  │  学习最佳策略"      │
└────────────────────┘  └────────────────────┘  └────────────────────┘
\`\`\`

**生活中的类比：**
- **监督学习** → 跟着老师学数学，老师给你题目和答案
- **无监督学习** → 自己整理房间，发现物品的分类规律
- **强化学习** → 学打游戏，通过得分知道什么操作是好的

---

## 完整的 AI 知识地图

\`\`\`
                          ┌─────────────┐
                          │  人工智能 AI │
                          │  (最大范畴)  │
                          └──────┬──────┘
                                 │
          ┌──────────────────────┼──────────────────────┐
          │                      │                      │
          ▼                      ▼                      ▼
   ┌──────────────┐      ┌──────────────┐      ┌──────────────┐
   │  传统 AI      │      │   机器学习   │      │   专家系统   │
   │ (规则系统)    │      │    (ML)     │      │ (知识库)     │
   └──────────────┘      └──────┬──────┘      └──────────────┘
                                │
          ┌─────────────────────┼─────────────────────┐
          │                     │                     │
          ▼                     ▼                     ▼
   ┌──────────────┐      ┌──────────────┐      ┌──────────────┐
   │  传统 ML      │      │   深度学习   │      │   其他 ML    │
   │ (决策树等)    │      │    (DL)     │      │ (SVM 等)     │
   └──────────────┘      └──────┬──────┘      └──────────────┘
                                │
          ┌─────────────────────┼─────────────────────┐
          │                     │                     │
          ▼                     ▼                     ▼
   ┌──────────────┐      ┌──────────────┐      ┌──────────────┐
   │     CNN      │      │     LLM     │      │     RNN      │
   │  (图像处理)   │      │  (大语言模型) │      │  (序列处理)   │
   │ 人脸识别、    │      │ ChatGPT     │      │ 语音识别、    │
   │ 图像分类      │      │ Claude 等   │      │ 机器翻译      │
   └──────────────┘      └──────────────┘      └──────────────┘
\`\`\`

---

## 小结

| 概念 | 简单理解 | 举例 |
|------|----------|------|
| 人工智能 | 让机器像人一样智能 | 所有 AI 技术 |
| 机器学习 | 让机器从数据中学习 | 推荐系统、垃圾邮件过滤 |
| 深度学习 | 用很深的神经网络学习 | 人脸识别、语音识别 |
| 大语言模型 | 超大的语言处理网络 | ChatGPT、Claude |

| 学习方式 | 特点 | 应用 |
|---------|------|-----|
| 监督学习 | 有标签数据 | 分类、预测 |
| 无监督学习 | 无标签数据 | 聚类、降维 |
| 强化学习 | 奖励反馈 | 游戏、控制 |

现在，当你再听到这些词时，就不会迷糊了！
            `,ja:`
人工知能、機械学習、深層学習、ニューラルネットワーク、大規模言語モデル...これらの言葉を聞いたことがあるかもしれません。それぞれの関係は？

## マトリョーシカの関係

実は、これらの関係はとてもシンプルで、**マトリョーシカ人形**のように、一つが別の一つを含んでいます：

\`\`\`
┌─────────────────────────────────────────┐
│           人工知能 (AI)                   │
│  ┌───────────────────────────────────┐  │
│  │       機械学習 (ML)                 │  │
│  │  ┌─────────────────────────────┐  │  │
│  │  │     深層学習 (DL)             │  │  │
│  │  │  ┌───────────────────────┐  │  │  │
│  │  │  │    大規模言語モデル     │  │  │  │
│  │  │  │    例: ChatGPT        │  │  │  │
│  │  │  └───────────────────────┘  │  │  │
│  │  └─────────────────────────────┘  │  │
│  └───────────────────────────────────┘  │
└─────────────────────────────────────────┘
\`\`\`

一つずつ理解していきましょう：

---

## 人工知能（AI）—— 最も大きな円

**人工知能**は最も広い概念で、**人間の知能を模倣できるあらゆるコンピュータシステム**を指します。

これには多くの異なる技術が含まれます：
- エキスパートシステム（ルールで専門家の判断を模倣）
- コンピュータビジョン（機械に「見る」能力を）
- 自然言語処理（機械が人間の言葉を理解）
- ロボット技術
- ……

**機械学習はAIを実現する方法の一つ**ですが、現在最も主流で効果的な方法です。

---

## 機械学習（ML）—— 中間の円

**機械学習**はAIの一分野で、その核心的な考え方は前のセクションで説明しました：

> 人がルールを手書きするのではなく、機械がデータから自動的に学習する。

機械学習には多くの異なるアルゴリズムがあります：
- 線形回帰
- 決定木
- サポートベクターマシン
- ニューラルネットワーク
- ……

その中で、**ニューラルネットワーク**は**深層学習**というより強力な分野に発展しました。

---

## 深層学習（DL）—— より小さな円

**深層学習**は機械学習の一種で、**多層のニューラルネットワーク**を使って学習します。

なぜ「深層」と呼ぶのか？ニューラルネットワークには多くの層があり、とても深い構造だからです：

- 浅いネットワーク：2-3層
- 深いネットワーク：数十層から数百層
- GPT-4：**120層以上**と言われています！

層が多いほど、より複雑なパターンを学習できます。これが、深層学習が以前はできなかったことを可能にした理由です：
- 正確な顔認識
- スムーズな音声認識
- 人間のような会話

---

## 大規模言語モデル（LLM）—— 最も核心の円

**大規模言語モデル**は、自然言語処理分野における深層学習の頂点です。

その特徴は：
- **大規模**：パラメータ数が膨大（GPT-3は1750億、GPT-4は1.8兆と言われる）
- **言語**：テキスト処理専門
- **モデル**：訓練されたニューラルネットワーク

ChatGPT、Claude、Geminiはすべて大規模言語モデルです。文章を書いたり、翻訳したり、コードを書いたり、質問に答えたり...ほぼ何でもできます。

---

## 機械学習の3つの分類

「マトリョーシカ」の関係に加えて、機械学習は**学習方法**で分類することもできます：

\`\`\`
┌──────────────────────────────────────────────────────────────────┐
│                   機械学習の3つの学習方法                         │
└──────────────────────────────────────────────────────────────────┘

┌────────────────────┐  ┌────────────────────┐  ┌────────────────────┐
│   教師あり学習       │  │   教師なし学習       │  │   強化学習          │
│ (Supervised)       │  │ (Unsupervised)     │  │ (Reinforcement)    │
├────────────────────┤  ├────────────────────┤  ├────────────────────┤
│                    │  │                    │  │                    │
│   📚 先生がいる     │  │   🔍 自己探索       │  │   🎮 試行錯誤       │
│                    │  │                    │  │                    │
│ データ：ラベル付き  │  │ データ：ラベルなし  │  │ データ：報酬信号    │
│ (これは猫/犬)      │  │ (画像の山)         │  │ (正解+1/不正解-1)  │
│                    │  │                    │  │                    │
│ 例：               │  │ 例：               │  │ 例：               │
│ • スパムメール検出  │  │ • ユーザー分類     │  │ • 囲碁AI           │
│ • 住宅価格予測     │  │ • 異常検知         │  │ • ゲームAI         │
│ • 画像分類         │  │ • トピック発見     │  │ • ロボット制御      │
│                    │  │                    │  │                    │
└────────────────────┘  └────────────────────┘  └────────────────────┘
        │                       │                       │
        ▼                       ▼                       ▼
┌────────────────────┐  ┌────────────────────┐  ┌────────────────────┐
│ 「先生が答えを教え、│  │ 「正解はなく、      │  │ 「試行とフィードバ  │
│  それを学ぶ」      │  │  自分でパターン発見」│  │  ックで最適戦略学習」│
└────────────────────┘  └────────────────────┘  └────────────────────┘
\`\`\`

**日常生活での例え：**
- **教師あり学習** → 先生と数学を学ぶ、問題と答えをもらう
- **教師なし学習** → 部屋を片付けながら物の分類規則を発見
- **強化学習** → ゲームを覚える、スコアで良い操作を学ぶ

---

## 完全なAI知識マップ

\`\`\`
                          ┌─────────────┐
                          │  人工知能 AI │
                          │  (最大範囲)  │
                          └──────┬──────┘
                                 │
          ┌──────────────────────┼──────────────────────┐
          │                      │                      │
          ▼                      ▼                      ▼
   ┌──────────────┐      ┌──────────────┐      ┌──────────────┐
   │  従来型 AI    │      │   機械学習   │      │ エキスパート  │
   │ (ルールベース) │      │    (ML)     │      │  システム    │
   └──────────────┘      └──────┬──────┘      └──────────────┘
                                │
          ┌─────────────────────┼─────────────────────┐
          │                     │                     │
          ▼                     ▼                     ▼
   ┌──────────────┐      ┌──────────────┐      ┌──────────────┐
   │  従来の ML    │      │   深層学習   │      │   その他 ML   │
   │ (決定木など)   │      │    (DL)     │      │ (SVMなど)     │
   └──────────────┘      └──────┬──────┘      └──────────────┘
                                │
          ┌─────────────────────┼─────────────────────┐
          │                     │                     │
          ▼                     ▼                     ▼
   ┌──────────────┐      ┌──────────────┐      ┌──────────────┐
   │     CNN      │      │     LLM     │      │     RNN      │
   │  (画像処理)   │      │  (大規模言語) │      │  (系列処理)   │
   │ 顔認識、      │      │ ChatGPT     │      │ 音声認識、    │
   │ 画像分類      │      │ Claude等    │      │ 機械翻訳      │
   └──────────────┘      └──────────────┘      └──────────────┘
\`\`\`

---

## まとめ

| 概念 | シンプルな理解 | 例 |
|------|----------|------|
| 人工知能 | 機械を人のように賢く | すべてのAI技術 |
| 機械学習 | 機械がデータから学ぶ | レコメンド、スパムフィルター |
| 深層学習 | 深いニューラルネットで学習 | 顔認識、音声認識 |
| 大規模言語モデル | 超大規模な言語処理ネットワーク | ChatGPT、Claude |

| 学習方法 | 特徴 | 応用 |
|---------|------|-----|
| 教師あり学習 | ラベル付きデータ | 分類、予測 |
| 教師なし学習 | ラベルなしデータ | クラスタリング、次元削減 |
| 強化学習 | 報酬フィードバック | ゲーム、制御 |

これで、これらの言葉を聞いても混乱しなくなりました！
            `}},{id:"ch1-neural-network-visual",title:{zh:'1.3 图解神经网络：AI如何"思考"',ja:"1.3 図解ニューラルネットワーク：AIはどう「考える」か"},content:{zh:`
上一节我们知道了深度学习使用"神经网络"，但神经网络到底是什么？让我们用图来理解。

---

## 神经网络的灵感来源：人脑

神经网络的设计灵感来自**人脑的神经元**：

\`\`\`
【人脑神经元】                    【人工神经元】

    树突（接收信号）                  输入 x₁ ──┐
       \\  |  /                      输入 x₂ ──┼──▶ [计算] ──▶ 输出
        \\ | /                       输入 x₃ ──┘
         \\|/                              ↑
    ┌─────●─────┐                    权重 w₁,w₂,w₃
    │  细胞体   │                    （重要程度）
    │ （处理）  │
    └─────┬─────┘
          │
       轴突（输出信号）
\`\`\`

**关键类比：**
- 树突 → 输入层（接收数据）
- 细胞体 → 计算（加权求和 + 激活）
- 轴突 → 输出（传递给下一层）

---

## 一个神经元的工作原理

\`\`\`
                    ┌─────────────────────────────────────┐
                    │           一个神经元                 │
                    │                                     │
输入              权重            加权求和         激活函数         输出
────              ────            ────────         ────────         ────

x₁ = 0.5 ──▶ × w₁ = 0.3 ─┐
                          │
x₂ = 0.8 ──▶ × w₂ = 0.5 ─┼──▶ Σ = 0.15+0.40+0.12 ──▶ f(0.67) ──▶ 0.66
                          │       = 0.67
x₃ = 0.6 ──▶ × w₃ = 0.2 ─┘

                    └─────────────────────────────────────┘

计算过程：
1. 每个输入 × 对应权重
2. 加起来得到总和
3. 通过激活函数转换
4. 输出结果
\`\`\`

**用生活来理解权重：**

想象你在决定今天穿什么，考虑三个因素：
- 天气（很重要） → 权重 0.5
- 心情（一般重要） → 权重 0.3
- 今天要见的人（稍微重要） → 权重 0.2

权重越大，这个因素对最终决定的影响就越大。

---

## 从单个神经元到神经网络

把很多神经元连接起来，就形成了**神经网络**：

\`\`\`
        输入层          隐藏层1         隐藏层2          输出层
       (Input)        (Hidden1)       (Hidden2)       (Output)

         ○─────────────○─────────────○
        ╱│╲           ╱│╲           ╱│╲
       ╱ │ ╲         ╱ │ ╲         ╱ │ ╲
  x₁ ○───┼───○─────○───┼───○─────○───┼───○─────○ → 猫的概率: 0.92
       ╲ │ ╱         ╲ │ ╱         ╲ │ ╱
        ╲│╱           ╲│╱           ╲│╱
  x₂ ○───┼───○─────○───┼───○─────○───┼───○─────○ → 狗的概率: 0.08
       ╲ │ ╱         ╲ │ ╱         ╲ │ ╱
        ╲│╱           ╲│╱           ╲│╱
  x₃ ○─────────────○─────────────○

       像素值          检测边缘        检测形状         最终判断
                      和颜色          和纹理
\`\`\`

**每一层学到什么？**

| 层级 | 学习的内容 | 比喻 |
|------|-----------|------|
| 输入层 | 接收原始数据（如像素） | 眼睛看到图片 |
| 隐藏层1 | 检测简单特征（边缘、颜色） | 注意到"有尖尖的东西" |
| 隐藏层2 | 组合成复杂特征（眼睛、耳朵） | 认出"这是耳朵的形状" |
| 输出层 | 做出最终判断 | 判断"这是一只猫" |

---

## AI 是怎么"学习"的？

神经网络的学习过程可以用一个简单的循环来理解：

\`\`\`
┌─────────────────────────────────────────────────────────────┐
│                     AI 的学习循环                            │
└─────────────────────────────────────────────────────────────┘

     ┌──────────┐
     │ 1. 输入  │   输入一张猫的图片
     │   数据   │
     └────┬─────┘
          │
          ▼
     ┌──────────┐
     │ 2. 预测  │   AI 猜测：这是狗（错了！）
     │   结果   │
     └────┬─────┘
          │
          ▼
     ┌──────────┐
     │ 3. 对比  │   正确答案是猫，AI 错了
     │   答案   │   计算"错多少" → 损失值
     └────┬─────┘
          │
          ▼
     ┌──────────┐
     │ 4. 调整  │   调整权重，让下次更准
     │   权重   │   （反向传播算法）
     └────┬─────┘
          │
          ▼
     ┌──────────┐
     │ 5. 重复  │   用更多数据重复这个过程
     │ 千万次   │   权重逐渐变得"正确"
     └──────────┘
\`\`\`

**形象比喻：学骑自行车**

1. **尝试**：你第一次骑，摇摇晃晃
2. **摔倒**：失去平衡，摔了（这是"错误"）
3. **反思**：意识到身体太偏了（计算"损失"）
4. **调整**：下次注意保持平衡（调整"权重"）
5. **重复**：练习很多次后，就会骑了

AI 学习的过程完全一样——**不断尝试、犯错、调整，直到足够准确**。

---

## 深度学习为什么"深"

\`\`\`
浅层网络（2层）              深层网络（多层）

输入 → ○ → 输出              输入 → ○ → ○ → ○ → ○ → ○ → 输出
         ↓                              ↓
    只能学简单规律                 可以学复杂规律


【识别手写数字 "3" 的例子】

浅层网络能学到：            深层网络能学到：
┌─────────────┐            ┌─────────────┐
│ 有曲线      │            │ 第1层：边缘  │  ╱  ─  ╲
│ 有两个弯    │            │ 第2层：曲线  │  ⌒  ∪  ⌓
│ ...大概是3？│            │ 第3层：笔画  │  ﹀ ∽
└─────────────┘            │ 第4层：组合  │  →  ③
     ↓                     │ 第5层：判断  │  这是 3！
  准确率 85%               └─────────────┘
                                ↓
                           准确率 99%
\`\`\`

**层数越多：**
- ✅ 能学习更复杂的模式
- ✅ 准确率更高
- ❌ 需要更多数据
- ❌ 需要更多计算资源
- ❌ 训练时间更长

---

## 一张图总结 AI 的工作流程

\`\`\`
┌──────────────────────────────────────────────────────────────────┐
│                    AI 从训练到使用的完整流程                       │
└──────────────────────────────────────────────────────────────────┘

【训练阶段】（需要大量数据和计算资源）

    大量数据                神经网络                  不断调整
  ┌─────────┐            ┌─────────┐             ┌─────────┐
  │ 🐱🐶🐱🐶│  ─────▶    │ ○→○→○→○ │  ─────▶     │ 对比答案 │
  │ 🐱🐶🐱🐶│    喂给     │ ○→○→○→○ │   预测      │ 调整权重 │
  │ (带标签) │            │ ○→○→○→○ │             │ 重复百万次│
  └─────────┘            └─────────┘             └─────────┘
                                                       │
                              ┌─────────────────────────┘
                              ▼
【使用阶段】（训练好的模型，快速推理）

    新的输入               训练好的模型              输出结果
  ┌─────────┐            ┌─────────┐             ┌─────────┐
  │   🐱?   │  ─────▶    │ ○→○→○→○ │  ─────▶     │  这是猫  │
  │ (新图片) │            │ (权重已固定)│           │ 置信度95%│
  └─────────┘            └─────────┘             └─────────┘

                    训练一次，使用无限次！
\`\`\`

---

## 图解：AI 如何找到最优权重

AI 训练的核心是**找到最好的权重**。这个过程叫做**梯度下降**，可以形象地理解为"下山找最低点"：

\`\`\`
┌──────────────────────────────────────────────────────────────────┐
│                    梯度下降：像下山一样找最优解                      │
└──────────────────────────────────────────────────────────────────┘

        损失值（错误程度）
           ▲
       高  │    ●  ← 初始位置（随机权重，错误很多）
           │   ╱
           │  ╱
           │ ● ← 第1次调整（错误减少了）
           │  ╲
           │   ● ← 第2次调整
           │    ╲
           │     ●
           │      ╲
       低  │       ★ ← 最优位置（错误最少）
           └────────────────────────────▶
                     权重值

步骤：
1. 从随机位置（权重）开始
2. 计算当前位置的"坡度"（梯度）
3. 往"下坡"方向走一小步
4. 重复直到到达"谷底"
\`\`\`

---

## 图解：训练过程中的损失变化

随着训练进行，AI 的错误（损失）会逐渐减少：

\`\`\`
损失值
  ▲
  │ ████
  │ ████ ███
  │ ████ ███ ██
  │ ████ ███ ███ ██
  │ ████ ███ ███ ███ ██
  │ ████ ███ ███ ███ ███ ██
  │ ████ ███ ███ ███ ███ ███ ██ █ █ █ █
  └──────────────────────────────────────▶ 训练轮数
     1    2    3    4    5    ...  100

┌──────────────────────────────────────┐
│           理想的训练曲线              │
│                                      │
│   开始时：损失高（错很多）            │
│      ↓                               │
│   中期：损失快速下降（学得最快）      │
│      ↓                               │
│   后期：损失趋于平稳（学得差不多了）  │
└──────────────────────────────────────┘
\`\`\`

---

## 常见问题：欠拟合与过拟合

训练 AI 时可能遇到两种问题：

\`\`\`
┌────────────────────────────────────────────────────────────────────┐
│                    三种情况对比                                      │
└────────────────────────────────────────────────────────────────────┘

  欠拟合                    刚刚好                    过拟合
 (Underfitting)          (Just Right)            (Overfitting)

     ●   ●                   ●   ●                   ●   ●
   ●       ●               ●       ●               ●       ●
  ●    ___  ●             ●   ~~~   ●             ●  ∿∿∿∿  ●
  ●   /   \\ ●             ● /     \\ ●             ●∿      ∿●
   ● /     ●                ●       ●               ●∿∿∿∿∿∿●

  太简单了！               完美！                   太复杂了！
  连训练数据               能泛化到                 死记硬背训练数据
  都学不好                 新数据                   遇到新数据就懵

【欠拟合原因】            【如何达到】             【过拟合原因】
• 模型太简单              • 合适的模型复杂度       • 模型太复杂
• 训练不够                • 足够的训练             • 数据太少
• 特征不够                • 正则化                 • 训练太久
\`\`\`

**生活类比：**
- **欠拟合**：考试只背了重点，结果重点都没记住
- **过拟合**：把答案原封不动背下来，换个问法就不会了
- **刚刚好**：理解了原理，换个题目也能做

---

## 关键理解

通过这些图示，你应该理解了：

1. **神经网络模仿人脑**：由很多简单的"神经元"连接组成
2. **权重决定重要性**：不同输入的影响力不同
3. **层层抽象**：从简单特征到复杂概念
4. **通过错误学习**：不断调整权重，减少错误
5. **深度带来能力**：层数越多，能处理越复杂的问题
6. **梯度下降找最优**：像下山一样找到最好的权重
7. **避免过拟合欠拟合**：让模型既能学好训练数据，又能泛化到新数据

现在你已经直观地理解了 AI 是如何"思考"和"学习"的！
            `,ja:`
前のセクションで深層学習が「ニューラルネットワーク」を使うことがわかりましたが、ニューラルネットワークとは一体何でしょうか？図を使って理解しましょう。

---

## ニューラルネットワークのインスピレーション：人間の脳

ニューラルネットワークの設計は**人間の脳のニューロン**からインスピレーションを受けています：

\`\`\`
【人間のニューロン】                【人工ニューロン】

    樹状突起（信号を受信）              入力 x₁ ──┐
       \\  |  /                        入力 x₂ ──┼──▶ [計算] ──▶ 出力
        \\ | /                         入力 x₃ ──┘
         \\|/                                ↑
    ┌─────●─────┐                      重み w₁,w₂,w₃
    │  細胞体   │                      （重要度）
    │ （処理）  │
    └─────┬─────┘
          │
       軸索（信号を出力）
\`\`\`

**重要な類似点：**
- 樹状突起 → 入力層（データを受信）
- 細胞体 → 計算（重み付き和 + 活性化）
- 軸索 → 出力（次の層へ伝達）

---

## 一つのニューロンの動作原理

\`\`\`
                    ┌─────────────────────────────────────┐
                    │           一つのニューロン            │
                    │                                     │
入力              重み            重み付き和       活性化関数        出力
────              ────            ────────         ────────         ────

x₁ = 0.5 ──▶ × w₁ = 0.3 ─┐
                          │
x₂ = 0.8 ──▶ × w₂ = 0.5 ─┼──▶ Σ = 0.15+0.40+0.12 ──▶ f(0.67) ──▶ 0.66
                          │       = 0.67
x₃ = 0.6 ──▶ × w₃ = 0.2 ─┘

                    └─────────────────────────────────────┘

計算プロセス：
1. 各入力 × 対応する重み
2. 合計を計算
3. 活性化関数で変換
4. 結果を出力
\`\`\`

**重みを日常生活で理解する：**

今日何を着るか決めるとき、3つの要素を考えると仮定：
- 天気（とても重要） → 重み 0.5
- 気分（普通に重要） → 重み 0.3
- 今日会う人（少し重要） → 重み 0.2

重みが大きいほど、その要素が最終決定に与える影響が大きくなります。

---

## 単一ニューロンからニューラルネットワークへ

多くのニューロンを接続すると、**ニューラルネットワーク**が形成されます：

\`\`\`
        入力層          隠れ層1         隠れ層2          出力層
       (Input)        (Hidden1)       (Hidden2)       (Output)

         ○─────────────○─────────────○
        ╱│╲           ╱│╲           ╱│╲
       ╱ │ ╲         ╱ │ ╲         ╱ │ ╲
  x₁ ○───┼───○─────○───┼───○─────○───┼───○─────○ → 猫の確率: 0.92
       ╲ │ ╱         ╲ │ ╱         ╲ │ ╱
        ╲│╱           ╲│╱           ╲│╱
  x₂ ○───┼───○─────○───┼───○─────○───┼───○─────○ → 犬の確率: 0.08
       ╲ │ ╱         ╲ │ ╱         ╲ │ ╱
        ╲│╱           ╲│╱           ╲│╱
  x₃ ○─────────────○─────────────○

       ピクセル値       エッジと        形状と           最終判断
                      色を検出        テクスチャ検出
\`\`\`

**各層は何を学ぶ？**

| 層 | 学習内容 | 比喩 |
|------|-----------|------|
| 入力層 | 生データを受信（例：ピクセル） | 目が画像を見る |
| 隠れ層1 | 単純な特徴を検出（エッジ、色） | 「尖ったものがある」と気づく |
| 隠れ層2 | 複雑な特徴に組み合わせ（目、耳） | 「これは耳の形」と認識 |
| 出力層 | 最終判断を下す | 「これは猫」と判断 |

---

## AIはどうやって「学習」するのか？

ニューラルネットワークの学習プロセスは、シンプルなループで理解できます：

\`\`\`
┌─────────────────────────────────────────────────────────────┐
│                     AIの学習ループ                           │
└─────────────────────────────────────────────────────────────┘

     ┌──────────┐
     │ 1. データ │   猫の画像を入力
     │   入力   │
     └────┬─────┘
          │
          ▼
     ┌──────────┐
     │ 2. 予測  │   AIの推測：これは犬（間違い！）
     │   結果   │
     └────┬─────┘
          │
          ▼
     ┌──────────┐
     │ 3. 答え  │   正解は猫、AIは間違えた
     │   と比較 │   「どれくらい間違ったか」を計算 → 損失値
     └────┬─────┘
          │
          ▼
     ┌──────────┐
     │ 4. 重み  │   次回より正確になるよう重みを調整
     │   を調整 │   （バックプロパゲーション）
     └────┬─────┘
          │
          ▼
     ┌──────────┐
     │ 5. 繰り  │   より多くのデータでこのプロセスを繰り返す
     │ 返し千万回│   重みが徐々に「正しく」なる
     └──────────┘
\`\`\`

**イメージしやすい比喩：自転車の乗り方を学ぶ**

1. **試す**：初めて乗ると、ふらふらする
2. **転ぶ**：バランスを失い、転んだ（これが「エラー」）
3. **振り返る**：体が傾きすぎたと気づく（「損失」を計算）
4. **調整**：次回はバランスを保つように注意（「重み」を調整）
5. **繰り返す**：何度も練習すると、乗れるようになる

AIの学習プロセスも全く同じです——**何度も試して、間違えて、調整し、十分正確になるまで続ける**。

---

## 深層学習はなぜ「深い」のか

\`\`\`
浅いネットワーク（2層）          深いネットワーク（多層）

入力 → ○ → 出力                入力 → ○ → ○ → ○ → ○ → ○ → 出力
         ↓                              ↓
    単純なパターンのみ学習可能        複雑なパターンを学習可能


【手書き数字「3」を認識する例】

浅いネットワークが学べること：    深いネットワークが学べること：
┌─────────────┐                ┌─────────────┐
│ 曲線がある   │                │ 第1層：エッジ│  ╱  ─  ╲
│ 2つの曲がり │                │ 第2層：曲線  │  ⌒  ∪  ⌓
│ ...たぶん3？ │                │ 第3層：ストローク│  ﹀ ∽
└─────────────┘                │ 第4層：組み合わせ│  →  ③
     ↓                         │ 第5層：判断  │  これは3！
  精度 85%                     └─────────────┘
                                    ↓
                               精度 99%
\`\`\`

**層が多いほど：**
- ✅ より複雑なパターンを学習可能
- ✅ 精度が高い
- ❌ より多くのデータが必要
- ❌ より多くの計算リソースが必要
- ❌ 訓練時間が長い

---

## AIのワークフローを一枚の図で要約

\`\`\`
┌──────────────────────────────────────────────────────────────────┐
│                   AIの訓練から使用までの完全なフロー               │
└──────────────────────────────────────────────────────────────────┘

【訓練段階】（大量のデータと計算リソースが必要）

    大量のデータ              ニューラルネットワーク          継続的な調整
  ┌─────────┐              ┌─────────┐               ┌─────────┐
  │ 🐱🐶🐱🐶│  ─────▶      │ ○→○→○→○ │  ─────▶       │ 答えと比較│
  │ 🐱🐶🐱🐶│    入力       │ ○→○→○→○ │   予測        │ 重みを調整│
  │ (ラベル付き)│           │ ○→○→○→○ │               │ 百万回繰返│
  └─────────┘              └─────────┘               └─────────┘
                                                         │
                                ┌─────────────────────────┘
                                ▼
【使用段階】（訓練済みモデル、高速推論）

    新しい入力               訓練済みモデル              出力結果
  ┌─────────┐              ┌─────────┐               ┌─────────┐
  │   🐱?   │  ─────▶      │ ○→○→○→○ │  ─────▶       │  これは猫│
  │ (新画像) │              │ (重み固定済み)│           │ 信頼度95%│
  └─────────┘              └─────────┘               └─────────┘

                    一度訓練すれば、無限に使用可能！
\`\`\`

---

## 図解：AIはどうやって最適な重みを見つけるか

AI訓練の核心は**最良の重みを見つけること**です。このプロセスは**勾配降下法**と呼ばれ、「山を下りて最低点を見つける」ようなものです：

\`\`\`
┌──────────────────────────────────────────────────────────────────┐
│                    勾配降下法：山を下りるように最適解を探す          │
└──────────────────────────────────────────────────────────────────┘

        損失値（エラーの程度）
           ▲
       高  │    ●  ← 初期位置（ランダムな重み、エラー多い）
           │   ╱
           │  ╱
           │ ● ← 1回目の調整（エラー減少）
           │  ╲
           │   ● ← 2回目の調整
           │    ╲
           │     ●
           │      ╲
       低  │       ★ ← 最適位置（エラー最小）
           └────────────────────────────▶
                     重みの値

手順：
1. ランダムな位置（重み）から開始
2. 現在位置の「傾き」（勾配）を計算
3. 「下り坂」方向に小さく進む
4. 「谷底」に到達するまで繰り返す
\`\`\`

---

## 図解：訓練中の損失の変化

訓練が進むにつれて、AIのエラー（損失）は徐々に減少します：

\`\`\`
損失値
  ▲
  │ ████
  │ ████ ███
  │ ████ ███ ██
  │ ████ ███ ███ ██
  │ ████ ███ ███ ███ ██
  │ ████ ███ ███ ███ ███ ██
  │ ████ ███ ███ ███ ███ ███ ██ █ █ █ █
  └──────────────────────────────────────▶ 訓練回数
     1    2    3    4    5    ...  100

┌──────────────────────────────────────┐
│           理想的な訓練曲線             │
│                                      │
│   最初：損失が高い（エラーが多い）     │
│      ↓                               │
│   中期：損失が急速に低下（最も学習中）  │
│      ↓                               │
│   後期：損失が安定（学習がほぼ完了）   │
└──────────────────────────────────────┘
\`\`\`

---

## よくある問題：アンダーフィッティングとオーバーフィッティング

AI訓練時に遭遇する可能性のある2つの問題：

\`\`\`
┌────────────────────────────────────────────────────────────────────┐
│                    3つの状況の比較                                   │
└────────────────────────────────────────────────────────────────────┘

 アンダーフィッティング          ちょうど良い           オーバーフィッティング
 (Underfitting)              (Just Right)            (Overfitting)

     ●   ●                   ●   ●                   ●   ●
   ●       ●               ●       ●               ●       ●
  ●    ___  ●             ●   ~~~   ●             ●  ∿∿∿∿  ●
  ●   /   \\ ●             ● /     \\ ●             ●∿      ∿●
   ● /     ●                ●       ●               ●∿∿∿∿∿∿●

  単純すぎる！              完璧！                   複雑すぎる！
  訓練データすら           新しいデータに            訓練データを丸暗記
  学習できない             汎化可能                  新データで混乱

【原因】                  【達成方法】              【原因】
• モデルが単純すぎる      • 適切なモデル複雑度       • モデルが複雑すぎる
• 訓練不足               • 十分な訓練               • データ不足
• 特徴不足               • 正則化                   • 過度な訓練
\`\`\`

**日常生活の例え：**
- **アンダーフィッティング**：試験で重要ポイントだけ暗記したが、そのポイントすら覚えられなかった
- **オーバーフィッティング**：答えをそのまま暗記したが、問い方が変わると答えられない
- **ちょうど良い**：原理を理解したので、別の問題でも解ける

---

## 重要な理解

これらの図を通じて、理解できたはずです：

1. **ニューラルネットワークは人間の脳を模倣**：多くの単純な「ニューロン」が接続
2. **重みが重要度を決定**：入力ごとに影響力が異なる
3. **層ごとの抽象化**：単純な特徴から複雑な概念へ
4. **エラーから学習**：重みを継続的に調整してエラーを減らす
5. **深さが能力をもたらす**：層が多いほど、複雑な問題に対処可能
6. **勾配降下で最適化**：山を下りるように最良の重みを見つける
7. **過学習・未学習を避ける**：訓練データを学習しつつ、新データにも汎化

これでAIがどう「考え」「学習」するかを直感的に理解できました！
            `}},{id:"ch1-llm-transformer",title:{zh:"1.4 图解大语言模型：ChatGPT 是怎么工作的",ja:"1.4 図解大規模言語モデル：ChatGPTはどう動くか"},content:{zh:`
上一节我们学习了神经网络的基础，现在让我们来看看当今最火的 AI 技术——**大语言模型（LLM）**是如何工作的。

ChatGPT、Claude、文心一言……这些 AI 助手的核心都是一种叫做 **Transformer** 的神经网络架构。

---

## 大语言模型的核心思想

大语言模型做的事情其实很简单：**预测下一个词**。

\`\`\`
┌──────────────────────────────────────────────────────────────────┐
│                    大语言模型的本质：预测下一个词                    │
└──────────────────────────────────────────────────────────────────┘

输入: "今天天气"
            │
            ▼
    ┌───────────────┐
    │   大语言模型   │
    │  (Transformer) │
    └───────┬───────┘
            │
            ▼
    预测下一个词的概率：
    ┌─────────────────────────────┐
    │  "很" → 35%                 │
    │  "不" → 25%                 │
    │  "真" → 20%                 │
    │  "好" → 15%                 │
    │  其他 → 5%                  │
    └─────────────────────────────┘
            │
            ▼
    选择概率最高的："很"

    输出: "今天天气很"

    然后继续预测... → "好" → "今天天气很好"
\`\`\`

**关键理解：** ChatGPT 并不是"理解"了你的问题，而是根据训练数据学到的模式，一个字一个字地预测最可能的回答。

---

## 图解 Transformer 架构

Transformer 是大语言模型的"引擎"，让我们用图来理解它的结构：

\`\`\`
┌──────────────────────────────────────────────────────────────────┐
│                    Transformer 整体架构                           │
└──────────────────────────────────────────────────────────────────┘

         输入                                              输出
    "我 爱 中国"                                      "I love China"
         │                                                  ▲
         ▼                                                  │
┌─────────────────┐                              ┌─────────────────┐
│                 │                              │                 │
│    Encoder      │         ────────────▶        │    Decoder      │
│   （编码器）     │          传递信息             │   （解码器）     │
│                 │                              │                 │
│  理解输入内容    │                              │  生成输出内容    │
│                 │                              │                 │
└─────────────────┘                              └─────────────────┘

    ┌─────────────────────────────────────────────────────────┐
    │                                                         │
    │   Encoder（编码器）           Decoder（解码器）          │
    │                                                         │
    │   ┌─────────────┐            ┌─────────────┐           │
    │   │ 注意力机制   │            │ 注意力机制   │           │
    │   └─────────────┘            └─────────────┘           │
    │         ↓                          ↓                   │
    │   ┌─────────────┐            ┌─────────────┐           │
    │   │ 前馈网络    │            │ 前馈网络    │           │
    │   └─────────────┘            └─────────────┘           │
    │         ↓                          ↓                   │
    │       × N层                      × N层                 │
    │                                                         │
    └─────────────────────────────────────────────────────────┘
\`\`\`

**现代 ChatGPT 等模型主要使用 Decoder 部分**，专注于文本生成。

---

## 图解注意力机制：AI 如何"关注"重要信息

**注意力机制（Attention）** 是 Transformer 最核心的创新，让 AI 能"关注"句子中最相关的部分。

\`\`\`
┌──────────────────────────────────────────────────────────────────┐
│                    注意力机制：AI 如何理解句子                      │
└──────────────────────────────────────────────────────────────────┘

问题："小明给小红送了一束花，她很开心"
                                    ↑
                               "她"指的是谁？

传统方法：按顺序读，可能搞混
注意力机制：直接"看向"相关的词

┌─────────────────────────────────────────────────────────────────┐
│                                                                 │
│    小明  给  小红  送了  一束  花  ，  她  很  开心              │
│                                                                 │
│                    注意力权重（"她"关注谁）                       │
│                                                                 │
│    小明 ──────────── 0.15 （较低，不太相关）                     │
│      │                                                          │
│    小红 ──────────── 0.75 ████████████████ （很高！）            │
│      │                                                          │
│     花 ──────────── 0.05 （低）                                  │
│      │                                                          │
│    开心 ──────────── 0.05 （低）                                  │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘

AI 通过注意力机制，自动学会了"她"应该关注"小红"！
\`\`\`

---

## 图解自注意力计算过程

自注意力的核心是三个概念：**Query（查询）、Key（键）、Value（值）**

\`\`\`
┌──────────────────────────────────────────────────────────────────┐
│                    自注意力：Q、K、V 的比喻                        │
└──────────────────────────────────────────────────────────────────┘

想象你在图书馆找书：

┌─────────────────────────────────────────────────────────────────┐
│                                                                 │
│  你的需求 ─────▶  Q (Query)   "我想找关于Python的书"            │
│                                                                 │
│  书架标签 ─────▶  K (Key)     "Python入门" "Java编程" "AI导论"  │
│                                                                 │
│  书的内容 ─────▶  V (Value)   每本书的实际内容                   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘

计算过程：
1. 用 Q（你的需求）和每个 K（书架标签）比较相似度
2. 相似度高的书（如"Python入门"）获得更高权重
3. 根据权重，综合所有 V（书的内容）得到最终结果

┌─────────────────────────────────────────────────────────────────┐
│                                                                 │
│   Q × K^T                  Softmax                  × V         │
│  ──────────  ─────▶  注意力权重  ─────▶  加权求和  ─────▶  输出  │
│  相似度计算           归一化             综合信息               │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
\`\`\`

---

## 图解多头注意力：从多个角度理解

**多头注意力（Multi-Head Attention）** 让 AI 从多个角度理解句子：

\`\`\`
┌──────────────────────────────────────────────────────────────────┐
│                    多头注意力：多角度理解                          │
└──────────────────────────────────────────────────────────────────┘

输入句子: "银行 倒闭了"

┌─────────────────────────────────────────────────────────────────┐
│                                                                 │
│  头1：关注语法关系        头2：关注语义关系        头3：关注位置  │
│  ┌───────────────┐       ┌───────────────┐       ┌───────────┐ │
│  │ "银行"是主语   │       │ "银行"=金融机构│       │ 词的顺序   │ │
│  │ "倒闭"是谓语   │       │ 不是河岸       │       │ 相邻关系   │ │
│  └───────────────┘       └───────────────┘       └───────────┘ │
│         │                       │                       │       │
│         └───────────────────────┼───────────────────────┘       │
│                                 ▼                               │
│                    ┌─────────────────────┐                      │
│                    │ 拼接 + 线性变换     │                      │
│                    └─────────────────────┘                      │
│                                 │                               │
│                                 ▼                               │
│                    综合多角度的理解结果                          │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘

就像多个专家一起讨论，每人从不同角度分析，最后综合意见。
\`\`\`

---

## 图解文本生成过程

ChatGPT 如何一步步生成回答：

\`\`\`
┌──────────────────────────────────────────────────────────────────┐
│                    文本生成：自回归过程                            │
└──────────────────────────────────────────────────────────────────┘

问题: "什么是AI？"

步骤1: 输入 "什么是AI？" → 预测第一个字 → "人"
步骤2: 输入 "什么是AI？人" → 预测下一个字 → "工"
步骤3: 输入 "什么是AI？人工" → 预测下一个字 → "智"
步骤4: 输入 "什么是AI？人工智" → 预测下一个字 → "能"
...

┌─────────────────────────────────────────────────────────────────┐
│                                                                 │
│  输入:    [什么是AI？]                                           │
│              │                                                  │
│              ▼                                                  │
│           ┌─────┐                                               │
│           │ LLM │ → "人"                                        │
│           └─────┘                                               │
│              │                                                  │
│  输入:    [什么是AI？人]                                         │
│              │                                                  │
│              ▼                                                  │
│           ┌─────┐                                               │
│           │ LLM │ → "工"                                        │
│           └─────┘                                               │
│              │                                                  │
│  ...继续直到生成完整回答...                                       │
│              │                                                  │
│              ▼                                                  │
│  输出:    "人工智能是让机器模拟人类智能的技术..."                 │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘

每一步都把之前生成的内容作为新输入，这叫"自回归"（Autoregressive）
\`\`\`

---

## 图解词嵌入：AI 如何理解文字

AI 不能直接处理文字，需要先把文字转换成数字（向量）：

\`\`\`
┌──────────────────────────────────────────────────────────────────┐
│                    词嵌入：文字变数字                              │
└──────────────────────────────────────────────────────────────────┘

    文字                    数字向量（词嵌入）

    "猫"   ───────▶   [0.2, 0.8, -0.1, 0.5, ...]   共512维
    "狗"   ───────▶   [0.3, 0.7, -0.2, 0.6, ...]
    "汽车" ───────▶   [-0.5, 0.1, 0.9, -0.3, ...]

┌─────────────────────────────────────────────────────────────────┐
│                                                                 │
│  神奇的是，相似的词在向量空间中距离更近：                          │
│                                                                 │
│           "狗" ●                                                │
│                  ╲                                              │
│                   ╲ 距离近                                      │
│                    ╲                                            │
│           "猫" ●────●                                           │
│                                                                 │
│                              ● "汽车"  (距离远)                  │
│                                                                 │
│  甚至可以做"词的算术"：                                          │
│  "国王" - "男人" + "女人" ≈ "女王"                               │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
\`\`\`

---

## 为什么叫"大"语言模型？

\`\`\`
┌──────────────────────────────────────────────────────────────────┐
│                    大语言模型的"大"                                │
└──────────────────────────────────────────────────────────────────┘

模型对比：

┌─────────────────┬────────────────┬──────────────────────────────┐
│     模型         │    参数量       │       相当于                  │
├─────────────────┼────────────────┼──────────────────────────────┤
│   简单神经网络    │   几千个        │   一本笔记本                  │
│   BERT          │   3.4亿         │   一个小型图书馆              │
│   GPT-3         │   1750亿        │   一座大型图书馆              │
│   GPT-4         │   ~1.8万亿      │   几十座图书馆                │
└─────────────────┴────────────────┴──────────────────────────────┘

参数越多：
✅ 能学习更复杂的知识
✅ 回答更准确、更流畅
❌ 需要更多计算资源
❌ 训练成本更高（GPT-4 训练成本估计超过1亿美元）
\`\`\`

---

## 关键理解

通过这些图解，你应该理解了：

1. **LLM 的本质是预测下一个词**：看起来像"对话"，实际是概率预测
2. **Transformer 是核心架构**：编码器理解输入，解码器生成输出
3. **注意力机制让 AI "关注"重要信息**：不再按顺序死读，而是看相关性
4. **多头注意力从多角度理解**：语法、语义、位置等多维度分析
5. **文本生成是自回归的**：一个字一个字地预测，每次都参考之前的内容
6. **参数量决定能力**：越大越强，但成本也越高

现在你已经直观地理解了 ChatGPT 等大语言模型是如何工作的！
            `,ja:`
前のセクションでニューラルネットワークの基礎を学びました。今度は最も注目されているAI技術——**大規模言語モデル（LLM）**がどのように動作するかを見ていきましょう。

ChatGPT、Claude、Gemini...これらのAIアシスタントの核心は、**Transformer**と呼ばれるニューラルネットワークアーキテクチャです。

---

## 大規模言語モデルの核心的な考え方

大規模言語モデルがやっていることは実はシンプルです：**次の単語を予測する**こと。

\`\`\`
┌──────────────────────────────────────────────────────────────────┐
│                    大規模言語モデルの本質：次の単語を予測            │
└──────────────────────────────────────────────────────────────────┘

入力: "今日の天気は"
            │
            ▼
    ┌───────────────┐
    │  大規模言語モデル │
    │  (Transformer) │
    └───────┬───────┘
            │
            ▼
    次の単語の確率を予測：
    ┌─────────────────────────────┐
    │  "良い" → 35%               │
    │  "悪い" → 25%               │
    │  "晴れ" → 20%               │
    │  "曇り" → 15%               │
    │  その他 → 5%                │
    └─────────────────────────────┘
            │
            ▼
    最も確率が高いものを選択："良い"

    出力: "今日の天気は良い"

    続けて予測... → "です" → "今日の天気は良いです"
\`\`\`

**重要な理解：** ChatGPTはあなたの質問を「理解」しているのではなく、訓練データから学んだパターンに基づいて、一文字ずつ最も可能性の高い回答を予測しています。

---

## 図解 Transformer アーキテクチャ

Transformerは大規模言語モデルの「エンジン」です。図で構造を理解しましょう：

\`\`\`
┌──────────────────────────────────────────────────────────────────┐
│                    Transformer 全体アーキテクチャ                  │
└──────────────────────────────────────────────────────────────────┘

         入力                                              出力
    "私は日本が好き"                                   "I like Japan"
         │                                                  ▲
         ▼                                                  │
┌─────────────────┐                              ┌─────────────────┐
│                 │                              │                 │
│    Encoder      │         ────────────▶        │    Decoder      │
│   （エンコーダ）  │          情報を伝達           │   （デコーダ）   │
│                 │                              │                 │
│  入力内容を理解   │                              │  出力内容を生成  │
│                 │                              │                 │
└─────────────────┘                              └─────────────────┘

    ┌─────────────────────────────────────────────────────────┐
    │                                                         │
    │   Encoder（エンコーダ）      Decoder（デコーダ）         │
    │                                                         │
    │   ┌─────────────┐            ┌─────────────┐           │
    │   │ 注意機構    │            │ 注意機構    │           │
    │   └─────────────┘            └─────────────┘           │
    │         ↓                          ↓                   │
    │   ┌─────────────┐            ┌─────────────┐           │
    │   │ フィードフォワード│         │ フィードフォワード│        │
    │   └─────────────┘            └─────────────┘           │
    │         ↓                          ↓                   │
    │       × N層                      × N層                 │
    │                                                         │
    └─────────────────────────────────────────────────────────┘
\`\`\`

**現代のChatGPTなどは主にDecoder部分を使用**し、テキスト生成に特化しています。

---

## 図解 注意機構：AIはどう「注目」するか

**注意機構（Attention）**はTransformerの最も重要な革新で、AIが文中の最も関連性の高い部分に「注目」できるようにします。

\`\`\`
┌──────────────────────────────────────────────────────────────────┐
│                    注意機構：AIはどう文を理解するか                  │
└──────────────────────────────────────────────────────────────────┘

質問：「太郎が花子に花をあげた。彼女は嬉しかった」
                                      ↑
                                「彼女」は誰を指す？

従来の方法：順番に読む、混乱する可能性あり
注意機構：直接関連する単語を「見る」

┌─────────────────────────────────────────────────────────────────┐
│                                                                 │
│    太郎  が  花子  に  花  を  あげた  彼女  は  嬉しかった        │
│                                                                 │
│                    注意の重み（「彼女」は誰に注目？）               │
│                                                                 │
│    太郎 ──────────── 0.15 （低め、あまり関連なし）                │
│      │                                                          │
│    花子 ──────────── 0.75 ████████████████ （とても高い！）       │
│      │                                                          │
│     花 ──────────── 0.05 （低い）                                │
│      │                                                          │
│   嬉しい ─────────── 0.05 （低い）                                │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘

AIは注意機構により、「彼女」が「花子」に注目すべきことを自動的に学習！
\`\`\`

---

## 図解 自己注意の計算プロセス

自己注意の核心は3つの概念：**Query（クエリ）、Key（キー）、Value（値）**

\`\`\`
┌──────────────────────────────────────────────────────────────────┐
│                    自己注意：Q、K、V の比喩                        │
└──────────────────────────────────────────────────────────────────┘

図書館で本を探すことを想像してください：

┌─────────────────────────────────────────────────────────────────┐
│                                                                 │
│  あなたの要求 ───▶  Q (Query)   "Pythonの本が欲しい"             │
│                                                                 │
│  棚のラベル ─────▶  K (Key)     "Python入門" "Java" "AI入門"     │
│                                                                 │
│  本の内容 ───────▶  V (Value)   各本の実際の内容                  │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘

計算プロセス：
1. Q（要求）と各K（棚ラベル）の類似度を比較
2. 類似度が高い本（例："Python入門"）は高い重みを得る
3. 重みに基づき、全V（本の内容）を総合して最終結果を得る

┌─────────────────────────────────────────────────────────────────┐
│                                                                 │
│   Q × K^T                  Softmax                  × V         │
│  ──────────  ─────▶  注意の重み  ─────▶  加重和  ─────▶  出力    │
│  類似度計算           正規化             情報統合               │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
\`\`\`

---

## 図解 マルチヘッド注意：複数の視点で理解

**マルチヘッド注意（Multi-Head Attention）**はAIが複数の角度から文を理解できるようにします：

\`\`\`
┌──────────────────────────────────────────────────────────────────┐
│                    マルチヘッド注意：多角的理解                     │
└──────────────────────────────────────────────────────────────────┘

入力文: "銀行 が 倒産した"

┌─────────────────────────────────────────────────────────────────┐
│                                                                 │
│  ヘッド1：文法関係に注目   ヘッド2：意味関係に注目   ヘッド3：位置 │
│  ┌───────────────┐       ┌───────────────┐       ┌───────────┐ │
│  │ "銀行"は主語   │       │ "銀行"=金融機関│       │ 語順      │ │
│  │ "倒産"は述語   │       │ 川岸ではない   │       │ 隣接関係   │ │
│  └───────────────┘       └───────────────┘       └───────────┘ │
│         │                       │                       │       │
│         └───────────────────────┼───────────────────────┘       │
│                                 ▼                               │
│                    ┌─────────────────────┐                      │
│                    │ 結合 + 線形変換     │                      │
│                    └─────────────────────┘                      │
│                                 │                               │
│                                 ▼                               │
│                    多角的理解の結果を統合                         │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘

複数の専門家が議論するように、それぞれ異なる角度から分析し、最後に意見を統合。
\`\`\`

---

## 図解 テキスト生成プロセス

ChatGPTはどのように段階的に回答を生成するか：

\`\`\`
┌──────────────────────────────────────────────────────────────────┐
│                    テキスト生成：自己回帰プロセス                   │
└──────────────────────────────────────────────────────────────────┘

質問: "AIとは何ですか？"

ステップ1: "AIとは何ですか？" を入力 → 最初の文字を予測 → "人"
ステップ2: "AIとは何ですか？人" を入力 → 次の文字を予測 → "工"
ステップ3: "AIとは何ですか？人工" を入力 → 次の文字を予測 → "知"
ステップ4: "AIとは何ですか？人工知" を入力 → 次の文字を予測 → "能"
...

┌─────────────────────────────────────────────────────────────────┐
│                                                                 │
│  入力:    [AIとは何ですか？]                                      │
│              │                                                  │
│              ▼                                                  │
│           ┌─────┐                                               │
│           │ LLM │ → "人"                                        │
│           └─────┘                                               │
│              │                                                  │
│  入力:    [AIとは何ですか？人]                                    │
│              │                                                  │
│              ▼                                                  │
│           ┌─────┐                                               │
│           │ LLM │ → "工"                                        │
│           └─────┘                                               │
│              │                                                  │
│  ...完全な回答が生成されるまで続く...                              │
│              │                                                  │
│              ▼                                                  │
│  出力:    "人工知能は機械が人間の知能を模倣する技術です..."         │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘

毎回、以前に生成した内容を新しい入力として使用。これを「自己回帰」と呼びます。
\`\`\`

---

## 図解 単語埋め込み：AIはどう文字を理解するか

AIは文字を直接処理できないので、まず数字（ベクトル）に変換する必要があります：

\`\`\`
┌──────────────────────────────────────────────────────────────────┐
│                    単語埋め込み：文字から数字へ                     │
└──────────────────────────────────────────────────────────────────┘

    文字                    数値ベクトル（埋め込み）

    "猫"   ───────▶   [0.2, 0.8, -0.1, 0.5, ...]   512次元
    "犬"   ───────▶   [0.3, 0.7, -0.2, 0.6, ...]
    "車"   ───────▶   [-0.5, 0.1, 0.9, -0.3, ...]

┌─────────────────────────────────────────────────────────────────┐
│                                                                 │
│  魔法のように、似た単語はベクトル空間で近くに配置：                  │
│                                                                 │
│           "犬" ●                                                │
│                  ╲                                              │
│                   ╲ 距離が近い                                   │
│                    ╲                                            │
│           "猫" ●────●                                           │
│                                                                 │
│                              ● "車"  (距離が遠い)                │
│                                                                 │
│  「単語の算術」も可能：                                           │
│  "王" - "男" + "女" ≈ "女王"                                     │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
\`\`\`

---

## なぜ「大規模」言語モデルと呼ぶのか？

\`\`\`
┌──────────────────────────────────────────────────────────────────┐
│                    大規模言語モデルの「大規模」                     │
└──────────────────────────────────────────────────────────────────┘

モデル比較：

┌─────────────────┬────────────────┬──────────────────────────────┐
│     モデル       │    パラメータ数  │       例えると                │
├─────────────────┼────────────────┼──────────────────────────────┤
│   単純なNN       │   数千個        │   ノート1冊                   │
│   BERT          │   3.4億         │   小さな図書館                │
│   GPT-3         │   1750億        │   大きな図書館                │
│   GPT-4         │   ~1.8兆        │   数十の図書館                │
└─────────────────┴────────────────┴──────────────────────────────┘

パラメータが多いほど：
✅ より複雑な知識を学習可能
✅ より正確で流暢な回答
❌ より多くの計算リソースが必要
❌ 訓練コストが高い（GPT-4の訓練コストは1億ドル以上と推定）
\`\`\`

---

## 重要な理解

これらの図を通じて、理解できたはずです：

1. **LLMの本質は次の単語を予測すること**：「会話」に見えても、実際は確率予測
2. **Transformerが核心アーキテクチャ**：エンコーダが入力を理解、デコーダが出力を生成
3. **注意機構でAIが重要な情報に「注目」**：順番に読むのではなく、関連性を見る
4. **マルチヘッド注意で多角的理解**：文法、意味、位置など多次元で分析
5. **テキスト生成は自己回帰的**：一文字ずつ予測、毎回以前の内容を参照
6. **パラメータ数が能力を決定**：大きいほど強力、しかしコストも高い

これでChatGPTなどの大規模言語モデルがどう動作するか直感的に理解できました！
            `}},{id:"ch1-can-cannot",title:{zh:"1.5 AI 能做什么，不能做什么",ja:"1.5 AIにできること、できないこと"},content:{zh:`
了解了 AI 的本质后，我们来看看它到底**能做什么**，以及**不能做什么**。建立正确的认知很重要，这样你才不会对 AI 有不切实际的期望，也不会低估它的能力。

## AI 擅长的事情

### 1. 处理海量数据

人类阅读一篇文章需要几分钟，AI 可以在一秒钟内"阅读"上千篇文章。

**应用场景：**
- 分析用户行为，推荐你可能喜欢的商品
- 扫描数百万张医学影像，辅助医生诊断
- 监控社交媒体上的舆情变化

### 2. 发现数据中的规律

人类很难从杂乱的数据中发现隐藏的模式，但 AI 很擅长这个。

**应用场景：**
- 预测股票价格走势（虽然不一定准）
- 发现欺诈交易
- 预测天气

### 3. 重复性工作

AI 不会累，不会走神，可以 24 小时不间断工作。

**应用场景：**
- 客服机器人回答常见问题
- 自动驾驶
- 工厂质量检测

### 4. 模式识别

识别图像、声音、文字中的模式是 AI 的强项。

**应用场景：**
- 人脸识别解锁手机
- 语音转文字
- 识别手写字

---

## AI 不擅长的事情

### 1. 真正"理解"含义

AI 看起来像是理解了你说的话，但实际上它只是在做**统计模式匹配**。

举个例子：

> **你问：** "我养了一只猫，它喜欢吃鱼。'它'指的是什么？"
>
> **AI 回答：** "猫"

AI 答对了，但它真的"理解"了吗？不，它只是根据大量文本中的统计规律，判断出"它"通常指代前面提到的动物。

### 2. 常识推理

人类有大量"不言而喻"的常识，但 AI 没有。

> **问：** "小明把杯子放进冰箱，第二天杯子还在冰箱里吗？"

人类会说"在"，因为杯子不会自己跑出来。但 AI 可能会困惑——它在训练数据中可能没见过这种情况。

### 3. 创造真正新颖的东西

AI 生成的内容，本质上是对已有数据的**重新组合**，而不是真正的"创造"。

AI 可以：
- 模仿莫奈的风格画画
- 用莎士比亚的风格写诗

但 AI 不能：
- 像莫奈一样，开创一个全新的艺术流派
- 像莎士比亚一样，创造出前所未有的文学形式

### 4. 情感共鸣

AI 可以识别情感（判断一句话是高兴还是悲伤），但它没有真正的感受。

当你伤心时，朋友的一个拥抱可能比任何话语都有效。AI 永远做不到这一点。

---

## 一个重要的认知

很多人在两个极端之间摇摆：

- **过度恐惧**："AI 会取代所有人的工作，人类要完蛋了！"
- **过度轻视**："AI 只是个工具，没什么大不了的。"

正确的认知是：

> **AI 是一个强大的工具，能极大增强人类的能力，但它不能取代人类的创造力、判断力和情感。**

最好的方式是：**让人类做人类擅长的事，让 AI 做 AI 擅长的事，两者协作**。

这就是为什么学习使用 AI 工具如此重要——它可以让你如虎添翼。
            `,ja:`
AIの本質を理解したところで、AIが**できること**と**できないこと**を見ていきましょう。正しい認識を持つことは重要です。そうすれば、AIに対して非現実的な期待を持つことも、その能力を過小評価することもなくなります。

## AIが得意なこと

### 1. 大量のデータ処理

人間が記事を1つ読むのに数分かかりますが、AIは1秒で何千もの記事を「読む」ことができます。

**応用例：**
- ユーザー行動を分析し、好みそうな商品をおすすめ
- 何百万もの医療画像をスキャンし、診断を支援
- SNS上の世論の変化をモニタリング

### 2. データのパターン発見

人間は雑然としたデータから隠れたパターンを見つけるのが難しいですが、AIはこれが得意です。

**応用例：**
- 株価のトレンド予測（必ずしも正確ではないが）
- 不正取引の検出
- 天気予報

### 3. 繰り返し作業

AIは疲れません、気が散りません、24時間休みなく働けます。

**応用例：**
- カスタマーサポートボットがよくある質問に回答
- 自動運転
- 工場での品質検査

### 4. パターン認識

画像、音声、テキストのパターンを認識することはAIの強みです。

**応用例：**
- 顔認識でスマートフォンをロック解除
- 音声をテキストに変換
- 手書き文字の認識

---

## AIが苦手なこと

### 1. 本当に「理解」すること

AIはあなたの言葉を理解しているように見えますが、実際は**統計的なパターンマッチング**をしているだけです。

例えば：

> **あなた：** 「私は猫を飼っています。それは魚が好きです。『それ』は何を指しますか？」
>
> **AI：** 「猫」

AIは正解しましたが、本当に「理解」したのでしょうか？いいえ、大量のテキストの統計的パターンから、「それ」は通常前に出てきた動物を指すと判断しただけです。

### 2. 常識的な推論

人間には「言わなくてもわかる」常識がたくさんありますが、AIにはありません。

> **質問：** 「太郎がコップを冷蔵庫に入れました。次の日、コップはまだ冷蔵庫にありますか？」

人間は「ある」と言います。コップは自分で出ていかないからです。しかしAIは困惑するかもしれません——訓練データでこのような状況を見たことがないかもしれません。

### 3. 本当に新しいものを創造すること

AIが生成するコンテンツは、本質的に既存のデータの**再結合**であり、本当の「創造」ではありません。

AIができること：
- モネのスタイルで絵を描く
- シェイクスピアのスタイルで詩を書く

AIができないこと：
- モネのように、全く新しい芸術運動を始める
- シェイクスピアのように、前例のない文学形式を創り出す

### 4. 感情的な共感

AIは感情を認識できます（文が嬉しいか悲しいかを判断）が、本当の感情を持っていません。

あなたが悲しいとき、友達からの一つのハグはどんな言葉よりも効果的かもしれません。AIには決してこれができません。

---

## 重要な認識

多くの人が2つの極端の間で揺れ動いています：

- **過度の恐怖**：「AIはすべての仕事を奪い、人類は終わりだ！」
- **過度の軽視**：「AIは単なるツールで、大したことない。」

正しい認識は：

> **AIは強力なツールで、人間の能力を大幅に強化できますが、人間の創造性、判断力、感情に取って代わることはできません。**

最良の方法は：**人間が得意なことは人間が、AIが得意なことはAIが、両者が協力する**ことです。

だからこそ、AIツールの使い方を学ぶことが重要なのです——あなたの力を何倍にもしてくれます。
            `}},{id:"ch1-history",title:{zh:"1.6 AI 的发展历史",ja:"1.6 AIの発展の歴史"},content:{zh:`
了解 AI 的历史，能帮助我们更好地理解今天的 AI 是怎么来的，以及它可能往哪里去。

---

## AI 发展时间线

\`\`\`
1950        1970        1980        1990        2000        2010        2020
  │           │           │           │           │           │           │
  ▼           ▼           ▼           ▼           ▼           ▼           ▼
┌─────┐   ┌─────┐   ┌─────┐   ┌─────┐   ┌─────┐   ┌─────┐   ┌─────┐
│诞生 │   │第一次│   │专家 │   │第二次│   │机器 │   │深度 │   │大模型│
│  期 │   │寒冬 │   │系统 │   │寒冬 │   │学习 │   │学习 │   │时代 │
└─────┘   └─────┘   └─────┘   └─────┘   └─────┘   └─────┘   └─────┘
  🌱         ❄️         💼         ❄️         📈         🚀         🌟
\`\`\`

---

## 1950年代：AI 的诞生

### 图灵测试（1950年）

英国数学家**艾伦·图灵**提出了一个著名的问题：

> "机器能思考吗？"

他设计了一个测试方法：如果一台机器和人对话，人无法分辨对方是机器还是人，那就可以认为这台机器有"智能"。

这就是著名的**图灵测试**，至今仍是评估 AI 的重要参考。

### 达特茅斯会议（1956年）

1956年夏天，一群科学家在美国达特茅斯学院开会，首次提出了"人工智能"（Artificial Intelligence）这个词。

这次会议被认为是 **AI 领域的诞生之日**。当时的科学家们非常乐观，认为很快就能创造出和人一样聪明的机器。

---

## 1970年代：第一次 AI 寒冬

### 为什么会有"寒冬"？

早期 AI 研究者的承诺太大，但实际成果太少：

| 承诺 | 现实 |
|------|------|
| "20年内机器能做任何人能做的事" | 连简单的视觉识别都做不好 |
| "翻译系统即将完成" | 翻译质量惨不忍睹 |
| "很快就能创造通用 AI" | 只能做非常简单的任务 |

政府和企业对 AI 失去信心，**资金大幅削减**，研究陷入低谷。

### 问题在哪里？

当时的 AI 主要靠**人工编写规则**，这种方法有很大局限：
- 规则太多，写不完
- 现实世界太复杂，无法穷举
- 计算机性能太弱

---

## 1980年代：专家系统的兴起

### 什么是专家系统？

既然通用 AI 太难，不如先做**专门领域的 AI**。

专家系统就是把某个领域专家的知识编写成规则，让计算机模拟专家做决策。

**例子：** 医疗诊断系统
- 如果病人发烧 + 咳嗽 + 喉咙痛 → 可能是感冒
- 如果病人发烧 + 皮疹 + 关节痛 → 可能是某种感染

### 兴衰

专家系统一度非常火，很多公司投入大量资金。但最终也遇到了瓶颈：
- **知识获取难**：专家的知识很难完全表达出来
- **维护成本高**：规则越来越多，系统越来越脆弱
- **无法处理不确定性**：现实世界充满模糊和例外

---

## 1990年代：第二次 AI 寒冬

专家系统的失败再次让 AI 陷入低谷。

但在这个"寒冷"的时期，一些重要的技术在悄悄发展：

- **统计方法**开始进入 AI 领域
- **神经网络**的理论在不断完善
- **互联网**开始普及，数据变得更容易获取
- **计算机性能**持续提升

这些都为后来的突破埋下了种子。

---

## 2000年代：机器学习的崛起

### 从规则到数据

AI 研究者开始意识到：

> 与其让人写规则，不如**让机器从数据中自动学习规则**。

这就是我们在 1.1 节讲过的核心思想。

### 重要突破

- **2006年**：杰弗里·辛顿提出深度学习的训练方法
- **大数据时代**：互联网产生了海量数据
- **GPU 加速**：原本用于游戏的显卡被用来训练 AI

---

## 2010年代：深度学习革命

### 里程碑事件

**2012年：ImageNet 突破**

在一个著名的图像识别比赛中，深度学习方法的错误率比传统方法低了 **10个百分点**。这是一个巨大的飞跃，震惊了整个领域。

**2016年：AlphaGo 击败围棋世界冠军**

Google DeepMind 的 AI 系统击败了韩国围棋大师李世石。围棋一直被认为是 AI 最难攻克的游戏之一。

**2017年：Transformer 诞生**

Google 发明了 Transformer 架构，这成为了后来所有大语言模型的基础，包括 ChatGPT。

---

## 2020年代：大模型时代

### ChatGPT 的横空出世

**2022年11月**，OpenAI 发布了 ChatGPT，AI 从此走进了普通人的生活。

为什么 ChatGPT 如此不同？

| 以前的 AI | ChatGPT |
|----------|---------|
| 只能做特定任务 | 能做各种任务 |
| 需要专业知识才能用 | 用自然语言就能交流 |
| 只有专家在用 | 普通人都能用 |

### 百花齐放

ChatGPT 之后，各大公司纷纷推出自己的大语言模型：
- Google 的 Gemini
- Anthropic 的 Claude
- Meta 的 LLaMA
- 国内的文心一言、通义千问等

我们正处于 AI 历史上**最激动人心的时代**。

---

## 从历史中学到什么？

### 1. AI 发展不是一帆风顺
经历了多次"寒冬"，才有了今天的繁荣。过度乐观和过度悲观都不可取。

### 2. 数据和算力是关键
现代 AI 的突破，很大程度上得益于海量数据和强大的计算能力。

### 3. 我们仍在早期
尽管进步惊人，但通用人工智能（AGI）仍未实现。AI 还有很长的路要走。

### 4. 机会属于会用 AI 的人
每一次技术革命都会创造新的机会。现在学习 AI，正是最好的时机。
            `,ja:`
AIの歴史を知ることで、今日のAIがどのように発展してきたか、そして今後どこへ向かうのかをより深く理解できます。

---

## AI発展タイムライン

\`\`\`
1950        1970        1980        1990        2000        2010        2020
  │           │           │           │           │           │           │
  ▼           ▼           ▼           ▼           ▼           ▼           ▼
┌─────┐   ┌─────┐   ┌─────┐   ┌─────┐   ┌─────┐   ┌─────┐   ┌─────┐
│誕生 │   │第一次│   │エキ │   │第二次│   │機械 │   │深層 │   │大規模│
│  期 │   │冬の時│   │スパー│   │冬の時│   │学習 │   │学習 │   │モデル│
│    │   │代   │   │ト   │   │代   │   │    │   │    │   │時代 │
└─────┘   └─────┘   └─────┘   └─────┘   └─────┘   └─────┘   └─────┘
  🌱         ❄️         💼         ❄️         📈         🚀         🌟
\`\`\`

---

## 1950年代：AIの誕生

### チューリングテスト（1950年）

イギリスの数学者**アラン・チューリング**は有名な問いを投げかけました：

> 「機械は思考できるか？」

彼はテスト方法を考案しました：機械と人間が会話し、人間が相手が機械か人間か判断できなければ、その機械には「知能」があると見なせる。

これが有名な**チューリングテスト**で、今日でもAI評価の重要な参考となっています。

### ダートマス会議（1956年）

1956年夏、科学者たちがアメリカのダートマス大学に集まり、初めて「人工知能」（Artificial Intelligence）という言葉を提唱しました。

この会議は**AI分野の誕生日**と見なされています。当時の科学者たちは非常に楽観的で、すぐに人間と同じくらい賢い機械を作れると考えていました。

---

## 1970年代：第一次AI冬

### なぜ「冬」が来たのか？

初期のAI研究者の約束は大きすぎましたが、実際の成果は少なすぎました：

| 約束 | 現実 |
|------|------|
| 「20年以内に機械は人間ができることは何でもできる」 | 単純な視覚認識さえできない |
| 「翻訳システムはもうすぐ完成」 | 翻訳品質は悲惨 |
| 「すぐに汎用AIを作れる」 | 非常に単純なタスクしかできない |

政府や企業はAIへの信頼を失い、**資金は大幅に削減**され、研究は低迷しました。

### 問題はどこにあったのか？

当時のAIは主に**人間がルールを手書き**する方法に頼っていましたが、この方法には大きな限界がありました：
- ルールが多すぎて書ききれない
- 現実世界は複雑すぎて網羅できない
- コンピュータの性能が弱すぎた

---

## 1980年代：エキスパートシステムの興隆

### エキスパートシステムとは？

汎用AIが難しすぎるなら、まず**特定分野のAI**を作ろう。

エキスパートシステムは、ある分野の専門家の知識をルールとして記述し、コンピュータに専門家のような意思決定をさせるものです。

**例：** 医療診断システム
- 患者が発熱 + 咳 + 喉の痛み → 風邪の可能性
- 患者が発熱 + 発疹 + 関節痛 → 何らかの感染症の可能性

### 興亡

エキスパートシステムは一時非常に人気があり、多くの企業が大量の資金を投入しました。しかし最終的には壁にぶつかりました：
- **知識獲得が困難**：専門家の知識を完全に表現するのは難しい
- **維持コストが高い**：ルールが増えるほどシステムは脆くなる
- **不確実性に対処できない**：現実世界は曖昧さと例外に満ちている

---

## 1990年代：第二次AI冬

エキスパートシステムの失敗により、AIは再び低迷しました。

しかしこの「寒い」時期に、いくつかの重要な技術が静かに発展していました：

- **統計的手法**がAI分野に入り始めた
- **ニューラルネットワーク**の理論が継続的に改善
- **インターネット**が普及し始め、データが入手しやすくなった
- **コンピュータ性能**が継続的に向上

これらすべてが、後のブレークスルーの種を蒔きました。

---

## 2000年代：機械学習の台頭

### ルールからデータへ

AI研究者たちは気づき始めました：

> 人間がルールを書くより、**機械がデータから自動的にルールを学習する**方がいい。

これが1.1節で説明した核心的な考え方です。

### 重要なブレークスルー

- **2006年**：ジェフリー・ヒントンが深層学習の訓練方法を提案
- **ビッグデータ時代**：インターネットが大量のデータを生成
- **GPUアクセラレーション**：元々ゲーム用のグラフィックカードがAI訓練に使われるように

---

## 2010年代：深層学習革命

### マイルストーンとなる出来事

**2012年：ImageNetブレークスルー**

有名な画像認識コンペティションで、深層学習手法のエラー率が従来手法より**10ポイント**低くなりました。これは大きな飛躍であり、分野全体を震撼させました。

**2016年：AlphaGoが囲碁世界チャンピオンに勝利**

Google DeepMindのAIシステムが韓国の囲碁名人イ・セドルを破りました。囲碁はAIにとって最も攻略が難しいゲームの一つと考えられていました。

**2017年：Transformerの誕生**

GoogleがTransformerアーキテクチャを発明しました。これがChatGPTを含むすべての大規模言語モデルの基盤となりました。

---

## 2020年代：大規模モデル時代

### ChatGPTの登場

**2022年11月**、OpenAIがChatGPTをリリースし、AIは一般の人々の生活に入り込みました。

なぜChatGPTはこれほど違うのか？

| 以前のAI | ChatGPT |
|----------|---------|
| 特定のタスクしかできない | 様々なタスクができる |
| 使うには専門知識が必要 | 自然言語でコミュニケーション可能 |
| 専門家だけが使用 | 一般の人も使える |

### 百花繚乱

ChatGPT以降、各大手企業が独自の大規模言語モデルを次々とリリース：
- Googleの Gemini
- Anthropicの Claude
- Metaの LLaMA
- その他多数

私たちは今、AI史上**最もエキサイティングな時代**にいます。

---

## 歴史から学ぶこと

### 1. AIの発展は順風満帆ではなかった
何度もの「冬」を経て、今日の繁栄があります。過度な楽観も過度な悲観も望ましくありません。

### 2. データと計算能力が鍵
現代AIのブレークスルーは、大量のデータと強力な計算能力のおかげです。

### 3. まだ初期段階にいる
驚異的な進歩にもかかわらず、汎用人工知能（AGI）はまだ実現していません。AIにはまだ長い道のりがあります。

### 4. 機会はAIを使いこなせる人のもの
技術革命は常に新しい機会を生み出します。今こそAIを学ぶ最高のタイミングです。
            `}},{id:"ch1-summary",title:{zh:"1.7 本章小结",ja:"1.7 この章のまとめ"},content:{zh:`
恭喜你！你已经完成了第一章的学习。让我们回顾一下这一章学到的关键内容：

---

## 核心要点

### 1. AI 的本质
> AI 是让机器从数据中自动学习规律，而不是人工编写规则。

### 2. 概念关系
> AI > 机器学习 > 深度学习 > 大语言模型
>
> 它们是一个套娃的关系，一个包含另一个。

### 3. 神经网络的工作原理
> 神经网络模仿人脑，由多个"神经元"连接组成。每个神经元接收输入、计算加权和、输出结果。通过"输入→预测→对比→调整"的循环不断学习。

### 4. 大语言模型与 Transformer
> ChatGPT 等大语言模型的本质是"预测下一个词"。Transformer 架构通过注意力机制让 AI 能"关注"句子中最相关的部分，多头注意力从多角度理解文本。

### 5. AI 的能与不能

| 擅长 | 不擅长 |
|------|--------|
| 处理海量数据 | 真正理解含义 |
| 发现隐藏规律 | 常识推理 |
| 重复性工作 | 创造全新事物 |
| 模式识别 | 情感共鸣 |

### 6. AI 的发展历史
> 从 1950 年代诞生，经历两次"寒冬"，到如今的大模型时代。AI 的发展告诉我们：技术进步不是一帆风顺的，数据和算力是现代 AI 的关键。

### 7. 正确的认知
> AI 是强大的工具，能增强人类能力，但不能取代人类的创造力、判断力和情感。人机协作是最佳方式。

---

## 下一章预告

在下一章，我们将进入动手环节：**实际使用 AI 工具**。

你将学会：
- 如何注册和使用 ChatGPT
- 如何和 AI 有效对话
- 如何用 AI 生成图片
- 各种 AI 工具的比较和选择

准备好把知识转化为实践了吗？翻到下一章，开始动手吧！

---

## 思考题

在继续之前，花几分钟思考这些问题：

1. 你今天使用的哪些 App 或服务背后有 AI？
2. 你觉得 AI 最可能帮到你的场景是什么？
3. 有什么事情是你不希望 AI 来做的？为什么？

不需要标准答案，只是帮助你建立自己的思考。
            `,ja:`
おめでとうございます！第1章の学習を終えました。この章で学んだ重要なポイントを振り返りましょう：

---

## 核心ポイント

### 1. AIの本質
> AIは、人がルールを手書きするのではなく、機械がデータから自動的にパターンを学習すること。

### 2. 概念の関係
> AI > 機械学習 > 深層学習 > 大規模言語モデル
>
> マトリョーシカのように、一つが別の一つを含んでいます。

### 3. ニューラルネットワークの仕組み
> ニューラルネットワークは人間の脳を模倣し、複数の「ニューロン」が接続されて構成されています。各ニューロンは入力を受け取り、重み付け和を計算し、結果を出力します。「入力→予測→比較→調整」のサイクルを繰り返し学習します。

### 4. 大規模言語モデルとTransformer
> ChatGPTなどの大規模言語モデルの本質は「次の単語を予測する」こと。Transformerアーキテクチャは注意機構によりAIが文中の最も関連性の高い部分に「注目」でき、マルチヘッド注意で多角的にテキストを理解します。

### 5. AIの得意・不得意

| 得意 | 不得意 |
|------|--------|
| 大量データ処理 | 本当の意味理解 |
| 隠れたパターン発見 | 常識的推論 |
| 繰り返し作業 | 全く新しいものの創造 |
| パターン認識 | 感情的共感 |

### 6. AIの発展の歴史
> 1950年代の誕生から、二度の「冬」を経て、今日の大規模モデル時代へ。AIの発展は、技術の進歩は順調ではないこと、データと計算能力が現代AIの鍵であることを教えてくれます。

### 7. 正しい認識
> AIは強力なツールで人間の能力を強化できますが、人間の創造性、判断力、感情に取って代わることはできません。人機協働が最善の方法です。

---

## 次章の予告

次の章では、実践に入ります：**AIツールを実際に使う**。

学ぶこと：
- ChatGPTの登録と使い方
- AIと効果的に会話する方法
- AIで画像を生成する方法
- 様々なAIツールの比較と選択

知識を実践に変える準備はできましたか？次の章をめくって、始めましょう！

---

## 考えてみよう

続ける前に、数分間これらの質問について考えてみてください：

1. 今日使ったアプリやサービスの中で、AIが使われているものは？
2. AIが最も役立つと思う場面は？
3. AIにやってほしくないことはある？それはなぜ？

正解は必要ありません。自分の考えを深めるためのものです。
            `}}]},{id:"chapter-2",number:2,title:{zh:"动手使用 AI 工具",ja:"AIツールを使ってみよう"},subtitle:{zh:"从观众变成玩家",ja:"観客からプレイヤーへ"},sections:[{id:"ch2-intro",title:{zh:"引言：百闻不如一试",ja:"序章：百聞は一見にしかず"},content:{zh:`
第一章我们了解了 AI 是什么，现在是时候**亲自体验**了。

就像学游泳一样，看再多游泳教程，不如跳进水里扑腾几下。学习 AI 工具也是一样——**动手试试，比什么都重要**。

---

## 本章你将学到

在这一章中，我们会手把手教你：

1. **注册和使用 ChatGPT** —— 最流行的 AI 对话工具
2. **写出好的提示词（Prompt）** —— 让 AI 更好地理解你
3. **使用 AI 生成图片** —— 体验创意 AI 的魔力
4. **了解更多 AI 工具** —— 找到适合你的工具

---

## 准备工作

在开始之前，请确保你有：

- ✅ 一台能上网的电脑或手机
- ✅ 一个邮箱（用于注册账号）
- ✅ 大约 30 分钟的时间
- ✅ 一颗好奇心

准备好了吗？让我们开始动手吧！
            `,ja:`
第1章でAIとは何かを学びました。今度は**実際に体験**する番です。

水泳を学ぶのと同じように、どれだけ教則ビデオを見ても、実際に水に入ってみるのが一番です。AIツールの学習も同じです——**実際に試すことが何よりも大切**です。

---

## この章で学ぶこと

この章では、手取り足取り教えます：

1. **ChatGPTの登録と使い方** —— 最も人気のAI会話ツール
2. **良いプロンプトの書き方** —— AIにより良く理解してもらう
3. **AIで画像を生成** —— クリエイティブAIの魔法を体験
4. **その他のAIツール** —— あなたに合ったツールを見つける

---

## 準備するもの

始める前に、以下を確認してください：

- ✅ インターネットに接続できるPCまたはスマートフォン
- ✅ メールアドレス（アカウント登録用）
- ✅ 約30分の時間
- ✅ 好奇心

準備はいいですか？始めましょう！
            `}},{id:"ch2-chatgpt-setup",title:{zh:"2.1 ChatGPT 注册与使用",ja:"2.1 ChatGPTの登録と使用"},content:{zh:`
ChatGPT 是目前最流行的 AI 对话工具，由 OpenAI 公司开发。让我们一步步来注册和使用它。

## 第一步：访问 ChatGPT

打开浏览器，访问：**https://chat.openai.com**

你会看到 ChatGPT 的首页。

## 第二步：创建账号

1. 点击 **"Sign up"（注册）** 按钮
2. 你可以选择：
   - 用 Google 账号登录（推荐，最方便）
   - 用 Microsoft 账号登录
   - 用邮箱注册

3. 如果用邮箱注册：
   - 输入你的邮箱地址
   - 设置密码
   - 去邮箱点击验证链接
   - 填写你的名字
   - 验证手机号（可能需要）

## 第三步：开始对话

注册成功后，你会看到一个对话界面。底部有一个输入框，这就是你和 AI 对话的地方。

### 试试你的第一次对话

在输入框中输入：

\`\`\`
你好！请用简单的话介绍一下你自己。
\`\`\`

按回车发送，看看 ChatGPT 怎么回复你。

### 再试几个例子

\`\`\`
帮我写一首关于春天的小诗，要求简短有趣
\`\`\`

\`\`\`
用小学生能懂的话解释一下为什么天空是蓝色的
\`\`\`

\`\`\`
我想学做菜，推荐三道简单易学的家常菜
\`\`\`

---

## 界面介绍

让我来介绍一下 ChatGPT 的界面：

### 左侧边栏
- **New chat（新对话）**：开始一个新的对话
- **历史记录**：你之前的所有对话都保存在这里

### 对话区域
- **你的消息**：显示在右侧
- **AI 的回复**：显示在左侧
- **复制按钮**：可以复制 AI 的回复
- **重新生成**：如果对回复不满意，可以让 AI 重新回答

### 底部输入框
- 输入你想说的话
- 按回车或点击发送按钮

---

## 免费版 vs 付费版

ChatGPT 有两个版本：

| 版本 | 价格 | 特点 |
|------|------|------|
| Free | 免费 | GPT-3.5 模型，基础功能 |
| Plus | $20/月 | GPT-4 模型，更智能，更多功能 |

**建议**：先用免费版熟悉一下，如果觉得好用且经常使用，再考虑付费。

---

## 常见问题

**Q：为什么有时候 AI 的回答很慢？**
A：可能是服务器繁忙，稍等一会儿再试。

**Q：为什么 AI 的回答有时候不准确？**
A：AI 可能会"幻觉"，即编造不存在的信息。重要信息请自行核实。

**Q：我的对话会被保存吗？**
A：对话会被保存用于改进服务，不要输入敏感个人信息。

---

恭喜你！你已经成功注册并开始使用 ChatGPT 了。接下来，让我们学习如何写出更好的提示词，让 AI 更好地帮助你。
            `,ja:`
ChatGPTは現在最も人気のあるAI会話ツールで、OpenAI社が開発しました。ステップバイステップで登録と使用方法を見ていきましょう。

## ステップ1：ChatGPTにアクセス

ブラウザを開いて、**https://chat.openai.com** にアクセスします。

ChatGPTのホームページが表示されます。

## ステップ2：アカウント作成

1. **"Sign up"（登録）** ボタンをクリック
2. 以下から選べます：
   - Googleアカウントでログイン（おすすめ、最も簡単）
   - Microsoftアカウントでログイン
   - メールアドレスで登録

3. メールアドレスで登録する場合：
   - メールアドレスを入力
   - パスワードを設定
   - メールで確認リンクをクリック
   - 名前を入力
   - 電話番号の確認（必要な場合）

## ステップ3：会話を始める

登録が完了すると、チャット画面が表示されます。下部に入力欄があり、ここでAIと会話します。

### 最初の会話を試してみよう

入力欄に入力：

\`\`\`
こんにちは！簡単に自己紹介してください。
\`\`\`

Enterを押して送信し、ChatGPTの返答を見てみましょう。

### もう少し試してみよう

\`\`\`
春についての短くて面白い詩を書いてください
\`\`\`

\`\`\`
小学生にもわかるように、なぜ空は青いのか説明してください
\`\`\`

\`\`\`
料理を学びたいです。簡単に作れる家庭料理を3つおすすめしてください
\`\`\`

---

## インターフェースの紹介

ChatGPTの画面を説明します：

### 左サイドバー
- **New chat（新規チャット）**：新しい会話を始める
- **履歴**：以前のすべての会話がここに保存

### 会話エリア
- **あなたのメッセージ**：右側に表示
- **AIの返答**：左側に表示
- **コピーボタン**：AIの返答をコピーできる
- **再生成**：返答に満足できなければ、AIに再回答させる

### 下部の入力欄
- 言いたいことを入力
- Enterを押すか送信ボタンをクリック

---

## 無料版 vs 有料版

ChatGPTには2つのバージョンがあります：

| バージョン | 価格 | 特徴 |
|------|------|------|
| Free | 無料 | GPT-3.5モデル、基本機能 |
| Plus | $20/月 | GPT-4モデル、より賢く、機能豊富 |

**おすすめ**：まず無料版で慣れてから、よく使うようなら有料版を検討。

---

## よくある質問

**Q：なぜAIの回答が遅いことがある？**
A：サーバーが混雑している可能性があります。少し待ってから再試行を。

**Q：なぜAIの回答が不正確なことがある？**
A：AIは「ハルシネーション」を起こすことがあり、存在しない情報を作り出すことがあります。重要な情報は自分で確認を。

**Q：会話は保存される？**
A：会話はサービス改善のために保存されます。個人情報は入力しないでください。

---

おめでとうございます！ChatGPTへの登録と使用を始めることができました。次は、より良いプロンプトの書き方を学んで、AIにもっと上手に手伝ってもらいましょう。
            `}},{id:"ch2-prompt-basics",title:{zh:"2.2 写好提示词的基础技巧",ja:"2.2 良いプロンプトの基本テクニック"},content:{zh:`
"提示词"（Prompt）就是你输入给 AI 的文字。**提示词写得好不好，直接决定了 AI 回答的质量**。

这就像问路一样：
- 问"附近有吃的吗？"→ 答案会很模糊
- 问"这附近 500 米内有什么评分 4.5 以上的川菜馆？"→ 答案会具体很多

## 基础技巧一：说清楚你要什么

### ❌ 不好的提示词
\`\`\`
帮我写个文章
\`\`\`

AI 会困惑：写什么主题？多长？给谁看？什么风格？

### ✅ 好的提示词
\`\`\`
帮我写一篇 800 字的文章，
主题是"如何养成早起的习惯"，
面向大学生读者，
语气轻松幽默，
要有具体可操作的建议。
\`\`\`

**记住：AI 不会读心术，你说得越清楚，它回答得越好。**

---

## 基础技巧二：给 AI 一个角色

让 AI 扮演一个特定角色，往往能得到更专业的回答。

### 示例：学习编程
\`\`\`
你是一位有 10 年经验的 Python 讲师，
擅长用简单易懂的方式教初学者。
请解释什么是"变量"。
\`\`\`

### 示例：健康咨询
\`\`\`
你是一位注册营养师，
我想减肥但不想节食，
请给我一周的健康饮食建议。
\`\`\`

### 示例：职业建议
\`\`\`
你是一位资深 HR，
帮我看看这份简历有什么可以改进的地方。
[粘贴简历内容]
\`\`\`

---

## 基础技巧三：给出例子

如果你想要特定格式或风格的输出，最好给 AI 一个例子。

### 示例：起名字
\`\`\`
帮我给一家咖啡店起名字。
风格参考：%Arabica、Manner、Blue Bottle
要求：简洁、现代、有格调
给我 10 个选择
\`\`\`

### 示例：写文案
\`\`\`
帮我写一条朋友圈文案，晒我新买的书。

风格参考："又给自己挖了一个坑，这周末哪儿也不去了。"

书名是《人类简史》
\`\`\`

---

## 基础技巧四：分步骤提问

复杂的任务，不要一次性问完，而是分步骤来。

### ❌ 一次问太多
\`\`\`
帮我写一个完整的商业计划书，包括市场分析、
竞争对手分析、财务预测、营销策略、团队介绍......
\`\`\`

### ✅ 分步骤来
\`\`\`
第一步：我想创业做一个在线教育平台，请帮我分析一下市场规模和趋势。
\`\`\`

等 AI 回答完后：
\`\`\`
第二步：基于上面的分析，请列出 5 个主要的竞争对手，并分析它们的优劣势。
\`\`\`

继续：
\`\`\`
第三步：根据以上信息，帮我设计一个差异化的定位策略。
\`\`\`

---

## 基础技巧五：追问和反馈

得到回答后，不要就结束了。你可以：

### 要求详细说明
\`\`\`
第三点能详细说说吗？
\`\`\`

### 要求换个角度
\`\`\`
能从用户的角度重新分析一下吗？
\`\`\`

### 要求调整风格
\`\`\`
这个回答太正式了，能用更轻松的语气重写吗？
\`\`\`

### 表达不满意
\`\`\`
这不是我想要的。我想要的是......
\`\`\`

---

## 进阶技巧：STAR 提示词框架

当你需要写复杂的提示词时，可以使用 **STAR 框架** 来组织：

| 要素 | 英文 | 含义 | 示例 |
|------|------|------|------|
| **S** | Style | 风格 | 专业报告风格 / 轻松对话风格 |
| **T** | Tone | 语气 | 正式、友好、幽默、严肃 |
| **A** | Audience | 受众 | 给小学生 / 给专业人士 |
| **R** | Response | 输出格式 | 列表、表格、分段落 |

### STAR 框架示例

\`\`\`
请帮我写一篇关于"健康饮食"的文章。

【风格】科普文章风格，像果壳网的文章
【语气】轻松有趣，但内容严谨
【受众】25-35岁的上班族
【格式】分3-4个小节，每节有小标题，
       文末有"划重点"总结，约1000字
\`\`\`

---

## 进阶技巧：思维链（Chain of Thought）

让 AI "一步步思考"，能显著提高复杂问题的回答质量。

### ❌ 普通提问
\`\`\`
小明有5个苹果，给了小红2个，又买了3个，问现在有几个？
\`\`\`

### ✅ 使用思维链
\`\`\`
小明有5个苹果，给了小红2个，又买了3个，问现在有几个？

请一步步分析：
1. 首先，小明原来有多少？
2. 然后，发生了什么变化？
3. 最后，计算结果
\`\`\`

### 更实用的思维链示例
\`\`\`
我想转行做产品经理，请帮我分析可行性。

请按以下步骤思考：
1. 产品经理需要哪些核心能力？
2. 我目前的背景（程序员3年）有哪些可迁移的技能？
3. 我还缺少什么？
4. 给出具体的学习和准备建议
\`\`\`

---

## 进阶技巧：少样本学习（Few-shot Learning）

给 AI 几个例子，让它学习你想要的模式。

### 示例：生成特定格式的内容
\`\`\`
请按照以下格式，给"时间管理"这个话题写3条金句：

示例格式：
📌 关于拖延：拖延不是懒，是对困难任务的逃避。
📌 关于专注：专注不是同时做很多事，而是不做其他事。

现在请写关于"时间管理"的：
\`\`\`

### 示例：学习特定的写作风格
\`\`\`
请学习以下文案风格，然后为新产品写一段：

【示例1】
产品：无线耳机
文案：告别线的束缚，让音乐如影随形。

【示例2】
产品：智能手表
文案：腕间方寸，掌控全天节奏。

【请写】
产品：便携投影仪
文案：
\`\`\`

---

## 本节要点

| 技巧 | 什么时候用 |
|------|-----------|
| 说清楚要什么 | 每次都用 |
| 给 AI 角色 | 需要专业回答时 |
| 给出例子 | 需要特定格式时 |
| 分步骤问 | 复杂任务时 |
| 追问反馈 | 不满意时 |
| STAR 框架 | 写复杂提示词时 |
| 思维链 | 推理分析问题时 |
| 少样本学习 | 需要特定风格时 |

---

## 小练习

现在，试试用上面学到的技巧，改写这个提示词：

**原始版本：**
\`\`\`
帮我写封邮件
\`\`\`

**你的改进版本：**
（试着加上：目的、收件人、语气、具体内容、长度要求等）

---

掌握这些基础技巧后，你已经能比大多数人更好地使用 AI 了。接下来，让我们看看如何用 AI 生成图片。
            `,ja:`
「プロンプト」とは、AIに入力するテキストのことです。**プロンプトの書き方が、AIの回答の質を直接決定します**。

道を尋ねるのと同じです：
- 「近くに食べ物ある？」→ 答えは曖昧
- 「ここから500m以内で、評価4.5以上の四川料理店はある？」→ 答えは具体的

## 基本テクニック1：何が欲しいか明確に

### ❌ 悪いプロンプト
\`\`\`
記事を書いて
\`\`\`

AIは困惑します：何のテーマ？長さは？誰向け？どんなスタイル？

### ✅ 良いプロンプト
\`\`\`
800字の記事を書いてください。
テーマは「早起きの習慣を身につける方法」、
大学生読者向け、
カジュアルでユーモアのある口調で、
具体的で実践可能なアドバイスを含めて。
\`\`\`

**覚えておいて：AIは心を読めません。明確に伝えるほど、良い回答が得られます。**

---

## 基本テクニック2：AIに役割を与える

AIに特定の役割を演じさせると、より専門的な回答が得られることが多いです。

### 例：プログラミング学習
\`\`\`
あなたは10年の経験を持つPython講師で、
初心者にわかりやすく教えるのが得意です。
「変数」とは何か説明してください。
\`\`\`

### 例：健康相談
\`\`\`
あなたは管理栄養士です。
私はダイエットしたいけど食事制限はしたくありません。
1週間の健康的な食事プランを提案してください。
\`\`\`

### 例：キャリアアドバイス
\`\`\`
あなたはベテランの人事担当者です。
この履歴書の改善点を教えてください。
[履歴書を貼り付け]
\`\`\`

---

## 基本テクニック3：例を示す

特定のフォーマットやスタイルの出力が欲しい場合、AIに例を示すのが効果的です。

### 例：名前を考える
\`\`\`
カフェの名前を考えてください。
スタイル参考：%Arabica、Blue Bottle、Starbucks Reserve
要件：シンプル、モダン、おしゃれ
10個の選択肢をください
\`\`\`

### 例：SNS投稿
\`\`\`
新しく買った本についてのInstagram投稿文を書いて。

スタイル参考：「また自分に課題を作ってしまった。今週末はどこにも行かない。」

本のタイトルは『サピエンス全史』
\`\`\`

---

## 基本テクニック4：段階的に質問する

複雑なタスクは一度に聞かず、段階的に進めましょう。

### ❌ 一度に多すぎる
\`\`\`
完全なビジネスプランを書いて、市場分析、
競合分析、財務予測、マーケティング戦略、チーム紹介...
\`\`\`

### ✅ 段階的に
\`\`\`
ステップ1：オンライン教育プラットフォームで起業したいです。市場規模とトレンドを分析してください。
\`\`\`

AIの回答後：
\`\`\`
ステップ2：上記の分析に基づいて、主な競合5社をリストアップし、長所と短所を分析してください。
\`\`\`

続けて：
\`\`\`
ステップ3：これらの情報を基に、差別化のポジショニング戦略を設計してください。
\`\`\`

---

## 基本テクニック5：フォローアップとフィードバック

回答を得た後、そこで終わりにしないでください：

### 詳細を求める
\`\`\`
3番目のポイントをもっと詳しく説明してもらえますか？
\`\`\`

### 別の視点を求める
\`\`\`
ユーザーの視点から再分析してもらえますか？
\`\`\`

### スタイルの調整を求める
\`\`\`
この回答は堅すぎます。もっとカジュアルなトーンで書き直してもらえますか？
\`\`\`

### 不満を表現する
\`\`\`
これは私が求めていたものではありません。私が欲しいのは...
\`\`\`

---

## 上級テクニック：STAR プロンプトフレームワーク

複雑なプロンプトを書く際は、**STAR フレームワーク**で整理できます：

| 要素 | 英語 | 意味 | 例 |
|------|------|------|------|
| **S** | Style | スタイル | 専門レポート / カジュアルな会話 |
| **T** | Tone | トーン | フォーマル、フレンドリー、ユーモラス |
| **A** | Audience | 読者 | 小学生向け / 専門家向け |
| **R** | Response | 出力形式 | リスト、表、段落分け |

### STAR フレームワークの例

\`\`\`
「健康的な食事」についての記事を書いてください。

【スタイル】科学記事風、専門的だが読みやすい
【トーン】軽快で面白いが、内容は正確に
【読者】25-35歳のオフィスワーカー
【形式】3-4セクションに分け、各セクションに見出し、
       最後に「まとめ」、約1000字
\`\`\`

---

## 上級テクニック：思考の連鎖（Chain of Thought）

AIに「ステップバイステップで考えて」もらうと、複雑な質問への回答品質が大幅に向上します。

### ❌ 通常の質問
\`\`\`
太郎は5個のりんごを持っていて、花子に2個あげて、また3個買いました。今いくつ持っていますか？
\`\`\`

### ✅ 思考の連鎖を使用
\`\`\`
太郎は5個のりんごを持っていて、花子に2個あげて、また3個買いました。今いくつ持っていますか？

ステップバイステップで分析してください：
1. まず、太郎は最初にいくつ持っていた？
2. 次に、何が起きた？
3. 最後に、結果を計算
\`\`\`

### より実用的な思考の連鎖の例
\`\`\`
プロダクトマネージャーに転職したいのですが、可能性を分析してください。

以下の順序で考えてください：
1. プロダクトマネージャーに必要なコアスキルは？
2. 現在の私の経歴（エンジニア3年）から移転可能なスキルは？
3. 何が足りない？
4. 具体的な学習と準備のアドバイス
\`\`\`

---

## 上級テクニック：少数サンプル学習（Few-shot Learning）

AIにいくつかの例を見せて、望むパターンを学習させます。

### 例：特定フォーマットのコンテンツ生成
\`\`\`
以下の形式で「タイムマネジメント」について3つの名言を書いてください：

形式の例：
📌 先延ばしについて：先延ばしは怠惰ではなく、難しいタスクからの逃避。
📌 集中について：集中とは多くのことを同時にやることではなく、他のことをやらないこと。

では「タイムマネジメント」について書いてください：
\`\`\`

### 例：特定のライティングスタイルを学習
\`\`\`
以下のコピーライティングスタイルを学習し、新製品用に書いてください：

【例1】
製品：ワイヤレスイヤホン
コピー：ケーブルの束縛を解き放ち、音楽と共に自由に。

【例2】
製品：スマートウォッチ
コピー：手首の小さな空間で、一日のリズムを掌握。

【書いてください】
製品：ポータブルプロジェクター
コピー：
\`\`\`

---

## このセクションのポイント

| テクニック | いつ使う？ |
|------|-----------|
| 明確に伝える | 毎回 |
| AIに役割を与える | 専門的な回答が必要な時 |
| 例を示す | 特定の形式が必要な時 |
| ステップで質問 | 複雑なタスクの時 |
| フォローアップ | 満足できない時 |
| STAR フレームワーク | 複雑なプロンプトを書く時 |
| 思考の連鎖 | 推論・分析問題の時 |
| 少数サンプル学習 | 特定のスタイルが必要な時 |

---

## 練習問題

さて、学んだテクニックを使って、このプロンプトを改善してみましょう：

**元のバージョン：**
\`\`\`
メールを書いて
\`\`\`

**あなたの改善版：**
（目的、宛先、トーン、具体的な内容、長さなどを追加してみて）

---

これらの基本テクニックをマスターすれば、ほとんどの人よりも上手にAIを使えるようになります。次は、実際の例を見てみましょう。
            `}},{id:"ch2-real-examples",title:{zh:"2.3 实战案例：AI 对话示范",ja:"2.3 実践例：AIとの会話デモ"},content:{zh:`
学习了基础技巧后，让我们看看一些**真实的 AI 对话案例**。这些例子展示了如何在日常生活和工作中有效使用 AI。

---

## 案例一：让 AI 帮你写邮件

### 场景
你需要给客户写一封道歉邮件，因为项目延期了。

### 对话过程

**你：**
\`\`\`
我需要给客户写一封邮件，解释项目延期的情况。
背景：
- 原定交付日期是 1 月 15 日
- 现在预计要延期到 1 月 25 日
- 原因是技术团队发现了一个重要的安全漏洞需要修复
- 客户是我们的重要合作伙伴
- 语气要专业但真诚

请帮我写这封邮件。
\`\`\`

**AI 的回复：**
\`\`\`
主题：关于项目交付时间调整的说明

尊敬的 [客户名称]：

感谢您一直以来对我们的信任与支持。

在此，我需要就项目交付时间向您做一个重要说明。
原定于 1 月 15 日交付的项目，现需延期至 1 月 25 日。

在最终测试阶段，我们的技术团队发现了一个
潜在的安全隐患。为确保交付给您的产品达到
最高的安全标准，我们决定优先处理这一问题。

我们深知这一调整可能给您带来不便，对此
深表歉意。为弥补延期造成的影响，我们将：
1. 提供每日进度更新
2. 延期期间的额外技术支持免费提供

如您有任何疑问，请随时联系我。
再次感谢您的理解与支持。

此致
敬礼
[你的名字]
\`\`\`

**你继续追问：**
\`\`\`
很好！能把语气改得更轻松一些吗？
这个客户和我们关系很熟。
\`\`\`

**AI 调整后：**
\`\`\`
Hi [客户名称]，

先说声抱歉！原定 15 号交付的项目要推迟到 25 号了。

原因是这样的：我们在最后测试时发现了一个安全漏洞，
虽然不是什么大问题，但本着对您负责的态度，
我们决定先把它修好再交付。

我知道延期肯定会给你们带来一些不便，
这几天我会保持每天更新进度，有任何问题随时找我！

感谢理解～

[你的名字]
\`\`\`

### 学到的技巧
1. **提供充足背景信息** —— AI 不知道你的具体情况
2. **可以多轮对话调整** —— 不满意就继续要求修改
3. **明确语气要求** —— "专业"和"轻松"差别很大

---

## 案例二：让 AI 帮你学习新概念

### 场景
你想了解什么是"区块链"，但维基百科的解释太复杂了。

### 对话过程

**你：**
\`\`\`
请用一个 10 岁小朋友能听懂的方式，
解释什么是区块链。
用生活中的例子来说明。
\`\`\`

**AI 的回复：**
\`\`\`
想象你和同学们在玩一个特别的记账游戏。

**普通的记账方式：**
班长一个人拿着账本，记录谁借了谁的橡皮、
谁给了谁零食。但问题是：
- 班长可能记错
- 班长可能偷偷改账本
- 账本丢了就全完了

**区块链的方式：**
每个同学都有一本一模一样的账本！

当小明借了小红一块橡皮时：
1. 小明大声宣布："我借了小红的橡皮！"
2. 所有同学都在自己的账本上记下这件事
3. 每个人的账本都要和其他人的对得上

这样有什么好处？
✅ 没有人能偷偷改账——因为要改就得改所有人的
✅ 账本不会丢——每个人都有备份
✅ 所有记录都是公开透明的

这就是区块链的基本原理！
区块链就是"大家一起记账的账本"。
\`\`\`

**你继续深入：**
\`\`\`
那比特币和区块链是什么关系？
\`\`\`

**AI 继续解释：**
\`\`\`
还是用记账本的例子：

区块链 = 那个"大家一起记"的账本方法
比特币 = 用这个方法记的"数字金币"

就像：
- 纸 = 一种写字的材料
- 日记 = 用纸写的一种东西

区块链是技术，比特币是用这个技术创造的
第一个产品。

后来人们发现，这个技术不只能记"谁有多少钱"，
还能记录：
- 谁拥有哪幅数字画（NFT）
- 合同条款是否被执行（智能合约）
- 商品从哪里来（供应链追踪）

所以区块链的用途远比比特币广泛！
\`\`\`

### 学到的技巧
1. **指定解释的难度级别** —— "10岁小朋友能懂"
2. **要求用类比** —— 抽象概念用具体例子更好理解
3. **循序渐进深入** —— 先懂基础再问进阶

---

## 案例三：让 AI 帮你解决代码问题

### 场景
你写的 Python 代码报错了，看不懂错误信息。

### 对话过程

**你：**
\`\`\`
我的 Python 代码报错了，帮我看看：

代码：
numbers = [1, 2, 3, 4, 5]
for i in range(6):
    print(numbers[i])

错误：
IndexError: list index out of range

这是什么意思？怎么修复？
\`\`\`

**AI 的回复：**
\`\`\`
这个错误是"索引超出范围"的意思。让我解释一下：

**问题原因：**
- 你的列表 numbers 有 5 个元素
- 索引是从 0 开始的，所以有效索引是 0, 1, 2, 3, 4
- 但 range(6) 会产生 0, 1, 2, 3, 4, 5
- 当 i=5 时，numbers[5] 不存在，就报错了

**图示：**
\`\`\`
索引:     0    1    2    3    4    ❌5
       ┌────┬────┬────┬────┬────┐
列表:  │ 1  │ 2  │ 3  │ 4  │ 5  │  没有了！
       └────┴────┴────┴────┴────┘
\`\`\`

**修复方法（三选一）：**

方法1：用列表长度
\`\`\`python
for i in range(len(numbers)):
    print(numbers[i])
\`\`\`

方法2：直接遍历（推荐）
\`\`\`python
for num in numbers:
    print(num)
\`\`\`

方法3：用 enumerate
\`\`\`python
for i, num in enumerate(numbers):
    print(f"索引{i}: {num}")
\`\`\`

建议使用方法2，更 Pythonic！
\`\`\`

### 学到的技巧
1. **提供完整信息** —— 代码 + 错误信息都要给
2. **AI 擅长解释错误** —— 比搜索引擎更直接
3. **可以要求多种解决方案** —— 选最适合你的

---

## 案例四：让 AI 帮你制定计划

### 场景
你想在 3 个月内学会基础日语。

### 对话过程

**你：**
\`\`\`
我想在 3 个月内学会基础日语，
目标是能进行简单的日常对话。

我的情况：
- 完全零基础
- 每天能学习 1 小时
- 周末可以多学 2 小时

请帮我制定一个学习计划。
\`\`\`

**AI 的回复：**
\`\`\`
# 3 个月日语学习计划

## 第一个月：基础入门

### 第 1-2 周：五十音
- 每天学习 10 个假名
- 平假名优先，再学片假名
- 推荐 App：多邻国、日语五十音图

### 第 3-4 周：基础句型
- 自我介绍句型
- 数字 1-100
- 常用问候语
- 推荐资源：《标准日本语》第 1-3 课

## 第二个月：词汇积累

### 目标：掌握 300 个常用词

**每周主题：**
- 第 1 周：食物、饮料
- 第 2 周：时间、日期
- 第 3 周：交通、位置
- 第 4 周：购物、数量

### 学习方法
- 每天 10 个新词
- 用 Anki 做间隔复习
- 周末看 10 分钟日语动画（带字幕）

## 第三个月：会话练习

### 目标：能进行 3 分钟对话

**练习场景：**
- 餐厅点餐
- 问路
- 自我介绍
- 购物

### 学习方法
- 找语言交换伙伴（HelloTalk）
- 每天跟读 15 分钟
- 周末尝试用日语写日记

## 每日时间分配（1小时）

| 时间 | 内容 |
|------|------|
| 15分钟 | 复习昨天内容 |
| 30分钟 | 学习新内容 |
| 15分钟 | 听力/口语练习 |

## 检验标准

3 个月后你应该能够：
✅ 认读所有假名
✅ 掌握 300+ 词汇
✅ 用日语做简单自我介绍
✅ 在餐厅用日语点餐
✅ 理解简单的日语对话
\`\`\`

### 学到的技巧
1. **说明你的限制条件** —— 时间、基础等
2. **明确具体目标** —— "简单日常对话"比"学日语"更清晰
3. **AI 擅长结构化规划** —— 但执行要靠你自己

---

## 小结

通过这些案例，你应该感受到了：

1. **AI 是很好的助手** —— 写作、学习、编程、规划
2. **关键是提问的方式** —— 背景越清晰，答案越好
3. **对话可以持续深入** —— 不满意就继续追问
4. **AI 不是万能的** —— 需要你来判断和执行

现在，打开 ChatGPT，试着用这些技巧解决你自己的问题吧！
            `,ja:`
基本テクニックを学んだところで、**実際のAI会話例**を見てみましょう。これらの例は、日常生活や仕事でAIを効果的に使う方法を示しています。

---

## ケース1：AIにメールを書いてもらう

### シナリオ
プロジェクトが遅延したため、クライアントにお詫びのメールを書く必要があります。

### 会話の流れ

**あなた：**
\`\`\`
クライアントにプロジェクト遅延を説明するメールを書く必要があります。
背景：
- 元の納期は1月15日
- 現在は1月25日に延期予定
- 理由：技術チームが重要なセキュリティ脆弱性を発見し、修正が必要
- クライアントは重要なパートナー
- トーンはプロフェッショナルだが誠実に

このメールを書いてください。
\`\`\`

**AIの返答：**
\`\`\`
件名：プロジェクト納期変更のお知らせ

[クライアント名] 様

いつもお世話になっております。

プロジェクトの納期について重要なお知らせがございます。
当初1月15日を予定しておりました納品日を、
1月25日に変更させていただきたくご連絡いたしました。

最終テスト段階において、技術チームが
セキュリティ上の潜在的な問題を発見いたしました。
最高品質の製品をお届けするため、
この問題の解決を優先することといたしました。

ご不便をおかけし、誠に申し訳ございません。
遅延の影響を軽減するため、以下を実施いたします：
1. 毎日の進捗報告
2. 遅延期間中の追加技術サポートを無償提供

ご不明な点がございましたら、お気軽にご連絡ください。
ご理解いただけますと幸いです。

敬具
[あなたの名前]
\`\`\`

**あなたの追加リクエスト：**
\`\`\`
いいですね！もう少しカジュアルなトーンにできますか？
このクライアントとは親しい関係なので。
\`\`\`

**AI調整後：**
\`\`\`
[クライアント名] さん

お疲れ様です！
まずはお詫びから—15日納品予定のプロジェクト、
25日まで延期させてください。

実は最終テストでセキュリティの穴を見つけまして、
大きな問題ではないんですが、ちゃんと直してから
お渡ししたいと思っています。

ご迷惑おかけして申し訳ないです。
毎日進捗をお伝えするので、何かあればいつでも
連絡ください！

[あなたの名前]
\`\`\`

### 学んだテクニック
1. **十分な背景情報を提供** —— AIはあなたの状況を知らない
2. **複数回の会話で調整可能** —— 満足いかなければ修正を依頼
3. **トーンを明確に** —— 「プロフェッショナル」と「カジュアル」は大違い

---

## ケース2：AIに新しい概念を教えてもらう

### シナリオ
「ブロックチェーン」について知りたいが、Wikipediaの説明が複雑すぎる。

### 会話の流れ

**あなた：**
\`\`\`
10歳の子供でもわかるように、
ブロックチェーンとは何か説明してください。
日常生活の例を使って。
\`\`\`

**AIの返答：**
\`\`\`
クラスのみんなで特別な記帳ゲームをしていると想像してください。

**普通の記帳方法：**
学級委員が一人で帳簿を持って、誰が誰に消しゴムを
貸したか、誰が誰にお菓子をあげたか記録します。
でも問題があります：
- 学級委員が間違えるかも
- 学級委員がこっそり帳簿を書き換えるかも
- 帳簿をなくしたら全部おしまい

**ブロックチェーンの方法：**
全員が同じ帳簿のコピーを持っています！

太郎くんが花子さんから消しゴムを借りたとき：
1. 太郎くんが大声で「花子さんから消しゴム借りた！」と言う
2. みんなが自分の帳簿にこれを書き込む
3. 全員の帳簿が一致していないといけない

これの何がいいの？
✅ 誰もこっそり書き換えられない——全員分を変えないといけないから
✅ 帳簿をなくさない——みんながバックアップを持っている
✅ すべての記録が公開で透明

これがブロックチェーンの基本原理です！
ブロックチェーンは「みんなで一緒に記帳する帳簿」なのです。
\`\`\`

### 学んだテクニック
1. **説明の難易度を指定** —— 「10歳の子供でもわかる」
2. **たとえを要求** —— 抽象的な概念は具体例でわかりやすく
3. **段階的に深掘り** —— 基礎を理解してから応用へ

---

## ケース3：AIにコードの問題を解決してもらう

### シナリオ
書いたPythonコードがエラーになり、エラーメッセージが理解できない。

### 会話の流れ

**あなた：**
\`\`\`
Pythonコードがエラーになりました。見てください：

コード：
numbers = [1, 2, 3, 4, 5]
for i in range(6):
    print(numbers[i])

エラー：
IndexError: list index out of range

これはどういう意味？どう直せばいい？
\`\`\`

**AIの返答：**
\`\`\`
このエラーは「インデックスが範囲外」という意味です。説明しますね：

**問題の原因：**
- リスト numbers には5つの要素がある
- インデックスは0から始まるので、有効なインデックスは 0, 1, 2, 3, 4
- でも range(6) は 0, 1, 2, 3, 4, 5 を生成する
- i=5 のとき、numbers[5] は存在しないのでエラー

**図示：**
\`\`\`
インデックス:  0    1    2    3    4    ❌5
           ┌────┬────┬────┬────┬────┐
リスト:    │ 1  │ 2  │ 3  │ 4  │ 5  │  ない！
           └────┴────┴────┴────┴────┘
\`\`\`

**修正方法（3つから選んで）：**

方法1：リストの長さを使う
\`\`\`python
for i in range(len(numbers)):
    print(numbers[i])
\`\`\`

方法2：直接イテレート（おすすめ）
\`\`\`python
for num in numbers:
    print(num)
\`\`\`

方法3：enumerate を使う
\`\`\`python
for i, num in enumerate(numbers):
    print(f"インデックス{i}: {num}")
\`\`\`

方法2がおすすめです。よりPythonicです！
\`\`\`

### 学んだテクニック
1. **完全な情報を提供** —— コードとエラーメッセージ両方
2. **AIはエラー説明が得意** —— 検索エンジンより直接的
3. **複数の解決策を求める** —— 自分に合ったものを選ぶ

---

## ケース4：AIに計画を立ててもらう

### シナリオ
3ヶ月で基礎的な英語を学びたい。

### 会話の流れ

**あなた：**
\`\`\`
3ヶ月で基礎的な英語を学びたいです。
目標は簡単な日常会話ができること。

私の状況：
- 完全な初心者
- 毎日1時間学習できる
- 週末はさらに2時間追加可能

学習計画を作ってください。
\`\`\`

**AIの返答：**
\`\`\`
# 3ヶ月英語学習プラン

## 第1月：基礎入門

### 第1-2週：基本フレーズ
- 挨拶と自己紹介
- 数字 1-100
- 曜日と月
- 推奨アプリ：Duolingo

### 第3-4週：基本文法
- be動詞
- 現在形
- 疑問文の作り方

## 第2月：語彙の蓄積

### 目標：300語の習得

**週ごとのテーマ：**
- 第1週：食べ物、飲み物
- 第2週：時間、日付
- 第3週：交通、場所
- 第4週：買い物、数量

### 学習方法
- 毎日10個の新単語
- Ankiで間隔反復
- 週末に10分間の英語動画視聴

## 第3月：会話練習

### 目標：3分間の会話ができる

**練習シナリオ：**
- レストランで注文
- 道を尋ねる
- 自己紹介
- 買い物

## 毎日の時間配分（1時間）

| 時間 | 内容 |
|------|------|
| 15分 | 昨日の復習 |
| 30分 | 新しい内容の学習 |
| 15分 | リスニング/スピーキング練習 |
\`\`\`

### 学んだテクニック
1. **制約条件を伝える** —— 時間、レベルなど
2. **具体的な目標を明確に** —— 「簡単な日常会話」は「英語を学ぶ」より明確
3. **AIは構造化された計画が得意** —— でも実行はあなた次第

---

## まとめ

これらの例を通じて、感じ取れたはずです：

1. **AIは優秀なアシスタント** —— ライティング、学習、コーディング、計画
2. **質問の仕方がカギ** —— 背景が明確なほど、回答も良くなる
3. **会話は継続して深掘りできる** —— 満足いかなければ追加質問
4. **AIは万能ではない** —— 判断と実行はあなた次第

さあ、ChatGPTを開いて、これらのテクニックで自分の問題を解決してみましょう！
            `}},{id:"ch2-image-generation",title:{zh:"2.4 用 AI 生成图片",ja:"2.4 AIで画像を生成する"},content:{zh:`
除了文字对话，AI 还能帮你**创作图片**。即使你完全不会画画，也能用 AI 生成各种风格的图像。

---

## 主流 AI 绘画工具

### 1. DALL-E 3（推荐新手）

**开发者：** OpenAI（和 ChatGPT 同一家公司）

**特点：**
- 直接在 ChatGPT 中使用
- 理解自然语言描述能力强
- 生成的图片质量高
- 免费用户每天有使用次数限制

**使用方式：**
在 ChatGPT 中直接输入：
\`\`\`
帮我画一只穿着宇航服的柴犬，在月球上行走，
背景是地球，卡通风格
\`\`\`

### 2. Midjourney（最受艺术家欢迎）

**特点：**
- 艺术性最强，画风精美
- 需要通过 Discord 使用
- 付费服务（约 $10/月起）
- 学习曲线稍陡

**适合：** 追求艺术品质的用户

### 3. Stable Diffusion（开源免费）

**特点：**
- 完全免费开源
- 可以在自己电脑上运行
- 高度可定制
- 需要一定技术基础

**适合：** 技术爱好者、有显卡的用户

---

## 动手试试：在 ChatGPT 中生成图片

### 第一步：打开 ChatGPT

访问 chat.openai.com，登录你的账号。

### 第二步：输入图片描述

尝试输入以下提示词：

**示例 1：简单描述**
\`\`\`
画一只可爱的橘猫，正在睡觉
\`\`\`

**示例 2：详细描述（效果更好）**
\`\`\`
画一只橘色的猫咪，
正蜷缩在窗台上睡觉，
阳光透过窗户照在它身上，
温馨的室内场景，
水彩画风格
\`\`\`

### 第三步：查看和调整

AI 会生成图片。如果不满意，你可以：
- "背景换成蓝色的"
- "让猫咪睁开眼睛"
- "换成油画风格"

---

## 写好图片提示词的技巧

### 基本公式

\`\`\`
[主体] + [动作/状态] + [环境/背景] + [风格] + [其他细节]
\`\`\`

### 示例对比

**❌ 模糊的提示词：**
\`\`\`
画一个女孩
\`\`\`

**✅ 详细的提示词：**
\`\`\`
一个亚洲女孩，
长发飘逸，穿着白色连衣裙，
站在樱花树下，
花瓣飘落，
日系动漫风格，
柔和的光线，
高清细节
\`\`\`

### 常用风格关键词

| 风格 | 关键词 |
|------|--------|
| 照片写实 | realistic, photorealistic, 8K, detailed |
| 动漫风格 | anime style, manga, 日系 |
| 油画 | oil painting, impressionist |
| 水彩 | watercolor, soft colors |
| 卡通 | cartoon, pixar style, 3D |
| 像素风 | pixel art, 8-bit, retro |
| 赛博朋克 | cyberpunk, neon, futuristic |

---

## 图片生成的局限性

### AI 不擅长的：
- ❌ 精确的文字（经常出错）
- ❌ 准确的手指数量（可能多或少）
- ❌ 复杂的空间关系
- ❌ 特定真实人物的肖像

### 使用建议：
- ✅ 用于创意启发和概念设计
- ✅ 生成插图、背景、素材
- ✅ 快速可视化想法
- ⚠️ 商用前注意版权问题

---

现在就去试试吧！从一个简单的描述开始，慢慢学会控制 AI 的创作方向。
            `,ja:`
テキストチャットだけでなく、AIは**画像を作成**することもできます。絵が全く描けなくても、AIを使ってさまざまなスタイルの画像を生成できます。

---

## 主流のAI画像生成ツール

### 1. DALL-E 3（初心者におすすめ）

**開発者：** OpenAI（ChatGPTと同じ会社）

**特徴：**
- ChatGPT内で直接使える
- 自然言語の理解力が高い
- 生成される画像の品質が高い
- 無料ユーザーは1日の使用回数制限あり

**使い方：**
ChatGPTで直接入力：
\`\`\`
宇宙服を着た柴犬が月面を歩いている絵を描いて、
背景に地球、カートゥーンスタイルで
\`\`\`

### 2. Midjourney（アーティストに最も人気）

**特徴：**
- 芸術性が最も高く、美しいスタイル
- Discordを通じて使用
- 有料サービス（約$10/月〜）
- 学習曲線がやや急

**向いている人：** 芸術的な品質を求めるユーザー

### 3. Stable Diffusion（オープンソース・無料）

**特徴：**
- 完全無料でオープンソース
- 自分のPCで実行可能
- 高度にカスタマイズ可能
- ある程度の技術知識が必要

**向いている人：** 技術愛好家、GPUを持っているユーザー

---

## 実践：ChatGPTで画像を生成

### ステップ1：ChatGPTを開く

chat.openai.comにアクセスし、ログイン。

### ステップ2：画像の説明を入力

以下のプロンプトを試してみてください：

**例1：シンプルな説明**
\`\`\`
かわいい茶トラ猫が寝ている絵を描いて
\`\`\`

**例2：詳細な説明（より良い結果）**
\`\`\`
オレンジ色の猫が、
窓辺で丸くなって寝ている、
窓から差し込む日光が猫を照らしている、
温かみのある室内シーン、
水彩画スタイル
\`\`\`

### ステップ3：確認と調整

AIが画像を生成します。満足いかなければ：
- 「背景を青に変えて」
- 「猫の目を開けて」
- 「油絵スタイルに変えて」

---

## 画像プロンプトのコツ

### 基本公式

\`\`\`
[主体] + [動作/状態] + [環境/背景] + [スタイル] + [その他の詳細]
\`\`\`

### 比較例

**❌ 曖昧なプロンプト：**
\`\`\`
女の子を描いて
\`\`\`

**✅ 詳細なプロンプト：**
\`\`\`
アジア人の女の子、
長い髪がなびいている、白いワンピースを着ている、
桜の木の下に立っている、
花びらが舞い落ちている、
日本のアニメスタイル、
柔らかい光、
高精細ディテール
\`\`\`

### よく使うスタイルキーワード

| スタイル | キーワード |
|------|--------|
| 写実的 | realistic, photorealistic, 8K, detailed |
| アニメ | anime style, manga, 日本風 |
| 油絵 | oil painting, impressionist |
| 水彩 | watercolor, soft colors |
| カートゥーン | cartoon, pixar style, 3D |
| ピクセル | pixel art, 8-bit, retro |
| サイバーパンク | cyberpunk, neon, futuristic |

---

## 画像生成の限界

### AIが苦手なこと：
- ❌ 正確な文字（よく間違える）
- ❌ 正確な指の数（多かったり少なかったり）
- ❌ 複雑な空間関係
- ❌ 特定の実在人物の肖像

### 使用のアドバイス：
- ✅ 創作のインスピレーションやコンセプトデザインに
- ✅ イラスト、背景、素材の生成に
- ✅ アイデアの素早い視覚化に
- ⚠️ 商用利用前に著作権を確認

---

さあ、試してみましょう！シンプルな説明から始めて、AIの創作をコントロールする方法を学んでいきましょう。
            `}},{id:"ch2-tools-comparison",title:{zh:"2.5 AI 工具大比较",ja:"2.5 AIツール比較"},content:{zh:`
市面上有很多 AI 工具，到底该选哪个？这一节帮你梳理主流工具的特点和适用场景。

---

## 对话类 AI 工具对比

| 工具 | 开发商 | 价格 | 特点 | 适合场景 |
|------|--------|------|------|----------|
| **ChatGPT** | OpenAI | 免费/Plus $20 | 最通用，生态最好 | 日常问答、写作、编程 |
| **Claude** | Anthropic | 免费/Pro $20 | 长文处理强，更谨慎 | 文档分析、学术写作 |
| **Gemini** | Google | 免费/Advanced $20 | 与 Google 服务整合 | 搜索、邮件、文档 |
| **文心一言** | 百度 | 免费 | 中文理解好，国内可用 | 中文写作、国内用户 |
| **通义千问** | 阿里 | 免费 | 中文能力强，响应快 | 中文场景、电商相关 |
| **Copilot** | 微软 | 免费/Pro $20 | 整合 Office/Bing | Office 办公、搜索 |

---

## 我该选哪个？

### 如果你是完全新手
👉 **推荐：ChatGPT**
- 最多人用，教程最多
- 功能全面，上手简单
- 免费版够用

### 如果你需要处理长文档
👉 **推荐：Claude**
- 能处理更长的文本
- 总结和分析能力强
- 回答更加谨慎可靠

### 如果你主要用中文
👉 **推荐：文心一言 或 通义千问**
- 中文理解更地道
- 国内访问稳定
- 无需翻墙

### 如果你重度使用 Google/微软服务
👉 **推荐：Gemini / Copilot**
- 与已有工具无缝整合
- 可以直接操作邮件、文档

---

## 图片生成工具对比

| 工具 | 价格 | 特点 | 适合人群 |
|------|------|------|----------|
| **DALL-E 3** | ChatGPT Plus 内含 | 简单易用，质量好 | 新手、轻度使用 |
| **Midjourney** | $10-60/月 | 艺术性强，社区活跃 | 设计师、艺术创作 |
| **Stable Diffusion** | 免费 | 开源可控，需要显卡 | 技术爱好者 |
| **Adobe Firefly** | 免费/付费 | 商用安全，整合 PS | 专业设计师 |

---

## 特定场景工具推荐

### 写代码
- **GitHub Copilot** ($10/月) - 最强代码助手
- **Cursor** (免费/付费) - AI 代码编辑器
- **ChatGPT** - 解释代码、debug

### 写作和文档
- **Notion AI** ($10/月) - 笔记和文档
- **Jasper** - 营销文案
- **Grammarly** - 语法检查

### 视频和音频
- **Runway** - AI 视频生成
- **ElevenLabs** - AI 语音克隆
- **Suno** - AI 音乐生成

### 效率工具
- **Perplexity** - AI 搜索引擎
- **Otter.ai** - 会议记录转写
- **Tome** - AI 做 PPT

---

## 实用建议

### 1. 从免费开始
大多数工具都有免费版，先试用再决定付费。

### 2. 不要贪多
选 1-2 个工具深入学习，比什么都试一点强。

### 3. 关注更新
AI 工具更新很快，定期了解新功能。

### 4. 保持批判
AI 会犯错，重要内容要自己核实。

---

## 小结

没有"最好"的 AI 工具，只有"最适合你"的工具。

建议的入门路径：
1. **第一步**：用 ChatGPT 熟悉 AI 对话
2. **第二步**：尝试图片生成（DALL-E 3）
3. **第三步**：根据需求探索专业工具

记住：**工具是为你服务的，不是让你为工具服务**。
            `,ja:`
市場にはたくさんのAIツールがありますが、どれを選べばいいのでしょうか？このセクションでは、主流ツールの特徴と適用シーンを整理します。

---

## 対話型AIツール比較

| ツール | 開発元 | 価格 | 特徴 | 適したシーン |
|------|--------|------|------|----------|
| **ChatGPT** | OpenAI | 無料/Plus $20 | 最も汎用的、エコシステム充実 | 日常Q&A、ライティング、コーディング |
| **Claude** | Anthropic | 無料/Pro $20 | 長文処理に強い、慎重 | 文書分析、学術執筆 |
| **Gemini** | Google | 無料/Advanced $20 | Googleサービスと統合 | 検索、メール、ドキュメント |
| **Copilot** | Microsoft | 無料/Pro $20 | Office/Bingと統合 | オフィス作業、検索 |

---

## どれを選ぶべき？

### 完全な初心者なら
👉 **おすすめ：ChatGPT**
- 最も多くの人が使用、チュートリアル豊富
- 機能が包括的、使いやすい
- 無料版で十分

### 長文ドキュメントを扱うなら
👉 **おすすめ：Claude**
- より長いテキストを処理可能
- 要約と分析能力が強い
- より慎重で信頼性の高い回答

### Google/Microsoftサービスを多用するなら
👉 **おすすめ：Gemini / Copilot**
- 既存ツールとシームレスに統合
- メールやドキュメントを直接操作可能

---

## 画像生成ツール比較

| ツール | 価格 | 特徴 | 向いている人 |
|------|------|------|----------|
| **DALL-E 3** | ChatGPT Plus に含む | シンプル、高品質 | 初心者、ライトユーザー |
| **Midjourney** | $10-60/月 | 芸術性が高い | デザイナー、アート制作 |
| **Stable Diffusion** | 無料 | オープンソース、GPU必要 | 技術愛好家 |
| **Adobe Firefly** | 無料/有料 | 商用安全、PS連携 | プロデザイナー |

---

## 特定シーン向けツール

### コーディング
- **GitHub Copilot** ($10/月) - 最強のコードアシスタント
- **Cursor** (無料/有料) - AIコードエディタ
- **ChatGPT** - コード説明、デバッグ

### ライティングとドキュメント
- **Notion AI** ($10/月) - ノートとドキュメント
- **Jasper** - マーケティングコピー
- **Grammarly** - 文法チェック

### 動画と音声
- **Runway** - AI動画生成
- **ElevenLabs** - AI音声クローン
- **Suno** - AI音楽生成

### 生産性ツール
- **Perplexity** - AI検索エンジン
- **Otter.ai** - 会議の文字起こし
- **Tome** - AIでプレゼン作成

---

## 実用的なアドバイス

### 1. 無料から始める
ほとんどのツールに無料版があります。試してから有料を検討。

### 2. 欲張らない
1-2個のツールを深く学ぶ方が、すべてを浅くより効果的。

### 3. アップデートに注目
AIツールは急速に進化。定期的に新機能をチェック。

### 4. 批判的思考を保つ
AIは間違える。重要な内容は自分で確認。

---

## まとめ

「最高の」AIツールはありません。あるのは「あなたに最適な」ツールだけ。

おすすめの入門ステップ：
1. **ステップ1**：ChatGPTでAI対話に慣れる
2. **ステップ2**：画像生成を試す（DALL-E 3）
3. **ステップ3**：ニーズに応じて専門ツールを探索

覚えておいてください：**ツールはあなたのためにあるのであって、あなたがツールのためにあるのではありません**。
            `}},{id:"ch2-summary",title:{zh:"2.6 本章小结",ja:"2.6 この章のまとめ"},content:{zh:`
恭喜你完成了第二章的学习！让我们回顾一下这一章学到的关键内容。

---

## 核心要点

### 1. ChatGPT 使用
- 访问 chat.openai.com 注册
- 在输入框输入问题，按回车发送
- 可以多轮对话，不断深入

### 2. 提示词技巧
| 技巧 | 要点 |
|------|------|
| 说清楚 | 提供背景、目标、格式要求 |
| 给角色 | 让 AI 扮演专家，回答更专业 |
| 给例子 | 展示期望的输出格式 |
| 分步骤 | 复杂任务拆成小步 |
| 追问 | 不满意就继续要求调整 |

### 3. 图片生成
- DALL-E 3 最适合新手
- 描述越详细，效果越好
- 包含：主体 + 动作 + 环境 + 风格

### 4. 工具选择
- 新手首选 ChatGPT
- 根据需求选择专业工具
- 从免费版开始尝试

---

## 你现在会的技能

完成这一章后，你应该能够：

✅ 注册并使用 ChatGPT
✅ 写出清晰有效的提示词
✅ 用 AI 帮你写邮件、学概念、改代码
✅ 用 AI 生成简单的图片
✅ 了解主流 AI 工具的区别

---

## 下一章预告

第三章我们将学习 **AI 使用的进阶技巧**：

- 如何让 AI 输出更稳定
- 复杂任务的拆解方法
- AI 的常见问题和解决方案
- 如何识别 AI 的错误

准备好进阶了吗？继续往下读！
            `,ja:`
第2章の学習、おめでとうございます！この章で学んだ重要なポイントを振り返りましょう。

---

## 核心ポイント

### 1. ChatGPTの使用
- chat.openai.comで登録
- 入力欄に質問を入力してEnterで送信
- 複数回の会話で深掘り可能

### 2. プロンプトテクニック
| テクニック | ポイント |
|------|------|
| 明確に | 背景、目標、フォーマットを提供 |
| 役割を与える | AIに専門家を演じさせる |
| 例を示す | 期待する出力形式を見せる |
| ステップ分け | 複雑なタスクを小さく分割 |
| フォローアップ | 満足いかなければ調整を依頼 |

### 3. 画像生成
- DALL-E 3が初心者に最適
- 詳細な説明ほど良い結果
- 含める：主体 + 動作 + 環境 + スタイル

### 4. ツール選択
- 初心者はまずChatGPT
- ニーズに応じて専門ツールを選択
- 無料版から試す

---

## 習得したスキル

この章を終えて、できるようになったこと：

✅ ChatGPTの登録と使用
✅ 明確で効果的なプロンプトの作成
✅ AIでメール作成、概念学習、コード修正
✅ AIで簡単な画像を生成
✅ 主流AIツールの違いを理解

---

## 次章の予告

第3章では**AIの上級テクニック**を学びます：

- AIの出力をより安定させる方法
- 複雑なタスクの分解方法
- AIのよくある問題と解決策
- AIの間違いを見分ける方法

レベルアップの準備はできましたか？続きを読みましょう！
            `}}]},{id:"chapter-3",number:3,title:{zh:"AI 进阶技巧",ja:"AI上級テクニック"},subtitle:{zh:"从会用到用得好",ja:"使えるから上手に使えるへ"},sections:[{id:"ch3-intro",title:{zh:"引言：从入门到精通",ja:"序章：入門から熟練へ"},content:{zh:`
你已经学会了 AI 的基础用法，现在是时候**进阶**了。

这一章会教你一些高级技巧，让你的 AI 使用效率翻倍。同时，我们也会讨论 AI 的局限性，帮你避开常见的坑。

---

## 本章你将学到

1. **让 AI 输出更稳定的方法** —— 减少随机性
2. **复杂任务的拆解技巧** —— 大事化小
3. **AI 常见问题及解决方案** —— 避坑指南
4. **如何识别 AI 的错误** —— 保持警惕

---

## 谁适合这一章？

如果你：
- ✅ 已经会用 ChatGPT 等工具
- ✅ 想让 AI 回答更准确、更稳定
- ✅ 遇到过 AI "胡说八道"的情况
- ✅ 想用 AI 处理更复杂的任务

那这一章就是为你准备的。

让我们开始吧！
            `,ja:`
AIの基本的な使い方を学んだので、今度は**レベルアップ**の時です。

この章では、AI活用の効率を倍増させる高度なテクニックを教えます。同時に、AIの限界についても議論し、よくある落とし穴を避ける手助けをします。

---

## この章で学ぶこと

1. **AIの出力を安定させる方法** —— ランダム性を減らす
2. **複雑なタスクの分解テクニック** —— 大きな問題を小さく
3. **AIのよくある問題と解決策** —— 落とし穴回避ガイド
4. **AIの間違いを見分ける方法** —— 警戒心を持つ

---

## この章に向いている人

もしあなたが：
- ✅ ChatGPTなどのツールを使えるようになった
- ✅ AIの回答をより正確で安定させたい
- ✅ AIが「でたらめ」を言う状況に遭遇したことがある
- ✅ AIでより複雑なタスクを処理したい

なら、この章はあなたのためのものです。

始めましょう！
            `}},{id:"ch3-stable-output",title:{zh:"3.1 让 AI 输出更稳定",ja:"3.1 AIの出力を安定させる"},content:{zh:`
你可能发现，同样的问题问 AI 两次，回答可能不一样。这是因为 AI 有**随机性**。

如何让输出更稳定、更可控？这里有几个技巧。

---

## 技巧一：明确输出格式

### ❌ 不明确的问法
\`\`\`
分析一下苹果公司
\`\`\`
AI 可能从任何角度回答，每次都不同。

### ✅ 明确格式的问法
\`\`\`
分析苹果公司，请按以下格式输出：

## 公司概况
（2-3句话介绍）

## 主要产品
（列出3-5个）

## 优势
（3点）

## 挑战
（3点）

## 总结
（1段话）
\`\`\`

**效果：** AI 会严格按照你的格式输出，每次结构一致。

---

## 技巧二：使用"系统提示词"

在 ChatGPT 中，你可以在对话开始时设定 AI 的"身份"和"行为规则"。

### 示例：设定翻译助手
\`\`\`
从现在开始，你是一个中英翻译专家。
规则：
1. 用户输入中文，你翻译成英文
2. 用户输入英文，你翻译成中文
3. 只输出翻译结果，不加解释
4. 保持原文的语气和风格
\`\`\`

设定后，AI 的行为会更加稳定和一致。

---

## 技巧三：给出正反例子

告诉 AI 什么是你想要的，什么是不想要的。

### 示例：写产品描述
\`\`\`
帮我写一段产品描述，介绍我们的笔记本电脑。

✅ 好的例子（参考这个风格）：
"轻薄如羽，性能如虎。新一代 XX 笔记本，
让你的创意不再受限。"

❌ 不好的例子（避免这种风格）：
"本产品采用最新技术，具有高性能处理器，
支持多种接口，适合商务办公。"

产品特点：14寸、1.2kg、续航12小时
\`\`\`

---

## 技巧四：要求 AI 自检

让 AI 在回答后检查自己的输出。

### 示例
\`\`\`
请回答以下问题，然后检查你的回答是否：
1. 准确无误
2. 简洁清晰
3. 没有遗漏重要信息

如果有问题，请修正。

问题：Python 中 list 和 tuple 的区别是什么？
\`\`\`

---

## 技巧五：分步骤输出

让 AI 一步步思考，而不是直接给答案。

### 示例
\`\`\`
请计算这道数学题，要求：
1. 先列出已知条件
2. 写出解题思路
3. 一步步计算
4. 给出最终答案
5. 验算一遍

题目：一个水池有两个进水管...
\`\`\`

这种方法叫做 **Chain of Thought（思维链）**，能显著提高复杂问题的回答质量。

---

## 小结

| 技巧 | 适用场景 |
|------|----------|
| 明确格式 | 需要结构化输出 |
| 系统提示 | 重复性任务 |
| 正反例子 | 风格/质量要求高 |
| 要求自检 | 需要高准确度 |
| 分步输出 | 复杂推理任务 |

掌握这些技巧，你的 AI 使用效率会大大提升！
            `,ja:`
同じ質問をAIに2回しても、回答が違うことに気づいたかもしれません。これはAIに**ランダム性**があるからです。

出力をより安定させ、コントロールするには？いくつかのテクニックがあります。

---

## テクニック1：出力形式を明確に

### ❌ 曖昧な質問
\`\`\`
Appleについて分析して
\`\`\`
AIはどの角度からでも答える可能性があり、毎回違う。

### ✅ 形式を明確にした質問
\`\`\`
Apple社を分析してください。以下の形式で出力：

## 会社概要
（2-3文で紹介）

## 主要製品
（3-5個リスト）

## 強み
（3点）

## 課題
（3点）

## まとめ
（1段落）
\`\`\`

**効果：** AIは指定した形式に厳密に従い、毎回構造が一貫。

---

## テクニック2：「システムプロンプト」を使う

ChatGPTでは、会話の最初にAIの「役割」と「行動ルール」を設定できます。

### 例：翻訳アシスタントの設定
\`\`\`
今から、あなたは日英翻訳の専門家です。
ルール：
1. 日本語入力→英語に翻訳
2. 英語入力→日本語に翻訳
3. 翻訳結果のみ出力、説明不要
4. 原文のトーンとスタイルを維持
\`\`\`

設定後、AIの振る舞いがより安定し一貫します。

---

## テクニック3：良い例と悪い例を示す

AIに何が欲しいか、何が欲しくないかを伝える。

### 例：製品説明を書く
\`\`\`
製品説明を書いてください。ノートPCの紹介。

✅ 良い例（このスタイルを参考に）：
「羽のように軽く、虎のようにパワフル。
新世代XXノートで、創造性を解放しよう。」

❌ 悪い例（このスタイルは避けて）：
「本製品は最新技術を採用し、高性能プロセッサを搭載、
複数のインターフェースをサポート、ビジネスに最適。」

製品特徴：14インチ、1.2kg、12時間バッテリー
\`\`\`

---

## テクニック4：AIに自己チェックさせる

AIに回答後、自分の出力をチェックさせる。

### 例
\`\`\`
以下の質問に答えてから、回答が：
1. 正確か
2. 簡潔明瞭か
3. 重要な情報の漏れがないか

チェックしてください。問題があれば修正を。

質問：Pythonのlistとtupleの違いは？
\`\`\`

---

## テクニック5：ステップごとに出力

AIに直接答えを出させるのではなく、段階的に考えさせる。

### 例
\`\`\`
この数学問題を解いてください。要件：
1. まず既知の条件をリスト
2. 解法の方針を書く
3. ステップごとに計算
4. 最終回答を出す
5. 検算する

問題：プールに2つの給水管があり...
\`\`\`

この方法は **Chain of Thought（思考の連鎖）** と呼ばれ、複雑な問題の回答品質を大幅に向上させます。

---

## まとめ

| テクニック | 適用シーン |
|------|----------|
| 形式を明確に | 構造化出力が必要 |
| システムプロンプト | 繰り返しタスク |
| 良い例と悪い例 | スタイル/品質要求が高い |
| 自己チェック | 高い精度が必要 |
| ステップ出力 | 複雑な推論タスク |

これらのテクニックをマスターすれば、AI活用の効率が大幅にアップします！
            `}},{id:"ch3-task-decomposition",title:{zh:"3.2 复杂任务的拆解",ja:"3.2 複雑なタスクの分解"},content:{zh:`
AI 虽然强大，但一次处理太复杂的任务时，往往会出错或遗漏。

解决方法是：**把大任务拆成小任务**。

---

## 为什么要拆解？

### 一次性问太多的问题

\`\`\`
帮我写一个完整的电商网站，包括用户系统、商品管理、
购物车、支付、订单、物流追踪、评价系统、客服...
\`\`\`

AI 的回答会很笼统，容易遗漏细节，甚至前后矛盾。

### 拆解后的效果

把大任务拆成小任务，每个任务聚焦一个点：
1. 先设计用户系统
2. 再设计商品管理
3. 然后是购物车
4. ...

每一步都能得到更详细、更准确的回答。

---

## 拆解的原则

### 1. 按阶段拆

把项目分成：需求分析 → 设计 → 实现 → 测试

\`\`\`
第一步：帮我分析一个电商网站需要哪些核心功能。
\`\`\`

\`\`\`
第二步：针对"用户系统"这个功能，帮我设计数据库结构。
\`\`\`

\`\`\`
第三步：帮我实现用户注册的代码。
\`\`\`

### 2. 按模块拆

把系统分成独立的模块分别处理。

\`\`\`
我在做一个博客系统，现在只讨论"文章管理"模块。
请帮我设计这个模块的功能列表。
\`\`\`

### 3. 按难度拆

先解决简单的，再解决复杂的。

\`\`\`
我想学 Python，请给我一个学习路径，
从最简单的开始，逐步到进阶。
\`\`\`

---

## 实战案例：用 AI 写一篇长文章

### 目标
写一篇 3000 字的文章《如何高效学习编程》

### ❌ 一次性的方法
\`\`\`
帮我写一篇 3000 字的文章，主题是如何高效学习编程
\`\`\`
结果：可能结构混乱，内容空洞

### ✅ 拆解的方法

**Step 1：列大纲**
\`\`\`
我想写一篇《如何高效学习编程》的文章，
请帮我列出文章大纲，包括 5-7 个主要部分。
\`\`\`

**Step 2：逐节写作**
\`\`\`
基于这个大纲，请帮我写第一部分"选择合适的编程语言"，
要求 400-500 字，包含具体建议。
\`\`\`

**Step 3：逐节完成**
（重复 Step 2 直到所有部分完成）

**Step 4：整合润色**
\`\`\`
请帮我把以上所有部分整合成一篇完整的文章，
并润色过渡段落，确保文章流畅。
\`\`\`

**Step 5：检查修改**
\`\`\`
请检查这篇文章是否有：
1. 逻辑不通的地方
2. 重复的内容
3. 可以更简洁的表达

给出修改建议。
\`\`\`

---

## 拆解任务的万能模板

对于任何复杂任务，都可以用这个框架：

\`\`\`
1. 明确目标
   "我想要完成 XXX"

2. 列出步骤
   "请帮我把这个任务拆成 5-7 个步骤"

3. 逐步执行
   "现在我们执行第一步：..."

4. 整合检查
   "请帮我整合以上内容，并检查问题"
\`\`\`

---

## 小贴士

- **保持上下文**：在同一个对话中继续，AI 会记住之前的内容
- **及时总结**：完成几步后，让 AI 总结一下进度
- **灵活调整**：发现问题及时修正，不要等到最后

---

记住：**复杂的问题，简单的步骤**。这是用好 AI 的关键。
            `,ja:`
AIは強力ですが、一度に複雑すぎるタスクを処理すると、間違いや漏れが発生しがちです。

解決策は：**大きなタスクを小さなタスクに分割する**こと。

---

## なぜ分解が必要？

### 一度に多くを求める問題

\`\`\`
完全なECサイトを作って。ユーザーシステム、商品管理、
カート、決済、注文、配送追跡、レビュー、カスタマーサポート...
\`\`\`

AIの回答は大雑把になり、詳細が抜け落ち、矛盾することも。

### 分解後の効果

大きなタスクを小さなタスクに分け、各タスクは一点に集中：
1. まずユーザーシステムを設計
2. 次に商品管理を設計
3. それからカート
4. ...

各ステップでより詳細で正確な回答が得られます。

---

## 分解の原則

### 1. フェーズで分ける

プロジェクトを：要件分析 → 設計 → 実装 → テスト に分割

\`\`\`
ステップ1：ECサイトに必要なコア機能を分析して。
\`\`\`

\`\`\`
ステップ2：「ユーザーシステム」機能のDB設計を手伝って。
\`\`\`

\`\`\`
ステップ3：ユーザー登録のコードを実装して。
\`\`\`

### 2. モジュールで分ける

システムを独立したモジュールに分けて別々に処理。

\`\`\`
ブログシステムを作っています。今は「記事管理」モジュールだけ議論。
このモジュールの機能リストを設計して。
\`\`\`

### 3. 難易度で分ける

簡単なものから解決し、複雑なものへ。

\`\`\`
Pythonを学びたい。学習パスを教えて、
最も簡単なことから始めて、徐々に上級へ。
\`\`\`

---

## 実践例：AIで長い記事を書く

### 目標
3000字の記事「効率的なプログラミング学習法」を書く

### ❌ 一度にやる方法
\`\`\`
3000字の記事を書いて、テーマは効率的なプログラミング学習
\`\`\`
結果：構造が混乱、内容が薄くなりがち

### ✅ 分解する方法

**Step 1：アウトラインを作る**
\`\`\`
「効率的なプログラミング学習法」の記事を書きたい。
5-7の主要セクションのアウトラインを作って。
\`\`\`

**Step 2：セクションごとに書く**
\`\`\`
このアウトラインに基づいて、第1部
「適切なプログラミング言語の選択」を書いて、
400-500字、具体的なアドバイスを含めて。
\`\`\`

**Step 3：各セクションを完成**
（全セクション完了までStep 2を繰り返す）

**Step 4：統合と推敲**
\`\`\`
上記のすべてのセクションを完全な記事に統合して、
つなぎの段落を推敲し、流れをスムーズに。
\`\`\`

**Step 5：チェックと修正**
\`\`\`
この記事に以下がないかチェック：
1. 論理が通らない箇所
2. 重複した内容
3. より簡潔にできる表現

修正案を出して。
\`\`\`

---

## タスク分解の万能テンプレート

どんな複雑なタスクでも、このフレームワークが使えます：

\`\`\`
1. 目標を明確に
   「XXXを完成させたい」

2. ステップをリスト
   「このタスクを5-7ステップに分けて」

3. 順番に実行
   「ステップ1を実行しよう：...」

4. 統合とチェック
   「以上の内容を統合して、問題をチェック」
\`\`\`

---

## ヒント

- **コンテキストを維持**：同じ会話で続ける、AIは以前の内容を覚えている
- **こまめに要約**：数ステップ後にAIに進捗を要約させる
- **柔軟に調整**：問題を見つけたらすぐ修正、最後まで待たない

---

覚えておいてください：**複雑な問題、シンプルなステップ**。これがAIを上手に使う鍵です。
            `}},{id:"ch3-common-problems",title:{zh:"3.3 常见问题与解决",ja:"3.3 よくある問題と解決策"},content:{zh:`
使用 AI 过程中，你一定会遇到各种问题。这里列出最常见的问题和解决方法。

---

## 问题一：AI "一本正经地胡说八道"

### 表现
AI 回答得很自信，但内容是错的。比如：
- 编造不存在的人物、书籍、事件
- 给出错误的数据或公式
- 提供不存在的链接

### 原因
AI 是根据概率生成文字的，它不"知道"什么是真的。

### 解决方法
1. **重要信息一定要核实**
2. **让 AI 标注信息来源**
   \`\`\`
   请回答这个问题，并说明你的信息来源
   \`\`\`
3. **问具体的、可验证的问题**
   - ❌ "谁是最好的程序员？"
   - ✅ "Python 之父是谁？"

---

## 问题二：AI 的回答太笼统

### 表现
AI 只给出泛泛而谈的内容，缺少具体细节。

### 解决方法
1. **追问细节**
   \`\`\`
   能具体说说第三点吗？最好举个例子
   \`\`\`
2. **限定范围**
   \`\`\`
   只针对初学者，给出 3 个具体可操作的建议
   \`\`\`
3. **要求格式化输出**
   \`\`\`
   请用表格形式列出，包括：方法、难度、预计时间
   \`\`\`

---

## 问题三：AI 不理解我的问题

### 表现
AI 的回答答非所问，或者理解错了你的意图。

### 解决方法
1. **重新表述问题**
   用不同的方式描述同一个问题
2. **提供更多背景**
   \`\`\`
   背景：我是一个没有编程经验的大学生...
   问题：我应该先学什么语言？
   \`\`\`
3. **使用类比**
   \`\`\`
   我想实现一个功能，类似于微信的朋友圈
   \`\`\`

---

## 问题四：AI 拒绝回答

### 表现
AI 说"我不能回答这个问题"或给出警告。

### 可能原因
- 涉及敏感话题（政治、暴力等）
- 涉及隐私或安全
- AI 误判了你的意图

### 解决方法
1. **重新表述，避免敏感词**
2. **说明合法用途**
   \`\`\`
   我是一名安全研究员，需要了解 XXX 的原理用于防御
   \`\`\`
3. **换一个 AI 工具试试**

---

## 问题五：AI 的回答前后矛盾

### 表现
在同一对话中，AI 的说法不一致。

### 解决方法
1. **指出矛盾**
   \`\`\`
   你刚才说 A，现在又说 B，哪个是对的？
   \`\`\`
2. **要求总结确认**
   \`\`\`
   请总结我们讨论的结论，确保没有矛盾
   \`\`\`
3. **开新对话**
   如果对话太长，信息混乱，不如重新开始

---

## 问题六：AI 无法访问最新信息

### 表现
AI 不知道最近发生的事，或给出过时的信息。

### 原因
AI 的知识有截止日期，ChatGPT 不是实时联网的。

### 解决方法
1. **使用有联网功能的 AI**
   - ChatGPT 的浏览功能
   - Perplexity（专门的 AI 搜索）
   - Gemini（可访问 Google 信息）
2. **自己提供最新信息**
   \`\`\`
   根据以下最新数据，帮我分析：
   [粘贴数据]
   \`\`\`

---

## 通用的问题解决思路

遇到问题时，按这个顺序尝试：

1. **换个问法** → 重新表述问题
2. **加信息** → 提供更多背景
3. **减范围** → 缩小问题范围
4. **换工具** → 试试其他 AI
5. **人工介入** → 某些问题确实需要人来解决

---

记住：**AI 是工具，不是权威**。遇到问题时保持耐心，多尝试几种方法。
            `,ja:`
AIを使う過程で、様々な問題に遭遇するでしょう。ここでは最も一般的な問題と解決方法を紹介します。

---

## 問題1：AIが「自信満々にでたらめを言う」

### 症状
AIは自信を持って回答するが、内容が間違っている：
- 存在しない人物、本、出来事を作り上げる
- 間違ったデータや公式を出す
- 存在しないリンクを提供

### 原因
AIは確率に基づいてテキストを生成しており、何が「真実」かを「知らない」。

### 解決方法
1. **重要な情報は必ず確認**
2. **AIに情報源を明記させる**
   \`\`\`
   この質問に答えて、情報源も教えて
   \`\`\`
3. **具体的で検証可能な質問をする**
   - ❌ 「誰が最高のプログラマー？」
   - ✅ 「Pythonの作者は誰？」

---

## 問題2：AIの回答が大雑把すぎる

### 症状
AIは一般的な内容しか出さず、具体的な詳細がない。

### 解決方法
1. **詳細を追求**
   \`\`\`
   3番目のポイントを具体的に説明して。例も挙げて
   \`\`\`
2. **範囲を限定**
   \`\`\`
   初心者だけに向けて、具体的で実行可能な3つのアドバイスを
   \`\`\`
3. **フォーマット出力を要求**
   \`\`\`
   表形式でリストして。含める項目：方法、難易度、予想時間
   \`\`\`

---

## 問題3：AIが質問を理解しない

### 症状
AIの回答が的外れ、または意図を誤解。

### 解決方法
1. **質問を言い換える**
   同じ質問を違う方法で説明
2. **より多くの背景を提供**
   \`\`\`
   背景：私はプログラミング経験のない大学生です...
   質問：まず何の言語を学ぶべき？
   \`\`\`
3. **類似例を使う**
   \`\`\`
   ある機能を実装したい、LINEのタイムラインのような
   \`\`\`

---

## 問題4：AIが回答を拒否

### 症状
AIが「この質問には答えられません」と言うか、警告を出す。

### 考えられる原因
- センシティブな話題（政治、暴力など）
- プライバシーやセキュリティに関わる
- AIがあなたの意図を誤判断

### 解決方法
1. **センシティブな言葉を避けて言い換え**
2. **正当な目的を説明**
   \`\`\`
   私はセキュリティ研究者で、防御目的でXXXの原理を理解する必要があります
   \`\`\`
3. **別のAIツールを試す**

---

## 問題5：AIの回答が矛盾

### 症状
同じ会話の中でAIの発言が一貫しない。

### 解決方法
1. **矛盾を指摘**
   \`\`\`
   さっきはAと言ったのに、今はBと言っている。どっちが正しい？
   \`\`\`
2. **要約と確認を求める**
   \`\`\`
   議論の結論を要約して、矛盾がないか確認して
   \`\`\`
3. **新しい会話を開始**
   会話が長すぎて情報が混乱したら、やり直し

---

## 問題6：AIが最新情報にアクセスできない

### 症状
AIが最近の出来事を知らない、または古い情報を出す。

### 原因
AIの知識には締め切り日があり、ChatGPTはリアルタイムでネット接続していない。

### 解決方法
1. **ネット接続機能のあるAIを使う**
   - ChatGPTのブラウジング機能
   - Perplexity（専用のAI検索）
   - Gemini（Google情報にアクセス可能）
2. **自分で最新情報を提供**
   \`\`\`
   以下の最新データに基づいて分析して：
   [データを貼り付け]
   \`\`\`

---

## 一般的な問題解決アプローチ

問題に遭遇したら、この順番で試す：

1. **聞き方を変える** → 質問を言い換え
2. **情報を追加** → より多くの背景を提供
3. **範囲を狭める** → 問題の範囲を縮小
4. **ツールを変える** → 他のAIを試す
5. **人が介入** → 一部の問題は確かに人が解決する必要がある

---

覚えておいてください：**AIはツールであり、権威ではない**。問題に遭遇したら忍耐強く、いくつかの方法を試してみてください。
            `}},{id:"ch3-practice-case",title:{zh:"3.4 完整实战案例",ja:"3.4 完全な実践ケース"},content:{zh:`
理论讲了很多，现在让我们通过一个**完整的实战案例**，看看如何从头到尾使用AI完成一个真实项目。

---

## 案例背景

假设你是一名产品经理，需要为公司策划一个**新功能**：在APP中添加"每日健康打卡"功能。你需要完成：

1. 竞品分析报告
2. 功能需求文档
3. 用户调研问卷
4. 项目汇报PPT大纲

让我们看看如何用AI高效完成这些任务。

---

## 第一步：竞品分析

### 初始提问

\`\`\`
我是产品经理，正在调研"每日健康打卡"功能。
请帮我分析市面上主流的健康类APP是如何设计打卡功能的。

请从以下维度分析：
1. 打卡类型（运动、饮食、睡眠等）
2. 激励机制（积分、成就、社交等）
3. 用户体验设计亮点
4. 可能的改进空间

请分析3-5款主流APP。
\`\`\`

### AI 回答（部分）

> **Keep**
> - 打卡类型：运动为主，支持跑步、健身课程打卡
> - 激励机制：卡路里积累、勋章系统、排行榜
> - 亮点：与社交功能紧密结合，可发动态
> - 改进：可增加饮食和睡眠维度
>
> **薄荷健康**
> - 打卡类型：饮食记录为主
> - ...

### 追问深挖

\`\`\`
针对激励机制，请详细比较Keep和薄荷健康的做法，
用表格形式呈现，包含：
- 激励类型
- 实现方式
- 效果评估
- 对我们的启发
\`\`\`

### 要求总结

\`\`\`
基于以上分析，请给我一份简洁的竞品分析结论，
格式如下：

## 核心发现（3点）
## 值得借鉴（3点）
## 差异化机会（2点）
\`\`\`

---

## 第二步：需求文档

### 框架生成

\`\`\`
现在我要写"每日健康打卡"的产品需求文档（PRD）。

背景信息：
- 目标用户：25-40岁都市白领
- 核心场景：碎片时间记录健康行为
- 打卡维度：运动、饮水、睡眠三个维度

请给出PRD的完整框架，包含每个部分应该写什么内容。
\`\`\`

### 逐节填充

\`\`\`
请帮我写PRD的"功能需求"部分。

要求：
1. 列出所有功能点
2. 每个功能点包含：功能描述、优先级、验收标准
3. 用表格形式呈现
\`\`\`

### 细节完善

\`\`\`
针对"打卡提醒"功能，请补充以下内容：
1. 用户故事（User Story）
2. 流程图描述（用文字描述即可）
3. 异常情况处理
4. 数据埋点建议
\`\`\`

### 检查优化

\`\`\`
请检查这份需求文档，从以下角度提出改进建议：
1. 是否有遗漏的场景？
2. 是否有冲突的需求？
3. 技术实现上有什么风险点？
4. 用户体验上有什么需要注意的？

[粘贴已完成的PRD内容]
\`\`\`

---

## 第三步：用户问卷

### 问卷设计

\`\`\`
我需要设计一份用户调研问卷，了解用户对"健康打卡"功能的需求。

目标：
- 了解用户当前的健康管理习惯
- 了解对打卡功能的需求和偏好
- 收集功能优先级排序

要求：
- 问卷控制在10-15题
- 包含单选、多选、量表题
- 逻辑清晰，便于填写

请直接输出完整问卷内容。
\`\`\`

### 优化表述

\`\`\`
请检查这份问卷的措辞，确保：
1. 问题没有引导性
2. 选项完整且互斥
3. 语言简洁易懂

如有问题，请指出并给出修改建议。
\`\`\`

---

## 第四步：汇报PPT

### 大纲生成

\`\`\`
我需要在周会上汇报这个项目，请帮我规划PPT大纲。

听众：产品VP、技术负责人、运营经理
时长：15分钟

内容应包含：
- 项目背景和目标
- 竞品分析结论
- 功能设计方案
- 实施计划和里程碑
- 所需资源和支持

每页给出标题和要点。
\`\`\`

### 制作要点

\`\`\`
对于"竞品分析结论"这一页，请给我：
1. 建议的数据可视化方式
2. 关键信息的展示建议
3. 演讲时的讲解要点
\`\`\`

---

## 案例复盘

通过这个案例，我们用到了以下技巧：

| 步骤 | 用到的技巧 |
|------|-----------|
| 竞品分析 | 明确分析维度、追问深挖、要求特定格式 |
| 需求文档 | 先框架后细节、逐节填充、请AI检查 |
| 用户问卷 | 明确目标和约束、请AI优化措辞 |
| PPT大纲 | 明确听众和时长、请AI给建议 |

### 关键心得

1. **永远不要一步到位** - 拆成多步，逐步完善
2. **明确输出格式** - 告诉AI你想要什么样的结果
3. **迭代优化** - 不满意就追问，多轮对话
4. **人工把关** - AI生成的内容要检查和修改
5. **积累模板** - 好的提示词可以复用

---

## 练习任务

试着用AI完成以下任务（选一个）：

**任务A：个人周报**
- 用AI帮你整理本周工作内容
- 生成周报框架
- 润色语言表达

**任务B：会议准备**
- 让AI帮你准备明天会议的议程
- 生成讨论问题清单
- 准备可能的问答

**任务C：学习计划**
- 让AI帮你制定某个技能的学习计划
- 分解学习目标
- 安排时间表

完成后，回顾一下：哪些地方用得好？哪些地方可以改进？
            `,ja:`
理論はたくさん話しましたが、今度は**完全な実践ケース**を通して、AIを使って実際のプロジェクトを最初から最後まで完成させる方法を見てみましょう。

---

## ケース背景

あなたがプロダクトマネージャーで、会社のために**新機能**を企画する必要があると仮定しましょう：アプリに「毎日の健康チェックイン」機能を追加します。完成すべきタスク：

1. 競合分析レポート
2. 機能要件ドキュメント
3. ユーザー調査アンケート
4. プロジェクト報告PPTの概要

AIを使ってこれらのタスクを効率的に完成させる方法を見ていきましょう。

---

## ステップ1：競合分析

### 初期質問

\`\`\`
私はプロダクトマネージャーで、「毎日の健康チェックイン」機能を調査しています。
市場の主要な健康アプリがチェックイン機能をどのように設計しているか分析してください。

以下の観点から分析してください：
1. チェックインの種類（運動、食事、睡眠など）
2. 動機付けメカニズム（ポイント、達成、ソーシャルなど）
3. UXデザインのハイライト
4. 改善の余地

3〜5つの主要アプリを分析してください。
\`\`\`

### AI回答（一部）

> **Keep**
> - チェックインタイプ：主に運動、ランニングやフィットネスコースのチェックインをサポート
> - 動機付け：カロリー蓄積、バッジシステム、ランキング
> - ハイライト：ソーシャル機能と緊密に統合、投稿可能
> - 改善点：食事と睡眠の次元を追加可能
>
> **あすけん**
> - チェックインタイプ：主に食事記録
> - ...

### 深掘り質問

\`\`\`
動機付けメカニズムについて、Keepとあすけんのやり方を詳しく比較してください。
表形式で表示し、以下を含めてください：
- 動機付けタイプ
- 実装方法
- 効果評価
- 私たちへの示唆
\`\`\`

### まとめを要求

\`\`\`
上記の分析に基づいて、簡潔な競合分析結論をください。
形式：

## 核心発見（3点）
## 参考にすべき点（3点）
## 差別化の機会（2点）
\`\`\`

---

## ステップ2：要件ドキュメント

### フレームワーク生成

\`\`\`
「毎日の健康チェックイン」の製品要件ドキュメント（PRD）を書きます。

背景情報：
- ターゲットユーザー：25〜40歳の都市部ビジネスパーソン
- コアシナリオ：隙間時間に健康行動を記録
- チェックイン次元：運動、水分補給、睡眠の3次元

PRDの完全なフレームワークと、各部分に何を書くべきか教えてください。
\`\`\`

### セクションごとに記入

\`\`\`
PRDの「機能要件」部分を書いてください。

要件：
1. すべての機能ポイントをリストアップ
2. 各機能ポイントに：機能説明、優先度、受入基準を含める
3. 表形式で表示
\`\`\`

### 詳細を完善

\`\`\`
「チェックインリマインダー」機能について、以下を補足してください：
1. ユーザーストーリー
2. フローチャート説明（テキストで可）
3. 例外処理
4. データ埋め込み提案
\`\`\`

### チェックと最適化

\`\`\`
この要件ドキュメントをチェックし、以下の観点から改善提案をください：
1. 見落としているシナリオはあるか？
2. 矛盾する要件はあるか？
3. 技術実装でのリスクポイントは？
4. UXで注意すべき点は？

[完成したPRDを貼り付け]
\`\`\`

---

## ステップ3：ユーザーアンケート

### アンケート設計

\`\`\`
「健康チェックイン」機能へのユーザーニーズを理解するための
ユーザー調査アンケートを設計する必要があります。

目標：
- ユーザーの現在の健康管理習慣を理解
- チェックイン機能へのニーズと好みを理解
- 機能の優先順位を収集

要件：
- 質問を10〜15問に制限
- 単一選択、複数選択、尺度問を含める
- ロジックが明確で、記入しやすい

完全なアンケート内容を出力してください。
\`\`\`

### 表現を最適化

\`\`\`
このアンケートの言葉遣いをチェックし、確認してください：
1. 質問に誘導性がないか
2. 選択肢が完全で相互排他的か
3. 言葉が簡潔で分かりやすいか

問題があれば指摘し、修正案を出してください。
\`\`\`

---

## ステップ4：報告PPT

### 概要生成

\`\`\`
週次会議でこのプロジェクトを報告する必要があります。PPTの概要を計画してください。

聴衆：プロダクトVP、技術責任者、運営マネージャー
時間：15分

内容に含めるべき：
- プロジェクトの背景と目標
- 競合分析の結論
- 機能設計案
- 実施計画とマイルストーン
- 必要なリソースとサポート

各ページのタイトルと要点を示してください。
\`\`\`

### 作成のポイント

\`\`\`
「競合分析の結論」ページについて、以下を教えてください：
1. 推奨されるデータ可視化方法
2. キー情報の表示方法
3. プレゼン時の解説ポイント
\`\`\`

---

## ケース振り返り

このケースで使用したテクニック：

| ステップ | 使用したテクニック |
|---------|-------------------|
| 競合分析 | 分析次元を明確に、深掘り質問、特定形式を要求 |
| 要件ドキュメント | 先にフレームワーク後に詳細、セクション毎に記入、AIにチェック依頼 |
| ユーザーアンケート | 目標と制約を明確に、AIに言葉遣い最適化依頼 |
| PPT概要 | 聴衆と時間を明確に、AIにアドバイス依頼 |

### 重要な学び

1. **一度に完成させようとしない** - 複数ステップに分け、徐々に完成
2. **出力形式を明確に** - AIにどんな結果が欲しいか伝える
3. **反復最適化** - 満足できなければ追加質問、複数回の対話
4. **人間がチェック** - AI生成コンテンツは確認と修正が必要
5. **テンプレートを蓄積** - 良いプロンプトは再利用可能

---

## 練習タスク

AIを使って以下のタスクを完成させてみましょう（1つ選択）：

**タスクA：個人週報**
- AIに今週の業務内容を整理してもらう
- 週報フレームワークを生成
- 言葉遣いを推敲

**タスクB：会議準備**
- AIに明日の会議の議題を準備してもらう
- 議論すべき質問リストを生成
- 想定されるQ&Aを準備

**タスクC：学習計画**
- AIにあるスキルの学習計画を立ててもらう
- 学習目標を分解
- タイムスケジュールを作成

完了後、振り返りましょう：どこがうまくいった？どこが改善できる？
            `}},{id:"ch3-summary",title:{zh:"3.5 本章小结",ja:"3.5 この章のまとめ"},content:{zh:`
恭喜你完成了进阶技巧的学习！让我们回顾关键要点。

---

## 核心技巧回顾

### 让输出更稳定
| 方法 | 说明 |
|------|------|
| 明确格式 | 用模板规定输出结构 |
| 系统提示 | 设定 AI 的角色和规则 |
| 给出例子 | 展示期望的风格和质量 |
| 要求自检 | 让 AI 检查自己的回答 |
| 分步输出 | 思维链，提升推理质量 |

### 复杂任务拆解
1. **按阶段拆** - 需求→设计→实现→测试
2. **按模块拆** - 独立功能分开处理
3. **按难度拆** - 从简单到复杂

### 常见问题处理
- **AI 胡说八道** → 核实信息，要求来源
- **回答太笼统** → 追问细节，限定范围
- **理解有误** → 重新表述，多给背景
- **拒绝回答** → 换个问法，说明用途
- **前后矛盾** → 指出问题，要求总结

---

## 进阶心法

1. **永远保持怀疑** - AI 可能出错
2. **迭代优化** - 多试几次，逐步改进
3. **明确边界** - 知道 AI 能做和不能做什么
4. **人机协作** - 发挥各自优势

---

## 你已经学会了

✅ 让 AI 输出更稳定、更可控
✅ 拆解复杂任务，逐步完成
✅ 解决 AI 使用中的常见问题
✅ 识别和处理 AI 的错误

---

## 接下来

下一章我们将探索 **AI 在实际生活和工作中的应用场景**：
- 用 AI 提升工作效率
- 用 AI 辅助学习
- 用 AI 进行创意创作
- 未来的 AI 发展趋势

准备好看看 AI 能帮你做什么具体的事了吗？
            `,ja:`
上級テクニックの学習、おめでとうございます！重要なポイントを振り返りましょう。

---

## 核心テクニック復習

### 出力を安定させる
| 方法 | 説明 |
|------|------|
| 形式を明確に | テンプレートで出力構造を規定 |
| システムプロンプト | AIの役割とルールを設定 |
| 例を示す | 期待するスタイルと品質を見せる |
| 自己チェック | AIに自分の回答をチェックさせる |
| ステップ出力 | 思考の連鎖、推論品質向上 |

### 複雑なタスクの分解
1. **フェーズで分ける** - 要件→設計→実装→テスト
2. **モジュールで分ける** - 独立した機能を別々に処理
3. **難易度で分ける** - 簡単なものから複雑なものへ

### よくある問題への対処
- **AIのでたらめ** → 情報を確認、出典を要求
- **回答が大雑把** → 詳細を追求、範囲を限定
- **理解の誤り** → 言い換え、背景を追加
- **回答拒否** → 聞き方を変える、目的を説明
- **矛盾** → 問題を指摘、要約を求める

---

## 上級者の心得

1. **常に懐疑的に** - AIは間違える可能性がある
2. **反復改善** - 何度か試して徐々に改善
3. **境界を明確に** - AIにできること・できないことを知る
4. **人機協働** - それぞれの強みを活かす

---

## 習得したこと

✅ AIの出力をより安定させ、コントロールする
✅ 複雑なタスクを分解し、段階的に完成
✅ AI使用中のよくある問題を解決
✅ AIの間違いを識別し対処

---

## 次へ

次の章では**AIの実生活と仕事での応用シーン**を探ります：
- AIで仕事効率を上げる
- AIで学習を支援
- AIでクリエイティブ制作
- AIの将来のトレンド

AIが具体的に何を助けてくれるか、見る準備はできましたか？
            `}}]},{id:"chapter-4",number:4,title:{zh:"AI应用场景",ja:"AIの応用シーン"},subtitle:{zh:"把AI融入你的工作和生活",ja:"AIを仕事と生活に取り入れる"},sections:[{id:"ch4-work-efficiency",title:{zh:"4.1 用AI提升工作效率",ja:"4.1 AIで仕事効率を上げる"},content:{zh:`
学会了基础技巧后，让我们看看AI如何在实际工作中发挥作用。

---

## 文档处理

### 写作与编辑

**起草文档**
\`\`\`
我需要写一份项目周报，包含以下内容：
- 本周完成：用户系统重构、API优化
- 进行中：数据库迁移（进度60%）
- 遇到问题：第三方接口响应慢
- 下周计划：完成迁移、性能测试

请用简洁专业的风格，适合发给技术团队和管理层。
\`\`\`

**润色修改**
\`\`\`
请帮我修改这段文字，让它更简洁有力：
[粘贴你的原文]

要求：
1. 去掉冗余的词
2. 让逻辑更清晰
3. 保持原意不变
\`\`\`

**翻译与本地化**
\`\`\`
请将以下产品描述翻译成日语，注意：
1. 使用敬体（です/ます形）
2. 适合电商平台的风格
3. 保持专业术语准确

[粘贴原文]
\`\`\`

### 数据整理

**表格数据处理**
\`\`\`
以下是销售数据，请帮我：
1. 按地区汇总销售额
2. 找出销售额最高和最低的产品
3. 计算同比增长率

数据：
产品A, 北京, 50000
产品B, 上海, 80000
产品A, 上海, 60000
...
\`\`\`

**信息提取**
\`\`\`
从以下客户反馈中提取关键信息：

1. 主要投诉点（按出现频率排序）
2. 具体的产品问题
3. 改进建议

[粘贴客户反馈]
\`\`\`

---

## 邮件沟通

### 商务邮件

**写正式邮件**
\`\`\`
帮我写一封邮件：
- 收件人：客户公司的技术负责人
- 目的：说明项目延期一周的原因
- 原因：发现了一个安全漏洞需要修复
- 态度：诚恳道歉，表达解决问题的决心
- 语言：中文，正式商务风格
\`\`\`

AI 生成的邮件：
> 尊敬的张总：
>
> 感谢贵司一直以来的支持与信任。
>
> 关于我们正在进行的XX项目，经过仔细的安全审查，我们的团队发现了一个需要优先处理的安全隐患。为确保系统的稳定性和安全性，我们决定将上线时间推迟一周。
>
> 我们对此深表歉意。目前团队正在全力修复该问题，预计将于X月X日完成。我们会保持沟通，及时同步进展。
>
> 如有任何疑问，请随时联系。
>
> 此致
> 敬礼

**回复难处理的邮件**
\`\`\`
我收到一封投诉邮件，客户对服务不满意。
请帮我写一封回复，要求：
1. 先表示理解和歉意
2. 说明我们的处理方案
3. 提供补偿措施
4. 语气诚恳但专业

客户邮件内容：
[粘贴邮件]
\`\`\`

### 日程协调

\`\`\`
我需要和以下人员安排一个会议：
- 张经理：周二下午、周四全天有空
- 李工程师：周一到周三都可以
- 王设计师：周四上午不行，其他时间都可以

会议时长：1.5小时
请找出所有可能的时间段。
\`\`\`

---

## 会议辅助

### 会议准备

\`\`\`
明天要开产品需求评审会，参与者包括产品、开发、测试团队。
请帮我：
1. 设计会议议程（1小时）
2. 列出需要准备的材料清单
3. 拟定关键讨论问题
\`\`\`

### 会议纪要

\`\`\`
请根据以下会议记录，生成一份正式的会议纪要：

会议记录（口语化）：
"张总说这个季度销售目标是500万，李经理觉得有点高，
建议分阶段完成。王总同意了，说第一个月先做150万试试，
然后根据情况调整。另外市场部要配合做活动..."

格式要求：
- 会议主题
- 与会人员
- 讨论要点
- 决议事项
- 待办事项（包含负责人和截止日期）
\`\`\`

---

## 实际工作流示例

### 案例：准备客户提案

**第一步：收集信息**
\`\`\`
我要为一家电商公司准备数字化转型提案。
请帮我列出需要了解的关键信息：
1. 客户现状需要了解什么？
2. 行业趋势有哪些？
3. 我们能提供什么价值？
\`\`\`

**第二步：生成大纲**
\`\`\`
基于以下信息，为提案生成大纲：
- 客户：中型电商，年销售额5000万
- 痛点：库存管理混乱，客服效率低
- 预算：50万左右
- 时间：3个月内见效
\`\`\`

**第三步：完善内容**
\`\`\`
请为提案的"解决方案"部分写详细内容：
- 方案1：智能库存管理系统
- 方案2：AI客服机器人
包含：功能描述、预期效果、实施步骤
\`\`\`

**第四步：最终检查**
\`\`\`
请检查这份提案：
1. 逻辑是否通顺
2. 有没有前后矛盾的地方
3. 语言是否专业得体
4. 是否遗漏重要信息

[粘贴提案全文]
\`\`\`

---

## 本节要点

| 应用场景 | AI 帮助方式 |
|---------|------------|
| 文档写作 | 起草、润色、翻译 |
| 数据处理 | 整理、分析、提取 |
| 邮件沟通 | 写作、回复、语气把控 |
| 会议工作 | 准备议程、整理纪要 |

**效率提升秘诀**：
1. 把重复性工作交给 AI
2. 用 AI 处理初稿，自己做最终把关
3. 建立自己的提示语模板库
            `,ja:`
基本スキルを学んだ後は、AIが実際の仕事でどう役立つか見てみましょう。

---

## ドキュメント処理

### ライティングと編集

**ドキュメント起草**
\`\`\`
週報を書く必要があります。以下の内容を含めてください：
- 今週完了：ユーザーシステムのリファクタリング、API最適化
- 進行中：データベース移行（進捗60%）
- 問題：サードパーティAPIの応答が遅い
- 来週の計画：移行完了、パフォーマンステスト

簡潔でプロフェッショナルなスタイルで、技術チームと管理層向けに。
\`\`\`

**推敲と修正**
\`\`\`
この文章をより簡潔で力強くしてください：
[原文を貼り付け]

要件：
1. 冗長な言葉を削除
2. ロジックをより明確に
3. 元の意味を保持
\`\`\`

**翻訳とローカライゼーション**
\`\`\`
以下の製品説明を英語に翻訳してください：
1. プロフェッショナルなトーンで
2. Eコマースサイトに適したスタイル
3. 専門用語を正確に

[原文を貼り付け]
\`\`\`

### データ整理

**表データの処理**
\`\`\`
以下は売上データです。以下をお願いします：
1. 地域別に売上を集計
2. 売上最高と最低の商品を特定
3. 前年比成長率を計算

データ：
商品A, 東京, 50000
商品B, 大阪, 80000
商品A, 大阪, 60000
...
\`\`\`

**情報抽出**
\`\`\`
以下の顧客フィードバックから重要情報を抽出：

1. 主な苦情（出現頻度順）
2. 具体的な製品の問題
3. 改善提案

[フィードバックを貼り付け]
\`\`\`

---

## メールコミュニケーション

### ビジネスメール

**正式なメールを書く**
\`\`\`
メールを書いてください：
- 宛先：クライアント企業の技術責任者
- 目的：プロジェクト1週間延期の説明
- 理由：セキュリティ脆弱性の修正が必要
- 態度：誠実に謝罪、問題解決への決意
- 言語：日本語、正式なビジネススタイル
\`\`\`

AI生成のメール：
> 山田様
>
> いつも大変お世話になっております。
>
> 現在進行中のXXプロジェクトについてご連絡いたします。セキュリティ監査の結果、優先対応が必要な脆弱性が発見されました。システムの安定性と安全性を確保するため、リリース日を1週間延期させていただきたく存じます。
>
> ご不便をおかけし、誠に申し訳ございません。現在チームは全力で修正に取り組んでおり、X月X日には完了予定です。進捗は随時ご報告いたします。
>
> ご不明な点がございましたら、お気軽にお問い合わせください。
>
> 何卒よろしくお願いいたします。

**難しいメールへの返信**
\`\`\`
クレームメールを受け取りました。お客様がサービスに不満です。
返信を書いてください：
1. まず理解と謝罪を表明
2. 対応策を説明
3. 補償措置を提供
4. 誠実かつプロフェッショナルなトーン

お客様のメール：
[メールを貼り付け]
\`\`\`

### スケジュール調整

\`\`\`
以下のメンバーとの会議をスケジュールする必要があります：
- 田中マネージャー：火曜午後、木曜終日空き
- 鈴木エンジニア：月曜から水曜まで可能
- 佐藤デザイナー：木曜午前以外は全て可能

会議時間：1.5時間
全ての可能な時間帯を見つけてください。
\`\`\`

---

## 会議サポート

### 会議準備

\`\`\`
明日、製品要件レビュー会議があります。参加者は製品、開発、QAチーム。
以下をお願いします：
1. 会議アジェンダを設計（1時間）
2. 準備資料のチェックリスト
3. 主要な議論ポイントをリストアップ
\`\`\`

### 議事録

\`\`\`
以下の会議メモから正式な議事録を作成してください：

会議メモ（口語調）：
「田中部長は今四半期の売上目標を500万と言った。鈴木課長は
少し高いと思って、段階的に達成することを提案した。山田部長は
同意して、最初の月は150万を試してから調整しようと言った。
あとマーケティング部がキャンペーンで協力するって...」

形式：
- 会議テーマ
- 参加者
- 議論ポイント
- 決定事項
- アクションアイテム（担当者と期限含む）
\`\`\`

---

## 実際のワークフロー例

### ケース：顧客提案書の準備

**ステップ1：情報収集**
\`\`\`
EC企業向けのデジタル変革提案書を準備します。
把握すべき重要情報をリストアップしてください：
1. 顧客の現状について何を知るべき？
2. 業界トレンドは？
3. 私たちが提供できる価値は？
\`\`\`

**ステップ2：アウトライン生成**
\`\`\`
以下の情報に基づいて提案書のアウトラインを生成：
- 顧客：中規模EC、年商5000万
- 課題：在庫管理の混乱、カスタマーサービス効率低下
- 予算：約500万
- 期間：3ヶ月以内に効果
\`\`\`

**ステップ3：内容を充実**
\`\`\`
提案書の「ソリューション」セクションの詳細を書いてください：
- ソリューション1：スマート在庫管理システム
- ソリューション2：AIカスタマーサービスボット
含める内容：機能説明、期待効果、実施ステップ
\`\`\`

**ステップ4：最終チェック**
\`\`\`
この提案書をチェックしてください：
1. ロジックは通っているか
2. 矛盾はないか
3. 言葉遣いはプロフェッショナルか
4. 重要情報の漏れはないか

[提案書全文を貼り付け]
\`\`\`

---

## このセクションのポイント

| 応用シーン | AIの支援方法 |
|---------|------------|
| ドキュメント作成 | 起草、推敲、翻訳 |
| データ処理 | 整理、分析、抽出 |
| メール | 作成、返信、トーン調整 |
| 会議 | アジェンダ準備、議事録作成 |

**効率アップの秘訣**：
1. 繰り返し作業はAIに任せる
2. AIで初稿を作り、自分で最終チェック
3. 自分のプロンプトテンプレート集を作る
            `}},{id:"ch4-learning",title:{zh:"4.2 用AI辅助学习",ja:"4.2 AIで学習を支援"},content:{zh:`
AI是你的24小时私人教师，可以针对你的情况定制学习方案。

---

## 学习新技能

### 制定学习计划

\`\`\`
我想学习 Python 编程，目标是能独立开发简单的数据分析脚本。
我的背景：
- 完全没有编程经验
- 每天能学习1-2小时
- 希望3个月内入门

请帮我制定学习计划，包括：
1. 每周学习目标
2. 推荐的学习资源
3. 实践项目建议
\`\`\`

### 概念解释

**简单解释复杂概念**
\`\`\`
请用简单的语言解释"机器学习"，我没有技术背景。
如果可以，用一个日常生活的例子来帮助理解。
\`\`\`

AI 可能这样回答：
> 想象你教一个小朋友认识猫和狗。你不会告诉他"猫有这些特征..."，
> 而是给他看很多猫和狗的照片，说"这是猫，这是狗"。
>
> 看多了以后，小朋友自己就能分辨了，即使看到新的猫狗照片也能认出来。
>
> 机器学习就是这样——不是人写规则告诉计算机怎么做，
> 而是给它很多例子，让它自己"学会"规律。

**深入理解**
\`\`\`
我理解了基本概念，现在想深入了解。
请解释机器学习中的"监督学习"和"非监督学习"有什么区别？
用具体的应用例子说明。
\`\`\`

### 知识梳理

\`\`\`
我正在学习日语N3语法，请帮我整理：
1. 「ようにする」和「ことにする」的区别
2. 各自的典型用法
3. 容易混淆的情况
4. 练习例句（带中文翻译）
\`\`\`

---

## 考试备考

### 重点整理

\`\`\`
我在准备AWS云计算认证考试（SAA-C03）。
请帮我整理以下主题的核心知识点：
1. EC2实例类型和使用场景
2. S3存储类别对比
3. VPC网络基础

每个主题给出：
- 必须记住的关键概念
- 考试常见题型
- 容易混淆的地方
\`\`\`

### 模拟问答

\`\`\`
请扮演AWS考试的面试官，问我关于S3的问题。
从简单到困难，逐步提升难度。
如果我答错了，请解释正确答案。

准备好了，请开始提问。
\`\`\`

### 错题分析

\`\`\`
这道题我做错了，请帮我分析：

题目：[粘贴题目]
我的答案：B
正确答案：D

请解释：
1. 为什么B是错的？
2. 为什么D是对的？
3. 这道题考查的核心知识点是什么？
4. 类似的题目我应该怎么思考？
\`\`\`

---

## 阅读理解

### 文章解析

\`\`\`
请帮我分析这篇技术文章：
[粘贴文章]

我想了解：
1. 文章的主要观点是什么？
2. 作者的论据是什么？
3. 有哪些专业术语需要解释？
4. 文章的局限性或可商榷之处？
\`\`\`

### 论文阅读

\`\`\`
我在读这篇机器学习论文，但是很多地方看不懂：
[粘贴论文摘要或部分内容]

请帮我：
1. 用简单的语言概括这篇论文要解决什么问题
2. 解释核心方法是什么
3. 主要结论是什么
4. 这个研究有什么实际应用价值
\`\`\`

---

## 语言学习

### 口语练习

\`\`\`
请和我进行英语对话练习。
场景：在咖啡店点单
我的英语水平：中级（能简单交流）

规则：
1. 你扮演咖啡店店员
2. 如果我说的有语法错误，用括号指出
3. 每轮对话后，教我一个相关的常用表达

开始吧。
\`\`\`

### 写作练习

\`\`\`
请帮我修改这段英语作文：

[粘贴作文]

请指出：
1. 语法错误（用红色标注）
2. 不自然的表达（建议更地道的说法）
3. 可以用的高级词汇或句型
4. 整体写作建议
\`\`\`

### 词汇积累

\`\`\`
请帮我学习这10个日语单词：
働く、会議、資料、確認、報告、
提案、締め切り、優先、対応、検討

对于每个词，请提供：
1. 读音（平假名）
2. 中文意思
3. 一个实用例句（带翻译）
4. 相关的常用搭配
\`\`\`

---

## 学习方法指导

### 费曼学习法

\`\`\`
我刚学完"RESTful API"这个概念。
请让我向你解释，就像你是一个完全不懂技术的人。
如果我解释不清楚的地方，请提出问题，帮我发现理解的漏洞。
\`\`\`

### 知识关联

\`\`\`
我学了这些概念：HTTP、API、JSON、数据库
请帮我画一个概念图，说明这些知识点之间的关系，
以及它们在实际Web开发中是如何配合工作的。
\`\`\`

---

## 本节要点

| 学习场景 | AI 使用方式 |
|---------|------------|
| 新技能学习 | 制定计划、解释概念 |
| 考试备考 | 整理重点、模拟问答 |
| 阅读理解 | 分析文章、解释难点 |
| 语言学习 | 对话练习、作文批改 |

**学习效率秘诀**：
1. 不懂就问，没有"愚蠢的问题"
2. 让 AI 用不同方式解释同一概念
3. 主动用自己的话复述，让 AI 检验
4. 定期让 AI 测试你的知识掌握
            `,ja:`
AIは24時間対応の個人教師で、あなたの状況に合わせた学習プランを作れます。

---

## 新しいスキルを学ぶ

### 学習計画を立てる

\`\`\`
Pythonプログラミングを学びたいです。目標は簡単なデータ分析スクリプトを独自に開発すること。
私の背景：
- プログラミング経験なし
- 毎日1-2時間学習可能
- 3ヶ月で入門したい

学習計画を作ってください：
1. 週ごとの学習目標
2. おすすめの学習リソース
3. 実践プロジェクトの提案
\`\`\`

### 概念説明

**複雑な概念を簡単に説明**
\`\`\`
「機械学習」を簡単な言葉で説明してください。技術的な背景がありません。
できれば、日常生活の例を使って理解を助けてください。
\`\`\`

AIの回答例：
> 子供に猫と犬を見分けることを教えることを想像してください。
> 「猫にはこういう特徴がある...」と説明するのではなく、
> たくさんの猫と犬の写真を見せて「これは猫、これは犬」と言います。
>
> たくさん見た後、子供は自分で見分けられるようになり、
> 新しい猫や犬の写真でも認識できます。
>
> 機械学習もこれと同じです——人間がルールを書いてコンピュータに
> 指示するのではなく、多くの例を与えて、自分で「学習」させます。

**深く理解する**
\`\`\`
基本概念は理解しました。もっと深く知りたいです。
機械学習の「教師あり学習」と「教師なし学習」の違いを説明してください。
具体的な応用例で説明してください。
\`\`\`

### 知識の整理

\`\`\`
日本語のN3文法を勉強しています。以下を整理してください：
1. 「ようにする」と「ことにする」の違い
2. それぞれの典型的な使い方
3. 混同しやすい状況
4. 練習例文（英訳付き）
\`\`\`

---

## 試験対策

### 重要ポイント整理

\`\`\`
AWSクラウド認定試験（SAA-C03）の準備をしています。
以下のトピックの核心知識をまとめてください：
1. EC2インスタンスタイプと使用シーン
2. S3ストレージクラスの比較
3. VPCネットワークの基礎

各トピックについて：
- 必須の重要概念
- 試験でよく出る問題形式
- 混同しやすいポイント
\`\`\`

### 模擬Q&A

\`\`\`
AWS試験の面接官役をしてください。S3について質問してください。
簡単から難しいへ、徐々に難易度を上げてください。
間違えたら、正解を説明してください。

準備できました。質問を始めてください。
\`\`\`

### 間違い分析

\`\`\`
この問題を間違えました。分析してください：

問題：[問題を貼り付け]
私の答え：B
正解：D

説明してください：
1. なぜBが間違いか？
2. なぜDが正解か？
3. この問題が問う核心知識は何か？
4. 似た問題にどうアプローチすべきか？
\`\`\`

---

## 読解力

### 記事分析

\`\`\`
この技術記事を分析してください：
[記事を貼り付け]

知りたいこと：
1. 記事の主なポイントは何か？
2. 著者の論拠は何か？
3. どの専門用語を説明すべきか？
4. 記事の限界や議論の余地は？
\`\`\`

### 論文読解

\`\`\`
この機械学習の論文を読んでいますが、多くの部分が理解できません：
[論文の要約または一部を貼り付け]

助けてください：
1. この論文が解決しようとしている問題を簡単に要約
2. 核心手法を説明
3. 主な結論は何か
4. この研究の実用的な応用価値は
\`\`\`

---

## 言語学習

### 会話練習

\`\`\`
英語の会話練習をしてください。
シーン：カフェで注文
私の英語レベル：中級（簡単な会話可能）

ルール：
1. カフェの店員役をして
2. 文法エラーがあれば括弧で指摘
3. 各会話の後、関連する便利な表現を教えて

始めてください。
\`\`\`

### 作文練習

\`\`\`
この英作文を添削してください：

[作文を貼り付け]

指摘してください：
1. 文法エラー（赤でマーク）
2. 不自然な表現（より自然な言い方を提案）
3. 使える高度な語彙や構文
4. 全体的なライティングアドバイス
\`\`\`

### 語彙強化

\`\`\`
以下の10の英単語を学ぶのを手伝ってください：
implement, collaborate, optimize, integrate, prioritize,
evaluate, facilitate, escalate, delegate, consolidate

各単語について：
1. 発音記号
2. 日本語の意味
3. 実用例文（翻訳付き）
4. よく使う組み合わせ
\`\`\`

---

## 学習方法ガイド

### ファインマン・テクニック

\`\`\`
「RESTful API」の概念を学びました。
技術を全く知らない人に説明するように、私に説明させてください。
説明が不明確な部分があれば、質問して理解の穴を見つける手伝いをしてください。
\`\`\`

### 知識の関連付け

\`\`\`
以下の概念を学びました：HTTP、API、JSON、データベース
これらの知識ポイントの関係を示すコンセプトマップを描いて、
実際のWeb開発でどう連携するか説明してください。
\`\`\`

---

## このセクションのポイント

| 学習シーン | AIの活用方法 |
|---------|------------|
| 新スキル習得 | 計画作成、概念説明 |
| 試験対策 | 重点整理、模擬Q&A |
| 読解力 | 記事分析、難点説明 |
| 語学学習 | 会話練習、作文添削 |

**学習効率アップの秘訣**：
1. 分からないことは何でも聞く、「バカな質問」はない
2. AIに同じ概念を違う方法で説明させる
3. 自分の言葉で言い直して、AIにチェックしてもらう
4. 定期的にAIに知識のテストをしてもらう
            `}},{id:"ch4-creative",title:{zh:"4.3 用AI进行创意创作",ja:"4.3 AIでクリエイティブ制作"},content:{zh:`
AI 不仅能处理逻辑任务，还能成为你的创意伙伴。

---

## 内容创作

### 文案写作

**社交媒体文案**
\`\`\`
我们要推广一款新的智能手表，特点是：
- 超长续航（30天）
- 健康监测（血氧、心率、睡眠）
- 时尚设计

请为以下平台各写一条推广文案：
1. 微博（140字以内）
2. 小红书（种草风格）
3. LinkedIn（专业商务风格）
\`\`\`

**广告标语**
\`\`\`
为这款智能手表想10个广告标语：
- 目标人群：25-35岁职场人士
- 品牌调性：科技、健康、简约
- 要求：简短有力，容易记住
\`\`\`

### 故事创作

**故事大纲**
\`\`\`
我想写一个短篇科幻故事，主题是"人类和AI共处的未来"。
请帮我设计：
1. 故事背景设定
2. 主要角色（2-3个）
3. 核心冲突
4. 故事大纲（起承转合）
\`\`\`

**情节发展**
\`\`\`
我已经写好了开头：
[粘贴你写的开头]

请帮我：
1. 评价这个开头的优缺点
2. 建议后续情节发展的几个方向
3. 为最有趣的方向写下一段
\`\`\`

---

## 设计辅助

### 产品设计

**需求分析**
\`\`\`
我想设计一款面向老年人的健康管理App。
请帮我分析：
1. 目标用户的特点和需求
2. 核心功能建议
3. 界面设计原则（考虑老年用户）
4. 潜在的使用障碍
\`\`\`

**功能规划**
\`\`\`
基于之前的分析，为这款老年健康App设计功能列表：

格式：
- 功能名称
- 功能描述
- 使用场景
- 优先级（高/中/低）

请给出核心功能（5-8个）
\`\`\`

### 视觉创意

**配色方案**
\`\`\`
我在设计一个心理健康类的网站，请推荐配色方案：
- 品牌调性：温暖、安心、专业
- 目标用户：成年人
- 参考网站：Headspace, Calm

请提供：
1. 主色调
2. 辅助色
3. 强调色
4. 色彩含义说明
5. 使用建议
\`\`\`

**图片描述**（用于AI图像生成）
\`\`\`
我需要一张网站首页的配图，请帮我写描述：
主题：冥想与放松
风格：现代简约、温暖色调
元素：可以包含人物或抽象图形
尺寸：16:9 横版
\`\`\`

---

## 音乐与艺术

### 音乐创作辅助

**歌词创作**
\`\`\`
请帮我写一首关于"在城市中追寻梦想"的歌词：
- 风格：流行抒情
- 结构：主歌-副歌-主歌-副歌-桥段-副歌
- 情感：既有迷茫，也有坚定

副歌要朗朗上口，适合作为歌曲高潮部分。
\`\`\`

**乐曲分析**
\`\`\`
请分析这首歌的音乐结构：
歌曲名：[歌曲名]

我想了解：
1. 曲式结构
2. 和弦进行特点
3. 情绪变化
4. 这首歌为什么好听
\`\`\`

### 艺术欣赏

\`\`\`
请介绍梵高的《星月夜》：
1. 创作背景
2. 艺术特点（构图、色彩、笔触）
3. 艺术价值和影响
4. 如何欣赏这幅画

我是艺术入门者，请用通俗的语言。
\`\`\`

---

## 头脑风暴

### 创意发散

\`\`\`
我需要为公司年会策划一个主题。
公司情况：IT公司，员工200人，平均年龄30岁

请用发散思维，从不同角度给我20个创意方向：
- 传统正式的
- 轻松有趣的
- 科技感的
- 复古怀旧的
- 其他创新方向
\`\`\`

### 问题解决

\`\`\`
我们的产品用户留存率低，7天留存只有15%。
请用不同思维模型帮我分析：

1. 用"第一性原理"分析：问题的本质是什么？
2. 用"反向思维"分析：如果想让留存更差，会怎么做？
3. 用"类比思维"分析：其他行业怎么解决类似问题？
4. 给出具体可行的改进建议
\`\`\`

---

## 个人品牌

### 自我介绍

\`\`\`
请帮我写几个版本的自我介绍：

背景信息：
- 5年产品经理经验
- 擅长B端SaaS产品
- 带过3个产品从0到1
- 爱好：读书、跑步

版本：
1. 30秒电梯演讲
2. 面试正式版
3. 社交场合轻松版
4. 社交媒体简介
\`\`\`

### 内容规划

\`\`\`
我想在知乎建立产品经理领域的个人品牌。
请帮我规划内容策略：

1. 账号定位建议
2. 内容主题（10个选题方向）
3. 内容形式建议
4. 发布频率建议
5. 前3个月的内容计划
\`\`\`

---

## 本节要点

| 创意场景 | AI 辅助方式 |
|---------|------------|
| 文案写作 | 多版本、多风格文案 |
| 故事创作 | 大纲、情节、角色 |
| 设计辅助 | 需求分析、功能规划 |
| 头脑风暴 | 创意发散、问题分析 |

**创意工作秘诀**：
1. 用 AI 打破创作空白，获得起点
2. 让 AI 提供多个方向，自己选择
3. AI 出创意，人来判断和执行
4. 迭代修改比一次完美更有效
            `,ja:`
AIはロジックタスクだけでなく、クリエイティブなパートナーにもなれます。

---

## コンテンツ制作

### コピーライティング

**ソーシャルメディア投稿**
\`\`\`
新しいスマートウォッチを宣伝します。特徴：
- 超長バッテリー（30日）
- 健康モニタリング（血中酸素、心拍数、睡眠）
- スタイリッシュなデザイン

以下のプラットフォーム用に投稿を書いてください：
1. Twitter（140文字以内）
2. Instagram（ビジュアル重視スタイル）
3. LinkedIn（プロフェッショナルスタイル）
\`\`\`

**キャッチコピー**
\`\`\`
このスマートウォッチのキャッチコピーを10個考えてください：
- ターゲット：25-35歳の社会人
- ブランドトーン：テクノロジー、健康、シンプル
- 要件：短く力強く、記憶に残る
\`\`\`

### ストーリー創作

**ストーリー概要**
\`\`\`
「人類とAIが共存する未来」をテーマに短編SF小説を書きたいです。
デザインを手伝ってください：
1. 物語の背景設定
2. 主要キャラクター（2-3人）
3. 核心的な葛藤
4. ストーリー概要（起承転結）
\`\`\`

**プロット展開**
\`\`\`
冒頭を書きました：
[書いた冒頭を貼り付け]

手伝ってください：
1. この冒頭の長所と短所を評価
2. 続きの展開の方向性を提案
3. 最も興味深い方向の次のパラグラフを書く
\`\`\`

---

## デザイン支援

### プロダクトデザイン

**要件分析**
\`\`\`
高齢者向けの健康管理アプリを設計したいです。
分析を手伝ってください：
1. ターゲットユーザーの特徴とニーズ
2. コア機能の提案
3. UIデザイン原則（高齢ユーザーを考慮）
4. 潜在的な使用障壁
\`\`\`

**機能計画**
\`\`\`
先ほどの分析に基づいて、この高齢者向け健康アプリの機能リストを設計してください：

形式：
- 機能名
- 機能説明
- 使用シーン
- 優先度（高/中/低）

コア機能（5-8個）を提示してください
\`\`\`

### ビジュアルクリエイティブ

**カラーパレット**
\`\`\`
メンタルヘルス関連のウェブサイトをデザインしています。カラーパレットを推薦してください：
- ブランドトーン：温かい、安心、プロフェッショナル
- ターゲット：大人
- 参考サイト：Headspace, Calm

提供してください：
1. メインカラー
2. サブカラー
3. アクセントカラー
4. カラーの意味の説明
5. 使用アドバイス
\`\`\`

**画像説明**（AI画像生成用）
\`\`\`
ウェブサイトのトップページ用の画像が必要です。説明を書いてください：
テーマ：瞑想とリラックス
スタイル：モダンミニマル、温かい色調
要素：人物または抽象的なグラフィック
サイズ：16:9 横長
\`\`\`

---

## 音楽とアート

### 音楽制作支援

**歌詞作成**
\`\`\`
「都会で夢を追いかける」をテーマに歌詞を書いてください：
- スタイル：ポップバラード
- 構成：Aメロ-サビ-Aメロ-サビ-ブリッジ-サビ
- 感情：迷いと決意の両方

サビはキャッチーで、クライマックスに適したものに。
\`\`\`

**曲の分析**
\`\`\`
この曲の音楽構造を分析してください：
曲名：[曲名]

知りたいこと：
1. 曲の構造
2. コード進行の特徴
3. 感情の変化
4. なぜこの曲が良いのか
\`\`\`

### アート鑑賞

\`\`\`
ゴッホの『星月夜』を紹介してください：
1. 創作背景
2. 芸術的特徴（構図、色彩、筆致）
3. 芸術的価値と影響
4. この絵をどう鑑賞するか

アート初心者なので、分かりやすい言葉でお願いします。
\`\`\`

---

## ブレインストーミング

### アイデア発散

\`\`\`
会社の忘年会のテーマを企画する必要があります。
会社情報：IT企業、社員200人、平均年齢30歳

発散思考で、異なる視点から20のアイデア方向を出してください：
- 伝統的でフォーマルな
- 軽くて楽しい
- テック感のある
- レトロノスタルジック
- その他の革新的な方向
\`\`\`

### 問題解決

\`\`\`
製品のユーザーリテンション率が低く、7日リテンションが15%しかありません。
異なる思考モデルで分析を手伝ってください：

1. 「第一原理」分析：問題の本質は何か？
2. 「逆転思考」分析：リテンションをさらに悪くするには？
3. 「アナロジー思考」分析：他業界はどう解決している？
4. 具体的で実行可能な改善提案
\`\`\`

---

## パーソナルブランド

### 自己紹介

\`\`\`
自己紹介を複数バージョン書いてください：

背景情報：
- プロダクトマネージャー5年経験
- B2B SaaS製品が得意
- 3つの製品を0から1へ立ち上げ
- 趣味：読書、ランニング

バージョン：
1. 30秒エレベーターピッチ
2. 面接用フォーマル版
3. 社交場でのカジュアル版
4. SNSプロフィール
\`\`\`

### コンテンツ戦略

\`\`\`
Qiitaでプロダクトマネージャー領域のパーソナルブランドを構築したいです。
コンテンツ戦略を計画してください：

1. アカウントポジショニング提案
2. コンテンツテーマ（10のトピック方向）
3. コンテンツ形式の提案
4. 投稿頻度の提案
5. 最初の3ヶ月のコンテンツ計画
\`\`\`

---

## このセクションのポイント

| クリエイティブシーン | AI支援方法 |
|---------|------------|
| コピーライティング | 複数バージョン、複数スタイル |
| ストーリー創作 | 概要、プロット、キャラクター |
| デザイン支援 | 要件分析、機能計画 |
| ブレインストーミング | アイデア発散、問題分析 |

**クリエイティブ作業の秘訣**：
1. AIでライターズブロックを打破、出発点を得る
2. AIに複数の方向を出させ、自分で選ぶ
3. AIがアイデアを出し、人が判断と実行
4. 一度で完璧よりも反復修正が効果的
            `}},{id:"ch4-daily-life",title:{zh:"4.4 AI在日常生活中的应用",ja:"4.4 日常生活でのAI活用"},content:{zh:`
AI 不仅能帮助工作和学习，还能让日常生活更轻松。

---

## 旅行规划

### 行程规划

\`\`\`
我计划国庆期间去日本旅行，请帮我规划行程：

基本信息：
- 时间：10月1日-7日（7天）
- 人数：2人（情侣）
- 出发地：上海
- 预算：每人15000元左右
- 偏好：美食、文化体验、购物

请给出：
1. 推荐目的地
2. 每日行程安排
3. 必去景点和餐厅
4. 大致预算分配
\`\`\`

### 攻略整理

\`\`\`
我要去京都参观金阁寺和伏见稻荷大社，请给我实用攻略：

1. 怎么去（从京都站出发）
2. 开放时间和门票
3. 游览建议（最佳时间、拍照点）
4. 周边可以顺便去的地方
5. 需要注意的事项
\`\`\`

### 行前准备

\`\`\`
下周要去日本旅行，请帮我列一个行前准备清单：

需要考虑：
- 证件和文件
- 换汇和支付
- 通讯（电话卡/WiFi）
- 必备物品
- App下载
- 其他注意事项

我是第一次去日本。
\`\`\`

---

## 健康生活

### 饮食建议

\`\`\`
我最近想减脂，请帮我制定一周的饮食计划：

我的情况：
- 身高170cm，体重75kg，男
- 目标：一个月减5斤
- 每天在公司食堂吃午饭
- 对食物不太挑剔

请给出：
1. 每日热量目标
2. 三餐食谱建议
3. 可以吃的零食
4. 需要避免的食物
\`\`\`

### 健身计划

\`\`\`
我想开始健身，但只能在家练习：

我的情况：
- 完全没有运动基础
- 每天能练30-40分钟
- 没有任何器材
- 目标是增肌和提高体能

请给我一个4周入门计划：
- 每周训练安排
- 每次训练动作详解
- 循序渐进的难度提升
\`\`\`

### 健康咨询

\`\`\`
我最近总是睡不好，入睡困难。请给我一些建议：

我的情况：
- 通常晚上11点上床，但经常1点多才睡着
- 不喝咖啡，不熬夜
- 工作压力比较大

请从这几方面给建议：
1. 睡前习惯调整
2. 卧室环境改善
3. 白天的习惯
4. 是否需要就医的判断

注意：如果涉及用药，请提醒我咨询医生。
\`\`\`

---

## 家庭生活

### 美食烹饪

\`\`\`
我今天买了这些食材，请给我晚餐建议：
- 鸡胸肉 300g
- 西兰花 一颗
- 土豆 两个
- 鸡蛋 4个
- 米饭

要求：
1. 简单易做（厨房新手）
2. 健康低脂
3. 30分钟内完成
\`\`\`

**详细食谱**
\`\`\`
请给我详细的"香煎鸡胸肉"食谱：

格式：
1. 所需食材和调料（精确用量）
2. 准备工作
3. 步骤详解（每步操作+时间）
4. 小技巧和注意事项
5. 可能的变化做法

我是厨房新手，请写得详细些。
\`\`\`

### 家务整理

\`\`\`
我想整理家里的衣柜，东西太多太乱了。
请给我一个整理指南：

1. 整理前的准备
2. 分类方法
3. 断舍离的标准
4. 收纳技巧
5. 保持整洁的习惯
\`\`\`

### 宠物照顾

\`\`\`
我刚养了一只2个月大的金毛幼犬。
作为新手，我需要知道什么？

请告诉我：
1. 每天喂食安排
2. 疫苗和驱虫计划
3. 基础训练方法
4. 常见问题处理
5. 必备用品清单
\`\`\`

---

## 购物决策

### 产品对比

\`\`\`
我想买一台笔记本电脑，预算8000元左右。

用途：
- 日常办公（Word、Excel、网页）
- 偶尔修图（Lightroom）
- 希望轻薄便携

请推荐3-5款合适的型号，并给出对比：
- 价格
- 核心配置
- 优缺点
- 最适合的使用场景
\`\`\`

### 价格分析

\`\`\`
双十一想买一台iPhone，请帮我分析：
1. 往年iPhone双十一的折扣力度
2. 在哪个平台买最划算
3. 有哪些叠加优惠的方法
4. 什么时候下单最好
\`\`\`

---

## 解决问题

### 家电维修

\`\`\`
我家空调制冷效果变差了，开了一会儿不太凉。
可能是什么原因？
我能自己做哪些检查和处理？
什么情况需要找师傅？
\`\`\`

### 法律咨询

\`\`\`
我租的房子到期，但房东不肯退押金，说墙面有污渍。
请问：
1. 正常使用的损耗应该谁负责？
2. 我可以怎么维权？
3. 需要保留哪些证据？

注意：如果涉及诉讼，请提醒我咨询律师。
\`\`\`

### 情感倾诉

\`\`\`
最近工作压力很大，感觉很焦虑。
不想和朋友说怕麻烦他们。
你能陪我聊聊吗？

（AI会像一个耐心的倾听者，帮你理清思路，
提供一些建议，但会在适当时候建议你寻求专业帮助）
\`\`\`

---

## 本节要点

| 生活场景 | AI 帮助方式 |
|---------|------------|
| 旅行规划 | 行程、攻略、清单 |
| 健康生活 | 饮食、健身、睡眠 |
| 家庭生活 | 烹饪、整理、宠物 |
| 购物决策 | 对比、分析、建议 |
| 问题解决 | 诊断、指导、参考 |

**使用提示**：
1. AI 建议仅供参考，重要事项要核实
2. 健康问题严重请就医
3. 法律问题复杂请咨询律师
4. AI 是助手，不是专家替代
            `,ja:`
AIは仕事や学習だけでなく、日常生活も楽にできます。

---

## 旅行計画

### 旅程計画

\`\`\`
ゴールデンウィークに韓国旅行を計画しています。旅程を計画してください：

基本情報：
- 期間：5月1日-5日（5日間）
- 人数：2人（カップル）
- 出発地：東京
- 予算：1人15万円程度
- 好み：グルメ、文化体験、ショッピング

提供してください：
1. おすすめの目的地
2. 日ごとの旅程
3. 必見スポットとレストラン
4. おおよその予算配分
\`\`\`

### 攻略整理

\`\`\`
ソウルで景福宮と明洞を訪問予定です。実用的なガイドをください：

1. アクセス方法（ソウル駅から）
2. 営業時間と入場料
3. 観光のコツ（ベストタイム、フォトスポット）
4. 近くで立ち寄れる場所
5. 注意事項
\`\`\`

### 出発前準備

\`\`\`
来週韓国旅行に行きます。出発前チェックリストを作ってください：

考慮事項：
- 書類と証明書
- 両替と支払い
- 通信（SIMカード/WiFi）
- 必需品
- アプリのダウンロード
- その他の注意点

初めての韓国旅行です。
\`\`\`

---

## 健康生活

### 食事アドバイス

\`\`\`
最近ダイエットしたいです。1週間の食事プランを作ってください：

私の状況：
- 身長170cm、体重75kg、男性
- 目標：1ヶ月で2kg減
- 昼食は会社の食堂
- 食べ物は特に好き嫌いなし

提供してください：
1. 1日のカロリー目標
2. 3食のメニュー提案
3. 食べてもいい間食
4. 避けるべき食べ物
\`\`\`

### 筋トレプラン

\`\`\`
トレーニングを始めたいですが、自宅でしかできません：

私の状況：
- 運動経験なし
- 毎日30-40分トレーニング可能
- 器具なし
- 目標は筋肉増量と体力向上

4週間の入門プランをください：
- 週間トレーニングスケジュール
- 各トレーニングの動作詳細
- 段階的な難易度アップ
\`\`\`

### 健康相談

\`\`\`
最近よく眠れません。寝付きが悪いです。アドバイスをください：

私の状況：
- 普段11時に就寝、1時過ぎまで眠れないことが多い
- コーヒーは飲まない、夜更かしはしない
- 仕事のストレスが大きい

以下の観点からアドバイス：
1. 就寝前の習慣調整
2. 寝室環境の改善
3. 日中の習慣
4. 受診が必要かの判断

注意：薬に関しては医師に相談するよう促してください。
\`\`\`

---

## 家庭生活

### 料理

\`\`\`
今日買った食材で夕食の提案をしてください：
- 鶏むね肉 300g
- ブロッコリー 1株
- じゃがいも 2個
- 卵 4個
- ご飯

要件：
1. 簡単に作れる（料理初心者）
2. ヘルシー低脂肪
3. 30分以内で完成
\`\`\`

**詳細レシピ**
\`\`\`
「チキンソテー」の詳細レシピをください：

形式：
1. 必要な食材と調味料（正確な分量）
2. 下準備
3. 手順詳細（各ステップの操作と時間）
4. コツと注意点
5. バリエーション

料理初心者なので、詳しく書いてください。
\`\`\`

### 家事整理

\`\`\`
クローゼットを整理したいです。物が多すぎて散らかっています。
整理ガイドをください：

1. 整理前の準備
2. 分類方法
3. 断捨離の基準
4. 収納テクニック
5. きれいを保つ習慣
\`\`\`

### ペットの世話

\`\`\`
2ヶ月の柴犬の子犬を飼い始めました。
初心者として何を知っておくべきですか？

教えてください：
1. 毎日の給餌スケジュール
2. ワクチンと駆虫計画
3. 基本的なトレーニング方法
4. よくある問題への対処
5. 必需品リスト
\`\`\`

---

## 買い物の決定

### 製品比較

\`\`\`
ノートパソコンを買いたいです。予算は10万円程度。

用途：
- 日常業務（Word、Excel、ウェブ閲覧）
- たまに写真編集（Lightroom）
- 軽くて持ち運びやすい希望

3-5機種のおすすめと比較をください：
- 価格
- 主要スペック
- メリット・デメリット
- 最適な使用シーン
\`\`\`

### 価格分析

\`\`\`
ブラックフライデーにiPhoneを買いたいです。分析してください：
1. 過去のiPhoneのブラックフライデー割引状況
2. どのプラットフォームが最もお得か
3. 割引を重ねる方法
4. いつ注文するのがベストか
\`\`\`

---

## 問題解決

### 家電トラブル

\`\`\`
エアコンの冷房効果が悪くなりました。しばらく動かしてもあまり涼しくありません。
原因は何でしょうか？
自分でできるチェックと対処は？
業者を呼ぶべき状況は？
\`\`\`

### 法律相談

\`\`\`
借りていた部屋の契約が終わりましたが、大家が敷金を返してくれません。壁に汚れがあると言っています。
質問：
1. 通常使用による損耗は誰の責任？
2. どう権利を主張できる？
3. どんな証拠を残すべき？

注意：訴訟に関しては弁護士に相談するよう促してください。
\`\`\`

### 感情の吐露

\`\`\`
最近仕事のストレスが大きくて、とても不安です。
友達に言うと迷惑かけそうで言いたくない。
話を聞いてくれますか？

（AIは忍耐強いリスナーとして、考えを整理し、
アドバイスを提供しますが、適切なタイミングで
専門家の助けを求めることを勧めます）
\`\`\`

---

## このセクションのポイント

| 生活シーン | AIの支援方法 |
|---------|------------|
| 旅行計画 | 旅程、攻略、チェックリスト |
| 健康生活 | 食事、運動、睡眠 |
| 家庭生活 | 料理、整理、ペット |
| 買い物決定 | 比較、分析、アドバイス |
| 問題解決 | 診断、指導、参考 |

**使用のヒント**：
1. AIのアドバイスは参考程度、重要事項は確認を
2. 健康問題が深刻なら医師へ
3. 法律問題が複雑なら弁護士へ
4. AIはアシスタント、専門家の代わりではない
            `}},{id:"ch4-summary",title:{zh:"4.5 本章小结",ja:"4.5 この章のまとめ"},content:{zh:`
恭喜你学完了 AI 应用场景！让我们回顾一下。

---

## 应用场景回顾

### 工作效率
- **文档处理**：起草、润色、翻译
- **邮件沟通**：写作、回复、协调
- **会议辅助**：准备、记录、整理

### 学习辅助
- **技能学习**：计划、解释、梳理
- **考试备考**：重点、问答、错题
- **语言学习**：对话、写作、词汇

### 创意创作
- **内容创作**：文案、故事、设计
- **头脑风暴**：发散、分析、决策
- **个人品牌**：定位、策略、内容

### 日常生活
- **旅行规划**：行程、攻略、准备
- **健康生活**：饮食、健身、作息
- **购物决策**：对比、分析、时机

---

## 使用AI的核心原则

### 1. AI 是工具，不是专家
- 提供参考和起点
- 重要决策要人来把关
- 专业问题找专业人士

### 2. 给得越多，得到越多
- 详细的背景信息
- 明确的目标和要求
- 具体的约束条件

### 3. 迭代优化比一次完美更重要
- 先快速得到初稿
- 再逐步修改完善
- 多试几种方向

### 4. 保持批判性思维
- 核实重要信息
- 质疑不确定内容
- 结合自己的判断

---

## 你已经学会了

✅ 用 AI 提升工作效率
✅ 用 AI 辅助学习进步
✅ 用 AI 激发创意灵感
✅ 用 AI 解决生活问题

---

## AI 使用的边界

### AI 能做好的事
- 处理信息和文字
- 提供思路和建议
- 解答知识问题
- 辅助创意工作

### AI 不适合做的事
- 做重大人生决策
- 替代专业医疗/法律建议
- 处理高度敏感信息
- 完全替代人际交流

---

## 接下来

在最后一章，我们将探讨：
- AI 技术的发展趋势
- 如何持续提升 AI 使用能力
- AI 时代的学习与成长

让我们一起展望 AI 的未来！
            `,ja:`
AI応用シーンの学習完了おめでとうございます！振り返ってみましょう。

---

## 応用シーン振り返り

### 仕事効率
- **ドキュメント処理**：起草、推敲、翻訳
- **メールコミュニケーション**：作成、返信、調整
- **会議サポート**：準備、記録、整理

### 学習支援
- **スキル学習**：計画、説明、整理
- **試験対策**：重点、Q&A、間違い分析
- **語学学習**：会話、作文、語彙

### クリエイティブ制作
- **コンテンツ制作**：コピー、ストーリー、デザイン
- **ブレインストーミング**：発散、分析、意思決定
- **パーソナルブランド**：ポジショニング、戦略、コンテンツ

### 日常生活
- **旅行計画**：旅程、攻略、準備
- **健康生活**：食事、運動、生活習慣
- **買い物決定**：比較、分析、タイミング

---

## AI使用の核心原則

### 1. AIはツール、専門家ではない
- 参考と出発点を提供
- 重要な決定は人が確認
- 専門的な問題は専門家へ

### 2. 与えるほど、得られる
- 詳細な背景情報
- 明確な目標と要件
- 具体的な制約条件

### 3. 反復改善は一度の完璧より重要
- まず素早く初稿を得る
- 段階的に修正・改善
- 複数の方向を試す

### 4. 批判的思考を保つ
- 重要情報を確認
- 不確実な内容を疑う
- 自分の判断と組み合わせる

---

## 習得したこと

✅ AIで仕事効率を上げる
✅ AIで学習を進める
✅ AIでクリエイティブなインスピレーションを得る
✅ AIで生活の問題を解決

---

## AI使用の境界

### AIが得意なこと
- 情報とテキストの処理
- アイデアと提案の提供
- 知識の質問に回答
- クリエイティブ作業の支援

### AIに向かないこと
- 重大な人生の決定
- 専門的な医療/法律アドバイスの代替
- 高度に機密性の高い情報の処理
- 人間関係の完全な代替

---

## 次へ

最終章では以下を探ります：
- AI技術の発展トレンド
- AI活用能力を継続的に向上させる方法
- AI時代の学習と成長

AIの未来を一緒に展望しましょう！
            `}}]},{id:"chapter-5",number:5,title:{zh:"AI与未来",ja:"AIと未来"},subtitle:{zh:"在AI时代持续成长",ja:"AI時代に成長し続ける"},sections:[{id:"ch5-ai-trends",title:{zh:"5.1 AI技术发展趋势",ja:"5.1 AI技術の発展トレンド"},content:{zh:`
AI 技术正在快速发展，了解趋势能帮你更好地把握未来。

---

## 当前AI的能力边界

### AI 已经很擅长的事
- **文字处理**：写作、翻译、总结、问答
- **编程辅助**：代码生成、调试、解释
- **图像生成**：根据描述创作图片
- **数据分析**：处理和解读数据
- **对话交流**：理解上下文、有逻辑地回应

### AI 目前的局限
| 局限 | 说明 |
|------|------|
| 知识更新 | 训练数据有截止日期，不了解最新事件 |
| 准确性 | 可能产生"幻觉"，编造不存在的信息 |
| 深度推理 | 复杂逻辑推理可能出错 |
| 创造性 | 创意基于已有数据，难以真正创新 |
| 情感理解 | 可以模拟但不能真正理解情感 |

---

## 正在发展的方向

### 多模态融合
未来的 AI 将能同时处理：
- 文字 + 图片 + 语音 + 视频
- 理解复杂的场景和情境
- 生成更丰富的内容形式

**例子**：
- 描述一个场景，AI 生成配套的图片、音乐和视频
- 上传一张图片，AI 分析并给出详细建议
- 用语音交流，AI 同时看你的屏幕提供帮助

### AI Agent（智能代理）
让 AI 能够自主完成复杂任务：
- 分解任务、制定计划
- 调用各种工具和 API
- 根据反馈调整行动
- 最终达成目标

**例子**：
- "帮我订下周去北京的机票和酒店" → AI 自动搜索、比价、预订
- "分析这个市场的竞争情况" → AI 自动搜集数据、分析、生成报告

### 个性化 AI
根据个人需求定制的 AI：
- 记住你的偏好和习惯
- 了解你的工作背景
- 适应你的沟通风格
- 像一个了解你的助手

### 实时学习
AI 能够从交互中学习：
- 根据你的反馈改进
- 适应新的信息和变化
- 不断提升服务质量

---

## 各行业的AI应用趋势

### 办公自动化
- 智能文档处理
- 自动会议记录和总结
- 智能日程管理
- 邮件自动分类和建议回复

### 教育培训
- 个性化学习路径
- 智能辅导和答疑
- 自动评估和反馈
- 虚拟学习助手

### 医疗健康
- 辅助诊断
- 药物研发
- 健康监测和预警
- 个性化治疗方案

### 创意设计
- AI 辅助设计
- 内容自动生成
- 创意灵感激发
- 风格迁移和编辑

---

## 关于AI的常见担忧

### "AI会取代我的工作吗？"

更准确的说法是：**会用AI的人会取代不会用AI的人**

- 重复性、规则明确的工作更容易被自动化
- 需要创造力、同理心、复杂判断的工作相对安全
- 学会与 AI 协作的人更有竞争力

### "AI会变得太强大失控吗？"

这是研究者认真对待的问题：
- AI 公司都在投入资源确保 AI 安全
- 有专门的 AI 安全研究领域
- 目前的 AI 还是工具，没有自主意识
- 但保持警惕和负责任的开发很重要

### "AI生成的内容有版权问题吗？"

这是一个还在讨论中的话题：
- 各国法律规定不同
- 建议：
  - 用 AI 辅助，不要完全照搬
  - 注明 AI 参与创作
  - 关注相关法规动态

---

## 本节要点

1. **了解 AI 的能力边界** - 知道它能做什么、不能做什么
2. **关注发展趋势** - 多模态、Agent、个性化
3. **行业应用广泛** - 各领域都在融入 AI
4. **理性看待担忧** - 既不过度恐惧，也不盲目乐观
            `,ja:`
AI技術は急速に発展しています。トレンドを理解することで、未来をより良く把握できます。

---

## 現在のAIの能力境界

### AIが得意なこと
- **テキスト処理**：ライティング、翻訳、要約、Q&A
- **プログラミング支援**：コード生成、デバッグ、説明
- **画像生成**：説明に基づいて画像を作成
- **データ分析**：データの処理と解釈
- **対話**：文脈を理解し、論理的に応答

### AIの現在の限界
| 限界 | 説明 |
|------|------|
| 知識の更新 | トレーニングデータには締め切りがあり、最新の出来事を知らない |
| 正確性 | 「ハルシネーション」を起こし、存在しない情報を作り出す可能性 |
| 深い推論 | 複雑な論理推論でエラーが発生する可能性 |
| 創造性 | 創意は既存データに基づき、真の革新は難しい |
| 感情理解 | シミュレートできるが、本当に感情を理解することはできない |

---

## 発展中の方向性

### マルチモーダル融合
未来のAIは同時に処理できる：
- テキスト + 画像 + 音声 + 動画
- 複雑なシーンと状況を理解
- より豊かなコンテンツ形式を生成

**例**：
- シーンを説明すると、AIが画像、音楽、動画をセットで生成
- 画像をアップロードすると、AIが分析して詳細なアドバイス
- 音声で会話しながら、AIが画面を見て支援

### AI Agent（インテリジェントエージェント）
AIが複雑なタスクを自律的に完了：
- タスクを分解、計画を立てる
- 各種ツールとAPIを呼び出す
- フィードバックに基づいて行動を調整
- 最終的に目標を達成

**例**：
- 「来週の北京行きの航空券とホテルを予約して」→ AIが自動で検索、比較、予約
- 「この市場の競合状況を分析して」→ AIが自動でデータ収集、分析、レポート生成

### パーソナライズドAI
個人のニーズに合わせたAI：
- あなたの好みと習慣を覚える
- あなたの仕事の背景を理解
- あなたのコミュニケーションスタイルに適応
- あなたを知るアシスタントのように

### リアルタイム学習
AIがインタラクションから学習：
- あなたのフィードバックに基づいて改善
- 新しい情報と変化に適応
- サービス品質を継続的に向上

---

## 各業界のAI応用トレンド

### オフィス自動化
- スマートドキュメント処理
- 自動会議記録と要約
- スマートスケジュール管理
- メールの自動分類と返信提案

### 教育・研修
- パーソナライズされた学習パス
- スマートチュータリングとQ&A
- 自動評価とフィードバック
- バーチャル学習アシスタント

### 医療・ヘルスケア
- 診断支援
- 創薬
- 健康モニタリングと警告
- パーソナライズされた治療計画

### クリエイティブデザイン
- AIアシストデザイン
- コンテンツ自動生成
- クリエイティブインスピレーション
- スタイル変換と編集

---

## AIに関するよくある懸念

### 「AIは私の仕事を奪うか？」

より正確には：**AIを使える人が使えない人に取って代わる**

- 繰り返し的でルールが明確な仕事は自動化されやすい
- 創造性、共感、複雑な判断が必要な仕事は比較的安全
- AIと協働できる人はより競争力がある

### 「AIが強力すぎて制御不能になる？」

これは研究者が真剣に取り組んでいる問題：
- AI企業はAI安全性の確保にリソースを投入
- 専門のAI安全研究分野がある
- 現在のAIはまだツールで、自律意識はない
- しかし警戒と責任ある開発は重要

### 「AI生成コンテンツに著作権問題は？」

まだ議論中のトピック：
- 国によって法律が異なる
- 提案：
  - AIは補助として使い、完全にコピーしない
  - AI参加を明記
  - 関連法規の動向に注目

---

## このセクションのポイント

1. **AIの能力境界を理解** - 何ができて何ができないかを知る
2. **発展トレンドに注目** - マルチモーダル、Agent、パーソナライゼーション
3. **業界応用は広範** - あらゆる分野がAIを取り入れている
4. **懸念を理性的に見る** - 過度な恐怖も盲目的な楽観もしない
            `}},{id:"ch5-continuous-learning",title:{zh:"5.2 持续提升AI使用能力",ja:"5.2 AI活用能力を継続的に向上"},content:{zh:`
AI 技术在快速发展，保持学习是跟上节奏的关键。

---

## 建立学习习惯

### 日常使用
最好的学习就是在实际工作中使用 AI：

**养成习惯**
- 遇到问题先想想能否用 AI 帮助
- 每天至少尝试一个新的使用场景
- 记录有效的提示语，建立自己的模板库

**刻意练习**
- 对比不同提示语的效果
- 尝试让 AI 完成稍有挑战的任务
- 分析失败的原因，改进方法

### 关注更新
AI 工具经常更新新功能：

**信息来源**
- 官方博客和更新日志
- 科技新闻网站
- 技术社区讨论
- 社交媒体上的实用技巧分享

**推荐关注**
- OpenAI 博客
- Google AI 博客
- Anthropic 博客
- 科技媒体的 AI 专栏

---

## 进阶学习路径

### 第一阶段：熟练使用
- 掌握基本对话技巧
- 了解常见使用场景
- 能写出清晰的提示语
- 知道 AI 的能力边界

### 第二阶段：高效协作
- 掌握复杂任务拆解
- 能稳定获得高质量输出
- 建立个人提示语库
- 在工作中形成 AI 辅助流程

### 第三阶段：创新应用
- 探索新的使用场景
- 结合专业领域深度应用
- 分享经验帮助他人
- 参与 AI 工具的反馈改进

### 第四阶段：技术理解
（可选，适合对技术感兴趣的人）
- 了解 AI 基本原理
- 学习 API 调用
- 尝试简单的 AI 应用开发
- 关注前沿研究动态

---

## 学习资源推荐

### 免费资源

**入门课程**
- Coursera: AI For Everyone（吴恩达）
- Google: AI 基础课程
- Microsoft: AI 入门学习路径

**实践平台**
- ChatGPT / Claude - 对话实践
- DALL-E / Midjourney - 图像生成实践
- GitHub Copilot - 编程辅助实践

**社区和论坛**
- Reddit r/ChatGPT, r/artificial
- Discord AI 社区
- 知乎 AI 话题
- 技术博客平台

### 进阶资源

**书籍**
- 《AI 新世界》- 李开复
- 《深度学习入门》- 斋藤康毅
- 《提示工程指南》- 各平台官方文档

**在线课程**
- Coursera 机器学习专项课程
- fast.ai 实践课程
- DeepLearning.AI 系列课程

---

## 实践项目建议

### 个人效率项目
1. **建立提示语库**
   - 收集有效的提示语模板
   - 按场景分类整理
   - 持续优化和更新

2. **自动化日常任务**
   - 用 AI 处理固定格式的工作
   - 建立标准化的工作流程
   - 记录时间节省效果

3. **知识管理系统**
   - 用 AI 整理笔记和资料
   - 建立个人知识库
   - 定期回顾和更新

### 创意项目
1. **内容创作**
   - 用 AI 辅助写博客
   - 创作短视频脚本
   - 制作教程或指南

2. **学习项目**
   - 用 AI 学习新技能
   - 记录学习过程
   - 分享学习心得

---

## 避免常见误区

### ❌ 过度依赖
- 不验证 AI 的输出
- 失去独立思考能力
- 所有工作都交给 AI

### ❌ 完全排斥
- 认为 AI 没用
- 拒绝尝试新工具
- 错过效率提升机会

### ✅ 正确态度
- AI 是强大的工具，但需要人来使用
- 保持好奇心，持续探索
- 在实践中找到适合自己的使用方式
- 关注 AI 伦理和负责任使用

---

## 本节要点

1. **养成使用习惯** - 在日常工作中多用多练
2. **关注更新动态** - AI 工具在不断进化
3. **分阶段学习** - 从基础到进阶逐步提升
4. **动手实践** - 做项目是最好的学习方式
5. **保持平衡** - 既不过度依赖，也不完全排斥
            `,ja:`
AI技術は急速に発展しており、学び続けることがペースについていく鍵です。

---

## 学習習慣を確立する

### 日常使用
最良の学習は実際の仕事でAIを使うこと：

**習慣を身につける**
- 問題に遭遇したら、まずAIが助けられるか考える
- 毎日少なくとも1つの新しい使用シーンを試す
- 効果的なプロンプトを記録し、テンプレート集を作る

**意図的な練習**
- 異なるプロンプトの効果を比較
- AIに少し難しいタスクを完了させてみる
- 失敗の原因を分析し、方法を改善

### アップデートに注目
AIツールは頻繁に新機能を更新：

**情報源**
- 公式ブログとアップデートログ
- テックニュースサイト
- 技術コミュニティの議論
- SNSでの実用的なヒント共有

**フォローすべきもの**
- OpenAIブログ
- Google AIブログ
- Anthropicブログ
- テックメディアのAIコラム

---

## 上級学習パス

### 第1段階：熟練使用
- 基本的な対話テクニックをマスター
- 一般的な使用シーンを理解
- 明確なプロンプトが書ける
- AIの能力境界を知る

### 第2段階：効率的な協働
- 複雑なタスク分解をマスター
- 安定して高品質な出力を得られる
- 個人的なプロンプトライブラリを構築
- 仕事でAI支援フローを形成

### 第3段階：革新的な応用
- 新しい使用シーンを探索
- 専門分野での深い応用
- 経験を共有して他者を助ける
- AIツールのフィードバック改善に参加

### 第4段階：技術的理解
（オプション、技術に興味がある人向け）
- AIの基本原理を理解
- API呼び出しを学ぶ
- 簡単なAIアプリ開発を試す
- 最先端研究の動向に注目

---

## 学習リソース推薦

### 無料リソース

**入門コース**
- Coursera: AI For Everyone（アンドリュー・ング）
- Google: AI基礎コース
- Microsoft: AI入門学習パス

**実践プラットフォーム**
- ChatGPT / Claude - 対話実践
- DALL-E / Midjourney - 画像生成実践
- GitHub Copilot - プログラミング支援実践

**コミュニティとフォーラム**
- Reddit r/ChatGPT, r/artificial
- Discord AIコミュニティ
- Qiita AIトピック
- テックブログプラットフォーム

### 上級リソース

**書籍**
- 『AI新世界』- 李開復
- 『ゼロから作るDeep Learning』- 斎藤康毅
- 『プロンプトエンジニアリングガイド』- 各プラットフォーム公式ドキュメント

**オンラインコース**
- Coursera 機械学習専門課程
- fast.ai 実践コース
- DeepLearning.AI シリーズ

---

## 実践プロジェクト提案

### 個人効率プロジェクト
1. **プロンプトライブラリの構築**
   - 効果的なプロンプトテンプレートを収集
   - シーン別に分類整理
   - 継続的に最適化と更新

2. **日常タスクの自動化**
   - AIで定型フォーマットの仕事を処理
   - 標準化されたワークフローを構築
   - 時間節約効果を記録

3. **ナレッジマネジメントシステム**
   - AIでノートと資料を整理
   - 個人知識ベースを構築
   - 定期的なレビューと更新

### クリエイティブプロジェクト
1. **コンテンツ制作**
   - AIでブログ執筆を支援
   - ショート動画のスクリプト作成
   - チュートリアルやガイドの制作

2. **学習プロジェクト**
   - AIで新しいスキルを学ぶ
   - 学習プロセスを記録
   - 学習の気づきを共有

---

## よくある誤りを避ける

### ❌ 過度な依存
- AIの出力を検証しない
- 独立した思考能力を失う
- すべての仕事をAIに任せる

### ❌ 完全な拒絶
- AIは役に立たないと思う
- 新しいツールを試すのを拒否
- 効率向上の機会を逃す

### ✅ 正しい態度
- AIは強力なツールだが、人が使う必要がある
- 好奇心を持ち、探索し続ける
- 実践で自分に合った使い方を見つける
- AI倫理と責任ある使用に注意

---

## このセクションのポイント

1. **使用習慣を身につける** - 日常業務で多く使い、多く練習
2. **アップデート動向に注目** - AIツールは進化し続けている
3. **段階的に学習** - 基礎から上級へ徐々にレベルアップ
4. **ハンズオン実践** - プロジェクトをやることが最良の学習方法
5. **バランスを保つ** - 過度な依存も完全な拒絶もしない
            `}},{id:"ch5-ai-mindset",title:{zh:"5.3 AI时代的思维方式",ja:"5.3 AI時代の思考法"},content:{zh:`
在 AI 时代，除了技能，思维方式的转变同样重要。

---

## 人机协作思维

### 发挥各自优势

**AI 的优势**
- 快速处理大量信息
- 不知疲倦，随时可用
- 没有情绪波动
- 善于模式识别
- 能同时考虑多种可能

**人的优势**
- 创造性思维
- 情感理解和同理心
- 道德判断
- 灵活应对新情况
- 最终决策权

### 协作模式
\`\`\`
不是：人 vs AI
而是：人 + AI > 单独的人或AI
\`\`\`

**有效协作的例子**
| 任务 | AI 负责 | 人负责 |
|------|---------|--------|
| 写报告 | 初稿、数据整理 | 观点、最终把关 |
| 做决策 | 信息收集、方案分析 | 价值判断、最终决定 |
| 创作 | 素材、灵感激发 | 创意方向、审美把控 |
| 学习 | 解释、答疑、练习 | 理解、应用、思考 |

---

## 终身学习思维

### 拥抱变化
AI 时代的特点是快速变化：
- 今天的工具明天可能过时
- 新能力不断涌现
- 工作方式持续演变

**应对策略**
- 保持好奇心，愿意尝试新事物
- 关注趋势，但不盲目追逐
- 建立可迁移的基础能力
- 适应变化，而不是抗拒变化

### 学习方法升级

**传统学习**
- 记忆大量知识
- 独立完成任务
- 追求标准答案

**AI 时代学习**
- 知道如何获取和验证知识
- 善于与 AI 协作完成任务
- 追求解决问题的能力

---

## 批判性思维

### 验证和质疑
AI 不是全知全能的：

**养成习惯**
- 对 AI 的回答保持适度怀疑
- 重要信息要多方验证
- 发现错误时主动指出
- 不断追问以确保准确

### 信息素养
在 AI 生成内容泛滥的时代：
- 区分 AI 生成和人工创作
- 识别虚假信息
- 重视信息来源
- 保持独立判断

---

## 道德和责任感

### 负责任地使用 AI
- 不用 AI 做有害的事
- 诚实标注 AI 参与的创作
- 尊重版权和知识产权
- 保护隐私，不泄露敏感信息

### 关注社会影响
- AI 对就业市场的影响
- AI 偏见和公平性问题
- AI 环境成本
- AI 治理和监管

---

## 情绪和心态

### 健康的心态

**避免焦虑**
- AI 是工具，不是威胁
- 专注于自己能控制的事
- 逐步学习，不必一步到位

**避免傲慢**
- AI 确实能做很多事
- 承认它在某些方面超过人
- 保持谦虚和学习心态

**保持平衡**
- 不过度依赖 AI
- 维护人际关系
- 保留不用 AI 的时间
- 享受纯粹人类活动的乐趣

### 找到你的定位
思考：在 AI 时代，你的独特价值是什么？
- 你的专业知识 + AI = ？
- 你的人际网络 + AI = ？
- 你的创造力 + AI = ？

---

## 本节要点

1. **人机协作** - 发挥各自优势，而不是对立
2. **终身学习** - 拥抱变化，持续提升
3. **批判性思维** - 验证、质疑、独立判断
4. **道德责任** - 负责任地使用 AI
5. **健康心态** - 既不焦虑，也不傲慢
            `,ja:`
AI時代には、スキルだけでなく、思考法の転換も同様に重要です。

---

## 人機協働思考

### それぞれの強みを発揮

**AIの強み**
- 大量の情報を高速処理
- 疲れ知らず、いつでも利用可能
- 感情の波がない
- パターン認識が得意
- 複数の可能性を同時に考慮

**人の強み**
- 創造的思考
- 感情理解と共感
- 道徳的判断
- 新状況への柔軟な対応
- 最終決定権

### 協働モード
\`\`\`
ではない：人 vs AI
である：人 + AI > 単独の人またはAI
\`\`\`

**効果的な協働の例**
| タスク | AIが担当 | 人が担当 |
|------|---------|--------|
| レポート作成 | 初稿、データ整理 | 観点、最終チェック |
| 意思決定 | 情報収集、方案分析 | 価値判断、最終決定 |
| 創作 | 素材、インスピレーション | 創意方向、審美コントロール |
| 学習 | 説明、Q&A、練習 | 理解、応用、思考 |

---

## 生涯学習思考

### 変化を受け入れる
AI時代の特徴は急速な変化：
- 今日のツールは明日には時代遅れかも
- 新しい能力が次々と登場
- 仕事のやり方が継続的に進化

**対応策**
- 好奇心を持ち、新しいことを試す意欲
- トレンドに注目、でも盲目的に追いかけない
- 移転可能な基礎能力を構築
- 変化に抵抗するのではなく、適応する

### 学習方法のアップグレード

**従来の学習**
- 大量の知識を暗記
- 独立してタスクを完了
- 標準的な答えを追求

**AI時代の学習**
- 知識を取得し検証する方法を知る
- AIと協働してタスクを完了
- 問題解決能力を追求

---

## 批判的思考

### 検証と疑問
AIは全知全能ではない：

**習慣を身につける**
- AIの回答に適度な懐疑心を持つ
- 重要な情報は複数ソースで検証
- 間違いを発見したら積極的に指摘
- 正確性を確保するため継続的に質問

### 情報リテラシー
AI生成コンテンツが溢れる時代に：
- AI生成と人間の創作を区別
- フェイク情報を識別
- 情報源を重視
- 独立した判断を維持

---

## 道徳と責任感

### 責任あるAI使用
- AIを有害なことに使わない
- AIが参加した創作を正直に表示
- 著作権と知的財産を尊重
- プライバシーを保護、機密情報を漏らさない

### 社会的影響への関心
- AIの雇用市場への影響
- AIバイアスと公平性の問題
- AIの環境コスト
- AIガバナンスと規制

---

## 感情とマインドセット

### 健康的なマインドセット

**不安を避ける**
- AIはツール、脅威ではない
- 自分がコントロールできることに集中
- 徐々に学習、一度に完璧を目指さない

**傲慢を避ける**
- AIは確かに多くのことができる
- 一部の面でAIが人を超えることを認める
- 謙虚さと学習心を維持

**バランスを保つ**
- AIに過度に依存しない
- 人間関係を維持
- AIを使わない時間を確保
- 純粋に人間らしい活動を楽しむ

### 自分のポジションを見つける
考えてみよう：AI時代、あなたの独自の価値は何か？
- あなたの専門知識 + AI = ？
- あなたの人脈 + AI = ？
- あなたの創造性 + AI = ？

---

## このセクションのポイント

1. **人機協働** - 対立ではなく、それぞれの強みを発揮
2. **生涯学習** - 変化を受け入れ、継続的に向上
3. **批判的思考** - 検証、疑問、独立した判断
4. **道徳責任** - 責任あるAI使用
5. **健康的なマインドセット** - 不安にも傲慢にもならない
            `}},{id:"ch5-conclusion",title:{zh:"5.4 结语：你的AI之旅",ja:"5.4 おわりに：あなたのAI旅"},content:{zh:`
恭喜你完成了这本书的学习！

---

## 回顾你的学习之旅

### 第一章：什么是人工智能
- 理解了 AI 的基本概念
- 了解了 AI 能做什么、不能做什么
- 认识了常见的 AI 工具

### 第二章：动手使用AI工具
- 学会了与 AI 对话的基本技巧
- 掌握了写好提示语的方法
- 实践了各种使用场景

### 第三章：AI进阶技巧
- 学会了让 AI 输出更稳定
- 掌握了复杂任务的拆解方法
- 知道了如何处理常见问题

### 第四章：AI应用场景
- 在工作中使用 AI 提升效率
- 用 AI 辅助学习和创作
- 让 AI 帮助解决生活问题

### 第五章：AI与未来
- 了解了 AI 的发展趋势
- 建立了持续学习的方法
- 形成了正确的思维方式

---

## 你已经具备的能力

✅ **基础对话能力** - 能清晰表达需求，获得有用回答

✅ **提示语写作** - 能写出结构化的高质量提示语

✅ **任务协作** - 能把 AI 融入工作和学习流程

✅ **问题解决** - 能处理 AI 使用中的常见问题

✅ **批判思维** - 能验证信息、独立判断

✅ **学习能力** - 能持续跟进 AI 的发展

---

## 接下来做什么？

### 立即行动
1. **选择一个场景深入使用**
   - 从工作或学习中选一个具体任务
   - 连续一周用 AI 辅助完成
   - 记录效果和改进点

2. **建立你的提示语库**
   - 整理本书中有用的提示语模板
   - 根据自己的需求修改
   - 持续补充新发现的好用法

3. **加入社区**
   - 找到 AI 使用者社区
   - 分享你的经验
   - 学习他人的技巧

### 长期目标
- 成为团队中的 AI 使用专家
- 探索 AI 在专业领域的深度应用
- 关注 AI 伦理，负责任地使用技术

---

## 最后的话

AI 技术将继续快速发展，没有人能预测未来会怎样。

但有一点是确定的：**那些善于学习、拥抱变化、负责任地使用技术的人，将在 AI 时代找到自己的位置。**

你已经迈出了第一步。继续前进，保持好奇，享受与 AI 协作的乐趣。

---

## 感谢

感谢你阅读这本书。

希望它能帮助你：
- 更好地理解和使用 AI
- 提升工作和学习效率
- 在 AI 时代自信前行

**祝你在 AI 之旅中一切顺利！**

---

*"最好的时代属于那些善于学习和适应的人。"*
            `,ja:`
この本の学習を完了おめでとうございます！

---

## 学習の旅を振り返る

### 第1章：人工知能とは何か
- AIの基本概念を理解
- AIができること・できないことを学んだ
- 一般的なAIツールを知った

### 第2章：AIツールを使ってみよう
- AIとの対話の基本テクニックを学んだ
- 良いプロンプトの書き方をマスター
- 様々な使用シーンを実践

### 第3章：AI上級テクニック
- AIの出力を安定させる方法を学んだ
- 複雑なタスクの分解方法をマスター
- よくある問題への対処法を知った

### 第4章：AI応用シーン
- 仕事でAIを使って効率を上げる
- AIで学習と創作を支援
- AIで生活の問題を解決

### 第5章：AIと未来
- AIの発展トレンドを理解
- 継続学習の方法を確立
- 正しい思考法を形成

---

## あなたが身につけた能力

✅ **基本対話能力** - ニーズを明確に伝え、有用な回答を得られる

✅ **プロンプトライティング** - 構造化された高品質なプロンプトが書ける

✅ **タスク協働** - AIを仕事と学習のフローに組み込める

✅ **問題解決** - AI使用中のよくある問題に対処できる

✅ **批判的思考** - 情報を検証し、独立して判断できる

✅ **学習能力** - AIの発展に継続的についていける

---

## 次に何をする？

### 今すぐ行動
1. **一つのシーンを深く使い込む**
   - 仕事や学習から具体的なタスクを選ぶ
   - 1週間連続でAIの支援で完了
   - 効果と改善点を記録

2. **プロンプトライブラリを構築**
   - 本書の有用なプロンプトテンプレートを整理
   - 自分のニーズに合わせて修正
   - 新しい発見を継続的に追加

3. **コミュニティに参加**
   - AIユーザーコミュニティを見つける
   - 経験を共有
   - 他の人のテクニックを学ぶ

### 長期目標
- チームのAI活用エキスパートになる
- 専門分野でのAI深層応用を探索
- AI倫理に注目、責任ある技術使用

---

## 最後に

AI技術は急速な発展を続け、未来がどうなるか誰にも予測できません。

しかし一つ確かなことがあります：**学ぶことが得意で、変化を受け入れ、責任を持って技術を使う人は、AI時代に自分の居場所を見つけるでしょう。**

あなたはすでに最初の一歩を踏み出しました。前進し続け、好奇心を持ち、AIとの協働を楽しんでください。

---

## 感謝

この本を読んでいただきありがとうございます。

この本が以下の助けになれば幸いです：
- AIをより良く理解し活用
- 仕事と学習の効率を向上
- AI時代に自信を持って前進

**AI旅が素晴らしいものになりますように！**

---

*「最高の時代は、学び適応することが得意な人のものです。」*
            `}}]}]},OL=e=>{const t=[];return e.chapters.forEach((n,r)=>{n.sections.forEach((s,i)=>{t.push({chapter:n,section:s,chapterIndex:r,sectionIndex:i})})}),t},jL=e=>e.chapters.reduce((t,n)=>t+n.sections.length,0),RL={id:"ai-advanced",title:{zh:"AI 进阶实战",ja:"AI 実践上級編"},subtitle:{zh:"掌握 AI 的高级使用技巧",ja:"AIの上級テクニックをマスターする"},author:"StudyForge",chapters:[{id:"chapter-0",number:0,title:{zh:"AI 动态时间轴",ja:"AI動向タイムライン"},subtitle:{zh:"追踪最新 AI 技术发展",ja:"最新AI技術の動向を追跡"},sections:[{id:"ch0-timeline",title:{zh:"2025 年 AI 大事记",ja:"2025年AI大事記"},content:{zh:`
## 🚀 AI 技术动态时间轴

追踪最新 AI 发展动态，点击链接深入了解相关知识点。

---

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        2025 年 AI 技术发展时间轴                          │
└─────────────────────────────────────────────────────────────────────────┘

  2025.01                                                          2025.07
     │                                                                │
     ▼                                                                ▼
─────●────────●────────●────────●────────●────────●────────●────────●─────
     │        │        │        │        │        │        │        │
   Claude   GPT-5    Claude   Codex    Sub-     Skills   Chrome   最新
   Opus4.5  发布     Code     Agent    agents   标准化   集成     动态
   发布              MCP      Skills   功能     通用
\`\`\`

---

## 📅 详细时间轴

### 🔵 2025年1月 - Claude Opus 4.5 发布

Anthropic 发布 Claude Opus 4.5，成为当时最强大的 AI 模型。

| 特性 | 说明 |
|------|------|
| 知识截止 | 2025年1月 |
| 长文本能力 | 可输出 1.2 万+ 字符 |
| 创作能力 | 文学创作、设计感更强 |

📖 **相关知识点**：→ 本书 [3.10 AI 模型对比与选型]

---

### 🔵 2025年2月 - GPT-5 系列发布

OpenAI 陆续发布 GPT-5.0、5.1、5.2，持续迭代改进。

**实测对比要点**：
- 长文本生成：Claude 约 12000 字符，GPT-5.1 约 6900 字符
- 数学编程：GPT-5.1 更强
- 浏览器自动化：GPT-5.1 略胜

📖 **深入学习**：→ 本书 [3.10 Claude vs GPT 实测对比]

---

### 🟢 2025年3月 - Claude Code 正式发布

基于 MCP 协议的 AI 编程助手 Claude Code 发布，改变开发方式。

**核心特性**：
- ✅ 直接读写代码文件
- ✅ 运行终端命令
- ✅ MCP 协议扩展能力

**Boris 核心经验**：Opus 4.5 + Plan 模式、多实例并行、Slash 命令自动化

📖 **深入学习**：→ 本书 [3.3 MCP：AI 的"万能接口"] 和 [序章：大佬经验分享]

---

### 🟢 2025年4月 - Sub-agents 功能上线

Claude Code 支持 Sub-agents，实现专家团队协作模式。

\`\`\`
Sub-agents 架构示意
───────────────────
        主 Agent
            │
    ┌───────┼───────┐
    ▼       ▼       ▼
  代码    测试    文档
  审查    专家    生成
\`\`\`

**核心价值**：上下文隔离避免污染、专业化分工成功率更高、可复用团队共享

📖 **深入学习**：→ 本书 [3.3 Sub-agents 专家团队协作]

---

### 🟡 2025年5月 - Agent Skills 成为行业标准

OpenAI Codex 采用 Anthropic 的 Skills 规范，实现跨平台通用。

| 平台 | Skills 支持 |
|------|------------|
| Claude Code | ✅ 原生支持 |
| GPT Codex | ✅ 新增支持 |
| Cursor | ✅ 兼容 |

> 💡 你写的 Skills 可以在 Claude 和 Codex 之间通用！

**重大意义**：一次编写到处运行，团队知识资产化，AI 能力可定制

📖 **深入学习**：→ 本书 [3.9 OpenAI Codex 使用指南]

---

### 🟡 2025年6月 - Claude Code Chrome 集成

Claude Code 原生支持 Chrome 浏览器，实现端到端自动化测试。

**应用场景**：
- 前端 UI 调试
- 浏览器自动化测试
- 保留登录状态和扩展插件

**验证循环技巧**：让 Claude 通过 Chrome 自动验证 UI，直到功能和体验满意，质量可提升 2-3 倍

📖 **深入学习**：→ 本书 [3.3 Boris 的验证反馈循环]

---

### 🔴 2025年7月 - 最新动态

**最新工具一览：**

| 工具 | 功能 | 亮点 |
|------|------|------|
| Ralph Wiggum | AI 自动迭代修复 | Bug 到完美应用只需一条命令 |
| Claudia | Claude Code GUI | 告别命令行，可视化操作 |
| SuperClaude | 能力增强 300% | 19 个命令 + 9 大专家角色 |
| Claude Code PM | 并行开发 | GitHub Issues 秒变独立分支 |
| Kilo Code | 融合 Cline + Roo | 5 种智能模式切换 |

**发展趋势总结**：
- 📊 Spec-Driven 开发逐渐取代 Vibe Coding（先写规格再编码）
- 🔧 上下文工程比提示工程效果好 10 倍（给足背景信息）
- 🤝 多 AI 协作成为常态（Claude + GPT + Gemini 各取所长）

📖 **深入学习**：→ 本书 [序章：大佬经验分享] 有 SuperClaude 完整使用指南

---

## 🗺️ 本书导航

> 💡 **使用左侧目录可快速跳转到任意章节**

| 章节 | 内容 |
|------|------|
| 第0章 | AI 动态时间轴 - 追踪2025年AI发展 |
| 第2章 | 提示词工程进阶 - 与AI对话的艺术 |
| 第3章 | AI Agents 智能体 - MCP、Claude Code等 |
| 第4章 | RAG 检索增强生成 - 让AI获取新知识 |
| 第1章 | 大模型技术深度解析 - Transformer与微调 |

---

> 📌 **提示**：本时间轴会持续更新，记录 AI 领域重要发展节点。
            `,ja:`
## 🚀 AI技術動向タイムライン

最新のAI発展動向を追跡し、リンクをクリックして関連知識を深く理解しましょう。

---

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        2025年 AI技術発展タイムライン                       │
└─────────────────────────────────────────────────────────────────────────┘

  2025.01                                                          2025.07
     │                                                                │
─────●────────●────────●────────●────────●────────●────────●────────●─────
   Claude   GPT-5    Claude   Codex    Sub-     Skills   Chrome   最新
   Opus4.5  発表     Code     Agent    agents   標準化   統合     動向
\`\`\`

---

## 📅 詳細タイムライン

### 🔵 2025年1月 - Claude Opus 4.5 リリース

AnthropicがClaude Opus 4.5をリリース。当時最強のAIモデルに。

📖 **関連知識**：→ 本書 [3.5 AIモデル比較と選定]

---

### 🔵 2025年2月 - GPT-5シリーズ発表

OpenAIがGPT-5.0、5.1、5.2を順次リリース。

**実測比較ポイント**：長文生成はClaude優位（約12000字 vs 6900字）、数学・プログラミングはGPT-5.1が強い

📖 **詳細**：→ 本書 [3.10 Claude vs GPT 実測比較]

---

### 🟢 2025年3月 - Claude Code 正式リリース

MCPベースのAIプログラミングアシスタントがリリース。

**Borisの核心経験**：Opus 4.5 + Planモード、マルチインスタンス並列、Slashコマンド自動化

📖 **詳細**：→ 本書 [3.3 MCP] と [序章：エキスパートの知見]

---

### 🟢 2025年4月 - Sub-agents 機能リリース

Claude CodeがSub-agentsをサポート、専門家チーム協力を実現。

**コアバリュー**：コンテキスト分離で汚染防止、専門化分業で成功率向上、再利用可能

📖 **詳細**：→ 本書 [3.3 Sub-agents 専門家チーム協力]

---

### 🟡 2025年5月 - Agent Skills が業界標準に

OpenAI CodexがAnthropicのSkills仕様を採用。

**重要な意義**：一度作成すればどこでも使える、チーム知識の資産化、AIカスタマイズ可能

📖 **詳細**：→ 本書 [3.9 OpenAI Codex 使用ガイド]

---

### 🟡 2025年6月 - Claude Code Chrome 統合

Claude CodeがChromeブラウザをネイティブサポート。

**検証ループ技術**：ClaudeがChromeでUI自動検証、品質2〜3倍向上

---

### 🔴 2025年7月 - 最新動向

| ツール | 機能 | ハイライト |
|--------|------|-----------|
| Ralph Wiggum | AI自動反復修正 | バグから完璧なアプリへ一コマンド |
| Claudia | Claude Code GUI | コマンドライン不要、視覚化操作 |
| SuperClaude | 能力強化300% | 19コマンド + 9エキスパートロール |
| Kilo Code | Cline + Roo融合 | 5つのスマートモード |

**トレンド**：
- Spec-Driven開発がVibeコーディングに代わる
- コンテキストエンジニアリングはプロンプトの10倍効果的
- マルチAI協力が常態化（Claude + GPT + Gemini）

📖 **詳細**：→ 本書 [序章：エキスパートの知見] に SuperClaude 完全ガイド

---

## 🗺️ クイックナビゲーション

| 知りたいこと | ジャンプ先 |
|-------------|----------|
| モデル選び方 | → 第3章第10節 [AIモデル比較] |
| MCPとは？ | → 第3章第3節 [MCP] |
| プロンプト技術 | → 第2章 [プロンプトエンジニアリング] |
| RAG最適化 | → 第4章第3節 [RAG最適化] |
            `}},{id:"ch0-experts",title:{zh:"大佬经验分享",ja:"エキスパートの知見"},content:{zh:`
## 🎯 大佬经验分享

来自 AI 领域顶尖开发者的实战经验，帮你少走弯路。

---

## 👤 Boris Cherny - Claude Code 创始人

> "开箱即用才是最强工作流，复利工程思维让效率翻倍！"

Boris 是 Anthropic 的工程师，Claude Code 项目的核心创建者。他在社交媒体上分享了自己的工作流程，获得了数万点赞和转发。以下是他的核心经验总结。

### 核心理念：简单但极致

Boris 的哲学是"**让 AI 成为工作流的自然延伸**"，而不是过度定制。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                     Boris 的工作流核心                                   │
└─────────────────────────────────────────────────────────────────────────┘

  ┌─────────────┐    ┌─────────────┐    ┌─────────────┐
  │  开箱即用   │ ──▶│  复利思维   │ ──▶│  极致效率   │
  │  不过度配置 │    │  经验复用   │    │  一次搞定   │
  └─────────────┘    └─────────────┘    └─────────────┘
\`\`\`

---

### 🔧 Boris 的具体配置

#### 1. 大模型优先策略

| 选择 | 原因 |
|------|------|
| Opus 4.5 + thinking 模式 | "一次搞定"成功率更高 |
| 不用小模型 | 减少反复纠正的轮次 |

> 💡 **复利思维**：虽然大模型更慢，但减少了 3-5 次修改循环，总时间反而更短。

#### 2. Plan 模式驱动

\`\`\`
Boris 的工作流程
────────────────
1. Shift+Tab 两次 → 进入 Plan 模式
2. 和 Claude 把整个 PR 计划聊透
3. 满意后 → 切到 auto-accept 模式
4. 让 Claude 一次性完成所有工作

关键：架构设计优先于代码执行
\`\`\`

#### 3. 并行处理：5+10 实例

\`\`\`
Boris 的多实例布局
──────────────────
终端 (iTerm2)                  Web 版
─────────────                  ──────
Tab 1: 主功能开发              浏览器 Tab × 5-10
Tab 2: Bug 修复                用于长时间任务
Tab 3: 测试编写                如文档生成
Tab 4: 代码审查                架构设计等
Tab 5: 部署脚本

→ 避免上下文切换成本
→ 真正的并行开发
\`\`\`

#### 4. Slash 命令自动化

在 \`~/.claude/commands/\` 目录下创建命令：

\`\`\`bash
# /commit-push-pr 命令
自动执行：
1. 收集 git status
2. 生成 commit message
3. 推送代码
4. 创建 PR

# 使用
> /commit-push-pr
\`\`\`

#### 5. 验证闭环机制

> "给 Claude 一个验证自己的方式，是最关键的一环。"

\`\`\`
验证闭环
────────
代码生成 → 自动测试 → 失败?
                        ↓
                    自动修复
                        ↓
                    重新测试
                        ↓
                    通过 ✓
\`\`\`

#### 6. 团队知识库

\`\`\`bash
# 将 Claude 使用规范 check in 到 git
project/
├── .claude/
│   ├── CLAUDE.md          # 项目规范
│   ├── commands/          # 团队共享命令
│   └── agents/            # Sub-agents 配置

→ "教 AI 的经验"版本化管理
→ 团队效率集体提升
\`\`\`

---

## 🚀 SuperClaude 框架

> "让 Claude Code 编程能力暴增 300%！"

SuperClaude 是一个专门为 Claude Code 设计的综合配置框架，通过结构化的配置文件和专业化的工作流程，将 Claude Code 从通用 AI 助手转变为专业的开发伙伴。

### 安装

\`\`\`bash
git clone https://github.com/NomenAK/SuperClaude.git
cd SuperClaude
./install.sh

# 验证安装
ls ~/.claude/           # 4 个主文件
ls ~/.claude/commands/  # 17 个命令文件
\`\`\`

### 19 个专业命令

| 类别 | 命令 | 功能 |
|------|------|------|
| 构建 | /build | 项目构建 |
| 开发 | /user:dev-setup | 环境配置 |
| 测试 | /user:test | 测试执行 |
| 分析 | /user:analyze | 代码分析 |
| 调试 | /user:troubleshoot | 问题排查 |
| 优化 | /user:improve | 代码改进 |
| 部署 | /user:deploy | 项目部署 |
| 迁移 | /user:migrate | 数据迁移 |
| 安全 | /user:scan | 安全扫描 |
| 设计 | /user:design | 架构设计 |

### 9 大专家角色

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                      SuperClaude 专家角色                               │
└─────────────────────────────────────────────────────────────────────────┘

  architect        frontend         backend          security
  ┌────────┐      ┌────────┐      ┌────────┐      ┌────────┐
  │系统设计 │      │用户体验 │      │API性能 │      │安全审计 │
  │可扩展性 │      │React   │      │数据库  │      │威胁建模 │
  └────────┘      └────────┘      └────────┘      └────────┘

  qa               performance      analyzer         mentor
  ┌────────┐      ┌────────┐      ┌────────┐      ┌────────┐
  │测试质量 │      │性能优化 │      │问题分析 │      │技术指导 │
  │覆盖率   │      │瓶颈定位 │      │根因定位 │      │代码教学 │
  └────────┘      └────────┘      └────────┘      └────────┘

  refactorer
  ┌────────┐
  │代码重构 │
  │质量提升 │
  └────────┘
\`\`\`

### 使用示例

\`\`\`bash
# 创建 React 项目 + TDD + 前端专家
/build --react --magic --tdd --persona-frontend

# 安全扫描 + 安全专家
/user:scan --persona-security

# 性能优化 + 性能专家
/user:improve --persona-performance
\`\`\`

---

## 📚 上下文工程（Context Engineering）

> "效果比提示工程好 10 倍，比 Vibe Coding 好 100 倍。"

上下文工程是一种新的 AI 编程方法论，强调通过精心设计的上下文（而不仅仅是提示词）来引导 AI 产出高质量代码。

### 核心方法：PRP（产品需求提示）

\`\`\`
传统方式                          PRP 方式
──────────                        ──────────
"帮我写个登录功能"                1. 功能需求定义
        │                        2. 分解为可验证步骤
        ▼                        3. 每步完成后验证
  代码质量不可控                  4. 迭代直到完美
                                         │
                                         ▼
                                   代码质量有保障
\`\`\`

---

## 🔧 更多实用技巧

### Cursor + Claude Code 组合
同时使用 IDE 插件（Cursor）和命令行工具（Claude Code），分工协作：
- Cursor：快速补全、小范围修改
- Claude Code：大规模重构、复杂任务

### Spec-Driven 开发
先写规格文档（spec.md），再让 AI 按规格实现：
1. 需求分析 → 编写 spec
2. spec 评审 → 确认无误
3. 按 spec 实现 → 一次通过

### Output Styles 功能
Claude Code 支持多种输出风格：
- **Concise**：简洁模式，减少解释
- **Verbose**：详细模式，包含推理过程
- **Learning**：学习模式，边写代码边教学

### 多 AI 协作
Claude + GPT + Gemini 组合使用，取长补短。

---

## 💡 核心经验总结

| 大佬 | 核心理念 | 关键技巧 |
|------|----------|----------|
| Boris | 开箱即用 + 复利思维 | Plan 模式、并行实例、验证闭环 |
| SuperClaude | 专业化 + 模块化 | 19命令、9角色、MCP集成 |
| 上下文工程 | 结构化 + 可验证 | PRP 方法、迭代验证 |

> 📌 **记住**：选择适合自己的方法，不要盲目复制，找到自己的最佳实践。
            `,ja:`
## 🎯 エキスパートの知見

AI分野トップ開発者の実践経験から学びましょう。

---

## 👤 Boris Cherny - Claude Code 創設者

> "開封即用が最強ワークフロー、複利思考で効率倍増！"

Boris は Anthropic のエンジニアで、Claude Code プロジェクトの核心的な創設者です。彼がSNSで共有したワークフローは数万のいいねと転送を獲得しました。

### コア理念：シンプルだが極致

Borisの哲学は「**AIをワークフローの自然な延長にする**」こと。

---

### 🔧 Boris の具体的な設定

#### 1. 大規模モデル優先

| 選択 | 理由 |
|------|------|
| Opus 4.5 + thinking モード | 「一発成功」率が高い |
| 小規模モデルは使わない | 修正ループを減らす |

#### 2. Plan モード駆動

\`\`\`
Boris のワークフロー
────────────────
1. Shift+Tab 2回 → Plan モードへ
2. Claude と PR 計画を詳細に議論
3. 満足したら → auto-accept モードへ
4. Claude に一括で完成させる
\`\`\`

#### 3. 並列処理：5+10 インスタンス

ターミナル 5つ + Web版 5-10個を同時実行。

#### 4. 検証クローズドループ

> "Claude に自己検証の方法を与えることが最も重要。"

---

## 🚀 SuperClaude フレームワーク

> "Claude Code のプログラミング能力を 300% 向上！"

SuperClaude は Claude Code 専用の総合設定フレームワークで、構造化された設定ファイルと専門化されたワークフローにより、Claude Code を汎用AIアシスタントから専門的な開発パートナーに変革します。

### 19の専門コマンド + 9つの専門家ロール

| ロール | 専門領域 |
|--------|---------|
| architect | システム設計 |
| frontend | ユーザー体験、React |
| backend | API、パフォーマンス |
| security | セキュリティ監査 |
| qa | テスト品質 |
| performance | パフォーマンス最適化 |
| analyzer | 根本原因分析 |
| mentor | 技術指導 |
| refactorer | コードリファクタリング |

---

## 📚 コンテキストエンジニアリング

> "プロンプトエンジニアリングより10倍効果的。"

コンテキストエンジニアリングは新しいAIプログラミング方法論で、プロンプトだけでなく、精心に設計されたコンテキストを通じてAIに高品質コードを生成させることを重視します。

### PRP（製品要件プロンプト）方式

複雑な機能を検証可能なステップに分解し、各ステップ完了後に検証。

---

## 🔧 その他の実用テクニック

### Cursor + Claude Code 組み合わせ
IDEプラグイン（Cursor）とCLIツール（Claude Code）を同時使用：
- Cursor：素早い補完、小範囲の修正
- Claude Code：大規模リファクタリング、複雑なタスク

### Spec-Driven 開発
仕様書（spec.md）を先に書き、AIに仕様通り実装させる。

### Output Styles 機能
Claude Code は複数の出力スタイルをサポート：
- **Concise**：簡潔モード
- **Verbose**：詳細モード
- **Learning**：学習モード

---

## 💡 コア経験まとめ

| エキスパート | コア理念 | キーテクニック |
|-------------|---------|---------------|
| Boris | 開封即用 + 複利思考 | Plan モード、並列インスタンス |
| SuperClaude | 専門化 + モジュール化 | 19コマンド、9ロール |
| コンテキストエンジニアリング | 構造化 + 検証可能 | PRP 方式 |
            `}}]},{id:"chapter-1",number:1,title:{zh:"大模型技术深度解析",ja:"大規模言語モデル技術詳解"},subtitle:{zh:"理解 Transformer 与微调技术",ja:"Transformerとファインチューニング技術を理解する"},sections:[{id:"ch1-intro",title:{zh:"引言：走进大模型的内部",ja:"序章：大規模モデルの内部へ"},content:{zh:`
当我们使用 ChatGPT、Claude、Gemini 这些 AI 时，你是否好奇过：

- 它们是如何理解我们说的话的？
- 为什么它们能生成如此流畅的文字？
- "参数"到底是什么意思？7B、70B、405B 有什么区别？

本章将带你深入大模型的技术内核，理解这些神奇能力背后的原理。

---

## 为什么要了解技术原理？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        了解原理的三大价值                                 │
└─────────────────────────────────────────────────────────────────────────┘

  价值1：更好地使用 AI                价值2：做出正确的选型
  ┌──────────────────────┐           ┌──────────────────────┐
  │                      │           │                      │
  │  • 理解 AI 的能力边界  │           │  • 知道什么模型适合    │
  │  • 写出更有效的提示词  │           │    什么任务           │
  │  • 避免常见的使用误区  │           │  • 评估成本与效果      │
  │                      │           │  • 选择合适的部署方案  │
  └──────────────────────┘           └──────────────────────┘

                    价值3：具备微调和定制能力
                    ┌──────────────────────┐
                    │                      │
                    │  • 根据需求微调模型    │
                    │  • 构建领域专属 AI     │
                    │  • 优化性能和成本      │
                    │                      │
                    └──────────────────────┘
\`\`\`

---

## 本章你将学到

| 章节 | 内容 | 收获 |
|------|------|------|
| 4.1 | Transformer 架构 | 理解大模型的核心技术 |
| 4.2 | 注意力机制 | 理解 AI 如何"理解"语言 |
| 4.3 | 主流大模型对比 | 了解 GPT、Claude、Gemini 的区别 |
| 4.4 | 大模型微调入门 | 学会定制自己的 AI |
| 4.5 | LoRA 微调实战 | 低成本微调的最佳实践 |

让我们开始这段技术探索之旅！
            `,ja:`
ChatGPT、Claude、Geminiなどを使う時、こんな疑問を持ったことはありませんか：

- どうやって私たちの言葉を理解しているのか？
- なぜこんなに流暢な文章を生成できるのか？
- 「パラメータ」とは何？7B、70B、405Bの違いは？

この章では大規模モデルの技術コアに迫り、その魔法のような能力の原理を理解します。

---

## 技術原理を理解する価値

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        原理理解の3つの価値                               │
└─────────────────────────────────────────────────────────────────────────┘

  価値1：AIをより良く使う            価値2：正しいモデル選択
  ┌──────────────────────┐           ┌──────────────────────┐
  │  • AIの能力境界を理解  │           │  • タスクに適したモデル │
  │  • 効果的なプロンプト  │           │  • コストと効果の評価  │
  │  • よくある誤解を回避  │           │  • 適切なデプロイ方式  │
  └──────────────────────┘           └──────────────────────┘

                    価値3：ファインチューニング能力
                    ┌──────────────────────┐
                    │  • ニーズに応じた調整  │
                    │  • ドメイン特化AI構築  │
                    │  • 性能とコスト最適化  │
                    └──────────────────────┘
\`\`\`

---

## この章で学ぶこと

| セクション | 内容 | 得られるもの |
|----------|------|------------|
| 4.1 | Transformerアーキテクチャ | 大規模モデルのコア技術理解 |
| 4.2 | アテンション機構 | AIが言語を「理解」する仕組み |
| 4.3 | 主流モデル比較 | GPT、Claude、Geminiの違い |
| 4.4 | ファインチューニング入門 | 自分のAIをカスタマイズ |
| 4.5 | LoRA実践 | 低コストチューニングのベストプラクティス |
            `}},{id:"ch1-transformer",title:{zh:"1.1 Transformer 架构详解",ja:"1.1 Transformerアーキテクチャ詳解"},content:{zh:`
## Transformer：改变世界的架构

2017 年，Google 发表了著名论文《Attention Is All You Need》，提出了 Transformer 架构。今天的 GPT、Claude、Gemini 全部基于这个架构。

---

## 动手体验：Transformer 工作流程

在深入学习之前，先通过这个交互式演示直观感受 Transformer 是如何处理文本的：

::transformer-v2-viz::

---

## 文本生成的核心原理：下一个词预测

大语言模型的本质其实很简单：**预测下一个词**。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    文本生成 = 不断预测下一个词                              │
└─────────────────────────────────────────────────────────────────────────┘

  用户输入: "今天天气"

  Step 1: 模型预测下一个词
  ┌────────────────────────────────────────┐
  │ 输入: "今天天气"                        │
  │                                        │
  │ 输出概率分布:                           │
  │   "很"     → 45%                       │
  │   "不"     → 25%                       │
  │   "真"     → 15%                       │
  │   "特别"   → 10%                       │
  │   其他     → 5%                        │
  │                                        │
  │ 选择: "很" (概率最高)                   │
  └────────────────────────────────────────┘

  Step 2: 把预测的词加入输入，继续预测
  ┌────────────────────────────────────────┐
  │ 输入: "今天天气很"                      │
  │                                        │
  │ 输出概率分布:                           │
  │   "好"     → 60%                       │
  │   "热"     → 20%                       │
  │   "冷"     → 15%                       │
  │   ...                                  │
  │                                        │
  │ 选择: "好"                             │
  └────────────────────────────────────────┘

  Step 3, 4, 5... 重复直到生成结束符或达到长度限制

  最终输出: "今天天气很好，适合出门散步。"
\`\`\`

> 💡 **这就是所谓的"自回归生成"（Autoregressive Generation）**

---

## 完整处理流程：从文本到预测

文本是如何变成概率预测的？整个流程分为三大步骤：

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    Transformer 处理流程全景图                              │
└─────────────────────────────────────────────────────────────────────────┘

  "今天天气很好"
       │
       ▼
  ┌─────────────────────────────────────────────────────────────────────┐
  │  STEP 1: 嵌入层 (Embedding)                                          │
  │  ─────────────────────────────────────────────────────────────────  │
  │                                                                     │
  │  1.1 分词 (Tokenization)                                            │
  │      "今天天气很好" → ["今天", "天气", "很", "好"]                    │
  │                       或 → ["今", "天", "天", "气", "很", "好"]       │
  │                                                                     │
  │  1.2 Token Embedding (词嵌入)                                        │
  │      每个 token → 768 维向量                                         │
  │      "今天" → [0.12, -0.45, 0.78, ..., 0.33]                        │
  │                                                                     │
  │  1.3 Position Embedding (位置嵌入)                                   │
  │      位置 0 → [0.00, 1.00, 0.00, ..., 1.00]                         │
  │      位置 1 → [0.84, 0.54, 0.01, ..., 0.99]                         │
  │                                                                     │
  │  1.4 相加得到最终嵌入                                                │
  │      最终向量 = Token Embedding + Position Embedding                │
  │                                                                     │
  └─────────────────────────────────────────────────────────────────────┘
       │
       ▼
  ┌─────────────────────────────────────────────────────────────────────┐
  │  STEP 2: Transformer Block × N (GPT-2 有 12 层，GPT-4 有 120+ 层)     │
  │  ─────────────────────────────────────────────────────────────────  │
  │                                                                     │
  │  每个 Block 包含:                                                    │
  │                                                                     │
  │  ┌─────────────────────────────────────────────────────────────┐    │
  │  │  Multi-Head Self-Attention                                  │    │
  │  │  • 让每个词"看到"其他词                                      │    │
  │  │  • 计算词与词之间的关联程度                                   │    │
  │  └─────────────────────────────────────────────────────────────┘    │
  │                        │                                            │
  │                        ▼                                            │
  │  ┌─────────────────────────────────────────────────────────────┐    │
  │  │  Add & Norm (残差连接 + 层归一化)                            │    │
  │  │  • 残差: output = input + attention_output                  │    │
  │  │  • 帮助梯度流动，防止梯度消失                                 │    │
  │  └─────────────────────────────────────────────────────────────┘    │
  │                        │                                            │
  │                        ▼                                            │
  │  ┌─────────────────────────────────────────────────────────────┐    │
  │  │  Feed Forward Network (前馈神经网络)                         │    │
  │  │  • 两层全连接: 768 → 3072 → 768                             │    │
  │  │  • 对每个位置独立处理                                        │    │
  │  └─────────────────────────────────────────────────────────────┘    │
  │                        │                                            │
  │                        ▼                                            │
  │  ┌─────────────────────────────────────────────────────────────┐    │
  │  │  Add & Norm (再次残差连接 + 层归一化)                        │    │
  │  └─────────────────────────────────────────────────────────────┘    │
  │                                                                     │
  └─────────────────────────────────────────────────────────────────────┘
       │
       ▼
  ┌─────────────────────────────────────────────────────────────────────┐
  │  STEP 3: 输出层                                                     │
  │  ─────────────────────────────────────────────────────────────────  │
  │                                                                     │
  │  3.1 线性投影                                                       │
  │      768 维 → 50,257 维 (词汇表大小)                                │
  │      每个维度代表一个词的"得分"(logits)                              │
  │                                                                     │
  │  3.2 Softmax 归一化                                                 │
  │      logits → 概率分布 (所有概率之和 = 1)                            │
  │                                                                     │
  │      [3.1, 0.5, 1.8, ...] → [0.45, 0.08, 0.32, ...]                │
  │                                                                     │
  │  3.3 采样策略选择下一个词                                            │
  │      • Greedy: 选概率最高的                                         │
  │      • Top-k: 从前 k 个中随机选                                     │
  │      • Top-p: 从累积概率达到 p 的词中选                              │
  │                                                                     │
  └─────────────────────────────────────────────────────────────────────┘
       │
       ▼
  输出: "好" (下一个词的预测)
\`\`\`

---

## 深入理解：分词 (Tokenization)

分词是第一步，也是容易被忽视但非常重要的一步。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    分词方式对比                                          │
└─────────────────────────────────────────────────────────────────────────┘

  方式1: 按字符分词
  "Hello" → ["H", "e", "l", "l", "o"]
  ✗ 缺点: 序列太长，无法捕捉词义

  方式2: 按空格分词
  "I love AI" → ["I", "love", "AI"]
  ✗ 缺点: 词汇表太大，罕见词无法处理

  方式3: BPE (Byte Pair Encoding) - 现代模型采用
  "unhappiness" → ["un", "happiness"] 或 ["un", "happ", "iness"]
  ✓ 优点: 平衡词汇表大小和序列长度

  GPT-2 词汇表: 50,257 个 token
  GPT-4 词汇表: ~100,000 个 token
  Claude 词汇表: ~100,000 个 token
\`\`\`

**中文分词示例:**
\`\`\`python
# 使用 tiktoken (OpenAI 的分词器)
import tiktoken
enc = tiktoken.encoding_for_model("gpt-4")

text = "今天天气很好"
tokens = enc.encode(text)
# 可能输出: [12345, 23456, 34567, 45678]  (示意)

# 每个数字对应词汇表中的一个 token
# 中文通常 1-2 个字符 = 1 个 token
\`\`\`

---

## 深入理解：词嵌入 (Token Embedding)

每个 token 被映射到一个高维向量空间。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    词嵌入可视化                                          │
└─────────────────────────────────────────────────────────────────────────┘

  词嵌入矩阵:
  ┌─────────────────────────────────────────────────────────────────┐
  │                    50,257 × 768                                 │
  │  ─────────────────────────────────────────────────────────────  │
  │                                                                 │
  │  token 0 ("!")    → [0.12, -0.45, 0.78, ..., 0.33]  (768维)    │
  │  token 1 (""")   → [0.23, 0.56, -0.12, ..., 0.67]  (768维)    │
  │  token 2 ("#")    → [-0.34, 0.78, 0.45, ..., -0.89] (768维)    │
  │  ...                                                            │
  │  token 15339 ("今天") → [0.56, -0.23, 0.89, ..., 0.12]          │
  │  token 28965 ("天气") → [0.45, -0.34, 0.78, ..., 0.23]          │
  │  ...                                                            │
  │  token 50256 ("<|endoftext|>") → [0.11, 0.22, ..., 0.33]       │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  GPT-2 词嵌入矩阵参数量: 50,257 × 768 ≈ 3,860 万参数！
\`\`\`

**语义关系在向量空间中的体现:**
\`\`\`python
# 经典例子: 词向量的语义算术
king - man + woman ≈ queen

# 相似词在向量空间中距离接近
cos_similarity("cat", "dog") ≈ 0.85  # 高相似度
cos_similarity("cat", "car") ≈ 0.15  # 低相似度
\`\`\`

---

## 深入理解：位置编码 (Positional Encoding)

自注意力机制本身不包含位置信息，需要显式添加。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    为什么需要位置编码？                                   │
└─────────────────────────────────────────────────────────────────────────┘

  问题: 自注意力是"集合运算"，不区分顺序

  例子:
  句子1: "猫 追 狗"  →  注意力看到 {猫, 追, 狗}
  句子2: "狗 追 猫"  →  注意力看到 {狗, 追, 猫}

  如果不加位置信息，模型会认为这两句话是一样的！

  解决: 给每个位置一个独特的"指纹"
\`\`\`

**原始 Transformer 使用正弦/余弦位置编码:**
\`\`\`python
# 位置编码公式
PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))

# pos = 词的位置 (0, 1, 2, ...)
# i = 维度索引 (0, 1, 2, ..., d_model/2)
# d_model = 嵌入维度 (如 768)
\`\`\`

\`\`\`
位置编码可视化 (假设 d_model = 8):

位置    dim0   dim1   dim2   dim3   dim4   dim5   dim6   dim7
───────────────────────────────────────────────────────────────
pos=0   0.00   1.00   0.00   1.00   0.00   1.00   0.00   1.00
pos=1   0.84   0.54   0.10   0.99   0.01   1.00   0.00   1.00
pos=2   0.91  -0.42   0.20   0.98   0.02   1.00   0.00   1.00
pos=3   0.14  -0.99   0.30   0.95   0.03   1.00   0.00   1.00
...

特点:
• 每个位置有独特的编码
• 不同维度变化频率不同 (低维度变化快，高维度变化慢)
• 相对位置可以通过线性变换表示
\`\`\`

**现代模型 (GPT-2/3/4, Llama) 使用可学习位置编码:**
\`\`\`python
# 可学习位置编码
self.position_embedding = nn.Embedding(max_seq_len, d_model)
# max_seq_len: 最大序列长度 (GPT-2: 1024, GPT-4: 128K+)

# 位置嵌入也是训练出来的，而不是固定公式
\`\`\`

---

## 深入理解：残差连接与层归一化 (Add & Norm)

为什么每个子层后都有 Add & Norm？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    残差连接 (Residual Connection)                        │
└─────────────────────────────────────────────────────────────────────────┘

  普通网络:
  x → [子层] → y

  残差网络:
  x → [子层] → y
  │            │
  └─────(+)────┘  →  output = x + y

  为什么重要？
  1. 梯度可以直接流回早期层 (解决梯度消失)
  2. 网络可以学习"恒等映射" (如果子层不需要，可以输出 0)
  3. 让训练更稳定，可以堆叠更多层

  GPT-3 有 96 层，没有残差连接根本无法训练！
\`\`\`

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    层归一化 (Layer Normalization)                        │
└─────────────────────────────────────────────────────────────────────────┘

  作用: 将每一层的输出归一化到均值为 0、方差为 1

  LayerNorm(x) = γ * (x - μ) / σ + β

  • μ = x 的均值
  • σ = x 的标准差
  • γ, β = 可学习的缩放和偏移参数

  为什么重要？
  1. 稳定训练过程
  2. 加速收敛
  3. 允许使用更大的学习率
\`\`\`

---

## 三种 Transformer 变体

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    Transformer 架构变体                                  │
└─────────────────────────────────────────────────────────────────────────┘

     Encoder-Only              Decoder-Only              Encoder-Decoder
     ────────────              ────────────              ───────────────
     ┌──────────┐             ┌──────────┐              ┌─────┐  ┌─────┐
     │ Encoder  │             │ Decoder  │              │ Enc │→│ Dec │
     └──────────┘             └──────────┘              └─────┘  └─────┘

  注意力类型:                注意力类型:                注意力类型:
  双向 (全部可见)            单向 (只看左边)            编码双向+解码单向

  代表模型:                  代表模型:                  代表模型:
  • BERT                    • GPT-1/2/3/4             • T5
  • RoBERTa                 • Claude                  • BART
  • ALBERT                  • Llama 1/2/3             • mBART

  擅长任务:                  擅长任务:                  擅长任务:
  • 文本分类                 • 文本生成                 • 机器翻译
  • 命名实体识别              • 对话系统                 • 文本摘要
  • 句子相似度               • 代码生成                 • 问答系统
  • 情感分析                 • 创意写作                 • 语音识别

  训练目标:                  训练目标:                  训练目标:
  掩码语言模型 (MLM)         下一词预测 (NTP)           Seq2Seq

     [MASK] 填空              一个词接一个词生成          输入→输出映射
\`\`\`

> 💡 **2024-2025 年的主流**: GPT、Claude、Llama 全部是 **Decoder-Only** 架构

---

## 参数量与模型能力

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    大模型参数量全景图                                     │
└─────────────────────────────────────────────────────────────────────────┘

  模型                参数量        层数     注意力头   嵌入维度
  ─────────────────────────────────────────────────────────────────
  GPT-2 Small         124M         12        12        768
  GPT-2 Large         774M         36        20        1280
  GPT-2 XL            1.5B         48        25        1600
  ─────────────────────────────────────────────────────────────────
  GPT-3 Small         125M         12        12        768
  GPT-3 Medium        350M         24        16        1024
  GPT-3 Large         760M         24        16        1536
  GPT-3 XL            1.3B         24        24        2048
  GPT-3 6.7B          6.7B         32        32        4096
  GPT-3 13B           13B          40        40        5120
  GPT-3 175B          175B         96        96        12288
  ─────────────────────────────────────────────────────────────────
  Llama 2 7B          7B           32        32        4096
  Llama 2 13B         13B          40        40        5120
  Llama 2 70B         70B          80        64        8192
  ─────────────────────────────────────────────────────────────────
  Claude 3 Haiku      ~20B         ?         ?         ?
  Claude 3 Sonnet     ~70B         ?         ?         ?
  Claude 3 Opus       ~200B        ?         ?         ?

  参数主要分布在:
  • 词嵌入矩阵: vocab_size × d_model
  • 注意力层: 4 × n_layers × d_model²
  • 前馈层: 8 × n_layers × d_model²
  • 输出层: d_model × vocab_size
\`\`\`

**Scaling Law (扩展定律):**
\`\`\`
模型性能 ∝ (参数量)^0.076 × (数据量)^0.095 × (计算量)^0.050

核心发现:
• 参数量翻倍 → 性能提升约 5%
• 数据量翻倍 → 性能提升约 7%
• 目前还没有看到明显的天花板

这就是为什么各大公司都在疯狂扩大模型规模！
\`\`\`

---

## 本节要点

1. **文本生成本质** —— 自回归预测下一个词
2. **完整流程** —— 分词 → 嵌入 → Transformer Block × N → 输出概率
3. **关键组件** —— 词嵌入、位置编码、残差连接、层归一化
4. **三种架构** —— Encoder-Only、Decoder-Only、Encoder-Decoder
5. **现代趋势** —— GPT/Claude/Llama 都是 Decoder-Only，规模持续扩大
            `,ja:`
## Transformer：世界を変えたアーキテクチャ

2017年、Googleが論文「Attention Is All You Need」を発表し、Transformerを提案しました。今日のGPT、Claude、Geminiはすべてこのアーキテクチャに基づいています。

---

## 体験してみよう：Transformerの動作

詳しく学ぶ前に、このインタラクティブデモでTransformerがテキストを処理する様子を体験してみましょう：

::transformer-v2-viz::

---

## テキスト生成の核心原理：次のトークン予測

大規模言語モデルの本質はシンプル：**次の単語を予測する**こと。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    テキスト生成 = 次の単語を繰り返し予測                    │
└─────────────────────────────────────────────────────────────────────────┘

  入力: "今日は天気が"

  Step 1: 次の単語を予測
  ┌────────────────────────────────────────┐
  │ 入力: "今日は天気が"                    │
  │                                        │
  │ 出力確率分布:                           │
  │   "良い"   → 45%                       │
  │   "悪い"   → 25%                       │
  │   "いい"   → 15%                       │
  │   ...                                  │
  │                                        │
  │ 選択: "良い" (最高確率)                 │
  └────────────────────────────────────────┘

  Step 2: 予測した単語を入力に追加し、続けて予測
  ┌────────────────────────────────────────┐
  │ 入力: "今日は天気が良い"                │
  │ → 次の予測: "です" (60%)               │
  └────────────────────────────────────────┘

  最終出力: "今日は天気が良いですね。"
\`\`\`

> 💡 **これが「自己回帰生成」（Autoregressive Generation）です**

---

## 完全な処理フロー

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    Transformer処理フロー全体図                            │
└─────────────────────────────────────────────────────────────────────────┘

  "今日は天気が良い"
       │
       ▼
  ┌─────────────────────────────────────────────────────────────────────┐
  │  STEP 1: 埋め込み層 (Embedding)                                      │
  │  ─────────────────────────────────────────────────────────────────  │
  │  1.1 トークン化: "今日" "は" "天気" "が" "良い"                       │
  │  1.2 トークン埋め込み: 各トークン → 768次元ベクトル                    │
  │  1.3 位置埋め込み: 位置情報を追加                                     │
  │  1.4 最終埋め込み = トークン埋め込み + 位置埋め込み                    │
  └─────────────────────────────────────────────────────────────────────┘
       │
       ▼
  ┌─────────────────────────────────────────────────────────────────────┐
  │  STEP 2: Transformer Block × N層                                    │
  │  ─────────────────────────────────────────────────────────────────  │
  │  • Multi-Head Self-Attention: 単語間の関係を計算                     │
  │  • Add & Norm: 残差接続 + 層正規化                                   │
  │  • Feed Forward: 768 → 3072 → 768                                  │
  │  • Add & Norm: 再び残差接続 + 層正規化                               │
  └─────────────────────────────────────────────────────────────────────┘
       │
       ▼
  ┌─────────────────────────────────────────────────────────────────────┐
  │  STEP 3: 出力層                                                     │
  │  ─────────────────────────────────────────────────────────────────  │
  │  • 線形投影: 768次元 → 50,257次元（語彙サイズ）                       │
  │  • Softmax: 確率分布に変換                                          │
  │  • サンプリング: 次のトークンを選択                                   │
  └─────────────────────────────────────────────────────────────────────┘
\`\`\`

---

## トークン化 (Tokenization)

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    トークン化方式の比較                                   │
└─────────────────────────────────────────────────────────────────────────┘

  方式1: 文字単位
  "Hello" → ["H", "e", "l", "l", "o"]
  ✗ シーケンスが長すぎる

  方式2: スペース区切り
  "I love AI" → ["I", "love", "AI"]
  ✗ 語彙が大きすぎる

  方式3: BPE (現代モデルが採用)
  "unhappiness" → ["un", "happiness"]
  ✓ バランスが良い

  GPT-2語彙: 50,257トークン
  GPT-4語彙: ~100,000トークン
\`\`\`

---

## 位置エンコーディング

\`\`\`
なぜ位置情報が必要か？

セルフアテンションは順序を区別しない:
"猫が犬を追う" と "犬が猫を追う"
→ 位置情報なしでは同じに見える！

解決策: 各位置に固有の「指紋」を追加

PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))

現代モデル(GPT-2/3/4, Llama)は学習可能な位置埋め込みを使用
\`\`\`

---

## 残差接続と層正規化

\`\`\`
残差接続 (Residual Connection):
output = x + SubLayer(x)

利点:
• 勾配が直接初期層に流れる
• 深いネットワークの学習が可能に
• GPT-3は96層、残差なしでは学習不可能

層正規化 (Layer Normalization):
LayerNorm(x) = γ * (x - μ) / σ + β

利点:
• 学習の安定化
• 収束の加速
\`\`\`

---

## 3種類のTransformerアーキテクチャ

| タイプ | 代表モデル | 得意なタスク |
|--------|----------|------------|
| Encoder-Only | BERT, RoBERTa | 分類、NER、感情分析 |
| Decoder-Only | GPT, Claude, Llama | 生成、対話、コード |
| Encoder-Decoder | T5, BART | 翻訳、要約、Q&A |

> 💡 **2024-2025年の主流**: GPT、Claude、Llamaは全て **Decoder-Only**

---

## パラメータ数と規模

\`\`\`
モデル              パラメータ数    層数
─────────────────────────────────────
GPT-2 XL            1.5B         48
GPT-3 175B          175B         96
Llama 2 70B         70B          80
Claude 3 Opus       ~200B        ?

Scaling Law:
性能 ∝ (パラメータ)^0.076 × (データ)^0.095

パラメータ2倍 → 性能約5%向上
\`\`\`

---

## 本節のポイント

1. **テキスト生成の本質** —— 自己回帰で次のトークンを予測
2. **完全なフロー** —— トークン化 → 埋め込み → Transformer Block → 確率出力
3. **重要な構成要素** —— 埋め込み、位置エンコーディング、残差接続
4. **3種類のアーキテクチャ** —— Encoder-Only、Decoder-Only、Encoder-Decoder
5. **現代のトレンド** —— GPT/Claude/Llamaは全てDecoder-Only
            `}},{id:"ch1-attention",title:{zh:"1.2 注意力机制深入理解",ja:"1.2 アテンション機構の深い理解"},content:{zh:`
## 注意力机制：Transformer 的灵魂

注意力机制（Attention）是 Transformer 最核心的创新。它让模型能够动态地"关注"输入中最相关的部分。

### 交互式演示：注意力机制可视化

先通过这个交互式演示探索注意力是如何工作的：

::attention-viz::

---

## 用搜索引擎类比理解 Q/K/V

理解 Q、K、V 最好的方式是把它想象成**搜索引擎**：

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    Q/K/V 的搜索引擎类比                                   │
└─────────────────────────────────────────────────────────────────────────┘

  想象你在 Google 搜索：

  ┌─────────────────────────────────────────────────────────────────────┐
  │                                                                     │
  │  🔍 Query (查询)：你输入的搜索词                                      │
  │     "如何学习机器学习"                                               │
  │                                                                     │
  │  📑 Key (键)：网页的标题/关键词                                       │
  │     - "机器学习入门教程"                                             │
  │     - "深度学习实战指南"                                             │
  │     - "Python 编程基础"                                              │
  │     - "今日新闻头条"                                                 │
  │                                                                     │
  │  📄 Value (值)：网页的实际内容                                        │
  │     - [机器学习教程的完整内容...]                                     │
  │     - [深度学习指南的完整内容...]                                     │
  │     - [Python 基础的完整内容...]                                      │
  │     - [新闻的完整内容...]                                            │
  │                                                                     │
  └─────────────────────────────────────────────────────────────────────┘

  搜索过程：
  1. Query 和每个 Key 计算相似度（点击率预估）
  2. 相似度高的 Key 对应的 Value 获得更高权重
  3. 返回加权后的结果（相关网页排在前面）

  在 Transformer 中：
  • Query = "我现在想要什么信息？"
  • Key = "我有什么信息可以提供？"
  • Value = "这些信息的具体内容"
\`\`\`

---

## Self-Attention 完整计算流程

让我们用一个具体例子走一遍完整的注意力计算。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    完整示例：计算 "猫 吃 鱼" 的注意力                       │
└─────────────────────────────────────────────────────────────────────────┘

  输入句子: "猫 吃 鱼"

  ═══════════════════════════════════════════════════════════════════════
  Step 1: 将每个词转换为向量（词嵌入）
  ═══════════════════════════════════════════════════════════════════════

  "猫" → x₁ = [0.2, 0.5, 0.1, 0.8]  (假设 4 维，实际是 768 维)
  "吃" → x₂ = [0.6, 0.1, 0.9, 0.3]
  "鱼" → x₃ = [0.3, 0.7, 0.2, 0.5]

  ═══════════════════════════════════════════════════════════════════════
  Step 2: 通过线性变换生成 Q, K, V
  ═══════════════════════════════════════════════════════════════════════

  每个词都会生成自己的 Q, K, V：

  对于 "猫":
    Q₁ = x₁ × W_Q = [0.2, 0.5, 0.1, 0.8] × W_Q = [2.2, 0.8]
    K₁ = x₁ × W_K = [0.2, 0.5, 0.1, 0.8] × W_K = [0.9, 1.1]
    V₁ = x₁ × W_V = [0.2, 0.5, 0.1, 0.8] × W_V = [0.5, 0.3]

  对于 "吃":
    Q₂ = [0.7, 1.4]    K₂ = [2.0, 0.6]    V₂ = [0.8, 0.2]

  对于 "鱼":
    Q₃ = [2.0, 0.5]    K₃ = [0.4, 1.3]    V₃ = [0.2, 0.9]

  ═══════════════════════════════════════════════════════════════════════
  Step 3: 计算注意力分数 (Q × K^T)
  ═══════════════════════════════════════════════════════════════════════

  以 "猫" 为例，计算它对每个词的注意力分数：

  score(猫→猫) = Q₁ · K₁ = [2.2, 0.8] · [0.9, 1.1] = 1.08 + 0.88 = 1.96
  score(猫→吃) = Q₁ · K₂ = [2.2, 0.8] · [2.0, 0.6] = 1.20 + 0.48 = 1.68
  score(猫→鱼) = Q₁ · K₃ = [2.2, 0.8] · [0.4, 1.3] = 0.48 + 1.04 = 1.52

  完整的注意力分数矩阵：
                K₁(猫)  K₂(吃)  K₃(鱼)
              ┌───────────────────────┐
  Q₁(猫)      │  1.96    1.68    1.52  │
  Q₂(吃)      │  2.31    2.10    1.87  │
  Q₃(鱼)      │  1.45    1.30    1.15  │
              └───────────────────────┘

  ═══════════════════════════════════════════════════════════════════════
  Step 4: 缩放 (除以 √d_k)
  ═══════════════════════════════════════════════════════════════════════

  为什么要缩放？
  • 当维度 d_k 很大时，点积的值会很大
  • 大的值经过 softmax 后会产生极端的分布（接近 one-hot）
  • 这会导致梯度消失，训练不稳定

  缩放后 (d_k = 2, √d_k = 1.414)：
                K₁(猫)  K₂(吃)  K₃(鱼)
              ┌───────────────────────┐
  Q₁(猫)      │  1.39    1.19    1.07  │
  Q₂(吃)      │  1.63    1.48    1.32  │
  Q₃(鱼)      │  1.02    0.92    0.81  │
              └───────────────────────┘

  ═══════════════════════════════════════════════════════════════════════
  Step 5: Softmax 归一化
  ═══════════════════════════════════════════════════════════════════════

  对每一行应用 softmax，使每行和为 1：

  softmax([2.39, 1.19, 1.07]) = [0.40, 0.33, 0.27]

  注意力权重矩阵：
                   猫      吃      鱼
              ┌───────────────────────┐
  猫          │  0.40    0.33    0.27  │  → 行和 = 1
  吃          │  0.42    0.35    0.23  │  → 行和 = 1
  鱼          │  0.38    0.34    0.28  │  → 行和 = 1
              └───────────────────────┘

  解读：
  • "猫" 对自己的注意力是 0.40（最高）
  • "猫" 对 "吃" 的注意力是 0.33
  • "猫" 对 "鱼" 的注意力是 0.27

  ═══════════════════════════════════════════════════════════════════════
  Step 6: 加权求和得到输出
  ═══════════════════════════════════════════════════════════════════════

  "猫" 的新表示 = 0.40×V₁ + 0.33×V₂ + 0.27×V₃
                = 0.40×[0.5,0.3] + 0.33×[0.8,0.2] + 0.27×[0.2,0.9]
                = [0.20,0.12] + [0.26,0.07] + [0.05,0.24]
                = [0.51, 0.43]

  新的表示融合了整个句子的上下文信息！
\`\`\`

---

## 核心公式详解

\`\`\`python
Attention(Q, K, V) = softmax(Q · K^T / √d_k) · V
\`\`\`

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    公式分解                                              │
└─────────────────────────────────────────────────────────────────────────┘

  Q · K^T        计算相似度（哪些词和我相关）
     ↓
  / √d_k         缩放（防止梯度消失）
     ↓
  softmax()      归一化（转换为概率分布，和为1）
     ↓
  × V            加权求和（提取相关信息）

  ═══════════════════════════════════════════════════════════════════════

  为什么是 √d_k？

  假设 Q 和 K 的元素都是均值为 0、方差为 1 的随机变量：
  • Q·K 的方差 ≈ d_k
  • 当 d_k = 64 时，Q·K 的标准差 ≈ 8
  • 除以 √64 = 8 后，标准差变回 1

  这保证了无论维度多大，softmax 的输入都在合理范围内。
\`\`\`

---

## 多头注意力（Multi-Head Attention）

单个注意力头只能学习一种关系模式。多头注意力让模型同时从多个角度理解输入。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    多头注意力可视化                                       │
└─────────────────────────────────────────────────────────────────────────┘

  句子: "The cat sat on the mat because it was tired"

  不同的头学习不同的关系：

  Head 1: 主语-动词关系          Head 2: 代词指代
  ┌─────────────────────┐       ┌─────────────────────┐
  │                     │       │                     │
  │  cat ═══════► sat   │       │  it ═══════► cat    │
  │  (谁在坐？)          │       │  (it 指的是谁？)     │
  │                     │       │                     │
  └─────────────────────┘       └─────────────────────┘

  Head 3: 介词关系               Head 4: 因果关系
  ┌─────────────────────┐       ┌─────────────────────┐
  │                     │       │                     │
  │  sat ═══► on ═══► mat│       │  because ═══► tired │
  │  (坐在哪里？)         │       │  (为什么？)          │
  │                     │       │                     │
  └─────────────────────┘       └─────────────────────┘

  GPT-2: 12 个注意力头
  GPT-3: 96 个注意力头
  GPT-4: 估计 100+ 个注意力头
\`\`\`

\`\`\`python
# 多头注意力的完整实现
import torch
import torch.nn as nn
import math

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model=768, num_heads=12):
        super().__init__()
        self.num_heads = num_heads
        self.d_k = d_model // num_heads  # 每个头的维度: 768/12 = 64

        # 一次性计算所有头的 Q, K, V（效率更高）
        self.W_Q = nn.Linear(d_model, d_model)
        self.W_K = nn.Linear(d_model, d_model)
        self.W_V = nn.Linear(d_model, d_model)
        self.W_O = nn.Linear(d_model, d_model)

    def forward(self, x, mask=None):
        batch_size, seq_len, d_model = x.shape

        # Step 1: 线性变换
        Q = self.W_Q(x)  # [batch, seq_len, d_model]
        K = self.W_K(x)
        V = self.W_V(x)

        # Step 2: 拆分成多个头
        # [batch, seq_len, d_model] → [batch, num_heads, seq_len, d_k]
        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)

        # Step 3: 计算注意力
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)

        # Step 4: 应用掩码（如果有）
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        # Step 5: Softmax
        attn_weights = torch.softmax(scores, dim=-1)

        # Step 6: 加权求和
        output = torch.matmul(attn_weights, V)

        # Step 7: 合并所有头
        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)

        # Step 8: 最终线性变换
        return self.W_O(output)
\`\`\`

---

## Masked Self-Attention（掩码自注意力）

在文本生成任务中，模型在预测第 n 个词时，不能看到第 n+1, n+2, ... 位置的词。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    掩码注意力的工作原理                                    │
└─────────────────────────────────────────────────────────────────────────┘

  生成句子: "I love AI"

  训练时，我们有完整的句子，但需要模拟生成过程：

  预测位置    能看到的词           预测目标
  ─────────────────────────────────────────
  位置 1      [I]                  love
  位置 2      [I, love]            AI
  位置 3      [I, love, AI]        <END>

  掩码矩阵（下三角矩阵）：

           I     love    AI
        ┌─────────────────────┐
  I     │  1      0       0   │   1 = 可以看到
  love  │  1      1       0   │   0 = 看不到
  AI    │  1      1       1   │
        └─────────────────────┘

  在注意力分数上应用掩码：

  原始分数：              加掩码后（-∞）：         Softmax 后：
  ┌─────────────┐       ┌─────────────┐        ┌─────────────┐
  │ 2.1 1.5 0.8 │       │ 2.1  -∞  -∞ │        │ 1.0 0.0 0.0 │
  │ 1.2 2.8 1.1 │   →   │ 1.2 2.8  -∞ │   →    │ 0.2 0.8 0.0 │
  │ 0.9 1.3 2.5 │       │ 0.9 1.3 2.5 │        │ 0.1 0.3 0.6 │
  └─────────────┘       └─────────────┘        └─────────────┘

  -∞ 经过 softmax 后变成 0，实现了"看不见"的效果
\`\`\`

---

## Temperature 与采样策略

生成文本时，如何从概率分布中选择下一个词？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    Temperature 控制输出多样性                             │
└─────────────────────────────────────────────────────────────────────────┘

  原始 logits: [3.0, 1.0, 0.5, 0.1]
  对应词汇:     好    热    冷    坏

  Temperature 的作用：logits / temperature → softmax

  ┌─────────────────────────────────────────────────────────────────────┐
  │  Temperature = 0.5（低温，更确定）                                    │
  │  ─────────────────────────────────────────────────────────────────  │
  │  logits / 0.5 = [1.0, 2.0, 1.0, 0.2]                               │
  │  softmax → [0.84, 0.11, 0.04, 0.01]                                │
  │                                                                     │
  │  "好" 的概率高达 84%！输出更确定、更保守                              │
  └─────────────────────────────────────────────────────────────────────┘

  ┌─────────────────────────────────────────────────────────────────────┐
  │  Temperature = 1.0（默认）                                           │
  │  ─────────────────────────────────────────────────────────────────  │
  │  logits / 1.0 = [3.0, 1.0, 0.5, 0.1]                               │
  │  softmax → [0.53, 0.19, 0.12, 0.08]                                │
  │                                                                     │
  │  正常的概率分布                                                      │
  └─────────────────────────────────────────────────────────────────────┘

  ┌─────────────────────────────────────────────────────────────────────┐
  │  Temperature = 2.0（高温，更随机）                                    │
  │  ─────────────────────────────────────────────────────────────────  │
  │  logits / 2.0 = [2.0, 0.5, 0.25, 0.05]                             │
  │  softmax → [0.38, 0.23, 0.18, 0.15]                                │
  │                                                                     │
  │  概率分布更平坦，输出更多样、更有创意（但可能更不准确）                  │
  └─────────────────────────────────────────────────────────────────────┘

  使用建议：
  • 代码生成 / 数学题：Temperature = 0 (贪心解码)
  • 一般对话：Temperature = 0.7 ~ 1.0
  • 创意写作：Temperature = 1.0 ~ 1.5
\`\`\`

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    Top-k 和 Top-p 采样                                   │
└─────────────────────────────────────────────────────────────────────────┘

  Top-k 采样：只从概率最高的 k 个词中选择

  原始分布: [好: 0.53, 热: 0.19, 冷: 0.12, 坏: 0.08, ...]

  Top-k = 3:
  [好: 0.53, 热: 0.19, 冷: 0.12] → 重新归一化 → [好: 0.63, 热: 0.23, 冷: 0.14]

  ─────────────────────────────────────────────────────────────────────────

  Top-p 采样（Nucleus Sampling）：选择累积概率达到 p 的最小词集

  原始分布（按概率排序）:
  好: 0.53  累积: 0.53
  热: 0.19  累积: 0.72
  冷: 0.12  累积: 0.84
  坏: 0.08  累积: 0.92  ← Top-p = 0.9 时，选到这里
  ...

  Top-p = 0.9:
  从 {好, 热, 冷, 坏} 中采样

  ─────────────────────────────────────────────────────────────────────────

  实际使用中，通常组合使用：
  • temperature = 0.7
  • top_p = 0.9
  • top_k = 50
\`\`\`

---

## 注意力可视化

\`\`\`
实际的注意力模式可以通过工具可视化：

输入: "The cat sat on the mat"

Layer 6, Head 8（学习到的句法关系）：

        The   cat   sat    on   the   mat
The     ███   ░░░   ░░░   ░░░   ░░░   ░░░
cat     ░░░   ███   ░░░   ░░░   ░░░   ░░░
sat     ░░░   ███   ██░   ░░░   ░░░   ░░░  ← "sat" 关注 "cat"（主语）
on      ░░░   ░░░   ███   ███   ░░░   ░░░
the     ░░░   ░░░   ░░░   ░░░   ███   ░░░
mat     ░░░   ░░░   ░░░   ███   ░░░   ███  ← "mat" 关注 "on"（介词）

███ = 高注意力    ░░░ = 低注意力

推荐工具：BertViz, Attention Viz, Transformer Explainer
\`\`\`

---

## 本节要点

1. **Q/K/V 类比** —— 就像搜索引擎：Query 是搜索词，Key 是标题，Value 是内容
2. **注意力公式** —— Attention = softmax(QK^T/√d_k)V，缩放防止梯度消失
3. **多头注意力** —— 12-96 个头同时学习不同类型的关系
4. **掩码注意力** —— 通过 -∞ 掩码实现"看不到未来"
5. **Temperature** —— 低温更确定，高温更随机
6. **Top-k/Top-p** —— 限制采样范围，平衡质量和多样性
            `,ja:`
## アテンション機構：Transformerの魂

アテンション機構（Attention）はTransformerの最も核心的なイノベーションです。入力の中で最も関連性の高い部分に動的に「注目」することができます。

### インタラクティブデモ：アテンション機構の可視化

このインタラクティブデモでアテンションがどのように機能するかを探索してください：

::attention-viz::

---

## 検索エンジンで理解する Q/K/V

Q/K/V を理解する最良の方法は**検索エンジン**のアナロジーです：

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    Q/K/V の検索エンジンアナロジー                          │
└─────────────────────────────────────────────────────────────────────────┘

  Google検索を想像してください：

  🔍 Query (クエリ)：入力した検索ワード
     "機械学習 入門"

  📑 Key (キー)：ウェブページのタイトル/キーワード
     - "機械学習入門チュートリアル"
     - "深層学習実践ガイド"
     - "Python プログラミング基礎"

  📄 Value (値)：ウェブページの実際のコンテンツ
     - [機械学習チュートリアルの完全な内容...]
     - [深層学習ガイドの完全な内容...]

  Transformerでは：
  • Query = "今、どんな情報が欲しい？"
  • Key = "どんな情報を提供できる？"
  • Value = "その情報の具体的な内容"
\`\`\`

---

## Self-Attention 完全な計算フロー

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    完全な例："猫 食べる 魚" のアテンション計算              │
└─────────────────────────────────────────────────────────────────────────┘

  Step 1: 各単語をベクトルに変換
  "猫" → x₁ = [0.2, 0.5, 0.1, 0.8]
  "食べる" → x₂ = [0.6, 0.1, 0.9, 0.3]
  "魚" → x₃ = [0.3, 0.7, 0.2, 0.5]

  Step 2: 線形変換で Q, K, V を生成
  各単語に対して Q, K, V を計算

  Step 3: アテンションスコア計算 (Q × K^T)
  各単語ペアの類似度を計算

  Step 4: スケーリング (÷ √d_k)
  勾配消失を防ぐ

  Step 5: Softmax 正規化
  各行の和が1になる確率分布に変換

  Step 6: 加重和で出力を得る
  新しい表現 = Σ(アテンション重み × V)
\`\`\`

---

## コア公式

\`\`\`python
Attention(Q, K, V) = softmax(Q · K^T / √d_k) · V
\`\`\`

\`\`\`
なぜ √d_k で割るのか？

• d_k が大きいとき、ドット積の値が大きくなる
• 大きな値は softmax 後に極端な分布（one-hotに近い）を生む
• これは勾配消失を引き起こし、学習を不安定にする

√d_k で割ることで、入力を適切な範囲に保つ
\`\`\`

---

## Multi-Head Attention

\`\`\`
異なるヘッドが異なる関係を学習：

Head 1: 主語-動詞関係    Head 2: 代名詞参照
Head 3: 前置詞関係      Head 4: 因果関係

GPT-2: 12ヘッド
GPT-3: 96ヘッド
GPT-4: 推定100+ヘッド
\`\`\`

---

## Masked Self-Attention

\`\`\`
テキスト生成では、未来の単語を「カンニング」できない：

文: "I love AI"

予測位置    見える単語        予測対象
─────────────────────────────────────
位置 1      [I]               love
位置 2      [I, love]         AI
位置 3      [I, love, AI]     <END>

マスク行列（下三角）：
      I   love  AI
  ┌─────────────────┐
I │  1    0    0   │  1 = 見える
love│  1    1    0   │  0 = 見えない
AI│  1    1    1   │
  └─────────────────┘

-∞ を適用後、softmax で 0 になり「見えない」効果を実現
\`\`\`

---

## Temperature とサンプリング戦略

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    Temperature で出力の多様性を制御                        │
└─────────────────────────────────────────────────────────────────────────┘

Temperature = 0.5（低温）→ より確定的、保守的
Temperature = 1.0（デフォルト）→ 通常の分布
Temperature = 2.0（高温）→ より多様、創造的

使用ガイドライン：
• コード生成/数学: Temperature = 0（貪欲デコーディング）
• 一般的な会話: Temperature = 0.7 ~ 1.0
• 創作活動: Temperature = 1.0 ~ 1.5

Top-k サンプリング: 上位 k 個から選択
Top-p サンプリング: 累積確率が p に達するまでの単語から選択
\`\`\`

---

## 本節のポイント

1. **Q/K/V アナロジー** —— 検索エンジン：Query=検索語、Key=タイトル、Value=内容
2. **アテンション公式** —— Attention = softmax(QK^T/√d_k)V
3. **マルチヘッド** —— 12-96個のヘッドが異なる関係を学習
4. **マスクアテンション** —— -∞マスクで「未来が見えない」を実現
5. **Temperature** —— 低温=確定的、高温=多様性
6. **Top-k/Top-p** —— サンプリング範囲を制限、品質と多様性のバランス
            `}},{id:"ch1-finetune-intro",title:{zh:"1.3 大模型微调入门",ja:"1.3 大規模モデルファインチューニング入門"},content:{zh:`
## 什么是微调（Fine-tuning）？

微调是在预训练模型的基础上，使用特定领域的数据进行进一步训练，使模型更适合特定任务。

---

## 为什么需要微调？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        微调的必要性                                      │
└─────────────────────────────────────────────────────────────────────────┘

  通用大模型的局限：

  ┌─────────────────────────────────────────────────────────────────────┐
  │                                                                     │
  │  ❌ 不了解你的专业领域术语                                            │
  │  ❌ 不知道你公司的产品和流程                                          │
  │  ❌ 输出风格可能不符合要求                                            │
  │  ❌ 在特定任务上表现不够精确                                          │
  │                                                                     │
  └─────────────────────────────────────────────────────────────────────┘

  微调后：

  ┌─────────────────────────────────────────────────────────────────────┐
  │                                                                     │
  │  ✅ 掌握你的领域专业知识                                              │
  │  ✅ 了解特定的术语和表达                                              │
  │  ✅ 输出符合品牌风格                                                  │
  │  ✅ 在特定任务上表现出色                                              │
  │                                                                     │
  └─────────────────────────────────────────────────────────────────────┘
\`\`\`

---

## 微调的类型

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        微调方法对比                                      │
└─────────────────────────────────────────────────────────────────────────┘

  方法              训练参数      显存需求      效果       适用场景
  ────              ────────      ────────      ────       ────────
  全量微调           100%          极高         最好       资源充足
  LoRA              ~0.1%         低           很好       资源有限（推荐）
  QLoRA             ~0.1%         极低         较好       消费级显卡
  Adapter           ~1%           中等         好         多任务
  Prompt Tuning     ~0.01%        极低         一般       简单任务


  推荐学习路径：

  初学者 → LoRA（性价比最高）
  进阶 → QLoRA（更低显存）
  专业 → 全量微调（追求极致效果）
\`\`\`

---

## LoRA：低秩自适应

LoRA（Low-Rank Adaptation）是目前最流行的微调方法。

### 核心思想

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        LoRA 原理图解                                     │
└─────────────────────────────────────────────────────────────────────────┘

  原始权重矩阵 W（冻结，不更新）
  ┌────────────────────────────────────────┐
  │                                        │
  │           d × d 维                      │    比如 4096 × 4096
  │          （参数量巨大）                  │    = 1600 万参数
  │                                        │
  └────────────────────────────────────────┘

  LoRA 添加两个小矩阵 A 和 B：

  ┌────────┐   ┌────────┐
  │   B    │   │   A    │
  │ d × r  │ × │ r × d  │    r = 秩（通常 4~64）
  └────────┘   └────────┘
      ↓             ↓
    4096×8        8×4096        = 6.5 万参数（减少 99.6%！）

  最终输出 = W·x + B·A·x

  只训练 A 和 B，W 保持不变！
\`\`\`

### LoRA 的优势

| 优势 | 说明 |
|------|------|
| 参数高效 | 只需训练 0.1% 的参数 |
| 显存友好 | 可在消费级 GPU 上运行 |
| 效果接近 | 接近全量微调的效果 |
| 可插拔 | 可以随时加载/卸载 LoRA |
| 可组合 | 多个 LoRA 可以组合使用 |

---

## 微调数据准备

### 数据格式

\`\`\`json
// 指令微调格式（推荐）
{
  "instruction": "将以下英文翻译成中文",
  "input": "Hello, how are you?",
  "output": "你好，你怎么样？"
}

// 对话格式
{
  "conversations": [
    {"role": "user", "content": "什么是机器学习？"},
    {"role": "assistant", "content": "机器学习是..."}
  ]
}

// 补全格式
{
  "prompt": "写一首关于春天的诗：",
  "completion": "春风拂面暖阳照..."
}
\`\`\`

### 数据质量原则

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        高质量微调数据的特征                               │
└─────────────────────────────────────────────────────────────────────────┘

  ✅ 数量充足
     • 最少 100-1000 条
     • 复杂任务需要更多

  ✅ 质量高
     • 准确无误
     • 表达清晰
     • 格式一致

  ✅ 多样性
     • 覆盖各种情况
     • 包含边界案例
     • 长度分布合理

  ✅ 代表性
     • 反映真实使用场景
     • 符合目标任务需求
\`\`\`

---

## 微调工具推荐

### 1. Hugging Face Transformers + PEFT

\`\`\`bash
# 安装
pip install transformers peft datasets accelerate

# PEFT = Parameter-Efficient Fine-Tuning
# 支持 LoRA、QLoRA、Adapter 等多种方法
\`\`\`

### 2. LLaMA-Factory（推荐新手）

\`\`\`bash
# 开箱即用的微调框架
git clone https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -r requirements.txt

# 提供 Web UI，无需写代码
python src/train_web.py
\`\`\`

### 3. Axolotl

\`\`\`bash
# 配置驱动的微调框架
git clone https://github.com/OpenAccess-AI-Collective/axolotl.git
cd axolotl
pip install -e .

# 使用 YAML 配置文件
accelerate launch -m axolotl.cli.train config.yaml
\`\`\`

---

## 本节要点

1. **微调目的** —— 让通用模型适应特定领域/任务
2. **LoRA** —— 最推荐的微调方法，参数效率高
3. **数据准备** —— 质量比数量更重要
4. **工具选择** —— LLaMA-Factory 适合新手，PEFT 更灵活
5. **资源评估** —— 根据显存选择合适的微调方法
            `,ja:`
## ファインチューニングとは？

ファインチューニングは、事前学習済みモデルをベースに、特定ドメインのデータでさらに学習させ、特定タスクにより適したモデルにすることです。

---

## なぜファインチューニングが必要？

\`\`\`
汎用大規模モデルの限界：
❌ あなたの専門領域の用語を知らない
❌ 会社の製品やプロセスを知らない
❌ 出力スタイルが要件に合わない可能性
❌ 特定タスクで十分な精度がない

ファインチューニング後：
✅ ドメイン専門知識を習得
✅ 特定の用語と表現を理解
✅ ブランドスタイルに合った出力
✅ 特定タスクで優れたパフォーマンス
\`\`\`

---

## ファインチューニングの種類

| 方法 | 学習パラメータ | VRAM | 効果 | 適用場面 |
|------|-------------|------|------|---------|
| フルチューニング | 100% | 極高 | 最良 | リソース十分 |
| LoRA | ~0.1% | 低 | 良好 | リソース制限（推奨）|
| QLoRA | ~0.1% | 極低 | 良い | 消費者GPU |
| Prompt Tuning | ~0.01% | 極低 | 普通 | 簡単なタスク |

---

## LoRA：低ランク適応

LoRA（Low-Rank Adaptation）は現在最も人気のあるファインチューニング方法です。

### コアアイデア

\`\`\`
元の重み行列 W（凍結、更新しない）
┌────────────────────────────────┐
│        d × d 次元               │  例：4096 × 4096
│       （パラメータ数膨大）        │  = 1600万パラメータ
└────────────────────────────────┘

LoRAは2つの小さな行列 A と B を追加：

┌────────┐   ┌────────┐
│   B    │ × │   A    │    r = ランク（通常4~64）
│ d × r  │   │ r × d  │
└────────┘   └────────┘
  4096×8      8×4096     = 6.5万パラメータ（99.6%削減！）

A と B のみを学習、W は変更なし！
\`\`\`

---

## ファインチューニングツール推奨

### 1. LLaMA-Factory（初心者推奨）

\`\`\`bash
git clone https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -r requirements.txt

# Web UIを提供、コード不要
python src/train_web.py
\`\`\`

### 2. Hugging Face PEFT

\`\`\`bash
pip install transformers peft datasets accelerate
\`\`\`

---

## 本節のポイント

1. **ファインチューニングの目的** —— 汎用モデルを特定ドメイン/タスクに適応
2. **LoRA** —— 最も推奨される方法、パラメータ効率が高い
3. **データ準備** —— 量より質が重要
4. **ツール選択** —— LLaMA-Factoryは初心者向け、PEFTはより柔軟
            `}},{id:"ch1-lora-practice",title:{zh:"1.4 LoRA 微调实战",ja:"1.4 LoRA ファインチューニング実践"},content:{zh:`
## LoRA 微调实战指南

本节将手把手教你使用 LoRA 微调一个大模型。

---

## 环境准备

### 硬件要求

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        不同模型的显存需求                                 │
└─────────────────────────────────────────────────────────────────────────┘

  模型大小      LoRA 微调      QLoRA 微调     全量微调
  ────────      ─────────      ──────────     ────────
  7B            16 GB          8 GB           60+ GB
  13B           24 GB          12 GB          100+ GB
  70B           80+ GB         48 GB          500+ GB

  推荐配置：
  • 入门级：RTX 3090/4090（24GB）→ 7B-13B LoRA
  • 专业级：A100（40/80GB）→ 70B LoRA
  • 云端：使用 Colab Pro 或云 GPU
\`\`\`

### 安装依赖

\`\`\`bash
# 基础依赖
pip install torch transformers datasets
pip install peft accelerate bitsandbytes
pip install wandb  # 可选，用于训练监控

# 验证 CUDA
python -c "import torch; print(torch.cuda.is_available())"
\`\`\`

---

## 使用 PEFT 进行 LoRA 微调

### 完整代码示例

\`\`\`python
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import load_dataset

# 1. 加载基础模型和分词器
model_name = "meta-llama/Llama-2-7b-hf"
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

# 2. 配置 LoRA
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=8,                      # LoRA 秩，越大效果越好但参数越多
    lora_alpha=32,            # 缩放因子
    lora_dropout=0.1,         # Dropout 防止过拟合
    target_modules=[          # 要应用 LoRA 的模块
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
    ]
)

# 3. 创建 PEFT 模型
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
# 输出：trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06%

# 4. 准备数据集
def format_instruction(sample):
    return f"""### 指令：
{sample['instruction']}

### 输入：
{sample['input']}

### 回答：
{sample['output']}"""

def tokenize(sample):
    result = tokenizer(
        format_instruction(sample),
        truncation=True,
        max_length=512,
        padding="max_length"
    )
    result["labels"] = result["input_ids"].copy()
    return result

dataset = load_dataset("json", data_files="train_data.json")
tokenized_dataset = dataset.map(tokenize)

# 5. 配置训练参数
training_args = TrainingArguments(
    output_dir="./lora_output",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    fp16=True,
)

# 6. 开始训练
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    data_collator=DataCollatorForSeq2Seq(tokenizer, padding=True),
)

trainer.train()

# 7. 保存 LoRA 权重
model.save_pretrained("./lora_weights")
\`\`\`

---

## 使用 LLaMA-Factory（推荐新手）

### 通过 Web UI 微调

\`\`\`bash
# 1. 克隆仓库
git clone https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory

# 2. 安装依赖
pip install -r requirements.txt

# 3. 启动 Web UI
python src/train_web.py
\`\`\`

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    LLaMA-Factory Web UI                                  │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  模型选择: [Llama-2-7B ▼]                                               │
│                                                                         │
│  微调方法: ○ 全量  ● LoRA  ○ QLoRA                                      │
│                                                                         │
│  训练数据: [选择文件...]  train_data.json                                │
│                                                                         │
│  LoRA 秩:    [8    ]    学习率: [2e-4  ]                                │
│  训练轮数:   [3    ]    批次大小: [4   ]                                 │
│                                                                         │
│              [开始训练]    [导出模型]                                     │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
\`\`\`

### 通过命令行微调

\`\`\`bash
# 使用配置文件
CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\
    --stage sft \\
    --model_name_or_path meta-llama/Llama-2-7b-hf \\
    --do_train \\
    --dataset your_dataset \\
    --template llama2 \\
    --finetuning_type lora \\
    --lora_target q_proj,v_proj \\
    --output_dir ./lora_output \\
    --per_device_train_batch_size 4 \\
    --gradient_accumulation_steps 4 \\
    --lr_scheduler_type cosine \\
    --logging_steps 10 \\
    --save_steps 1000 \\
    --learning_rate 5e-5 \\
    --num_train_epochs 3.0 \\
    --fp16
\`\`\`

---

## QLoRA：更低显存的选择

QLoRA = 量化 + LoRA，可以在消费级显卡上微调大模型。

\`\`\`python
from transformers import BitsAndBytesConfig

# 4-bit 量化配置
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# 加载量化模型
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto"
)

# 然后正常应用 LoRA
model = get_peft_model(model, lora_config)

# 显存对比（7B 模型）：
# - FP16: ~14GB
# - 4-bit QLoRA: ~6GB
\`\`\`

---

## 微调后的模型使用

### 加载 LoRA 权重

\`\`\`python
from peft import PeftModel

# 加载基础模型
base_model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    torch_dtype=torch.float16,
    device_map="auto"
)

# 加载 LoRA 权重
model = PeftModel.from_pretrained(
    base_model,
    "./lora_weights"
)

# 推理
inputs = tokenizer("你好，请介绍一下你自己", return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=100)
print(tokenizer.decode(outputs[0]))
\`\`\`

### 合并 LoRA 权重

\`\`\`python
# 将 LoRA 权重合并到基础模型
merged_model = model.merge_and_unload()

# 保存合并后的完整模型
merged_model.save_pretrained("./merged_model")
tokenizer.save_pretrained("./merged_model")
\`\`\`

---

## 常见问题与解决

| 问题 | 可能原因 | 解决方案 |
|------|---------|---------|
| 显存不足 | 模型太大 | 使用 QLoRA 或减小 batch size |
| 训练不收敛 | 学习率不合适 | 尝试 1e-4 ~ 5e-5 |
| 过拟合 | 数据太少 | 增加数据或加大 dropout |
| 效果不好 | 数据质量差 | 清洗数据，提高质量 |
| 速度太慢 | 没有使用混合精度 | 开启 fp16/bf16 |

---

## 本节要点

1. **环境准备** —— 显存决定能微调多大的模型
2. **PEFT 库** —— 官方推荐的参数高效微调库
3. **LLaMA-Factory** —— 新手友好，提供 Web UI
4. **QLoRA** —— 显存不足时的最佳选择
5. **模型合并** —— 可以将 LoRA 权重合并到基础模型
            `,ja:`
## LoRAファインチューニング実践ガイド

このセクションでは、LoRAを使用して大規模モデルをファインチューニングする方法を手順を追って説明します。

---

## 環境準備

### ハードウェア要件

| モデルサイズ | LoRA | QLoRA | フルチューニング |
|------------|------|-------|---------------|
| 7B | 16 GB | 8 GB | 60+ GB |
| 13B | 24 GB | 12 GB | 100+ GB |
| 70B | 80+ GB | 48 GB | 500+ GB |

### 依存関係インストール

\`\`\`bash
pip install torch transformers datasets
pip install peft accelerate bitsandbytes
\`\`\`

---

## PEFTを使用したLoRAファインチューニング

\`\`\`python
from peft import LoraConfig, get_peft_model, TaskType

# LoRA設定
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=8,                      # LoRAランク
    lora_alpha=32,            # スケーリング係数
    lora_dropout=0.1,         # ドロップアウト
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)

# PEFTモデル作成
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
# 出力：trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06%
\`\`\`

---

## LLaMA-Factory使用（初心者推奨）

\`\`\`bash
# 1. リポジトリをクローン
git clone https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory

# 2. 依存関係インストール
pip install -r requirements.txt

# 3. Web UI起動
python src/train_web.py
\`\`\`

Web UIで簡単にファインチューニング設定が可能です。

---

## QLoRA：より低いVRAMでの選択

\`\`\`python
from transformers import BitsAndBytesConfig

# 4-bit量子化設定
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# VRAM比較（7Bモデル）：
# - FP16: ~14GB
# - 4-bit QLoRA: ~6GB
\`\`\`

---

## 本節のポイント

1. **環境準備** —— VRAMがファインチューニング可能なモデルサイズを決定
2. **PEFTライブラリ** —— 公式推奨のパラメータ効率的なファインチューニングライブラリ
3. **LLaMA-Factory** —— 初心者向け、Web UI提供
4. **QLoRA** —— VRAM不足時の最良の選択
            `}},{id:"ch1-case-study",title:{zh:"1.5 实战案例：客服机器人微调",ja:"1.5 実践例：カスタマーサービスボットのファインチューニング"},content:{zh:`
## 🎯 实战目标

微调一个专门回答电商客服问题的 AI 助手。

---

## 📊 案例背景

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                     电商客服机器人微调项目                                 │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  目标：让 AI 掌握公司产品知识，以专业客服语气回答问题                        │
│                                                                         │
│  基座模型：Qwen2-7B-Instruct（开源、中文能力强）                           │
│  微调方法：LoRA（显存友好）                                               │
│  训练数据：1000 条客服对话记录                                            │
│  硬件：RTX 4090 24GB                                                    │
│  预计时间：2-3 小时                                                      │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
\`\`\`

---

## 📝 Step 1: 准备训练数据

### 数据格式示例

\`\`\`json
{
  "conversations": [
    {
      "role": "system",
      "content": "你是XX电商的专业客服，态度友好、回答专业准确。"
    },
    {
      "role": "user",
      "content": "你们的退货政策是什么？"
    },
    {
      "role": "assistant",
      "content": "您好！我们提供7天无理由退货服务。商品需保持原包装完好，配件齐全。退货运费由我们承担，退款将在收到商品后3个工作日内原路返回。请问还有其他问题吗？"
    }
  ]
}
\`\`\`

### 数据采集来源

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        训练数据来源                                       │
└─────────────────────────────────────────────────────────────────────────┘

  1. 历史客服记录（脱敏处理）
     ├─ 筛选高质量对话
     ├─ 人工纠正错误回答
     └─ 统一格式

  2. 人工编写样本
     ├─ 覆盖常见问题
     ├─ 包含边界情况
     └─ 体现品牌调性

  3. GPT-4 辅助生成
     ├─ 提供场景描述
     ├─ 生成多样化问答
     └─ 人工审核筛选

  建议配比：真实数据 60% + 人工编写 25% + AI生成 15%
\`\`\`

### 数据质量检查清单

| 检查项 | 标准 | ✓/✗ |
|--------|------|-----|
| 格式正确 | JSON 可解析 | ✓ |
| 无空值 | 每个字段都有内容 | ✓ |
| 语气一致 | 符合客服专业调性 | ✓ |
| 信息准确 | 产品信息无误 | ✓ |
| 长度适中 | 回答 50-300 字 | ✓ |
| 覆盖全面 | 涵盖主要问题类型 | ✓ |

---

## 💻 Step 2: 使用 Unsloth 快速微调

### 为什么用 Unsloth？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    Unsloth vs 传统 PEFT 对比                             │
└─────────────────────────────────────────────────────────────────────────┘

  指标              Unsloth          传统 PEFT
  ────              ───────          ─────────
  训练速度           2-5x 更快        基准
  显存占用           减少 60%         基准
  安装难度           一行命令         需要配置
  支持模型           主流 LLM         更广泛

  结论：新手强烈推荐 Unsloth！
\`\`\`

### 安装 Unsloth

\`\`\`bash
# 一键安装（推荐）
pip install unsloth

# 或从源码安装（获取最新功能）
pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
\`\`\`

### 完整微调代码

\`\`\`python
from unsloth import FastLanguageModel
from trl import SFTTrainer
from transformers import TrainingArguments
from datasets import load_dataset
import torch

# ============================================
# 1. 加载模型（自动应用 Unsloth 优化）
# ============================================
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/Qwen2-7B-Instruct",  # 使用 Unsloth 优化版
    max_seq_length=2048,
    dtype=None,  # 自动检测
    load_in_4bit=True,  # 使用 4-bit 量化，显存友好
)

# ============================================
# 2. 添加 LoRA 适配器
# ============================================
model = FastLanguageModel.get_peft_model(
    model,
    r=16,                          # LoRA 秩，推荐 8-64
    target_modules=[               # 目标模块
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ],
    lora_alpha=16,                 # 缩放因子
    lora_dropout=0,                # Unsloth 优化，设为 0 更快
    bias="none",
    use_gradient_checkpointing="unsloth",  # 节省显存
)

print(f"可训练参数: {model.print_trainable_parameters()}")
# 输出: trainable params: 41,943,040 || all params: 7,657,616,384 || 0.55%

# ============================================
# 3. 准备数据集
# ============================================
def formatting_prompts(examples):
    """将对话格式化为模型输入"""
    texts = []
    for conv in examples["conversations"]:
        text = ""
        for msg in conv:
            if msg["role"] == "system":
                text += f"<|im_start|>system\\n{msg['content']}<|im_end|>\\n"
            elif msg["role"] == "user":
                text += f"<|im_start|>user\\n{msg['content']}<|im_end|>\\n"
            elif msg["role"] == "assistant":
                text += f"<|im_start|>assistant\\n{msg['content']}<|im_end|>\\n"
        texts.append(text)
    return {"text": texts}

# 加载数据
dataset = load_dataset("json", data_files="customer_service_data.json")
dataset = dataset.map(formatting_prompts, batched=True)

# ============================================
# 4. 配置训练参数
# ============================================
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset["train"],
    dataset_text_field="text",
    max_seq_length=2048,
    args=TrainingArguments(
        output_dir="./customer_service_lora",
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        warmup_steps=10,
        max_steps=500,               # 1000条数据约500步
        learning_rate=2e-4,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=10,
        save_steps=100,
        optim="adamw_8bit",          # 8-bit 优化器，节省显存
    ),
)

# ============================================
# 5. 开始训练！
# ============================================
print("开始训练...")
trainer.train()

# ============================================
# 6. 保存模型
# ============================================
model.save_pretrained("./customer_service_lora")
tokenizer.save_pretrained("./customer_service_lora")

print("✅ 训练完成！模型已保存。")
\`\`\`

---

## 📈 Step 3: 训练监控与调优

### 观察训练 Loss

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        训练 Loss 曲线解读                                 │
└─────────────────────────────────────────────────────────────────────────┘

  Loss
   3.0 ┤
       │╲
   2.5 ┤ ╲
       │  ╲
   2.0 ┤   ╲
       │    ╲──────
   1.5 ┤           ╲
       │            ╲
   1.0 ┤             ╲______________________
       │
   0.5 ┤
       └────────────────────────────────────────
        0   100   200   300   400   500  Step

  ✅ 健康曲线：快速下降后趋于平稳
  ⚠️ 不收敛：Loss 不下降 → 调大学习率
  ⚠️ 震荡大：上下波动剧烈 → 调小学习率
  ⚠️ 过拟合：训练 Loss 极低但验证 Loss 上升 → 减少训练步数
\`\`\`

### 超参数调优建议

| 场景 | 调整建议 |
|------|---------|
| Loss 不下降 | 增大 learning_rate 到 5e-4 |
| Loss 震荡 | 减小 learning_rate 到 1e-4 |
| 显存不足 | 减小 batch_size，增大 gradient_accumulation |
| 效果不好 | 增大 r 值（如 32 或 64） |
| 过拟合 | 减少 max_steps，增加数据 |

---

## 🧪 Step 4: 效果评估

### 自动评估

\`\`\`python
from transformers import pipeline

# 加载微调后的模型
pipe = pipeline("text-generation", model="./customer_service_lora")

# 测试用例
test_cases = [
    "你们支持花呗付款吗？",
    "我的订单还没发货，能催一下吗？",
    "商品有质量问题怎么办？",
    "能开发票吗？",
]

print("=" * 60)
print("模型效果测试")
print("=" * 60)

for question in test_cases:
    prompt = f"<|im_start|>system\\n你是XX电商的专业客服。<|im_end|>\\n<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant\\n"

    response = pipe(prompt, max_new_tokens=200, temperature=0.7)
    answer = response[0]['generated_text'].split("<|im_start|>assistant\\n")[-1]

    print(f"\\n用户: {question}")
    print(f"客服: {answer}")
\`\`\`

### 人工评估标准

| 维度 | 1分 | 3分 | 5分 |
|------|-----|-----|-----|
| 准确性 | 信息错误 | 基本正确 | 完全准确 |
| 专业度 | 口语化 | 一般 | 专业术语恰当 |
| 态度 | 生硬 | 普通 | 热情友好 |
| 完整性 | 答非所问 | 部分解答 | 完整全面 |

---

## 💰 成本估算

### 云 GPU 价格参考（2025年）

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        云 GPU 价格对比                                    │
└─────────────────────────────────────────────────────────────────────────┘

  平台              GPU              价格（$/小时）   适合模型
  ────              ───              ──────────────   ────────
  Lambda Labs       RTX 4090         $0.75            7B LoRA
  Vast.ai           RTX 4090         $0.50-0.80       7B LoRA
  RunPod            RTX 4090         $0.69            7B LoRA
  AWS               A10G             $1.00            7B-13B
  Google Cloud      A100 40GB        $3.67            13B-70B
  AutoDL (国内)     RTX 4090         ¥2.5/小时        7B LoRA

  本案例预估成本：
  ┌────────────────────────────────────────────┐
  │  RTX 4090 × 3小时 ≈ $2.25（约 ¥16）        │
  │  性价比极高！                              │
  └────────────────────────────────────────────┘
\`\`\`

---

## 🚀 Step 5: 部署上线

### 使用 Ollama 本地部署

\`\`\`bash
# 1. 安装 Ollama
curl -fsSL https://ollama.com/install.sh | sh

# 2. 创建 Modelfile
cat > Modelfile << 'EOF'
FROM ./customer_service_merged

SYSTEM "你是XX电商的专业客服，态度友好、回答专业准确。"

PARAMETER temperature 0.7
PARAMETER top_p 0.9
EOF

# 3. 创建 Ollama 模型
ollama create customer-service -f Modelfile

# 4. 运行
ollama run customer-service
\`\`\`

### 使用 vLLM 高性能部署

\`\`\`bash
# 安装 vLLM
pip install vllm

# 启动 API 服务
python -m vllm.entrypoints.openai.api_server \\
    --model ./customer_service_merged \\
    --host 0.0.0.0 \\
    --port 8000

# 调用 API
curl http://localhost:8000/v1/chat/completions \\
    -H "Content-Type: application/json" \\
    -d '{
        "model": "customer_service",
        "messages": [{"role": "user", "content": "退货怎么操作？"}]
    }'
\`\`\`

---

## ✅ 本节要点

1. **数据准备** —— 质量 > 数量，建议 1000+ 条
2. **Unsloth** —— 速度快 2-5x，显存省 60%
3. **训练监控** —— 观察 Loss 曲线，及时调参
4. **效果评估** —— 自动 + 人工双重验证
5. **成本控制** —— 云 GPU 微调成本低至 ¥20
6. **部署上线** —— Ollama（简单）或 vLLM（高性能）
            `,ja:`
## 🎯 実践目標

ECサイトのカスタマーサービス問い合わせに特化したAIアシスタントをファインチューニング。

---

## 📊 ケース背景

\`\`\`
目標：AIに自社製品知識を習得させ、プロの接客トーンで回答
ベースモデル：Qwen2-7B-Instruct
ファインチューニング方法：LoRA
トレーニングデータ：1000件の接客対話
ハードウェア：RTX 4090 24GB
予想時間：2-3時間
\`\`\`

---

## 💻 Unslothで高速ファインチューニング

### なぜUnsloth？

| 指標 | Unsloth | 従来のPEFT |
|------|---------|-----------|
| 学習速度 | 2-5倍高速 | 基準 |
| VRAM使用量 | 60%削減 | 基準 |
| インストール | 1行 | 設定必要 |

### インストール

\`\`\`bash
pip install unsloth
\`\`\`

### 完全なコード例

\`\`\`python
from unsloth import FastLanguageModel
from trl import SFTTrainer
from transformers import TrainingArguments

# モデル読み込み
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/Qwen2-7B-Instruct",
    max_seq_length=2048,
    load_in_4bit=True,
)

# LoRAアダプター追加
model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_alpha=16,
)

# トレーナー設定
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    args=TrainingArguments(
        output_dir="./lora_output",
        per_device_train_batch_size=2,
        max_steps=500,
        learning_rate=2e-4,
    ),
)

# 学習開始
trainer.train()
\`\`\`

---

## 💰 コスト見積もり

| プラットフォーム | GPU | 価格/時間 |
|----------------|-----|----------|
| Lambda Labs | RTX 4090 | $0.75 |
| RunPod | RTX 4090 | $0.69 |
| AWS | A10G | $1.00 |

本ケースの推定コスト：約$2-3（3時間）

---

## 🚀 デプロイ

### Ollamaでローカルデプロイ

\`\`\`bash
# インストール
curl -fsSL https://ollama.com/install.sh | sh

# モデル作成
ollama create customer-service -f Modelfile

# 実行
ollama run customer-service
\`\`\`

---

## ✅ 本節のポイント

1. **データ準備** —— 質 > 量、1000件以上推奨
2. **Unsloth** —— 2-5倍高速、60% VRAM削減
3. **コスト管理** —— クラウドGPUで約$2-3
4. **デプロイ** —— Ollama（簡単）またはvLLM（高性能）
            `}},{id:"ch1-local-deploy",title:{zh:"1.6 本地部署与 Ollama",ja:"1.6 ローカルデプロイとOllama"},content:{zh:`
## 为什么要本地部署？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                     本地部署 vs 云端 API                                  │
└─────────────────────────────────────────────────────────────────────────┘

                    本地部署                    云端 API
                    ────────                    ────────
  隐私性            ✅ 数据不离开本地            ❌ 数据发送到云端
  成本              ✅ 一次投入长期免费           ❌ 按调用付费
  速度              ✅ 无网络延迟                ❌ 依赖网络
  定制性            ✅ 可以微调                  ⚠️ 部分支持
  模型选择          ⚠️ 受硬件限制               ✅ 最强模型

  适用场景：
  • 隐私敏感数据处理
  • 离线环境使用
  • 高频调用节省成本
  • 学习研究目的
\`\`\`

---

## Ollama：最简单的本地部署方案

### 安装 Ollama

\`\`\`bash
# macOS / Linux
curl -fsSL https://ollama.com/install.sh | sh

# Windows
# 下载安装包：https://ollama.com/download/windows
\`\`\`

### 运行开源模型

\`\`\`bash
# 运行 Llama 3.1 8B（推荐入门）
ollama run llama3.1

# 运行 Qwen2 7B（中文能力强）
ollama run qwen2

# 运行 CodeLlama（代码生成）
ollama run codellama

# 运行 DeepSeek Coder（代码能力强）
ollama run deepseek-coder:6.7b
\`\`\`

### 常用模型推荐

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                     Ollama 热门模型                                      │
└─────────────────────────────────────────────────────────────────────────┘

  模型名称              大小        特点                  显存需求
  ────────              ────        ────                  ────────
  llama3.1:8b           4.7GB       通用能力强             8GB+
  qwen2:7b              4.4GB       中文能力最强           8GB+
  mistral:7b            4.1GB       速度快、效果好         8GB+
  codellama:7b          3.8GB       代码生成专用           8GB+
  deepseek-coder:6.7b   3.8GB       代码能力强             8GB+
  phi3:3.8b             2.2GB       小巧但能力强           4GB+
  gemma2:2b             1.6GB       超小模型入门           4GB+

  选择建议：
  • 8GB 显存：选 7B 以下模型
  • 16GB 显存：可以跑 13B 模型
  • 24GB 显存：可以跑量化的 70B 模型
\`\`\`

### Ollama 命令速查

\`\`\`bash
# 查看已下载模型
ollama list

# 下载模型（不运行）
ollama pull llama3.1

# 删除模型
ollama rm llama3.1

# 查看模型信息
ollama show llama3.1

# 复制模型
ollama cp llama3.1 my-llama

# 启动 API 服务（默认 11434 端口）
ollama serve

# API 调用示例
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.1",
  "prompt": "为什么天空是蓝色的？",
  "stream": false
}'
\`\`\`

---

## 使用自定义模型

### 导入 GGUF 模型

\`\`\`bash
# 1. 下载 GGUF 格式模型（从 Hugging Face）
# 例如：TheBloke/Llama-2-7B-GGUF

# 2. 创建 Modelfile
cat > Modelfile << 'EOF'
FROM ./llama-2-7b.Q4_K_M.gguf

# 设置系统提示词
SYSTEM "你是一个专业的AI助手，回答问题时要准确、简洁。"

# 设置参数
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER num_ctx 4096
EOF

# 3. 创建 Ollama 模型
ollama create my-llama -f Modelfile

# 4. 运行
ollama run my-llama
\`\`\`

### 导入微调后的模型

\`\`\`bash
# 1. 合并 LoRA 权重（如果还没合并）
python -c "
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer

base = AutoModelForCausalLM.from_pretrained('Qwen/Qwen2-7B-Instruct')
model = PeftModel.from_pretrained(base, './my_lora')
merged = model.merge_and_unload()
merged.save_pretrained('./merged_model')
"

# 2. 转换为 GGUF 格式
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp

python convert_hf_to_gguf.py ../merged_model --outtype q4_k_m

# 3. 创建 Ollama 模型
ollama create my-finetuned -f Modelfile
\`\`\`

---

## 配合 Open WebUI 使用

\`\`\`bash
# 使用 Docker 一键部署
docker run -d -p 3000:8080 \\
  --add-host=host.docker.internal:host-gateway \\
  -v open-webui:/app/backend/data \\
  --name open-webui \\
  ghcr.io/open-webui/open-webui:main

# 访问 http://localhost:3000
# 自动连接本地 Ollama
\`\`\`

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                     Open WebUI 界面                                      │
├─────────────────────────────────────────────────────────────────────────┤
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  🤖 Select a model: [llama3.1 ▼]                                │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  User: 帮我写一首关于春天的诗                                     │   │
│  │  ────────────────────────────────────────────────────────────   │   │
│  │  Assistant: 春风拂柳绿意浓，                                      │   │
│  │            花开满园香飘送。                                       │   │
│  │            蝴蝶翩翩舞枝头，                                       │   │
│  │            燕子归来筑新梦。                                       │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  Type a message...                                    [Send]    │   │
│  └─────────────────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────────────────┘
\`\`\`

---

## 性能优化技巧

### GPU 加速

\`\`\`bash
# 确保 NVIDIA 驱动已安装
nvidia-smi

# Ollama 会自动检测并使用 GPU
# 如需指定 GPU：
CUDA_VISIBLE_DEVICES=0 ollama run llama3.1

# 查看 GPU 使用情况
watch -n 1 nvidia-smi
\`\`\`

### 模型量化选择

| 量化等级 | 大小 | 质量 | 速度 | 推荐场景 |
|---------|------|------|------|---------|
| Q8_0 | 100% | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | 质量优先 |
| Q6_K | 75% | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | 平衡 |
| Q4_K_M | 50% | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | **推荐** |
| Q4_0 | 45% | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ | 速度优先 |
| Q2_K | 30% | ⭐⭐ | ⭐⭐⭐⭐⭐ | 极限压缩 |

---

## 本节要点

1. **Ollama** —— 最简单的本地 LLM 部署方案
2. **模型选择** —— 根据显存选择合适大小的模型
3. **自定义模型** —— 支持导入 GGUF 格式和微调模型
4. **Open WebUI** —— 提供 ChatGPT 风格的 Web 界面
5. **性能优化** —— 使用 Q4_K_M 量化获得最佳性价比
            `,ja:`
## なぜローカルデプロイ？

| 項目 | ローカル | クラウドAPI |
|------|---------|------------|
| プライバシー | ✅ データはローカルに | ❌ クラウドに送信 |
| コスト | ✅ 一度の投資で長期無料 | ❌ 従量課金 |
| 速度 | ✅ ネットワーク遅延なし | ❌ ネットワーク依存 |

---

## Ollama：最もシンプルなローカルデプロイ

### インストール

\`\`\`bash
# macOS / Linux
curl -fsSL https://ollama.com/install.sh | sh
\`\`\`

### モデル実行

\`\`\`bash
# Llama 3.1 8B実行
ollama run llama3.1

# Qwen2 7B（中国語に強い）
ollama run qwen2
\`\`\`

### おすすめモデル

| モデル | サイズ | 特徴 | VRAM |
|--------|-------|------|------|
| llama3.1:8b | 4.7GB | 汎用性高い | 8GB+ |
| qwen2:7b | 4.4GB | 中国語最強 | 8GB+ |
| phi3:3.8b | 2.2GB | 小型で高性能 | 4GB+ |

---

## カスタムモデル

\`\`\`bash
# GGUFモデルをインポート
cat > Modelfile << 'EOF'
FROM ./model.gguf
SYSTEM "あなたはプロのAIアシスタントです。"
EOF

ollama create my-model -f Modelfile
\`\`\`

---

## Open WebUI

\`\`\`bash
# Docker で一発デプロイ
docker run -d -p 3000:8080 \\
  --add-host=host.docker.internal:host-gateway \\
  ghcr.io/open-webui/open-webui:main

# http://localhost:3000 でアクセス
\`\`\`

---

## 本節のポイント

1. **Ollama** —— 最もシンプルなローカルLLMデプロイ
2. **モデル選択** —— VRAMに応じて適切なサイズを選択
3. **Open WebUI** —— ChatGPT風のWebインターフェース
            `}},{id:"ch1-quantization",title:{zh:"1.7 模型量化压缩",ja:"1.7 モデル量子化圧縮"},content:{zh:`
## 让大模型"瘦身"：量化技术详解

当你想在消费级显卡上运行 70B 大模型时，量化就是你的救星。

---

## 🎯 什么是量化？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        模型量化原理                                       │
└─────────────────────────────────────────────────────────────────────────┘

  原始模型 (FP32)                    量化后模型 (INT4)
  ┌────────────────┐                ┌────────────────┐
  │ 参数: 3.14159  │   ───量化───▶   │ 参数: 3        │
  │ 占用: 32 bits  │                │ 占用: 4 bits   │
  │ 精度: 最高     │                │ 精度: 略降     │
  └────────────────┘                └────────────────┘
         │                                  │
         ▼                                  ▼
   70B 模型 ≈ 140GB                  70B 模型 ≈ 35GB
   需要 A100 80G×2                   可用 RTX 4090 24G
\`\`\`

**核心思想**：用更少的 bit 数存储参数，牺牲少量精度换取巨大的内存节省。

---

## 📊 主流量化方法对比

| 方法 | 精度 | 速度 | VRAM占用 | 适用场景 |
|------|------|------|----------|----------|
| **FP32** | ⭐⭐⭐⭐⭐ | ⭐⭐ | 100% | 训练基准 |
| **FP16** | ⭐⭐⭐⭐ | ⭐⭐⭐ | 50% | 推理标准 |
| **INT8** | ⭐⭐⭐ | ⭐⭐⭐⭐ | 25% | 部署优化 |
| **INT4** | ⭐⭐ | ⭐⭐⭐⭐⭐ | 12.5% | 极限压缩 |

---

## 🔧 主流量化格式详解

### 1️⃣ GGUF (原 GGML)

**最流行的 CPU/GPU 混合推理格式**，由 llama.cpp 开发者创建。

\`\`\`bash
# 使用 llama.cpp 运行 GGUF 模型
./llama-cli -m llama-3-8b-Q4_K_M.gguf -p "你好" -n 100

# 常见量化级别
# Q2_K  - 极限压缩，质量损失大
# Q4_K_M - 推荐！平衡压缩和质量
# Q5_K_M - 高质量，稍大
# Q8_0  - 几乎无损
\`\`\`

**GGUF 量化级别选择指南**：

| 量化级别 | 7B 模型大小 | 质量 | 推荐用途 |
|----------|-------------|------|----------|
| Q2_K | ~2.5GB | ⭐⭐ | 极限内存场景 |
| Q4_K_M | ~4GB | ⭐⭐⭐⭐ | **日常使用首选** |
| Q5_K_M | ~5GB | ⭐⭐⭐⭐⭐ | 质量优先 |
| Q8_0 | ~7GB | ⭐⭐⭐⭐⭐ | 几乎无损 |

---

### 2️⃣ GPTQ

**GPU 专用量化**，推理速度快，需要 GPU。

\`\`\`python
from transformers import AutoModelForCausalLM, AutoTokenizer

# 加载 GPTQ 量化模型
model_id = "TheBloke/Llama-2-7B-GPTQ"
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    trust_remote_code=True
)
tokenizer = AutoTokenizer.from_pretrained(model_id)

# 正常使用
output = model.generate(
    tokenizer("Hello", return_tensors="pt").input_ids.cuda(),
    max_new_tokens=50
)
\`\`\`

---

### 3️⃣ AWQ (Activation-aware Weight Quantization)

**更智能的量化**：根据激活值重要性决定量化精度。

\`\`\`python
from awq import AutoAWQForCausalLM
from transformers import AutoTokenizer

# 加载 AWQ 模型
model_id = "TheBloke/Llama-2-7B-AWQ"
model = AutoAWQForCausalLM.from_quantized(
    model_id,
    fuse_layers=True,  # 融合层加速
    device_map="auto"
)

# AWQ 优势：
# - 比 GPTQ 更好的精度保持
# - 更快的推理速度
# - 更低的内存占用
\`\`\`

---

### 4️⃣ bitsandbytes (QLoRA)

**训练和推理都能用**，HuggingFace 官方支持。

\`\`\`python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 4-bit 量化配置
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",        # NormalFloat4
    bnb_4bit_compute_dtype="float16",
    bnb_4bit_use_double_quant=True    # 双重量化
)

# 加载模型
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3-8B",
    quantization_config=bnb_config,
    device_map="auto"
)

# 8-bit 量化 (更简单)
model_8bit = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3-8B",
    load_in_8bit=True,
    device_map="auto"
)
\`\`\`

---

## 🎮 实战：不同显存的最佳选择

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    显存 vs 可运行模型                                     │
└─────────────────────────────────────────────────────────────────────────┘

  显存容量          推荐模型 & 量化方案
  ─────────────────────────────────────────────────────

  4GB  (GTX 1650)   → Qwen2-1.5B (FP16) 或 7B (Q2_K)
       │
       ▼
  8GB  (RTX 3060)   → Llama3-8B (Q4_K_M) 或 Qwen2-7B (Q5_K_M)
       │
       ▼
  12GB (RTX 3060Ti) → Llama3-8B (Q8_0) 或 13B (Q4_K_M)
       │
       ▼
  24GB (RTX 4090)   → Llama3-70B (Q4_K_M) 或 Qwen2-72B (Q4_K_M)
       │
       ▼
  48GB (A6000×2)    → Llama3-70B (FP16) 或 更大模型
\`\`\`

---

## 🛠️ 自己动手量化

### 使用 llama.cpp 量化

\`\`\`bash
# 1. 克隆 llama.cpp
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp && make -j

# 2. 转换 HuggingFace 模型到 GGUF
python convert_hf_to_gguf.py /path/to/model --outfile model.gguf

# 3. 量化
./llama-quantize model.gguf model-Q4_K_M.gguf Q4_K_M
\`\`\`

### 使用 AutoGPTQ 量化

\`\`\`python
from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig

# 配置量化参数
quantize_config = BaseQuantizeConfig(
    bits=4,
    group_size=128,
    damp_percent=0.1,
    desc_act=False
)

# 加载模型并量化
model = AutoGPTQForCausalLM.from_pretrained(
    "meta-llama/Llama-3-8B",
    quantize_config
)

# 准备校准数据
examples = [tokenizer(text, return_tensors="pt") for text in calibration_texts]

# 执行量化
model.quantize(examples)

# 保存
model.save_quantized("llama3-8b-gptq-4bit")
\`\`\`

---

## ⚖️ 如何选择量化方案？

\`\`\`
                        选择决策树
                            │
              ┌─────────────┴─────────────┐
              │      你有 GPU 吗？         │
              └─────────────┬─────────────┘
                   ╱                ╲
                 是                  否
                 │                   │
        ┌────────┴────────┐         │
        │  推理还是训练？  │         ▼
        └────────┬────────┘      GGUF
             ╱         ╲        (CPU推理)
           推理        训练
            │           │
     ┌──────┴──────┐    ▼
     │ 追求速度？  │  bitsandbytes
     └──────┬──────┘  (QLoRA训练)
        ╱        ╲
      是          否
       │           │
       ▼           ▼
     AWQ        GPTQ
  (最快推理)   (兼容性好)
\`\`\`

---

## 💡 小贴士

> 🎯 **推荐组合**：
> - 本地部署：Ollama + GGUF Q4_K_M
> - GPU 推理：vLLM + AWQ
> - 微调训练：bitsandbytes + QLoRA
> - 生产环境：TensorRT-LLM + INT8
            `,ja:`
## 大規模モデルを「スリム化」：量子化技術詳解

消費者向けGPUで70Bモデルを動かしたい時、量子化があなたの救世主です。

---

## 🎯 量子化とは？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        モデル量子化の原理                                  │
└─────────────────────────────────────────────────────────────────────────┘

  元のモデル (FP32)                  量子化後 (INT4)
  ┌────────────────┐                ┌────────────────┐
  │ パラメータ: 3.14159 │  ──量子化──▶  │ パラメータ: 3   │
  │ 容量: 32 bits  │                │ 容量: 4 bits   │
  │ 精度: 最高     │                │ 精度: やや低下  │
  └────────────────┘                └────────────────┘
         │                                  │
         ▼                                  ▼
   70B モデル ≈ 140GB                70B モデル ≈ 35GB
   A100 80G×2 が必要                 RTX 4090 24G で動作
\`\`\`

**核心思想**：少ないbit数でパラメータを保存し、わずかな精度を犠牲に大きなメモリ節約を実現。

---

## 📊 主要な量子化手法の比較

| 手法 | 精度 | 速度 | VRAM使用 | 適用シーン |
|------|------|------|----------|------------|
| **FP32** | ⭐⭐⭐⭐⭐ | ⭐⭐ | 100% | 学習基準 |
| **FP16** | ⭐⭐⭐⭐ | ⭐⭐⭐ | 50% | 推論標準 |
| **INT8** | ⭐⭐⭐ | ⭐⭐⭐⭐ | 25% | デプロイ最適化 |
| **INT4** | ⭐⭐ | ⭐⭐⭐⭐⭐ | 12.5% | 極限圧縮 |

---

## 🔧 主要な量子化フォーマット詳解

### 1️⃣ GGUF（旧 GGML）

**最も人気のCPU/GPUハイブリッド推論形式**、llama.cpp開発者が作成。

\`\`\`bash
# llama.cpp で GGUF モデルを実行
./llama-cli -m llama-3-8b-Q4_K_M.gguf -p "こんにちは" -n 100

# 一般的な量子化レベル
# Q2_K  - 極限圧縮、品質低下大
# Q4_K_M - おすすめ！圧縮と品質のバランス
# Q5_K_M - 高品質、やや大きい
# Q8_0  - ほぼ無損失
\`\`\`

**GGUF 量子化レベル選択ガイド**：

| 量子化レベル | 7B モデルサイズ | 品質 | 推奨用途 |
|--------------|-----------------|------|----------|
| Q2_K | ~2.5GB | ⭐⭐ | 極限メモリ制限 |
| Q4_K_M | ~4GB | ⭐⭐⭐⭐ | **日常使用に最適** |
| Q5_K_M | ~5GB | ⭐⭐⭐⭐⭐ | 品質優先 |
| Q8_0 | ~7GB | ⭐⭐⭐⭐⭐ | ほぼ無損失 |

---

### 2️⃣ GPTQ

**GPU専用量子化**、高速推論、GPUが必要。

\`\`\`python
from transformers import AutoModelForCausalLM, AutoTokenizer

# GPTQ量子化モデルをロード
model_id = "TheBloke/Llama-2-7B-GPTQ"
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    trust_remote_code=True
)
tokenizer = AutoTokenizer.from_pretrained(model_id)

# 通常通り使用
output = model.generate(
    tokenizer("Hello", return_tensors="pt").input_ids.cuda(),
    max_new_tokens=50
)
\`\`\`

---

### 3️⃣ AWQ（Activation-aware Weight Quantization）

**よりスマートな量子化**：活性化値の重要性に基づいて量子化精度を決定。

\`\`\`python
from awq import AutoAWQForCausalLM
from transformers import AutoTokenizer

# AWQモデルをロード
model_id = "TheBloke/Llama-2-7B-AWQ"
model = AutoAWQForCausalLM.from_quantized(
    model_id,
    fuse_layers=True,  # レイヤー融合で高速化
    device_map="auto"
)

# AWQの利点：
# - GPTQより良い精度維持
# - より速い推論速度
# - より少ないメモリ使用
\`\`\`

---

### 4️⃣ bitsandbytes（QLoRA）

**学習と推論両方で使用可能**、HuggingFace公式サポート。

\`\`\`python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 4-bit量子化設定
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",        # NormalFloat4
    bnb_4bit_compute_dtype="float16",
    bnb_4bit_use_double_quant=True    # 二重量子化
)

# モデルをロード
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3-8B",
    quantization_config=bnb_config,
    device_map="auto"
)
\`\`\`

---

## 🎮 実践：VRAM別の最適な選択

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    VRAM vs 実行可能モデル                                 │
└─────────────────────────────────────────────────────────────────────────┘

  VRAM容量           推奨モデル & 量子化方式
  ─────────────────────────────────────────────────────

  4GB  (GTX 1650)   → Qwen2-1.5B (FP16) または 7B (Q2_K)
       │
       ▼
  8GB  (RTX 3060)   → Llama3-8B (Q4_K_M) または Qwen2-7B (Q5_K_M)
       │
       ▼
  24GB (RTX 4090)   → Llama3-70B (Q4_K_M) または Qwen2-72B (Q4_K_M)
\`\`\`

---

## 💡 ヒント

> 🎯 **おすすめの組み合わせ**：
> - ローカルデプロイ：Ollama + GGUF Q4_K_M
> - GPU推論：vLLM + AWQ
> - ファインチューニング：bitsandbytes + QLoRA
> - 本番環境：TensorRT-LLM + INT8
            `}},{id:"ch1-summary",title:{zh:"1.8 本章小结",ja:"1.8 この章のまとめ"},content:{zh:`
## 大模型技术核心回顾

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    本章知识地图                                          │
└─────────────────────────────────────────────────────────────────────────┘

                        大模型技术深度解析
                              │
          ┌───────────────────┼───────────────────┐
          ▼                   ▼                   ▼
     Transformer           注意力机制            微调技术
          │                   │                   │
     ┌────┴────┐         ┌────┴────┐         ┌────┴────┐
     │ Encoder │         │  Q/K/V  │         │  LoRA   │
     │ Decoder │         │ 多头注意力│         │  QLoRA  │
     │ 位置编码 │         │ Mask注意力│         │ 数据准备 │
     └─────────┘         └─────────┘         └─────────┘
\`\`\`

---

## 关键知识点总结

### Transformer 架构

| 组件 | 作用 | 重要性 |
|------|------|--------|
| Self-Attention | 建模全局依赖 | ⭐⭐⭐⭐⭐ |
| Feed Forward | 特征变换 | ⭐⭐⭐⭐ |
| 位置编码 | 提供位置信息 | ⭐⭐⭐⭐ |
| Layer Norm | 稳定训练 | ⭐⭐⭐ |

### 注意力机制

| 类型 | 特点 | 应用 |
|------|------|------|
| Self-Attention | 序列内部关系 | 所有 Transformer |
| Cross-Attention | 序列间关系 | 翻译、问答 |
| Multi-Head | 多视角 | 增强表达能力 |
| Masked | 防止信息泄露 | 语言生成 |

### 微调方法

| 方法 | 参数量 | 显存 | 推荐度 |
|------|--------|------|--------|
| LoRA | ~0.1% | 低 | ⭐⭐⭐⭐⭐ |
| QLoRA | ~0.1% | 极低 | ⭐⭐⭐⭐ |
| 全量微调 | 100% | 极高 | ⭐⭐⭐ |

---

## 实践路线图

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    学习路线建议                                          │
└─────────────────────────────────────────────────────────────────────────┘

  第一步：理解原理
  ─────────────────
  • 阅读 "Attention Is All You Need" 论文
  • 理解 Self-Attention 计算过程
  • 了解 Encoder/Decoder 区别

  第二步：动手实践
  ─────────────────
  • 使用 LLaMA-Factory 完成第一次微调
  • 准备自己的数据集
  • 尝试不同的 LoRA 参数

  第三步：深入优化
  ─────────────────
  • 学习数据清洗和增强
  • 尝试 QLoRA 降低成本
  • 探索模型合并技术

  第四步：生产部署
  ─────────────────
  • 学习模型量化
  • 了解推理优化
  • 部署到生产环境
\`\`\`

---

## 推荐资源

### 必读论文

| 论文 | 内容 | 链接 |
|------|------|------|
| Attention Is All You Need | Transformer 原论文 | arXiv:1706.03762 |
| BERT | Encoder 代表作 | arXiv:1810.04805 |
| GPT 系列 | Decoder 代表作 | OpenAI Blog |
| LoRA | 低秩微调 | arXiv:2106.09685 |

### 实践工具

| 工具 | 用途 | 难度 |
|------|------|------|
| LLaMA-Factory | 一站式微调 | ⭐⭐ |
| PEFT | 官方参数高效库 | ⭐⭐⭐ |
| Axolotl | 配置驱动微调 | ⭐⭐⭐ |
| vLLM | 高效推理 | ⭐⭐⭐⭐ |

---

## 写在最后

理解大模型的技术原理，不是为了成为 AI 研究员，而是为了：

1. **更好地使用 AI** —— 知其然，知其所以然
2. **做出明智的选型** —— 根据需求选择合适的模型和方法
3. **定制专属 AI** —— 掌握微调能力，构建领域专属模型

技术在不断发展，但核心原理是稳定的。掌握了这些基础，你就能更好地跟上 AI 的发展步伐！

*"理解原理，才能真正驾驭工具。"*
            `,ja:`
## 大規模モデル技術コア復習

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    この章の知識マップ                                     │
└─────────────────────────────────────────────────────────────────────────┘

                        大規模モデル技術詳解
                              │
          ┌───────────────────┼───────────────────┐
          ▼                   ▼                   ▼
     Transformer           アテンション           ファインチューニング
          │                   │                   │
     ┌────┴────┐         ┌────┴────┐         ┌────┴────┐
     │ Encoder │         │  Q/K/V  │         │  LoRA   │
     │ Decoder │         │マルチヘッド│         │  QLoRA  │
     │位置エンコード│        │ Maskアテンション│       │ データ準備 │
     └─────────┘         └─────────┘         └─────────┘
\`\`\`

---

## 重要ポイントまとめ

### Transformerアーキテクチャ

| コンポーネント | 役割 | 重要度 |
|-------------|------|--------|
| Self-Attention | グローバル依存性モデリング | ⭐⭐⭐⭐⭐ |
| Feed Forward | 特徴変換 | ⭐⭐⭐⭐ |
| 位置エンコーディング | 位置情報提供 | ⭐⭐⭐⭐ |

### ファインチューニング方法

| 方法 | パラメータ量 | VRAM | 推奨度 |
|------|-----------|------|--------|
| LoRA | ~0.1% | 低 | ⭐⭐⭐⭐⭐ |
| QLoRA | ~0.1% | 極低 | ⭐⭐⭐⭐ |
| フルチューニング | 100% | 極高 | ⭐⭐⭐ |

---

## 実践ロードマップ

1. **原理理解** —— 論文を読み、アテンション計算を理解
2. **実践開始** —— LLaMA-Factoryで最初のファインチューニング
3. **深い最適化** —— データ準備、QLoRAでコスト削減
4. **本番デプロイ** —— 量子化、推論最適化

---

## 最後に

大規模モデルの技術原理を理解することは、AI研究者になるためではなく：

1. **AIをより良く使う** —— 原理を知れば、より効果的に活用できる
2. **賢い選択** —— ニーズに応じた適切なモデルと方法を選択
3. **専用AIをカスタマイズ** —— ファインチューニング能力でドメイン特化モデルを構築

*「原理を理解してこそ、ツールを本当に使いこなせる。」*
            `}}]},{id:"chapter-2",number:2,title:{zh:"提示词工程进阶",ja:"プロンプトエンジニアリング上級編"},subtitle:{zh:"掌握与AI对话的艺术",ja:"AIとの対話術をマスターする"},sections:[{id:"ch2-intro",title:{zh:"引言：提示词的力量",ja:"序章：プロンプトの力"},content:{zh:`
同样的 AI，不同的提示词，结果可能天差地别。

**提示词工程（Prompt Engineering）** 是一门让 AI 更好地理解你、更精准地完成任务的技术。掌握它，你就能让 AI 发挥出 10 倍的能力。

---

## 为什么提示词如此重要？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        提示词的影响力                                    │
└─────────────────────────────────────────────────────────────────────────┘

  普通提示词                              优秀提示词
  ┌─────────────┐                        ┌─────────────┐
  │  写一篇文章  │                        │ 角色 + 任务  │
  │             │                        │ + 格式 + 约束│
  └─────────────┘                        └─────────────┘
        │                                      │
        ▼                                      ▼
  ┌─────────────┐                        ┌─────────────┐
  │  泛泛而谈    │                        │ 专业、精准   │
  │  结构混乱    │                        │ 结构清晰     │
  │  需要多次修改│                        │ 一次到位     │
  └─────────────┘                        └─────────────┘

  效率: ★★☆☆☆                           效率: ★★★★★
\`\`\`

---

## 本章你将学到

1. **提示词的核心结构** —— 如何组织一个好的提示词
2. **常用提示词模式** —— 可以直接套用的模板
3. **角色扮演技巧** —— 让 AI 变成各种专家
4. **思维链与推理** —— 让 AI 一步步思考
5. **高级技巧与实战** —— 真实场景的应用

让我们开始掌握这门"与 AI 对话的艺术"！
            `,ja:`
同じAIでも、プロンプトが違えば結果は天と地ほど違います。

**プロンプトエンジニアリング（Prompt Engineering）** は、AIにあなたをより良く理解させ、より正確にタスクを完了させる技術です。これをマスターすれば、AIの能力を10倍引き出せます。

---

## なぜプロンプトがこれほど重要なのか？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        プロンプトの影響力                                │
└─────────────────────────────────────────────────────────────────────────┘

  普通のプロンプト                        優れたプロンプト
  ┌─────────────┐                        ┌─────────────┐
  │  記事を書いて │                        │ 役割 + タスク │
  │             │                        │ + 形式 + 制約 │
  └─────────────┘                        └─────────────┘
        │                                      │
        ▼                                      ▼
  ┌─────────────┐                        ┌─────────────┐
  │  漠然とした内容│                        │ 専門的で正確 │
  │  構造が乱雑   │                        │ 構造が明確   │
  │  何度も修正必要│                        │ 一発OK      │
  └─────────────┘                        └─────────────┘

  効率: ★★☆☆☆                           効率: ★★★★★
\`\`\`

---

## この章で学ぶこと

1. **プロンプトのコア構造** —— 良いプロンプトの組み立て方
2. **よく使うプロンプトパターン** —— そのまま使えるテンプレート
3. **ロールプレイテクニック** —— AIを様々な専門家に変える
4. **思考の連鎖と推論** —— AIに段階的に考えさせる
5. **上級テクニックと実践** —— 実際のシーンでの応用

「AIとの対話術」をマスターしましょう！
            `}},{id:"ch2-structure",title:{zh:"2.1 提示词的核心结构",ja:"2.1 プロンプトのコア構造"},content:{zh:`
一个好的提示词通常包含这些要素：

---

## CRISPE 框架

这是业界广泛使用的提示词结构：

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                         CRISPE 提示词框架                                │
└─────────────────────────────────────────────────────────────────────────┘

  C - Capacity (角色)      "你是一位资深的..."
  ────────────────────────────────────────────────────────

  R - Role (任务)          "你的任务是..."
  ────────────────────────────────────────────────────────

  I - Insight (背景)       "背景信息是..."
  ────────────────────────────────────────────────────────

  S - Statement (要求)     "请按照以下要求..."
  ────────────────────────────────────────────────────────

  P - Personality (风格)   "语气要专业/亲切/幽默..."
  ────────────────────────────────────────────────────────

  E - Experiment (示例)    "输出格式示例：..."
\`\`\`

---

## 实际例子对比

### ❌ 不完整的提示词

\`\`\`
帮我写一封邮件
\`\`\`

AI 不知道：写给谁？什么目的？什么语气？

### ✅ 使用 CRISPE 的提示词

\`\`\`
【角色】你是一位专业的商务沟通专家

【任务】帮我写一封跟进邮件

【背景】
- 上周与客户开会讨论了新项目合作
- 客户对价格有顾虑，需要进一步说服
- 客户是科技公司的采购总监

【要求】
- 字数控制在200字以内
- 重申我们的核心优势
- 提供一个限时优惠方案
- 预约下次沟通时间

【风格】专业但亲切，不要过于推销

【格式】
主题：xxx
正文：xxx
\`\`\`

---

## 简化版：三要素法则

如果觉得 CRISPE 太复杂，至少要包含这三个要素：

\`\`\`
┌───────────────────────────────────────────────────────┐
│                三要素法则                              │
├───────────────────────────────────────────────────────┤
│                                                       │
│   1. 角色 (Role)                                      │
│      └── "你是一个..."                                │
│                                                       │
│   2. 任务 (Task)                                      │
│      └── "请帮我..."                                  │
│                                                       │
│   3. 格式 (Format)                                    │
│      └── "输出格式为..."                              │
│                                                       │
└───────────────────────────────────────────────────────┘
\`\`\`

**例子**：
\`\`\`
你是一位Python专家。
请帮我优化下面这段代码，提高运行效率。
输出格式：先给出优化后的代码，再解释改动原因。
\`\`\`

---

## 本节要点

1. **CRISPE 框架** —— 完整的提示词结构
2. **三要素法则** —— 简化版：角色、任务、格式
3. **背景信息很重要** —— 提供足够的上下文
4. **明确输出格式** —— 减少来回修改
            `,ja:`
良いプロンプトには通常、以下の要素が含まれます：

---

## CRISPE フレームワーク

業界で広く使われているプロンプト構造：

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                         CRISPE プロンプトフレームワーク                   │
└─────────────────────────────────────────────────────────────────────────┘

  C - Capacity (役割)      "あなたはベテランの..."
  ────────────────────────────────────────────────────────

  R - Role (タスク)        "あなたのタスクは..."
  ────────────────────────────────────────────────────────

  I - Insight (背景)       "背景情報は..."
  ────────────────────────────────────────────────────────

  S - Statement (要件)     "以下の要件に従って..."
  ────────────────────────────────────────────────────────

  P - Personality (スタイル) "トーンは専門的/親しみやすい/ユーモラス..."
  ────────────────────────────────────────────────────────

  E - Experiment (例)      "出力形式の例：..."
\`\`\`

---

## 実際の例の比較

### ❌ 不完全なプロンプト

\`\`\`
メールを書いて
\`\`\`

AIには分からない：誰に？何の目的？どんなトーン？

### ✅ CRISPE を使ったプロンプト

\`\`\`
【役割】あなたはビジネスコミュニケーションの専門家です

【タスク】フォローアップメールを書いてください

【背景】
- 先週、新プロジェクトの協力について顧客と会議
- 顧客は価格に懸念があり、さらに説得が必要
- 顧客はテック企業の調達部長

【要件】
- 200文字以内
- 当社のコア優位性を再確認
- 期間限定オファーを提供
- 次回のコミュニケーション時間を予約

【スタイル】専門的だが親しみやすく、押し売り感なし

【形式】
件名：xxx
本文：xxx
\`\`\`

---

## 簡略版：3要素ルール

CRISPE が複雑すぎると感じたら、少なくとも3要素を含めましょう：

\`\`\`
┌───────────────────────────────────────────────────────┐
│                3要素ルール                             │
├───────────────────────────────────────────────────────┤
│                                                       │
│   1. 役割 (Role)                                      │
│      └── "あなたは..."                                │
│                                                       │
│   2. タスク (Task)                                    │
│      └── "〜してください..."                          │
│                                                       │
│   3. 形式 (Format)                                    │
│      └── "出力形式は..."                              │
│                                                       │
└───────────────────────────────────────────────────────┘
\`\`\`

**例**：
\`\`\`
あなたはPythonの専門家です。
以下のコードを最適化し、実行効率を向上させてください。
出力形式：最適化後のコードを先に、次に変更理由を説明。
\`\`\`

---

## このセクションのポイント

1. **CRISPE フレームワーク** —— 完全なプロンプト構造
2. **3要素ルール** —— 簡略版：役割、タスク、形式
3. **背景情報が重要** —— 十分なコンテキストを提供
4. **出力形式を明確に** —— やり取りを減らす
            `}},{id:"ch2-patterns",title:{zh:"2.2 常用提示词模式",ja:"2.2 よく使うプロンプトパターン"},content:{zh:`
掌握这些常用模式，可以直接套用到各种场景。

---

## 模式一：Few-Shot（少样本示例）

给 AI 几个例子，让它学会规律后处理新问题。

\`\`\`
请将以下中文翻译成日语，保持口语化风格：

例子1:
中文：今天天气真好
日语：今日はいい天気だね

例子2:
中文：我肚子饿了
日语：お腹空いたな

现在请翻译：
中文：这个电影太好看了
日语：
\`\`\`

**适用场景**：翻译、格式转换、风格模仿

---

## 模式二：Chain of Thought（思维链）

让 AI 一步步思考，而不是直接给答案。

\`\`\`
请一步步分析这个问题：

问题：一个商店进货价是80元，售价是100元，
打8折后还能赚多少钱？

请按以下步骤分析：
1. 首先，计算打折后的售价
2. 然后，计算每件商品的利润
3. 最后，给出结论
\`\`\`

**适用场景**：数学计算、逻辑分析、复杂决策

---

## 模式三：角色扮演

让 AI 扮演特定角色，获得更专业的回答。

\`\`\`
你现在是一位有20年经验的营养师。

一位30岁的程序员来咨询你：
- 他经常加班熬夜
- 饮食不规律，常吃外卖
- 最近感觉很疲惫

请从专业角度给出建议。
\`\`\`

**适用场景**：专业咨询、创意写作、模拟面试

---

## 模式四：约束条件

明确告诉 AI 什么不要做。

\`\`\`
写一段产品介绍，要求：

✅ 要做的：
- 突出三个核心功能
- 使用简洁的语言
- 包含一个用户案例

❌ 不要：
- 不要使用专业术语
- 不要超过200字
- 不要使用夸张的形容词
\`\`\`

**适用场景**：内容创作、代码生成、格式限制

---

## 模式五：迭代优化

让 AI 自己改进结果。

\`\`\`
第一步：请写一段产品介绍

第二步：以用户体验专家的视角，
找出上面文案的三个问题

第三步：根据问题重新改写文案
\`\`\`

**适用场景**：写作润色、方案优化、自我检查

---

## 模式速查表

| 模式 | 关键词 | 适用场景 |
|------|--------|----------|
| Few-Shot | "例如..."、"参考这个格式..." | 格式统一、风格模仿 |
| 思维链 | "一步步"、"首先...然后..." | 复杂推理、计算 |
| 角色扮演 | "你是一位..."、"作为专家..." | 专业建议、创意写作 |
| 约束条件 | "不要..."、"限制在..." | 控制输出、避免问题 |
| 迭代优化 | "先...再...然后改进" | 质量提升、自我完善 |

---

## 本节要点

1. **Few-Shot** —— 用例子教会 AI
2. **思维链** —— 让 AI 分步骤思考
3. **角色扮演** —— 获得专业视角
4. **约束条件** —— 明确什么不要做
5. **迭代优化** —— 让 AI 自我改进
            `,ja:`
これらの一般的なパターンをマスターすれば、様々なシーンに直接適用できます。

---

## パターン1：Few-Shot（少数例示）

AIにいくつかの例を与え、パターンを学んでから新しい問題を処理させます。

\`\`\`
以下の中国語を日本語に翻訳してください。口語スタイルを維持：

例1:
中国語：今天天气真好
日本語：今日はいい天気だね

例2:
中国語：我肚子饿了
日本語：お腹空いたな

では翻訳してください：
中国語：这个电影太好看了
日本語：
\`\`\`

**適用シーン**：翻訳、フォーマット変換、スタイル模倣

---

## パターン2：Chain of Thought（思考の連鎖）

AIに直接答えを出させず、段階的に考えさせます。

\`\`\`
この問題を段階的に分析してください：

問題：ある店の仕入れ価格は80元、販売価格は100元、
20%割引後、どれだけ利益が出ますか？

以下のステップで分析してください：
1. まず、割引後の販売価格を計算
2. 次に、各商品の利益を計算
3. 最後に、結論を出す
\`\`\`

**適用シーン**：数学計算、論理分析、複雑な意思決定

---

## パターン3：ロールプレイ

AIに特定の役割を演じさせ、より専門的な回答を得ます。

\`\`\`
あなたは20年の経験を持つ栄養士です。

30歳のプログラマーが相談に来ました：
- よく残業で夜更かし
- 食事が不規則で、よく外食
- 最近とても疲れを感じている

専門家の視点からアドバイスをください。
\`\`\`

**適用シーン**：専門相談、クリエイティブライティング、模擬面接

---

## パターン4：制約条件

AIに何をしないかを明確に伝えます。

\`\`\`
製品紹介文を書いてください。要件：

✅ すること：
- 3つのコア機能を強調
- 簡潔な言葉を使用
- ユーザー事例を1つ含める

❌ しないこと：
- 専門用語を使わない
- 200文字を超えない
- 誇張した形容詞を使わない
\`\`\`

**適用シーン**：コンテンツ作成、コード生成、フォーマット制限

---

## パターン5：反復改善

AIに結果を自己改善させます。

\`\`\`
ステップ1：製品紹介文を書いてください

ステップ2：UX専門家の視点から、
上記のコピーの3つの問題を指摘

ステップ3：問題に基づいてコピーを書き直す
\`\`\`

**適用シーン**：ライティング改善、プラン最適化、セルフチェック

---

## パターン早見表

| パターン | キーワード | 適用シーン |
|----------|------------|------------|
| Few-Shot | "例えば..."、"このフォーマットを参考に..." | フォーマット統一、スタイル模倣 |
| 思考の連鎖 | "段階的に"、"まず...次に..." | 複雑な推論、計算 |
| ロールプレイ | "あなたは..."、"専門家として..." | 専門アドバイス、創作 |
| 制約条件 | "〜しない"、"〜に制限" | 出力制御、問題回避 |
| 反復改善 | "まず...次に...改善" | 品質向上、自己完善 |

---

## このセクションのポイント

1. **Few-Shot** —— 例でAIに教える
2. **思考の連鎖** —— AIに段階的に考えさせる
3. **ロールプレイ** —— 専門家の視点を得る
4. **制約条件** —— 何をしないかを明確に
5. **反復改善** —— AIに自己改善させる
            `}},{id:"ch2-advanced",title:{zh:"2.3 实战案例与高级技巧",ja:"2.3 実践例と上級テクニック"},content:{zh:`
## 真实场景：提示词实战

让我们通过几个真实场景，看看如何应用提示词技巧解决实际问题。

---

## 案例1：代码审查助手

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                          代码审查提示词                                   │
└─────────────────────────────────────────────────────────────────────────┘

你是一位资深的代码审查专家，有10年的软件开发经验。

## 任务
请审查以下代码，重点关注：
1. 代码质量和可读性
2. 潜在的 bug 和安全问题
3. 性能优化建议
4. 最佳实践遵循情况

## 输出格式
请按以下结构输出：

### 🔴 必须修复（阻塞性问题）
- [问题描述] + [具体位置] + [修复建议]

### 🟡 建议改进（非阻塞）
- [改进点] + [原因] + [改进方案]

### 🟢 做得好的地方
- [亮点]

### 代码
[粘贴代码]
\`\`\`

**这个提示词的技巧：**
- ✅ 设定专家角色
- ✅ 明确任务目标
- ✅ 结构化输出格式
- ✅ 使用emoji增强可读性

---

## 案例2：文档生成助手

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                         技术文档生成提示词                                │
└─────────────────────────────────────────────────────────────────────────┘

你是一位技术文档专家。请根据以下函数生成完整的 API 文档。

## 要求
1. 简洁的功能描述（1-2句）
2. 参数说明表格（参数名、类型、必填、说明）
3. 返回值说明
4. 使用示例（包含正常用法和边界情况）
5. 注意事项

## 输出格式：Markdown

## 函数代码
[粘贴函数]
\`\`\`

---

## 高级技巧：Self-Consistency（自洽性）

当需要更可靠的答案时，可以让 AI 多次尝试，然后综合结果。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        Self-Consistency 方法                            │
└─────────────────────────────────────────────────────────────────────────┘

                    ┌───────────────┐
                    │   同一问题     │
                    └───────┬───────┘
                            │
        ┌───────────────────┼───────────────────┐
        │                   │                   │
        ▼                   ▼                   ▼
  ┌───────────┐       ┌───────────┐       ┌───────────┐
  │  思路 A    │       │  思路 B    │       │  思路 C    │
  └─────┬─────┘       └─────┬─────┘       └─────┬─────┘
        │                   │                   │
        ▼                   ▼                   ▼
  ┌───────────┐       ┌───────────┐       ┌───────────┐
  │  答案 A    │       │  答案 B    │       │  答案 A    │
  └───────────┘       └───────────┘       └───────────┘
                            │
                            ▼
                    ┌───────────────┐
                    │  投票：答案 A  │
                    │  （出现2次）   │
                    └───────────────┘
\`\`\`

**实际应用：**
\`\`\`
请用3种不同的思路解决这个问题，然后比较各个答案，
给出你最有信心的最终答案。

问题：[你的问题]
\`\`\`

---

## 高级技巧：Tree of Thought（思维树）

对于复杂问题，让 AI 探索多个分支，评估每个分支，选择最优路径。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        Tree of Thought 示例                             │
└─────────────────────────────────────────────────────────────────────────┘

                         问题
                           │
         ┌─────────────────┼─────────────────┐
         │                 │                 │
         ▼                 ▼                 ▼
     方案 A            方案 B            方案 C
    ┌────────┐        ┌────────┐        ┌────────┐
    │可行性:高│        │可行性:中│        │可行性:低│
    │成本:低  │        │成本:高  │        │成本:中  │
    │风险:低  │        │风险:中  │        │风险:高  │
    └────┬───┘        └────────┘        └────────┘
         │
         │    ← 选择最优分支继续深入
         ▼
    ┌─────────┐
    │ 详细规划 │
    └─────────┘
\`\`\`

**提示词模板：**
\`\`\`
请使用思维树方法解决这个问题：

1. 首先，生成3个可能的解决方案
2. 对每个方案评估：可行性、成本、风险
3. 选择最优方案并详细展开
4. 给出最终的完整解决方案

问题：[你的问题]
\`\`\`

---

## 实用提示词模板库

### 学习类
\`\`\`
我想学习 [主题]，我的背景是 [你的基础]。
请设计一个 [时间] 的学习计划，包含：
1. 学习路线图
2. 推荐资源
3. 练习项目
4. 检验标准
\`\`\`

### 写作类
\`\`\`
请帮我写一篇关于 [主题] 的 [文章类型]。

目标读者：[受众]
篇幅：[字数]
风格：[专业/轻松/幽默]
重点：[核心观点]

请先给出大纲，确认后再写正文。
\`\`\`

### 分析类
\`\`\`
请分析 [主题/数据/现象]：

1. 现状概述
2. 关键因素分析
3. 主要发现（用数据支撑）
4. 可行建议
5. 潜在风险

输出格式：结构化报告，使用图表说明关键数据
\`\`\`

---

## 本节要点

1. **代码审查** —— 角色 + 任务 + 结构化输出
2. **Self-Consistency** —— 多次尝试取共识
3. **Tree of Thought** —— 探索多分支选最优
4. **模板复用** —— 积累自己的提示词库
            `,ja:`
## 実際のシナリオ：プロンプト実践

いくつかの実際のシナリオを通じて、プロンプトテクニックで問題を解決する方法を見てみましょう。

---

## ケース1：コードレビューアシスタント

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        コードレビュープロンプト                           │
└─────────────────────────────────────────────────────────────────────────┘

あなたは10年の経験を持つシニアコードレビュアーです。

## タスク
以下のコードをレビューしてください：
1. コード品質と可読性
2. 潜在的なバグとセキュリティ問題
3. パフォーマンス改善の提案
4. ベストプラクティスの遵守状況

## 出力形式
以下の構造で出力してください：

### 🔴 必須修正（ブロッキング問題）
- [問題] + [場所] + [修正案]

### 🟡 改善提案（非ブロッキング）
- [改善点] + [理由] + [提案]

### 🟢 良い点
- [ハイライト]

### コード
[コードを貼り付け]
\`\`\`

**このプロンプトのテクニック：**
- ✅ 専門家の役割を設定
- ✅ タスク目標を明確化
- ✅ 構造化された出力形式
- ✅ 絵文字で可読性向上

---

## ケース2：ドキュメント生成アシスタント

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        技術ドキュメント生成プロンプト                      │
└─────────────────────────────────────────────────────────────────────────┘

あなたは技術ドキュメントの専門家です。
以下の関数の完全なAPIドキュメントを生成してください。

## 要件
1. 簡潔な機能説明（1-2文）
2. パラメータ説明表（名前、型、必須、説明）
3. 戻り値の説明
4. 使用例（通常の使い方とエッジケース）
5. 注意事項

## 出力形式：Markdown

## 関数コード
[関数を貼り付け]
\`\`\`

---

## 上級テクニック：Self-Consistency（自己整合性）

より信頼性の高い回答が必要な場合、AIに複数回試行させて結果を統合します。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        Self-Consistency 方法                            │
└─────────────────────────────────────────────────────────────────────────┘

                    ┌───────────────┐
                    │   同じ問題     │
                    └───────┬───────┘
                            │
        ┌───────────────────┼───────────────────┐
        │                   │                   │
        ▼                   ▼                   ▼
  ┌───────────┐       ┌───────────┐       ┌───────────┐
  │  アプローチA │       │  アプローチB │       │  アプローチC │
  └─────┬─────┘       └─────┬─────┘       └─────┬─────┘
        │                   │                   │
        ▼                   ▼                   ▼
  ┌───────────┐       ┌───────────┐       ┌───────────┐
  │  回答 A    │       │  回答 B    │       │  回答 A    │
  └───────────┘       └───────────┘       └───────────┘
                            │
                            ▼
                    ┌───────────────┐
                    │  投票：回答 A  │
                    │ （2回出現）    │
                    └───────────────┘
\`\`\`

**実践的な使い方：**
\`\`\`
3つの異なるアプローチでこの問題を解決し、
各回答を比較して、最も確信のある最終回答を出してください。

問題：[あなたの質問]
\`\`\`

---

## 上級テクニック：Tree of Thought（思考の木）

複雑な問題に対して、AIに複数の分岐を探索させ、評価して最適なパスを選択します。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        Tree of Thought 例                               │
└─────────────────────────────────────────────────────────────────────────┘

                         問題
                           │
         ┌─────────────────┼─────────────────┐
         │                 │                 │
         ▼                 ▼                 ▼
     方案 A            方案 B            方案 C
    ┌────────┐        ┌────────┐        ┌────────┐
    │実現性:高│        │実現性:中│        │実現性:低│
    │コスト:低│        │コスト:高│        │コスト:中│
    │リスク:低│        │リスク:中│        │リスク:高│
    └────┬───┘        └────────┘        └────────┘
         │
         │    ← 最適な分岐を選んで深掘り
         ▼
    ┌─────────┐
    │ 詳細計画 │
    └─────────┘
\`\`\`

**プロンプトテンプレート：**
\`\`\`
思考の木メソッドでこの問題を解決してください：

1. まず、3つの可能な解決策を生成
2. 各方案を評価：実現性、コスト、リスク
3. 最適な方案を選択し詳細に展開
4. 最終的な完全な解決策を提示

問題：[あなたの質問]
\`\`\`

---

## 実用プロンプトテンプレート集

### 学習系
\`\`\`
[トピック]を学びたいです。私の背景は[あなたの基礎]です。
[期間]の学習計画を設計してください：
1. 学習ロードマップ
2. おすすめリソース
3. 練習プロジェクト
4. 習得度チェック基準
\`\`\`

### ライティング系
\`\`\`
[トピック]についての[記事タイプ]を書いてください。

ターゲット読者：[対象]
文字数：[長さ]
スタイル：[専門的/カジュアル/ユーモア]
重点：[コアメッセージ]

まずアウトラインを提示し、確認後に本文を書いてください。
\`\`\`

### 分析系
\`\`\`
[トピック/データ/現象]を分析してください：

1. 現状の概要
2. 主要因分析
3. 主な発見（データで裏付け）
4. 実行可能な提案
5. 潜在的リスク

出力形式：構造化レポート、図表で主要データを説明
\`\`\`

---

## このセクションのポイント

1. **コードレビュー** —— 役割 + タスク + 構造化出力
2. **Self-Consistency** —— 複数回試行で合意を得る
3. **Tree of Thought** —— 複数分岐を探索し最適を選ぶ
4. **テンプレート再利用** —— 自分のプロンプトライブラリを蓄積
            `}},{id:"ch2-claude-tips",title:{zh:"2.4 Claude 专属技巧",ja:"2.4 Claude専用テクニック"},content:{zh:`
## Claude 的独特能力

Claude 有一些独特的功能，掌握这些技巧可以大幅提升使用效果。

---

## 1. Extended Thinking（深度思考）

Claude 可以在回答前进行深度思考，特别适合复杂问题。

**触发方式**：
\`\`\`
请仔细思考这个问题，考虑多种可能性，然后给出你的分析。

问题：[你的复杂问题]
\`\`\`

**适用场景**：数学证明、代码Debug、复杂业务分析、策略规划

---

## 2. Artifacts（可视化输出）

Claude 可以生成独立的可视化输出。

| 类型 | 说明 | 示例 |
|------|------|------|
| 代码 | 可运行的代码片段 | React组件、Python脚本 |
| SVG | 矢量图形 | 图标、插图 |
| HTML | 网页 | 落地页、表单 |
| Mermaid | 流程图 | 架构图、流程图 |

---

## 3. System Prompt 最佳实践

\`\`\`
## 身份定义
你是 [角色名称]，专注于 [领域]。

## 核心能力
- 能力1
- 能力2

## 行为规范
- 总是 [正面行为]
- 永远不要 [禁止行为]

## 输出格式
回答时请遵循以下格式：
1. 简要总结
2. 详细分析
3. 行动建议
\`\`\`

---

## 4. Claude Code 集成技巧

### Slash 命令
\`\`\`bash
/init      # 初始化项目记忆
/compact   # 紧凑对话，节省 token
/clear     # 清除上下文
/help      # 查看帮助
\`\`\`

### CLAUDE.md 项目配置

在项目根目录创建 CLAUDE.md 文件，Claude Code 会自动读取：

\`\`\`markdown
# CLAUDE.md

## 项目概述
这是一个 [项目类型] 项目，使用 [技术栈]。

## 常用命令
- npm run dev - 启动开发服务器
- npm run build - 构建生产版本

## 代码规范
- 使用 TypeScript
- 组件使用函数式写法
\`\`\`

---

## 5. 多轮对话策略

- **第1轮**：建立上下文（项目背景、技术栈）
- **第2轮**：具体任务（实现功能、解决问题）
- **第3轮**：迭代优化（基于上面的代码再添加...）
- **第4轮**：总结确认（请总结完成的功能）

**技巧**：保持上下文连贯，适时用 /compact 压缩

---

## 6. 输出控制技巧

### 控制长度
\`\`\`
请用3句话概括。
请控制在100字以内。
\`\`\`

### 控制格式
\`\`\`
请用 Markdown 表格输出。
请用代码块输出。
\`\`\`

### 控制语言
\`\`\`
请用中文回答。
请用技术性语言回答（面向开发者）。
\`\`\`

---

## 本节要点

1. **Extended Thinking** —— 复杂问题让 Claude 深度思考
2. **Artifacts** —— 生成可视化、可运行的输出
3. **System Prompt** —— 精心设计系统提示词
4. **Claude Code** —— 利用 /命令 和 CLAUDE.md
5. **多轮对话** —— 保持上下文连贯，适时总结
6. **输出控制** —— 明确指定长度、格式、语言
            `,ja:`
## Claudeの独自機能

Claudeにはいくつかの独自機能があり、これらのテクニックをマスターすると使用効果が大幅に向上します。

---

## 1. Extended Thinking（深い思考）

Claudeは回答前に深い思考を行うことができ、複雑な問題に特に適しています。

**トリガー方法**：
\`\`\`
この問題について慎重に考え、複数の可能性を検討してから分析を提供してください。

質問：[あなたの複雑な質問]
\`\`\`

**適用シーン**：数学的証明、コードデバッグ、複雑なビジネス分析、戦略プランニング

---

## 2. Artifacts（視覚的出力）

Claudeは独立した視覚的出力を生成できます。

| タイプ | 説明 | 例 |
|--------|------|-----|
| コード | 実行可能なコードスニペット | Reactコンポーネント、Pythonスクリプト |
| SVG | ベクターグラフィックス | アイコン、イラスト |
| HTML | ウェブページ | ランディングページ、フォーム |
| Mermaid | フローチャート | アーキテクチャ図、プロセスフロー |

---

## 3. System Prompt のベストプラクティス

\`\`\`
## アイデンティティ定義
あなたは[役割名]で、[分野]に専門性があります。

## コア能力
- 能力1
- 能力2

## 行動規範
- 常に[ポジティブな行動]
- 決して[禁止行動]しない

## 出力形式
回答する際は以下の形式に従ってください：
1. 簡潔な要約
2. 詳細な分析
3. アクションの提案
\`\`\`

---

## 4. Claude Code 統合テクニック

### Slashコマンド
\`\`\`bash
/init      # プロジェクトメモリの初期化
/compact   # 会話をコンパクトに
/clear     # コンテキストをクリア
/help      # ヘルプを表示
\`\`\`

### CLAUDE.md プロジェクト設定

プロジェクトルートにCLAUDE.mdファイルを作成すると、Claude Codeが自動的に読み取ります。

---

## 5. マルチターン対話戦略

- **第1ターン**：コンテキストの確立（プロジェクト背景、技術スタック）
- **第2ターン**：具体的なタスク（機能実装、問題解決）
- **第3ターン**：反復改善（上のコードに基づいて追加...）
- **第4ターン**：まとめと確認（完成した機能をまとめて）

**テクニック**：コンテキストの一貫性を維持、適時 /compact で圧縮

---

## 6. 出力制御テクニック

### 長さの制御
\`\`\`
3文で要約してください。
100文字以内で。
\`\`\`

### 形式の制御
\`\`\`
Markdownテーブルで出力してください。
コードブロックで出力してください。
\`\`\`

---

## このセクションのポイント

1. **Extended Thinking** —— 複雑な問題にはClaudeに深く考えさせる
2. **Artifacts** —— 視覚的で実行可能な出力を生成
3. **System Prompt** —— システムプロンプトを慎重に設計
4. **Claude Code** —— /コマンドとCLAUDE.mdを活用
5. **マルチターン対話** —— コンテキストを一貫させ、適時まとめる
6. **出力制御** —— 長さ、形式、言語を明確に指定
            `}},{id:"ch2-structured-output",title:{zh:"2.5 结构化输出：JSON Mode",ja:"2.5 構造化出力：JSON Mode"},content:{zh:`
## 让 AI 输出可解析的数据

当你需要程序自动处理 AI 的输出时，结构化输出是必备技能。

---

## 🎯 为什么需要结构化输出？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    传统输出 vs 结构化输出                                  │
└─────────────────────────────────────────────────────────────────────────┘

  传统文本输出                          结构化 JSON 输出
  ┌────────────────────────┐           ┌────────────────────────┐
  │ "这个产品很好，我给     │           │ {                      │
  │  5颗星，推荐购买。"    │           │   "sentiment": "正面", │
  │                        │           │   "rating": 5,         │
  │  ❌ 难以程序化解析      │           │   "recommend": true    │
  │  ❌ 格式不稳定          │           │ }                      │
  │  ❌ 需要正则提取        │           │                        │
  └────────────────────────┘           │  ✅ 直接 JSON.parse()  │
                                       │  ✅ 格式稳定可靠       │
                                       │  ✅ 类型安全           │
                                       └────────────────────────┘
\`\`\`

---

## 📊 主流 API 的 JSON Mode 对比

| 平台 | 方式 | 可靠性 | 备注 |
|------|------|--------|------|
| **OpenAI** | response_format | ⭐⭐⭐⭐⭐ | 原生支持，强制 JSON |
| **Claude** | Prompt 引导 | ⭐⭐⭐⭐ | 通过指令实现 |
| **Gemini** | response_schema | ⭐⭐⭐⭐⭐ | 支持 Schema 验证 |

---

## 🔧 OpenAI JSON Mode

\`\`\`python
from openai import OpenAI
import json

client = OpenAI()

# 方法1：简单 JSON Mode
response = client.chat.completions.create(
    model="gpt-4o",
    response_format={"type": "json_object"},  # 开启 JSON Mode
    messages=[
        {"role": "system", "content": "你是一个情感分析助手，以 JSON 格式输出分析结果。"},
        {"role": "user", "content": "分析这条评论的情感：这个产品太棒了，物超所值！"}
    ]
)

result = json.loads(response.choices[0].message.content)
print(result)
# {"sentiment": "positive", "score": 0.95, "keywords": ["棒", "物超所值"]}
\`\`\`

### Structured Output (更严格)

\`\`\`python
from pydantic import BaseModel
from openai import OpenAI

# 定义数据结构
class SentimentAnalysis(BaseModel):
    sentiment: str  # positive, negative, neutral
    score: float    # 0.0 - 1.0
    keywords: list[str]
    summary: str

client = OpenAI()

# 使用 Structured Output
response = client.beta.chat.completions.parse(
    model="gpt-4o-2024-08-06",
    messages=[
        {"role": "system", "content": "分析用户评论的情感"},
        {"role": "user", "content": "这个产品质量一般，但价格很便宜"}
    ],
    response_format=SentimentAnalysis  # 传入 Pydantic 模型
)

# 自动解析为 Python 对象
result = response.choices[0].message.parsed
print(f"情感: {result.sentiment}")
print(f"评分: {result.score}")
print(f"关键词: {result.keywords}")
\`\`\`

---

## 🔧 Claude 结构化输出

Claude 没有原生 JSON Mode，但可以通过 Prompt 实现：

\`\`\`python
import anthropic
import json

client = anthropic.Anthropic()

# 方法1：Prompt 引导
response = client.messages.create(
    model="claude-sonnet-4-20250514",
    max_tokens=1024,
    system="""你是一个数据提取助手。
请严格按照以下 JSON Schema 输出，不要包含任何其他文字：

{
  "name": "string",
  "email": "string",
  "phone": "string",
  "address": "string"
}""",
    messages=[
        {"role": "user", "content": "请从以下文本提取联系信息：张三，邮箱 zhangsan@example.com，电话 13800138000，地址：北京市朝阳区xxx"}
    ]
)

# 解析结果
result = json.loads(response.content[0].text)
print(result)
\`\`\`

### Claude Tool Use (更可靠)

\`\`\`python
import anthropic

client = anthropic.Anthropic()

# 定义工具 Schema
tools = [
    {
        "name": "extract_contact",
        "description": "提取联系人信息",
        "input_schema": {
            "type": "object",
            "properties": {
                "name": {"type": "string", "description": "姓名"},
                "email": {"type": "string", "description": "邮箱"},
                "phone": {"type": "string", "description": "电话"},
                "address": {"type": "string", "description": "地址"}
            },
            "required": ["name"]
        }
    }
]

response = client.messages.create(
    model="claude-sonnet-4-20250514",
    max_tokens=1024,
    tools=tools,
    tool_choice={"type": "tool", "name": "extract_contact"},  # 强制使用工具
    messages=[
        {"role": "user", "content": "提取联系信息：张三，zhangsan@example.com"}
    ]
)

# 获取结构化结果
tool_use = response.content[0]
result = tool_use.input
print(result)  # {"name": "张三", "email": "zhangsan@example.com"}
\`\`\`

---

## 🔧 Gemini 结构化输出

\`\`\`python
import google.generativeai as genai
from google.generativeai import types

genai.configure(api_key="YOUR_API_KEY")

# 定义输出 Schema
response_schema = {
    "type": "object",
    "properties": {
        "sentiment": {"type": "string", "enum": ["positive", "negative", "neutral"]},
        "score": {"type": "number"},
        "keywords": {"type": "array", "items": {"type": "string"}}
    },
    "required": ["sentiment", "score"]
}

model = genai.GenerativeModel(
    "gemini-1.5-pro",
    generation_config=types.GenerationConfig(
        response_mime_type="application/json",
        response_schema=response_schema
    )
)

response = model.generate_content("分析：这个电影太精彩了！")
print(response.text)  # 保证符合 Schema 的 JSON
\`\`\`

---

## 💡 通用 Prompt 技巧

当 API 不支持原生 JSON Mode 时：

\`\`\`markdown
## 输出格式要求

请严格按照以下 JSON 格式输出，不要包含任何解释性文字：

\`\`\`json
{
  "field1": "说明1",
  "field2": "说明2",
  "field3": ["数组", "示例"]
}
\`\`\`

重要规则：
1. 只输出 JSON，不要输出其他内容
2. 确保 JSON 格式正确，可以被解析
3. 所有字段都是必填的
4. 字符串使用双引号
\`\`\`

---

## 🛡️ 错误处理最佳实践

\`\`\`python
import json
from typing import TypeVar, Type
from pydantic import BaseModel, ValidationError

T = TypeVar('T', bound=BaseModel)

def parse_ai_response(response: str, model: Type[T]) -> T | None:
    """安全解析 AI 返回的 JSON"""
    try:
        # 尝试提取 JSON 块
        if '\`\`\`json' in response:
            json_str = response.split('\`\`\`json')[1].split('\`\`\`')[0]
        elif '\`\`\`' in response:
            json_str = response.split('\`\`\`')[1].split('\`\`\`')[0]
        else:
            json_str = response

        # 解析并验证
        data = json.loads(json_str.strip())
        return model.model_validate(data)

    except json.JSONDecodeError as e:
        print(f"JSON 解析失败: {e}")
        return None
    except ValidationError as e:
        print(f"数据验证失败: {e}")
        return None

# 使用示例
class ProductInfo(BaseModel):
    name: str
    price: float
    in_stock: bool

result = parse_ai_response(ai_response, ProductInfo)
if result:
    print(f"产品: {result.name}, 价格: {result.price}")
\`\`\`

---

## 📊 选择决策树

\`\`\`
                需要结构化输出？
                      │
            ┌─────────┴─────────┐
            │   使用什么 API？   │
            └─────────┬─────────┘
               ╱      │      ╲
           OpenAI  Claude  Gemini
              │       │       │
              ▼       ▼       ▼
        Structured  Tool   response_
          Output    Use    schema
              │       │       │
              └───────┼───────┘
                      ▼
              Pydantic 验证
                      │
                      ▼
                 业务逻辑
\`\`\`
            `,ja:`
## AIの出力をパース可能なデータに

プログラムでAIの出力を自動処理する必要がある場合、構造化出力は必須スキルです。

---

## 🎯 なぜ構造化出力が必要？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    従来の出力 vs 構造化出力                               │
└─────────────────────────────────────────────────────────────────────────┘

  従来のテキスト出力                    構造化JSON出力
  ┌────────────────────────┐           ┌────────────────────────┐
  │ "この商品は素晴らしい、 │           │ {                      │
  │  星5つ、おすすめ！"    │           │   "sentiment": "正面", │
  │                        │           │   "rating": 5,         │
  │  ❌ プログラム解析困難  │           │   "recommend": true    │
  │  ❌ フォーマット不安定  │           │ }                      │
  │  ❌ 正規表現が必要     │           │                        │
  └────────────────────────┘           │  ✅ JSON.parse()可能  │
                                       │  ✅ フォーマット安定   │
                                       │  ✅ 型安全            │
                                       └────────────────────────┘
\`\`\`

---

## 📊 主要APIのJSON Mode比較

| プラットフォーム | 方式 | 信頼性 | 備考 |
|------------------|------|--------|------|
| **OpenAI** | response_format | ⭐⭐⭐⭐⭐ | ネイティブ対応 |
| **Claude** | Prompt誘導 | ⭐⭐⭐⭐ | 指示で実現 |
| **Gemini** | response_schema | ⭐⭐⭐⭐⭐ | Schema検証対応 |

---

## 🔧 OpenAI JSON Mode

\`\`\`python
from openai import OpenAI
import json

client = OpenAI()

# 方法1：シンプルなJSON Mode
response = client.chat.completions.create(
    model="gpt-4o",
    response_format={"type": "json_object"},  # JSON Mode有効化
    messages=[
        {"role": "system", "content": "感情分析アシスタントとして、JSON形式で結果を出力してください。"},
        {"role": "user", "content": "このレビューの感情を分析：この商品は最高、コスパ抜群！"}
    ]
)

result = json.loads(response.choices[0].message.content)
print(result)
\`\`\`

### Structured Output（より厳密）

\`\`\`python
from pydantic import BaseModel
from openai import OpenAI

# データ構造を定義
class SentimentAnalysis(BaseModel):
    sentiment: str  # positive, negative, neutral
    score: float    # 0.0 - 1.0
    keywords: list[str]
    summary: str

client = OpenAI()

# Structured Outputを使用
response = client.beta.chat.completions.parse(
    model="gpt-4o-2024-08-06",
    messages=[
        {"role": "system", "content": "ユーザーレビューの感情を分析"},
        {"role": "user", "content": "品質は普通だが、価格は安い"}
    ],
    response_format=SentimentAnalysis
)

# 自動的にPythonオブジェクトに解析
result = response.choices[0].message.parsed
print(f"感情: {result.sentiment}")
\`\`\`

---

## 🔧 Claude 構造化出力

ClaudeにはネイティブのJSON Modeがありませんが、Promptで実現可能：

\`\`\`python
import anthropic
import json

client = anthropic.Anthropic()

response = client.messages.create(
    model="claude-sonnet-4-20250514",
    max_tokens=1024,
    system="""データ抽出アシスタントです。
以下のJSON Schemaに厳密に従って出力してください：

{
  "name": "string",
  "email": "string",
  "phone": "string"
}""",
    messages=[
        {"role": "user", "content": "連絡先を抽出：田中太郎、tanaka@example.com"}
    ]
)

result = json.loads(response.content[0].text)
\`\`\`

### Claude Tool Use（より信頼性が高い）

\`\`\`python
import anthropic

client = anthropic.Anthropic()

tools = [
    {
        "name": "extract_contact",
        "description": "連絡先情報を抽出",
        "input_schema": {
            "type": "object",
            "properties": {
                "name": {"type": "string"},
                "email": {"type": "string"}
            },
            "required": ["name"]
        }
    }
]

response = client.messages.create(
    model="claude-sonnet-4-20250514",
    max_tokens=1024,
    tools=tools,
    tool_choice={"type": "tool", "name": "extract_contact"},
    messages=[
        {"role": "user", "content": "連絡先を抽出：田中太郎、tanaka@example.com"}
    ]
)

result = response.content[0].input
\`\`\`

---

## 💡 汎用Promptテクニック

APIがネイティブJSON Modeをサポートしていない場合：

\`\`\`markdown
## 出力フォーマット要件

以下のJSON形式で厳密に出力し、説明文は含めないでください：

\`\`\`json
{
  "field1": "説明1",
  "field2": "説明2"
}
\`\`\`

重要なルール：
1. JSONのみを出力
2. JSON形式が正しいことを確認
3. すべてのフィールドは必須
\`\`\`

---

## 📊 選択決定木

\`\`\`
                構造化出力が必要？
                      │
            ┌─────────┴─────────┐
            │   どのAPIを使用？  │
            └─────────┬─────────┘
               ╱      │      ╲
           OpenAI  Claude  Gemini
              │       │       │
              ▼       ▼       ▼
        Structured  Tool   response_
          Output    Use    schema
              │       │       │
              └───────┼───────┘
                      ▼
              Pydantic検証
\`\`\`
            `}},{id:"ch2-summary",title:{zh:"2.6 本章小结",ja:"2.6 この章のまとめ"},content:{zh:`
## 提示词工程核心要点

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                       提示词工程知识地图                                  │
└─────────────────────────────────────────────────────────────────────────┘

                              提示词工程
                                  │
            ┌─────────────────────┼─────────────────────┐
            │                     │                     │
            ▼                     ▼                     ▼
       ┌─────────┐           ┌─────────┐          ┌─────────┐
       │ 结构框架 │           │ 常用模式 │          │ 实践技巧 │
       └─────────┘           └─────────┘          └─────────┘
            │                     │                     │
     ┌──────┴──────┐       ┌──────┴──────┐       ┌──────┴──────┐
     │             │       │             │       │             │
     ▼             ▼       ▼             ▼       ▼             ▼
  CRISPE        三要素   Few-Shot    思维链    角色扮演    迭代优化
  框架          法则     少样本      推理      专家视角    持续改进
\`\`\`

---

## 快速行动清单

- [ ] 下次使用 AI 时，尝试使用"三要素法则"
- [ ] 遇到复杂问题，让 AI "一步步分析"
- [ ] 需要专业建议时，让 AI 扮演相关专家
- [ ] 保存你觉得好用的提示词模板

---

## 关键金句

> "好的提示词不是在命令 AI，而是在与 AI 协作。"

> "给 AI 足够的信息，它才能给你满意的答案。"

> "提示词工程的本质，是把模糊的需求变成清晰的指令。"

---

下一章，我们将学习更激动人心的内容：**AI Agents（智能体）** —— 让 AI 不只是回答问题，而是能够自主完成复杂任务！
            `,ja:`
## プロンプトエンジニアリングのコアポイント

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    プロンプトエンジニアリング知識マップ                    │
└─────────────────────────────────────────────────────────────────────────┘

                          プロンプトエンジニアリング
                                  │
            ┌─────────────────────┼─────────────────────┐
            │                     │                     │
            ▼                     ▼                     ▼
       ┌─────────┐           ┌─────────┐          ┌─────────┐
       │構造フレーム│           │一般パターン│          │実践テクニック│
       └─────────┘           └─────────┘          └─────────┘
            │                     │                     │
     ┌──────┴──────┐       ┌──────┴──────┐       ┌──────┴──────┐
     │             │       │             │       │             │
     ▼             ▼       ▼             ▼       ▼             ▼
  CRISPE        3要素   Few-Shot    思考の連鎖  ロールプレイ  反復改善
  フレーム      ルール   少数例示    推論      専門家視点    継続改善
\`\`\`

---

## クイックアクションリスト

- [ ] 次にAIを使うとき、「3要素ルール」を試す
- [ ] 複雑な問題に遭遇したら、AIに「段階的に分析」させる
- [ ] 専門アドバイスが必要なとき、AIに関連専門家を演じさせる
- [ ] 使えると思ったプロンプトテンプレートを保存する

---

## 重要な格言

> 「良いプロンプトはAIに命令するのではなく、AIと協力することです。」

> 「AIに十分な情報を与えれば、満足のいく答えが得られます。」

> 「プロンプトエンジニアリングの本質は、曖昧なニーズを明確な指示に変えることです。」

---

次の章では、さらにエキサイティングな内容を学びます：**AI Agents（インテリジェントエージェント）** —— AIが質問に答えるだけでなく、複雑なタスクを自律的に完了できるようになります！
            `}}]},{id:"chapter-3",number:3,title:{zh:"AI Agents 智能体",ja:"AI Agents インテリジェントエージェント"},subtitle:{zh:"让AI自主完成复杂任务",ja:"AIに複雑なタスクを自律的に完了させる"},sections:[{id:"ch3-intro",title:{zh:"引言：从对话到行动",ja:"序章：対話から行動へ"},content:{zh:`
到目前为止，我们学习的都是**与 AI 对话**。你问，AI 答。

但是，如果 AI 能**自己动手**做事呢？

这就是 **AI Agent（智能体）** 的概念：让 AI 不只是回答问题，而是能够**自主规划、调用工具、完成任务**。

---

## 普通 AI vs AI Agent

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                     普通 AI  vs  AI Agent                               │
└─────────────────────────────────────────────────────────────────────────┘

    普通 AI（ChatGPT 等）                    AI Agent
    ┌──────────────────────┐               ┌──────────────────────┐
    │                      │               │                      │
    │   用户提问            │               │   用户设定目标        │
    │      ↓               │               │      ↓               │
    │   AI 回答            │               │   Agent 分析任务     │
    │      ↓               │               │      ↓               │
    │   结束               │               │   制定计划           │
    │                      │               │      ↓               │
    │                      │               │   调用工具执行        │
    │                      │               │      ↓               │
    │                      │               │   检查结果           │
    │                      │               │      ↓               │
    │                      │               │   继续或完成         │
    │                      │               │                      │
    └──────────────────────┘               └──────────────────────┘

    特点：一问一答                          特点：自主规划执行
    能力：回答问题                          能力：完成任务
\`\`\`

---

## 生活中的 Agent 类比

想象你有一个**超级助理**：

**普通 AI** 像一个知识渊博的顾问：
- 你问："今天北京天气怎么样？"
- 它答："今天北京晴，25度"

**AI Agent** 像一个能干的私人助理：
- 你说："帮我安排明天去北京的出差"
- 它会：
  1. 查询明天的天气
  2. 搜索航班和价格
  3. 推荐合适的酒店
  4. 把行程整理成表格发给你
  5. 甚至帮你预订（如果有权限）

---

## 本章你将学到

1. **什么是 AI Agent** —— 核心概念和工作原理
2. **Agent 的关键能力** —— 规划、工具使用、记忆
3. **常见 Agent 类型** —— 不同场景的 Agent
4. **实用 Agent 工具** —— 现在就能用的 Agent
5. **Agent 的未来** —— 发展趋势和机遇

让我们进入 AI Agent 的世界！
            `,ja:`
これまで私たちが学んできたのは、**AIとの対話**でした。あなたが質問し、AIが答える。

しかし、もしAIが**自分で行動**できたら？

これが**AI Agent（インテリジェントエージェント）**の概念です：AIが質問に答えるだけでなく、**自律的に計画を立て、ツールを使い、タスクを完了**できるようにすることです。

---

## 通常のAI vs AI Agent

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                     通常のAI  vs  AI Agent                              │
└─────────────────────────────────────────────────────────────────────────┘

    通常のAI（ChatGPT等）                   AI Agent
    ┌──────────────────────┐               ┌──────────────────────┐
    │                      │               │                      │
    │   ユーザーが質問     │               │   ユーザーが目標設定  │
    │      ↓               │               │      ↓               │
    │   AIが回答           │               │   Agentがタスク分析   │
    │      ↓               │               │      ↓               │
    │   終了               │               │   計画を立てる        │
    │                      │               │      ↓               │
    │                      │               │   ツールを呼び出して実行│
    │                      │               │      ↓               │
    │                      │               │   結果をチェック      │
    │                      │               │      ↓               │
    │                      │               │   継続または完了      │
    │                      │               │                      │
    └──────────────────────┘               └──────────────────────┘

    特徴：一問一答                          特徴：自律的に計画・実行
    能力：質問に答える                      能力：タスクを完了する
\`\`\`

---

## 日常生活でのAgentの例え

**スーパーアシスタント**がいると想像してください：

**通常のAI** は知識豊富なアドバイザーのよう：
- 「今日の東京の天気は？」と聞くと
- 「今日の東京は晴れ、25度です」と答える

**AI Agent** は有能なパーソナルアシスタントのよう：
- 「明日の北京出張を手配して」と言うと
- 以下を行います：
  1. 明日の天気を確認
  2. フライトと価格を検索
  3. 適切なホテルを推薦
  4. 旅程を表にまとめて送信
  5. 権限があれば予約まで行う

---

## この章で学ぶこと

1. **AI Agentとは** —— コアコンセプトと仕組み
2. **Agentの重要な能力** —— 計画、ツール使用、記憶
3. **一般的なAgentタイプ** —— 様々なシーンのAgent
4. **実用的なAgentツール** —— 今すぐ使えるAgent
5. **Agentの未来** —— 発展トレンドと機会

AI Agentの世界に入りましょう！
            `}},{id:"ch3-how-it-works",title:{zh:"3.1 Agent 的工作原理",ja:"3.1 Agentの仕組み"},content:{zh:`
AI Agent 是如何工作的？让我们拆解它的核心组件。

---

## Agent 的四大核心能力

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        AI Agent 核心架构                                │
└─────────────────────────────────────────────────────────────────────────┘

                         ┌─────────────┐
                         │   大脑      │
                         │  (LLM)     │
                         └──────┬──────┘
                                │
         ┌──────────────────────┼──────────────────────┐
         │                      │                      │
         ▼                      ▼                      ▼
  ┌──────────────┐      ┌──────────────┐      ┌──────────────┐
  │    规划      │      │    工具      │      │    记忆      │
  │  Planning   │      │   Tools     │      │   Memory    │
  └──────────────┘      └──────────────┘      └──────────────┘
         │                      │                      │
         ▼                      ▼                      ▼
   分解任务              执行动作              存储信息
   制定步骤              调用API              上下文记忆
   调整计划              读写文件              长期记忆
\`\`\`

---

## 1. 规划能力（Planning）

Agent 能够把复杂任务分解成可执行的步骤：

**例子：**"帮我写一篇关于AI的博客文章"

Agent 的规划：
\`\`\`
1. 确定文章主题和目标读者
2. 搜索相关资料和最新动态
3. 制定文章大纲
4. 撰写各个章节
5. 优化和润色
6. 生成配图建议
7. 输出最终文章
\`\`\`

---

## 2. 工具使用（Tools）

Agent 可以调用各种外部工具：

| 工具类型 | 例子 | 用途 |
|----------|------|------|
| 搜索 | Google、Bing | 获取最新信息 |
| 代码 | Python 解释器 | 计算、数据处理 |
| 文件 | 读写本地文件 | 存储和访问数据 |
| API | 天气、地图、支付 | 连接外部服务 |
| 浏览器 | 网页自动化 | 操作网页 |

---

## 3. 记忆能力（Memory）

Agent 需要记住信息来完成复杂任务：

\`\`\`
┌─────────────────────────────────────────────────────┐
│                  Agent 记忆系统                     │
├─────────────────────────────────────────────────────┤
│                                                     │
│  短期记忆（对话上下文）                              │
│  └── 当前对话的历史                                 │
│  └── 刚才执行的步骤                                 │
│                                                     │
│  长期记忆（持久存储）                                │
│  └── 用户偏好                                       │
│  └── 历史任务记录                                   │
│  └── 学到的知识                                     │
│                                                     │
└─────────────────────────────────────────────────────┘
\`\`\`

---

## 4. 反思与调整

Agent 会根据执行结果调整策略：

\`\`\`
执行步骤 ──▶ 检查结果 ──▶ 结果OK？
                              │
                    ┌─────────┴─────────┐
                    │                   │
                   是                   否
                    │                   │
                    ▼                   ▼
               继续下一步          分析问题
                                       │
                                       ▼
                                  调整策略
                                       │
                                       ▼
                                  重新执行
\`\`\`

---

## ReAct 模式

大多数 Agent 采用 **ReAct（Reasoning + Acting）** 模式：

### 交互式演示：Agent 工作循环

体验一下 Agent 是如何思考、行动和观察的：

::agent-viz::

\`\`\`
循环过程：
┌──────────────────────────────────────────────────────┐
│                                                      │
│  思考 (Thought) ──▶ 行动 (Action) ──▶ 观察 (Observe) │
│     ▲                                          │     │
│     └──────────────────────────────────────────┘     │
│                                                      │
└──────────────────────────────────────────────────────┘

例子：
Thought: 用户想知道今天的股价，我需要搜索最新数据
Action: 调用搜索工具，搜索"苹果公司今日股价"
Observe: 搜索结果显示苹果股价为 $178.50
Thought: 我已经获得了数据，可以回答用户了
Action: 输出答案给用户
\`\`\`

---

## 本节要点

1. **四大核心** —— 规划、工具、记忆、反思
2. **规划能力** —— 把复杂任务分解成步骤
3. **工具使用** —— 连接外部世界的能力
4. **记忆系统** —— 短期和长期记忆
5. **ReAct 模式** —— 思考-行动-观察的循环
            `,ja:`
AI Agentはどのように動作するのでしょうか？コアコンポーネントを分解してみましょう。

---

## Agentの4つのコア能力

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        AI Agent コアアーキテクチャ                       │
└─────────────────────────────────────────────────────────────────────────┘

                         ┌─────────────┐
                         │    脳       │
                         │   (LLM)    │
                         └──────┬──────┘
                                │
         ┌──────────────────────┼──────────────────────┐
         │                      │                      │
         ▼                      ▼                      ▼
  ┌──────────────┐      ┌──────────────┐      ┌──────────────┐
  │    計画      │      │   ツール    │      │    記憶      │
  │  Planning   │      │   Tools     │      │   Memory    │
  └──────────────┘      └──────────────┘      └──────────────┘
         │                      │                      │
         ▼                      ▼                      ▼
   タスク分解             アクション実行          情報保存
   ステップ策定           API呼び出し            コンテキスト記憶
   計画調整              ファイル読み書き         長期記憶
\`\`\`

---

## 1. 計画能力（Planning）

Agentは複雑なタスクを実行可能なステップに分解できます：

**例：**「AIについてのブログ記事を書いて」

Agentの計画：
\`\`\`
1. 記事のテーマとターゲット読者を決定
2. 関連資料と最新動向を検索
3. 記事のアウトラインを作成
4. 各セクションを執筆
5. 最適化と推敲
6. 配置する画像を提案
7. 最終記事を出力
\`\`\`

---

## 2. ツール使用（Tools）

Agentは様々な外部ツールを呼び出せます：

| ツールタイプ | 例 | 用途 |
|-------------|-----|------|
| 検索 | Google、Bing | 最新情報の取得 |
| コード | Pythonインタープリター | 計算、データ処理 |
| ファイル | ローカルファイルの読み書き | データの保存とアクセス |
| API | 天気、地図、決済 | 外部サービスへの接続 |
| ブラウザ | Webページ自動化 | Webページの操作 |

---

## 3. 記憶能力（Memory）

Agentは複雑なタスクを完了するために情報を記憶する必要があります：

\`\`\`
┌─────────────────────────────────────────────────────┐
│                  Agent 記憶システム                  │
├─────────────────────────────────────────────────────┤
│                                                     │
│  短期記憶（対話コンテキスト）                        │
│  └── 現在の対話履歴                                 │
│  └── 直前に実行したステップ                         │
│                                                     │
│  長期記憶（永続ストレージ）                          │
│  └── ユーザーの好み                                 │
│  └── 過去のタスク記録                               │
│  └── 学んだ知識                                     │
│                                                     │
└─────────────────────────────────────────────────────┘
\`\`\`

---

## 4. 振り返りと調整

Agentは実行結果に基づいて戦略を調整します：

\`\`\`
ステップ実行 ──▶ 結果チェック ──▶ 結果OK？
                                   │
                    ┌──────────────┴──────────────┐
                    │                             │
                   はい                           いいえ
                    │                             │
                    ▼                             ▼
               次のステップへ                問題を分析
                                               │
                                               ▼
                                           戦略を調整
                                               │
                                               ▼
                                            再実行
\`\`\`

---

## ReAct パターン

ほとんどのAgentは**ReAct（Reasoning + Acting）**パターンを採用：

### インタラクティブデモ：Agent動作ループ

Agentがどのように思考、行動、観察するかを体験してください：

::agent-viz::

\`\`\`
ループプロセス：
┌──────────────────────────────────────────────────────┐
│                                                      │
│  思考 (Thought) ──▶ 行動 (Action) ──▶ 観察 (Observe) │
│     ▲                                          │     │
│     └──────────────────────────────────────────┘     │
│                                                      │
└──────────────────────────────────────────────────────┘

例：
Thought: ユーザーは今日の株価を知りたい、最新データを検索する必要がある
Action: 検索ツールを呼び出し、「Apple社今日の株価」を検索
Observe: 検索結果はApple株価が$178.50と表示
Thought: データを取得した、ユーザーに回答できる
Action: ユーザーに答えを出力
\`\`\`

---

## このセクションのポイント

1. **4つのコア** —— 計画、ツール、記憶、振り返り
2. **計画能力** —— 複雑なタスクをステップに分解
3. **ツール使用** —— 外部世界への接続能力
4. **記憶システム** —— 短期と長期記憶
5. **ReAct パターン** —— 思考-行動-観察のループ
            `}},{id:"ch3-tools",title:{zh:"3.2 实用 Agent 工具",ja:"3.2 実用的なAgentツール"},content:{zh:`
现在已经有很多可以直接使用的 AI Agent 工具，让我们来看看最实用的几个。

---

## 代码开发类 Agent

### Claude Code / Cursor / GitHub Copilot

这些工具可以：
- 自动编写代码
- 理解整个项目结构
- 自动修复 bug
- 执行命令和测试

**使用场景**：
- "帮我实现一个用户登录功能"
- "修复这个报错"
- "给这个函数写单元测试"

---

## 通用任务 Agent

### ChatGPT Plugins / GPTs

OpenAI 的插件系统让 ChatGPT 可以：
- 搜索网页
- 分析数据
- 生成图片
- 连接第三方服务

### Claude with Tools

Claude 也可以：
- 执行代码
- 搜索信息
- 分析文件

---

## 自动化工作流

### Zapier / Make

这些工具可以把 AI 集成到工作流中：

\`\`\`
触发器 ──▶ AI 处理 ──▶ 执行动作

例子：
收到邮件 ──▶ AI分析内容 ──▶ 自动分类并回复
\`\`\`

---

## 浏览器自动化

### 浏览器 Agent

可以自动操作网页的 AI：

- 自动填写表单
- 抓取信息
- 执行重复性任务

**注意**：使用时要遵守网站规则和法律法规

---

## Agent 能力对比

| 工具 | 代码 | 搜索 | 文件 | 浏览器 | 价格 |
|------|------|------|------|--------|------|
| Claude Code | ✅ | ✅ | ✅ | ✅ | 付费 |
| ChatGPT Plus | ✅ | ✅ | ✅ | ❌ | $20/月 |
| Cursor | ✅ | ✅ | ✅ | ❌ | 免费/付费 |
| Copilot | ✅ | ❌ | ✅ | ❌ | $10/月 |

---

## 如何选择 Agent 工具

\`\`\`
你的需求是什么？
      │
      ├── 写代码 ──▶ Claude Code / Cursor
      │
      ├── 日常任务 ──▶ ChatGPT / Claude
      │
      ├── 自动化工作流 ──▶ Zapier + AI
      │
      └── 数据分析 ──▶ ChatGPT 代码解释器
\`\`\`

---

## 本节要点

1. **代码 Agent** —— 自动写代码、修复 bug
2. **通用 Agent** —— ChatGPT、Claude 的工具功能
3. **自动化工具** —— 把 AI 集成到工作流
4. **根据需求选择** —— 不同工具有不同优势
            `,ja:`
今すぐ使えるAI Agentツールがたくさんあります。最も実用的なものをいくつか見てみましょう。

---

## コード開発系Agent

### Claude Code / Cursor / GitHub Copilot

これらのツールができること：
- 自動でコードを書く
- プロジェクト構造全体を理解
- 自動でバグを修正
- コマンドとテストを実行

**使用シーン**：
- 「ユーザーログイン機能を実装して」
- 「このエラーを修正して」
- 「この関数のユニットテストを書いて」

---

## 汎用タスクAgent

### ChatGPT Plugins / GPTs

OpenAIのプラグインシステムでChatGPTができること：
- Webページを検索
- データを分析
- 画像を生成
- サードパーティサービスに接続

### Claude with Tools

Claudeもできること：
- コードを実行
- 情報を検索
- ファイルを分析

---

## 自動化ワークフロー

### Zapier / Make

これらのツールはAIをワークフローに統合できます：

\`\`\`
トリガー ──▶ AI処理 ──▶ アクション実行

例：
メール受信 ──▶ AIが内容分析 ──▶ 自動分類と返信
\`\`\`

---

## ブラウザ自動化

### ブラウザAgent

Webページを自動操作できるAI：

- フォームを自動入力
- 情報を収集
- 繰り返しタスクを実行

**注意**：使用時はWebサイトのルールと法規制を遵守してください

---

## Agent能力比較

| ツール | コード | 検索 | ファイル | ブラウザ | 価格 |
|--------|--------|------|----------|----------|------|
| Claude Code | ✅ | ✅ | ✅ | ✅ | 有料 |
| ChatGPT Plus | ✅ | ✅ | ✅ | ❌ | $20/月 |
| Cursor | ✅ | ✅ | ✅ | ❌ | 無料/有料 |
| Copilot | ✅ | ❌ | ✅ | ❌ | $10/月 |

---

## Agentツールの選び方

\`\`\`
あなたのニーズは？
      │
      ├── コードを書く ──▶ Claude Code / Cursor
      │
      ├── 日常タスク ──▶ ChatGPT / Claude
      │
      ├── ワークフロー自動化 ──▶ Zapier + AI
      │
      └── データ分析 ──▶ ChatGPT コードインタープリター
\`\`\`

---

## このセクションのポイント

1. **コードAgent** —— 自動でコード作成、バグ修正
2. **汎用Agent** —— ChatGPT、Claudeのツール機能
3. **自動化ツール** —— AIをワークフローに統合
4. **ニーズに応じて選択** —— ツールごとに異なる強み
            `}},{id:"ch3-mcp",title:{zh:'3.3 MCP：AI 的"万能接口"',ja:"3.3 MCP：AIの「万能インターフェース」"},content:{zh:`
## 什么是 MCP？

**MCP（Model Context Protocol）** 是 Anthropic 推出的开放协议，让 AI 能够安全地连接各种外部工具和数据源。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                         MCP 架构概览                                     │
└─────────────────────────────────────────────────────────────────────────┘

                           ┌───────────────┐
                           │    AI 模型    │
                           │  (Claude等)   │
                           └───────┬───────┘
                                   │
                           ┌───────▼───────┐
                           │  MCP 协议层   │
                           └───────┬───────┘
                                   │
      ┌────────────────────────────┼────────────────────────────┐
      │                            │                            │
      ▼                            ▼                            ▼
┌───────────┐              ┌───────────┐              ┌───────────┐
│ 文件系统  │              │  数据库   │              │  API服务  │
│ MCP服务器 │              │ MCP服务器 │              │ MCP服务器 │
└───────────┘              └───────────┘              └───────────┘
      │                            │                            │
      ▼                            ▼                            ▼
  本地文件                    PostgreSQL                   GitHub/Slack
                               MySQL                      Notion/Jira
\`\`\`

---

## 为什么 MCP 很重要？

之前的问题：
- 每个 AI 产品都要单独开发工具集成
- 数据源连接方式各不相同
- 难以复用和标准化

MCP 解决了：
\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                         统一的工具接口                                   │
└─────────────────────────────────────────────────────────────────────────┘

  旧模式                                新模式（MCP）
  ─────────                            ──────────────
  ChatGPT ─┬─ 文件读取                  Claude ──┐
           ├─ 网页搜索                  ChatGPT ─┼── MCP ──┬─ 任何工具
           └─ API调用                   其他AI ──┘         └─ 任何数据

  Claude ──┬─ 文件读取
           ├─ 网页搜索     ──▶         一次开发，处处可用
           └─ API调用

  重复开发 ✗                           标准化复用 ✓
\`\`\`

---

## Claude Code：MCP 的实战应用

**Claude Code** 是基于 MCP 的 AI 编程助手，可以直接操作你的开发环境。

### 核心能力

| 能力 | 说明 | 示例 |
|------|------|------|
| 🔍 代码理解 | 读取和分析整个项目 | "解释这个模块的架构" |
| ✏️ 代码修改 | 直接编辑文件 | "修复这个 bug" |
| 💻 命令执行 | 运行终端命令 | "运行测试并修复失败用例" |
| 🔗 工具集成 | 通过 MCP 扩展能力 | "查询数据库中的用户数据" |

### 实际工作流程

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                     Claude Code 工作流程                                │
└─────────────────────────────────────────────────────────────────────────┘

用户: "帮我实现用户登录功能"
                │
                ▼
        ┌───────────────┐
        │ 1. 分析项目   │  ← 读取现有代码、理解架构
        └───────┬───────┘
                │
                ▼
        ┌───────────────┐
        │ 2. 制定计划   │  ← 确定需要创建/修改的文件
        └───────┬───────┘
                │
                ▼
        ┌───────────────┐
        │ 3. 编写代码   │  ← 创建路由、控制器、模型
        └───────┬───────┘
                │
                ▼
        ┌───────────────┐
        │ 4. 测试验证   │  ← 运行测试、检查错误
        └───────┬───────┘
                │
                ▼
        ┌───────────────┐
        │ 5. 迭代改进   │  ← 根据反馈继续优化
        └───────────────┘
\`\`\`

---

## 常用 MCP 服务器

| MCP 服务器 | 功能 | 使用场景 |
|-----------|------|----------|
| filesystem | 文件读写 | 代码编辑、文档处理 |
| postgres/mysql | 数据库查询 | 数据分析、报表生成 |
| github | 代码仓库操作 | PR 审查、Issue 管理 |
| slack | 消息发送 | 团队通知、自动化 |
| browser | 网页浏览 | 信息收集、测试 |
| memory | 长期记忆 | 保存对话上下文 |

---

## 如何开始使用

### 1. 安装 Claude Code
\`\`\`bash
# macOS/Linux
npm install -g @anthropic-ai/claude-code

# 启动
claude
\`\`\`

### 2. 配置 MCP 服务器
\`\`\`json
// ~/.claude/claude_desktop_config.json
{
  "mcpServers": {
    "filesystem": {
      "command": "npx",
      "args": ["-y", "@anthropic-ai/mcp-server-filesystem", "/path/to/project"]
    },
    "github": {
      "command": "npx",
      "args": ["-y", "@anthropic-ai/mcp-server-github"],
      "env": {
        "GITHUB_TOKEN": "your-token"
      }
    }
  }
}
\`\`\`

### 3. 开始对话
\`\`\`
你: 帮我看看这个项目的代码结构
Claude: [读取项目文件...] 这是一个 React + TypeScript 项目...

你: 添加一个深色模式切换功能
Claude: [分析现有代码...] [创建 ThemeContext...] [修改组件...]
\`\`\`

---

## Boris 的高效工作流（来自 Claude Code 创始人）

Claude Code 创始人 Boris Cherny 分享了他的实战经验，这些技巧能让效率提升数倍。

### 1. 并行处理：5 个实例同时跑

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        Boris 的多实例工作流                              │
└─────────────────────────────────────────────────────────────────────────┘

  终端                                    Web 版
  ─────                                  ──────
  Tab 1: 功能开发                        Tab A: 代码审查
  Tab 2: Bug 修复                        Tab B: 文档生成
  Tab 3: 测试编写                        Tab C: 架构设计
  Tab 4: 重构优化                        ...
  Tab 5: 部署脚本

  ↓ 每个实例独立处理子任务 ↓

  并行开发 → 效率翻倍
\`\`\`

### 2. Plan 模式优先

> "大多数会话从 Plan 模式开始（Shift+Tab 两次），先设计再执行。"

\`\`\`
普通模式                              Plan 模式
────────                             ──────────
直接写代码                            先分析需求
边写边改                              设计方案
来回修改 3-5 次                       一次通过

效率: 低                              效率: 高
\`\`\`

### 3. Slash 命令自动化

将重复操作封装成命令：

\`\`\`bash
/commit-push-pr    # 一键：提交 + 推送 + 创建 PR
/verify-app        # 自动化端到端测试
/code-simplifier   # 写完代码自动简化
\`\`\`

### 4. 验证反馈循环

> "给 Claude 一个验证自己的方式，质量能提升 2-3 倍。"

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                          验证反馈循环                                    │
└─────────────────────────────────────────────────────────────────────────┘

                    ┌───────────────┐
                    │  编写代码     │
                    └───────┬───────┘
                            │
                            ▼
                    ┌───────────────┐
                    │  自动测试     │ ← 单元测试 / E2E 测试
                    └───────┬───────┘
                            │
                    ┌───────┴───────┐
                    │               │
                    ▼               ▼
              ┌─────────┐     ┌─────────┐
              │  通过   │     │  失败   │
              └─────────┘     └────┬────┘
                                   │
                                   ▼
                            ┌───────────────┐
                            │  自动修复     │
                            └───────┬───────┘
                                    │
                                    └──────▶ 重新测试
\`\`\`

---

## Sub-agents：专家团队协作

Claude Code 支持创建专门的 Sub-agents，每个专注一个领域：

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                       Sub-agents 架构                                   │
└─────────────────────────────────────────────────────────────────────────┘

                           主 Agent
                              │
        ┌─────────────────────┼─────────────────────┐
        │                     │                     │
        ▼                     ▼                     ▼
  ┌───────────┐         ┌───────────┐         ┌───────────┐
  │ 代码审查员 │         │  测试专家  │         │ 文档作者  │
  │           │         │           │         │           │
  │ - 检查质量 │         │ - 编写测试 │         │ - 生成文档 │
  │ - 安全审计 │         │ - 覆盖率   │         │ - API 说明 │
  │ - 性能建议 │         │ - 边界用例 │         │ - 示例代码 │
  └───────────┘         └───────────┘         └───────────┘
        │                     │                     │
        └─────────────────────┼─────────────────────┘
                              │
                              ▼
                        整合所有结果
\`\`\`

### 配置 Sub-agent

\`\`\`yaml
# .claude/agents/code-reviewer.md
---
name: code-reviewer
description: 专门审查代码质量和安全性
tools:
  - Read
  - Grep
  - Glob
---

你是一位资深代码审查专家。重点关注：
1. 代码质量和可读性
2. 安全漏洞
3. 性能问题
\`\`\`

---

## Spec-Driven 开发：告别 Vibe Coding

传统的 "Vibe Coding"（边想边写）效率低下，Spec-Driven 开发是更好的选择：

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                   Spec-Driven vs Vibe Coding                            │
└─────────────────────────────────────────────────────────────────────────┘

  Vibe Coding                           Spec-Driven
  ────────────                          ───────────
  ┌─────────────┐                      ┌─────────────┐
  │  想到什么    │                      │  需求分析   │
  │  写什么     │                      └──────┬──────┘
  └──────┬──────┘                             │
         │                                    ▼
         ▼                             ┌─────────────┐
  ┌─────────────┐                      │  编写规格   │
  │  出 bug     │                      │  (spec.md)  │
  └──────┬──────┘                      └──────┬──────┘
         │                                    │
         ▼                                    ▼
  ┌─────────────┐                      ┌─────────────┐
  │  反复修改   │                      │  按规格实现 │
  └──────┬──────┘                      └──────┬──────┘
         │                                    │
         ▼                                    ▼
  ┌─────────────┐                      ┌─────────────┐
  │  又出 bug   │                      │  一次通过   │
  └─────────────┘                      └─────────────┘

  效率: ★★☆☆☆                         效率: ★★★★★
\`\`\`

---

## 本节要点

1. **MCP** —— AI 与外部工具的标准化接口
2. **Claude Code** —— 基于 MCP 的 AI 编程助手
3. **并行处理** —— 多实例同时工作，效率翻倍
4. **验证循环** —— 自动测试确保代码质量
5. **Sub-agents** —— 专业分工，各司其职
6. **Spec-Driven** —— 先设计后实现，一次通过
            `,ja:`
## MCPとは？

**MCP（Model Context Protocol）** は、Anthropicが開発したオープンプロトコルで、AIが様々な外部ツールやデータソースに安全に接続できるようにします。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                         MCP アーキテクチャ概要                           │
└─────────────────────────────────────────────────────────────────────────┘

                           ┌───────────────┐
                           │   AIモデル    │
                           │  (Claude等)   │
                           └───────┬───────┘
                                   │
                           ┌───────▼───────┐
                           │  MCPプロトコル │
                           └───────┬───────┘
                                   │
      ┌────────────────────────────┼────────────────────────────┐
      │                            │                            │
      ▼                            ▼                            ▼
┌───────────┐              ┌───────────┐              ┌───────────┐
│ファイルシステム│              │ データベース │              │ APIサービス │
│ MCPサーバー │              │ MCPサーバー │              │ MCPサーバー │
└───────────┘              └───────────┘              └───────────┘
      │                            │                            │
      ▼                            ▼                            ▼
  ローカルファイル                PostgreSQL                  GitHub/Slack
                               MySQL                      Notion/Jira
\`\`\`

---

## なぜMCPが重要なのか？

以前の問題：
- 各AI製品が個別にツール統合を開発
- データソース接続方法がバラバラ
- 再利用や標準化が困難

MCPが解決したこと：
\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                         統一されたツールインターフェース                    │
└─────────────────────────────────────────────────────────────────────────┘

  旧モデル                              新モデル（MCP）
  ─────────                            ──────────────
  ChatGPT ─┬─ ファイル読取              Claude ──┐
           ├─ Web検索                   ChatGPT ─┼── MCP ──┬─ 任意のツール
           └─ API呼出                   他のAI ──┘         └─ 任意のデータ

  Claude ──┬─ ファイル読取
           ├─ Web検索     ──▶          一度開発、どこでも利用可能
           └─ API呼出

  重複開発 ✗                           標準化再利用 ✓
\`\`\`

---

## Claude Code：MCPの実践活用

**Claude Code** はMCPベースのAIプログラミングアシスタントで、開発環境を直接操作できます。

### コア機能

| 機能 | 説明 | 例 |
|------|------|------|
| 🔍 コード理解 | プロジェクト全体を読み取り分析 | 「このモジュールのアーキテクチャを説明して」 |
| ✏️ コード修正 | ファイルを直接編集 | 「このバグを修正して」 |
| 💻 コマンド実行 | ターミナルコマンドを実行 | 「テストを実行して失敗を修正して」 |
| 🔗 ツール統合 | MCPで機能拡張 | 「DBのユーザーデータを取得して」 |

### 実際のワークフロー

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                     Claude Code ワークフロー                             │
└─────────────────────────────────────────────────────────────────────────┘

ユーザー: 「ユーザーログイン機能を実装して」
                │
                ▼
        ┌───────────────┐
        │ 1. プロジェクト分析 │  ← 既存コードを読み取り、アーキテクチャを理解
        └───────┬───────┘
                │
                ▼
        ┌───────────────┐
        │ 2. 計画立案    │  ← 作成/修正が必要なファイルを特定
        └───────┬───────┘
                │
                ▼
        ┌───────────────┐
        │ 3. コード作成  │  ← ルート、コントローラー、モデルを作成
        └───────┬───────┘
                │
                ▼
        ┌───────────────┐
        │ 4. テスト検証  │  ← テスト実行、エラー確認
        └───────┬───────┘
                │
                ▼
        ┌───────────────┐
        │ 5. 反復改善    │  ← フィードバックに基づき最適化
        └───────────────┘
\`\`\`

---

## よく使うMCPサーバー

| MCPサーバー | 機能 | 使用シーン |
|-----------|------|----------|
| filesystem | ファイル読み書き | コード編集、ドキュメント処理 |
| postgres/mysql | DBクエリ | データ分析、レポート生成 |
| github | コードリポジトリ操作 | PRレビュー、Issue管理 |
| slack | メッセージ送信 | チーム通知、自動化 |
| browser | Webブラウジング | 情報収集、テスト |
| memory | 長期記憶 | 会話コンテキストの保存 |

---

## 始め方

### 1. Claude Codeをインストール
\`\`\`bash
# macOS/Linux
npm install -g @anthropic-ai/claude-code

# 起動
claude
\`\`\`

### 2. MCPサーバーを設定
\`\`\`json
// ~/.claude/claude_desktop_config.json
{
  "mcpServers": {
    "filesystem": {
      "command": "npx",
      "args": ["-y", "@anthropic-ai/mcp-server-filesystem", "/path/to/project"]
    },
    "github": {
      "command": "npx",
      "args": ["-y", "@anthropic-ai/mcp-server-github"],
      "env": {
        "GITHUB_TOKEN": "your-token"
      }
    }
  }
}
\`\`\`

### 3. 対話を開始
\`\`\`
あなた: このプロジェクトのコード構造を見て
Claude: [プロジェクトファイルを読み取り中...] React + TypeScriptプロジェクトです...

あなた: ダークモード切替機能を追加して
Claude: [既存コードを分析中...] [ThemeContextを作成...] [コンポーネントを修正...]
\`\`\`

---

## Borisの効率的ワークフロー（Claude Code創設者より）

Claude Code創設者Boris Chernyが実践的なテクニックを共有。効率が数倍向上します。

### 1. 並列処理：5インスタンス同時実行

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                      Borisのマルチインスタンスワークフロー                  │
└─────────────────────────────────────────────────────────────────────────┘

  ターミナル                              Web版
  ────────                              ─────
  Tab 1: 機能開発                        Tab A: コードレビュー
  Tab 2: バグ修正                        Tab B: ドキュメント生成
  Tab 3: テスト作成                      Tab C: アーキテクチャ設計
  Tab 4: リファクタリング                ...
  Tab 5: デプロイスクリプト

  ↓ 各インスタンスが独立してサブタスクを処理 ↓

  並列開発 → 効率倍増
\`\`\`

### 2. Planモード優先

> "ほとんどのセッションはPlanモードから開始（Shift+Tab 2回）、設計してから実行。"

### 3. Slashコマンド自動化

\`\`\`bash
/commit-push-pr    # ワンクリック：コミット + プッシュ + PR作成
/verify-app        # 自動E2Eテスト
/code-simplifier   # コード作成後に自動簡素化
\`\`\`

### 4. 検証フィードバックループ

> "Claudeに自己検証の方法を与えると、品質が2-3倍向上する。"

---

## Sub-agents：専門家チーム協力

Claude Codeは専門のSub-agentsをサポート、各自が1つの領域に集中：

\`\`\`yaml
# .claude/agents/code-reviewer.md
---
name: code-reviewer
description: コード品質とセキュリティを専門にレビュー
tools:
  - Read
  - Grep
  - Glob
---

あなたはシニアコードレビュアーです。重点：
1. コード品質と可読性
2. セキュリティ脆弱性
3. パフォーマンス問題
\`\`\`

---

## Spec-Driven開発：Vibe Codingからの脱却

従来の"Vibe Coding"（思いつきでコーディング）は非効率。Spec-Driven開発がより良い選択：

| アプローチ | 特徴 | 効率 |
|-----------|------|------|
| Vibe Coding | 思いつきで書く→バグ→修正の繰り返し | ★★☆☆☆ |
| Spec-Driven | 要件分析→仕様作成→実装 | ★★★★★ |

---

## このセクションのポイント

1. **MCP** —— AIと外部ツールの標準化インターフェース
2. **Claude Code** —— MCPベースのAIプログラミングアシスタント
3. **並列処理** —— 複数インスタンス同時作業で効率倍増
4. **検証ループ** —— 自動テストでコード品質を確保
5. **Sub-agents** —— 専門分業、各自の役割
6. **Spec-Driven** —— 設計先行、一発で通す
            `}},{id:"ch3-claude-code-commands",title:{zh:"3.4 Claude Code 命令详解",ja:"3.4 Claude Code コマンド詳解"},content:{zh:`
## Claude Code 命令大全

Claude Code 提供了丰富的斜杠命令（Slash Commands）来提升开发效率。掌握这些命令能让你的 AI 编程体验更上一层楼。

---

## 核心命令一览

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                     Claude Code 命令分类                                 │
└─────────────────────────────────────────────────────────────────────────┘

  基础命令                  MCP 命令               会话管理
  ────────                  ────────               ────────
  /help                    /mcp                   /clear
  /status                  /mcp add               /compact
  /config                  /mcp remove            /memory
  /model                   /mcp list              /resume
                           /mcp logs

  开发命令                  文件操作               工具命令
  ────────                  ────────               ────────
  /init                    /read                  /terminal
  /plan                    /edit                  /web
  /commit                  /write                 /browser
  /pr                      /search                /cost
\`\`\`

---

## 🔧 MCP 命令详解

MCP（Model Context Protocol）命令是 Claude Code 最强大的扩展能力。

### /mcp - 查看 MCP 状态

\`\`\`bash
/mcp
# 显示当前已连接的 MCP 服务器列表和状态
\`\`\`

**使用场景**：
- 检查 MCP 服务器是否正常运行
- 查看可用的工具和资源
- 调试连接问题

### /mcp add - 添加 MCP 服务器

\`\`\`bash
# 添加 GitHub MCP 服务器
/mcp add github

# 添加数据库 MCP 服务器
/mcp add postgres

# 添加自定义 MCP 服务器
/mcp add my-custom-server --command "node server.js"
\`\`\`

**使用场景**：
- 连接 GitHub 进行 PR 管理
- 连接数据库进行数据分析
- 添加自定义工具扩展能力

### /mcp logs - 查看 MCP 日志

\`\`\`bash
/mcp logs
# 查看 MCP 服务器的运行日志，用于调试

/mcp logs github
# 查看特定服务器的日志
\`\`\`

---

## 📝 会话管理命令

### /clear - 清空会话

\`\`\`bash
/clear
# 清空当前对话历史，开始全新会话
\`\`\`

**使用场景**：
- 切换到新任务时清理上下文
- 对话过长导致响应变慢时重置
- 避免旧对话影响新任务

### /compact - 压缩上下文

\`\`\`bash
/compact
# 智能压缩对话历史，保留关键信息
\`\`\`

**使用场景**：
- 对话过长时节省 tokens
- 保留重要上下文同时减少内存占用
- 长时间编程会话中定期执行

### /memory - 长期记忆

\`\`\`bash
/memory
# 查看和管理长期记忆

/memory add "项目使用 React + TypeScript"
# 添加长期记忆，后续会话都会记住
\`\`\`

**使用场景**：
- 保存项目架构信息
- 记住编码偏好（如缩进风格）
- 跨会话保持一致性

---

## 🚀 开发命令

### /init - 初始化项目

\`\`\`bash
/init
# Claude 分析当前目录，生成 CLAUDE.md 配置文件
\`\`\`

**使用场景**：
- 新项目首次使用 Claude Code
- 让 Claude 理解项目结构和技术栈
- 生成项目特定的指令配置

### /plan - 进入计划模式

\`\`\`bash
/plan
# 切换到计划模式，只做分析和设计，不执行修改

# 快捷键：Shift + Tab 两次
\`\`\`

**使用场景**：
- 复杂功能开发前的架构设计
- 代码审查和分析
- 学习理解代码库

### /commit - 提交代码

\`\`\`bash
/commit
# 自动生成 commit message 并提交
\`\`\`

**使用场景**：
- 完成功能开发后快速提交
- AI 自动分析变更生成 commit 信息
- 保持 commit 历史清晰

### /pr - 创建 Pull Request

\`\`\`bash
/pr
# 创建 PR 并生成描述
\`\`\`

**使用场景**：
- 完成功能后自动创建 PR
- AI 生成清晰的 PR 描述
- 包含变更摘要和测试说明

---

## 🔍 文件操作命令

### /search - 搜索代码

\`\`\`bash
/search "function handleClick"
# 在项目中搜索代码

/search --type ts "useState"
# 只搜索 TypeScript 文件
\`\`\`

**使用场景**：
- 快速定位代码位置
- 查找函数定义和引用
- 分析代码模式

---

## ⚙️ 配置命令

### /config - 查看/修改配置

\`\`\`bash
/config
# 查看当前配置

/config set model claude-opus-4-5-20250514
# 设置默认模型

/config set theme dark
# 设置主题
\`\`\`

### /model - 切换模型

\`\`\`bash
/model
# 查看可用模型

/model claude-opus-4-5-20250514
# 切换到 Opus 4.5
\`\`\`

**使用场景**：
- 简单任务用 Haiku 节省成本
- 复杂任务切换到 Opus 获得最佳效果
- 根据任务类型灵活选择

---

## 💰 实用命令

### /cost - 查看使用成本

\`\`\`bash
/cost
# 显示当前会话的 token 使用和费用
\`\`\`

**使用场景**：
- 监控 API 使用量
- 优化 prompt 降低成本
- 预算管理

### /status - 查看状态

\`\`\`bash
/status
# 显示连接状态、模型信息、配置等
\`\`\`

---

## 🎯 高效工作流示例

### 示例 1：新功能开发

\`\`\`bash
# 1. 先进入计划模式，设计方案
/plan
你: 我需要实现用户认证功能

# 2. 确认方案后退出计划模式执行
# Shift + Tab 切换到 Auto 模式

# 3. 开发完成后提交
/commit

# 4. 创建 PR
/pr
\`\`\`

### 示例 2：调试 MCP 问题

\`\`\`bash
# 1. 检查 MCP 状态
/mcp

# 2. 查看日志定位问题
/mcp logs

# 3. 重新添加服务器
/mcp remove github
/mcp add github
\`\`\`

### 示例 3：长会话管理

\`\`\`bash
# 1. 对话过长时压缩
/compact

# 2. 添加重要信息到长期记忆
/memory add "使用 pnpm 而非 npm"

# 3. 切换任务时清空
/clear
\`\`\`

---

## 📋 命令速查表

| 命令 | 快捷方式 | 说明 |
|------|----------|------|
| /help | - | 显示帮助信息 |
| /clear | Cmd/Ctrl+K | 清空会话 |
| /compact | - | 压缩上下文 |
| /mcp | - | 查看 MCP 状态 |
| /plan | Shift+Tab×2 | 进入计划模式 |
| /commit | - | 提交代码 |
| /pr | - | 创建 PR |
| /cost | - | 查看费用 |
| /model | - | 切换模型 |

---

## 本节要点

1. **MCP 命令** —— 管理外部工具连接，扩展 AI 能力
2. **会话管理** —— /clear、/compact、/memory 控制对话
3. **开发命令** —— /plan、/commit、/pr 覆盖完整开发流程
4. **配置命令** —— /config、/model 自定义工作环境
5. **实用技巧** —— 组合使用命令实现高效工作流
            `,ja:`
## Claude Code コマンド大全

Claude Codeは豊富なスラッシュコマンドを提供し、開発効率を向上させます。これらのコマンドをマスターすれば、AIプログラミング体験がさらに向上します。

---

## コアコマンド一覧

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                     Claude Code コマンド分類                             │
└─────────────────────────────────────────────────────────────────────────┘

  基本コマンド              MCPコマンド             セッション管理
  ──────────              ──────────             ────────────
  /help                    /mcp                   /clear
  /status                  /mcp add               /compact
  /config                  /mcp remove            /memory
  /model                   /mcp list              /resume
                           /mcp logs

  開発コマンド              ファイル操作            ツールコマンド
  ──────────              ──────────              ────────────
  /init                    /read                  /terminal
  /plan                    /edit                  /web
  /commit                  /write                 /browser
  /pr                      /search                /cost
\`\`\`

---

## 🔧 MCPコマンド詳解

MCP（Model Context Protocol）コマンドはClaude Codeの最強の拡張機能です。

### /mcp - MCP状態確認

\`\`\`bash
/mcp
# 接続中のMCPサーバーリストと状態を表示
\`\`\`

**使用シーン**：
- MCPサーバーが正常に動作しているか確認
- 利用可能なツールとリソースを確認
- 接続問題のデバッグ

### /mcp add - MCPサーバー追加

\`\`\`bash
# GitHub MCPサーバーを追加
/mcp add github

# データベースMCPサーバーを追加
/mcp add postgres

# カスタムMCPサーバーを追加
/mcp add my-custom-server --command "node server.js"
\`\`\`

**使用シーン**：
- GitHubに接続してPR管理
- データベースに接続してデータ分析
- カスタムツールで機能拡張

---

## 📝 セッション管理コマンド

### /clear - セッションクリア

\`\`\`bash
/clear
# 現在の会話履歴をクリアし、新しいセッションを開始
\`\`\`

**使用シーン**：
- 新しいタスクに切り替える時にコンテキストをクリア
- 会話が長くなり応答が遅くなった時にリセット
- 古い会話が新しいタスクに影響するのを防ぐ

### /compact - コンテキスト圧縮

\`\`\`bash
/compact
# 会話履歴をスマートに圧縮、重要な情報を保持
\`\`\`

**使用シーン**：
- 会話が長くなった時にtokensを節約
- 重要なコンテキストを保持しつつメモリ使用量を削減
- 長時間のプログラミングセッションで定期的に実行

---

## 🚀 開発コマンド

### /plan - 計画モードに入る

\`\`\`bash
/plan
# 計画モードに切り替え、分析と設計のみ、変更は実行しない

# ショートカット：Shift + Tab 2回
\`\`\`

**使用シーン**：
- 複雑な機能開発前のアーキテクチャ設計
- コードレビューと分析
- コードベースの理解と学習

### /commit - コードコミット

\`\`\`bash
/commit
# 自動的にcommit messageを生成してコミット
\`\`\`

### /pr - Pull Request作成

\`\`\`bash
/pr
# PRを作成して説明を生成
\`\`\`

---

## 📋 コマンド早見表

| コマンド | ショートカット | 説明 |
|---------|--------------|------|
| /help | - | ヘルプ情報を表示 |
| /clear | Cmd/Ctrl+K | セッションをクリア |
| /compact | - | コンテキストを圧縮 |
| /mcp | - | MCP状態を確認 |
| /plan | Shift+Tab×2 | 計画モードに入る |
| /commit | - | コードをコミット |
| /pr | - | PRを作成 |
| /cost | - | 費用を確認 |
| /model | - | モデルを切り替え |

---

## 本節のポイント

1. **MCPコマンド** —— 外部ツール接続を管理、AI能力を拡張
2. **セッション管理** —— /clear、/compact、/memoryで会話を制御
3. **開発コマンド** —— /plan、/commit、/prで開発フロー全体をカバー
4. **設定コマンド** —— /config、/modelで作業環境をカスタマイズ
5. **実用テクニック** —— コマンドを組み合わせて効率的なワークフローを実現
            `}},{id:"ch3-agent-skills",title:{zh:"3.5 Agent Skills 详解",ja:"3.5 Agent Skills 詳解"},content:{zh:`
## Agent Skills：让 AI 学会新技能

Agent Skills 是 Claude Code 最强大的定制化功能。通过自定义 Skills，你可以让 AI 掌握特定的工作流程，实现一键执行复杂任务。

---

## 什么是 Agent Skills？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        Agent Skills 架构                                 │
└─────────────────────────────────────────────────────────────────────────┘

  用户请求                Skills 定义               AI 执行
  ────────               ──────────               ────────
  "运行 /deploy"    →    .claude/skills/         →   自动执行
                         deploy.md                   部署流程
                              │
                              ▼
                    ┌─────────────────┐
                    │ • 触发条件       │
                    │ • 执行步骤       │
                    │ • 工具调用       │
                    │ • 输出格式       │
                    └─────────────────┘
\`\`\`

**核心概念**：
- **Skills** = 可复用的 AI 工作流程定义
- **存储位置**：\`.claude/skills/\` 目录下的 Markdown 文件
- **触发方式**：通过 \`/skillname\` 或自然语言触发
- **跨平台通用**：OpenAI Codex 已采用相同规范

---

## 创建你的第一个 Skill

### 示例 1：代码审查 Skill

\`\`\`markdown
# .claude/skills/review.md

---
name: review
description: 执行全面的代码审查
triggers:
  - /review
  - 帮我审查代码
  - code review
---

# 代码审查流程

## 执行步骤

1. **获取变更文件**
   - 运行 \`git diff --name-only HEAD~1\` 获取变更文件列表
   - 如果用户指定了文件，则只审查指定文件

2. **代码分析**
   对每个文件检查以下方面：
   - 代码逻辑正确性
   - 潜在的 Bug 和边界情况
   - 性能问题
   - 安全漏洞（SQL 注入、XSS 等）
   - 代码风格一致性

3. **生成报告**
   使用以下格式输出：

   \`\`\`
   ## 📝 代码审查报告

   ### 文件：{filename}

   #### ✅ 优点
   - ...

   #### ⚠️ 建议改进
   - 行 {line}: {issue}

   #### 🔴 必须修复
   - 行 {line}: {critical_issue}
   \`\`\`

4. **提供修复建议**
   对于每个问题，提供具体的修复代码示例
\`\`\`

### 示例 2：自动化部署 Skill

\`\`\`markdown
# .claude/skills/deploy.md

---
name: deploy
description: 一键部署到生产环境
triggers:
  - /deploy
  - 部署到生产
  - deploy to production
---

# 部署流程

## 前置检查

1. 确认当前分支是 main 或 master
2. 确认没有未提交的更改
3. 确认所有测试通过

## 执行步骤

1. **运行测试**
   \`\`\`bash
   npm test
   \`\`\`
   如果测试失败，停止部署并报告错误

2. **构建项目**
   \`\`\`bash
   npm run build
   \`\`\`

3. **版本更新**
   - 读取 package.json 中的版本号
   - 询问用户选择版本更新类型（patch/minor/major）
   - 更新版本号

4. **创建 Git Tag**
   \`\`\`bash
   git tag v{version}
   git push origin v{version}
   \`\`\`

5. **部署确认**
   输出部署摘要，等待用户确认后执行部署命令
\`\`\`

---

## Skill 高级技巧

### 1. 使用变量和参数

\`\`\`markdown
# .claude/skills/create-component.md

---
name: create-component
description: 创建 React 组件
triggers:
  - /component {name}
  - 创建组件 {name}
---

# 创建 React 组件

## 参数
- \`{name}\`: 组件名称（必需）

## 执行步骤

1. 在 \`src/components/{name}/\` 目录下创建：
   - \`{name}.tsx\` - 组件代码
   - \`{name}.test.tsx\` - 测试文件
   - \`{name}.css\` - 样式文件
   - \`index.ts\` - 导出文件

2. 组件模板：
   \`\`\`tsx
   import React from 'react';
   import './{name}.css';

   interface {name}Props {
     // 定义 props
   }

   export const {name}: React.FC<{name}Props> = (props) => {
     return (
       <div className="{name}">
         {/* 组件内容 */}
       </div>
     );
   };
   \`\`\`
\`\`\`

### 2. 条件分支执行

\`\`\`markdown
# .claude/skills/fix.md

---
name: fix
description: 智能修复问题
triggers:
  - /fix
  - 帮我修复
---

# 智能修复流程

## 问题诊断

1. **检测问题类型**
   - 如果是 TypeScript 错误：运行 \`npx tsc --noEmit\` 获取错误列表
   - 如果是 ESLint 错误：运行 \`npm run lint\` 获取警告
   - 如果是测试失败：运行 \`npm test\` 查看失败用例
   - 如果是构建错误：分析构建日志

2. **根据类型执行修复**

   ### TypeScript 错误
   - 定位错误文件和行号
   - 分析类型错误原因
   - 提供类型修复方案

   ### ESLint 错误
   - 尝试自动修复：\`npm run lint -- --fix\`
   - 手动修复无法自动处理的问题

   ### 测试失败
   - 分析失败原因
   - 修复代码或更新测试用例
\`\`\`

### 3. 团队共享 Skills

\`\`\`bash
# 项目级 Skills（团队共享）
.claude/
└── skills/
    ├── review.md      # 团队代码审查规范
    ├── deploy.md      # 部署流程
    └── onboard.md     # 新人引导

# 用户级 Skills（个人专用）
~/.claude/
└── skills/
    ├── my-shortcuts.md
    └── personal-workflow.md
\`\`\`

---

## 🔥 推荐开源 Skills 资源

### awesome-claude-skills

GitHub 上最全的 Claude Skills 资源集合：

\`\`\`bash
# 仓库地址
https://github.com/travisvn/awesome-claude-skills

# 包含内容
├── 📁 skills/           # 数百个即用 Skills
│   ├── code-review/     # 代码审查
│   ├── deployment/      # 部署自动化
│   ├── testing/         # 测试生成
│   └── documentation/   # 文档生成
├── 📁 templates/        # Skills 模板
└── 📁 examples/         # 使用示例
\`\`\`

**快速使用**：
\`\`\`bash
# 克隆仓库
git clone https://github.com/travisvn/awesome-claude-skills.git

# 复制你需要的 Skills 到项目
cp -r awesome-claude-skills/skills/code-review .claude/skills/
\`\`\`

### awesome-claude-code

Claude Code 综合资源列表：

\`\`\`bash
# 仓库地址
https://github.com/hesreallyhim/awesome-claude-code

# 包含资源
- Skills 集合
- MCP 服务器列表
- 最佳实践指南
- 社区工具推荐
\`\`\`

---

## 实战案例：完整项目 Skills 配置

\`\`\`bash
# 项目目录结构
my-project/
├── .claude/
│   ├── skills/
│   │   ├── review.md        # 代码审查
│   │   ├── deploy.md        # 部署流程
│   │   ├── component.md     # 组件生成
│   │   ├── api.md           # API 开发
│   │   └── debug.md         # 调试助手
│   └── settings.json        # Hooks 配置
├── CLAUDE.md                # 项目说明
└── src/
\`\`\`

### 推荐的 Skills 集合

| Skill | 命令 | 用途 |
|-------|------|------|
| review | /review | 代码审查 |
| deploy | /deploy | 一键部署 |
| component | /component Name | 创建组件 |
| api | /api endpoint | 创建 API 端点 |
| debug | /debug | 智能调试 |
| test | /test | 运行并修复测试 |
| doc | /doc | 生成文档 |
| refactor | /refactor | 代码重构 |

---

## 本节要点

1. **Skills 定义** —— 在 \`.claude/skills/\` 目录下创建 Markdown 文件
2. **触发方式** —— 使用 \`/skillname\` 或自然语言触发
3. **参数传递** —— 使用 \`{参数名}\` 语法接收用户输入
4. **开源资源** —— awesome-claude-skills 提供数百个即用 Skills
5. **团队共享** —— 项目级 Skills 实现团队标准化
            `,ja:`
## Agent Skills：AIに新しいスキルを学ばせる

Agent SkillsはClaude Codeの最も強力なカスタマイズ機能です。カスタムSkillsを通じて、AIに特定のワークフローを習得させ、複雑なタスクをワンクリックで実行できます。

---

## Agent Skillsとは？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        Agent Skills アーキテクチャ                        │
└─────────────────────────────────────────────────────────────────────────┘

  ユーザーリクエスト        Skills定義                AI実行
  ──────────────         ──────────               ────────
  "/deployを実行"    →    .claude/skills/         →   自動で
                         deploy.md                   デプロイ実行
                              │
                              ▼
                    ┌─────────────────┐
                    │ • トリガー条件   │
                    │ • 実行ステップ   │
                    │ • ツール呼び出し │
                    │ • 出力形式      │
                    └─────────────────┘
\`\`\`

**コアコンセプト**：
- **Skills** = 再利用可能なAIワークフロー定義
- **保存場所**：\`.claude/skills/\` ディレクトリ内のMarkdownファイル
- **トリガー方法**：\`/skillname\` または自然言語でトリガー
- **クロスプラットフォーム互換**：OpenAI Codexも同じ仕様を採用

---

## 最初のSkillを作成する

### 例1：コードレビュー Skill

\`\`\`markdown
# .claude/skills/review.md

---
name: review
description: 包括的なコードレビューを実行
triggers:
  - /review
  - コードレビューして
  - code review
---

# コードレビュープロセス

## 実行ステップ

1. **変更ファイルを取得**
   - \`git diff --name-only HEAD~1\` で変更ファイルリストを取得
   - ユーザーが指定した場合は、指定ファイルのみレビュー

2. **コード分析**
   各ファイルで以下をチェック：
   - コードロジックの正確性
   - 潜在的なバグと境界ケース
   - パフォーマンス問題
   - セキュリティ脆弱性（SQLインジェクション、XSSなど）
   - コードスタイルの一貫性

3. **レポート生成**
   以下の形式で出力：

   \`\`\`
   ## 📝 コードレビューレポート

   ### ファイル：{filename}

   #### ✅ 良い点
   - ...

   #### ⚠️ 改善提案
   - 行 {line}: {issue}

   #### 🔴 必須修正
   - 行 {line}: {critical_issue}
   \`\`\`
\`\`\`

---

## Skill 高度なテクニック

### 1. 変数とパラメータの使用

\`\`\`markdown
# .claude/skills/create-component.md

---
name: create-component
description: Reactコンポーネントを作成
triggers:
  - /component {name}
  - コンポーネント作成 {name}
---

# Reactコンポーネント作成

## パラメータ
- \`{name}\`: コンポーネント名（必須）

## 実行ステップ

1. \`src/components/{name}/\` ディレクトリに作成：
   - \`{name}.tsx\` - コンポーネントコード
   - \`{name}.test.tsx\` - テストファイル
   - \`{name}.css\` - スタイルファイル
   - \`index.ts\` - エクスポートファイル
\`\`\`

### 2. チーム共有Skills

\`\`\`bash
# プロジェクトレベルSkills（チーム共有）
.claude/
└── skills/
    ├── review.md      # チームコードレビュー規範
    ├── deploy.md      # デプロイフロー
    └── onboard.md     # 新人ガイド

# ユーザーレベルSkills（個人専用）
~/.claude/
└── skills/
    ├── my-shortcuts.md
    └── personal-workflow.md
\`\`\`

---

## 🔥 おすすめオープンソースSkillsリソース

### awesome-claude-skills

GitHubで最も完全なClaude Skillsリソース集：

\`\`\`bash
# リポジトリURL
https://github.com/travisvn/awesome-claude-skills

# 含まれる内容
├── 📁 skills/           # 数百の即戦力Skills
├── 📁 templates/        # Skillsテンプレート
└── 📁 examples/         # 使用例
\`\`\`

### awesome-claude-code

Claude Code総合リソースリスト：

\`\`\`bash
# リポジトリURL
https://github.com/hesreallyhim/awesome-claude-code
\`\`\`

---

## 本節のポイント

1. **Skills定義** —— \`.claude/skills/\` ディレクトリにMarkdownファイルを作成
2. **トリガー方法** —— \`/skillname\` または自然言語でトリガー
3. **パラメータ渡し** —— \`{パラメータ名}\` 構文でユーザー入力を受け取る
4. **オープンソースリソース** —— awesome-claude-skillsで数百の即戦力Skills
5. **チーム共有** —— プロジェクトレベルSkillsでチーム標準化を実現
            `}},{id:"ch3-subagents",title:{zh:"3.6 Sub-agents 多 Agent 协作",ja:"3.6 Sub-agents マルチAgent協力"},content:{zh:`
## Sub-agents：专家团队协作模式

Sub-agents 是 Claude Code 的高级功能，让你可以将复杂任务分解给多个专家 Agent 并行处理，大幅提升开发效率。

---

## Sub-agents 架构

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        Sub-agents 协作架构                               │
└─────────────────────────────────────────────────────────────────────────┘

                    ┌─────────────────┐
                    │   主 Agent      │
                    │  (Coordinator)  │
                    │  协调分配任务    │
                    └────────┬────────┘
                             │
           ┌─────────────────┼─────────────────┐
           ▼                 ▼                 ▼
   ┌───────────────┐ ┌───────────────┐ ┌───────────────┐
   │   代码专家     │ │   测试专家     │ │   文档专家     │
   │  (Code Agent) │ │ (Test Agent)  │ │  (Doc Agent)  │
   │   独立上下文   │ │   独立上下文   │ │   独立上下文   │
   └───────────────┘ └───────────────┘ └───────────────┘
           │                 │                 │
           ▼                 ▼                 ▼
     编写功能代码      编写测试用例       生成 API 文档
\`\`\`

**核心优势**：
- **上下文隔离** —— 每个 Agent 独立上下文，避免信息污染
- **专业化分工** —— 专注特定任务，成功率更高
- **并行处理** —— 多任务同时进行，效率倍增

---

## 使用 Sub-agents

### 对话中触发

\`\`\`markdown
用户：请帮我完成用户认证功能，需要代码、测试和文档

Claude：我会安排三个专家 Agent 并行工作：

1. **代码专家** - 实现 JWT 认证逻辑
2. **测试专家** - 编写单元测试和集成测试
3. **文档专家** - 生成 API 文档和使用说明

[启动 Sub-agents...]

// 主 Agent 协调，各专家独立工作
// 最终汇总所有输出
\`\`\`

### 通过 Skill 定义

\`\`\`markdown
# .claude/skills/full-feature.md

---
name: full-feature
description: 完整功能开发（代码+测试+文档）
---

## 执行步骤

1. **启动代码 Agent**
   - 专注于功能实现
   - 遵循项目代码规范
   - 输出：功能代码文件

2. **启动测试 Agent**
   - 等待代码 Agent 完成
   - 编写单元测试覆盖所有函数
   - 编写集成测试覆盖主要流程
   - 输出：测试文件

3. **启动文档 Agent**
   - 分析代码结构
   - 生成 JSDoc 注释
   - 更新 README
   - 输出：文档更新

4. **汇总报告**
   合并三个 Agent 的输出，生成完成报告
\`\`\`

---

## 🔥 多 Agent 管理工具

### Claude Squad ⭐ 4.3k

管理多个 AI Agent 的终端工具，支持 Claude Code、Codex、Gemini、Aider 等：

\`\`\`bash
# 安装
go install github.com/smtg-ai/claude-squad@latest

# 或使用 Homebrew
brew install smtg-ai/tap/claude-squad

# 启动
claude-squad
\`\`\`

**核心功能**：
\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│  Claude Squad - 多 Agent 终端管理器                                      │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  Workspace 1: [Claude Code] 正在实现用户认证...                          │
│  Workspace 2: [Codex] 正在编写测试用例...                                │
│  Workspace 3: [Aider] 正在重构数据库层...                                │
│  Workspace 4: [Gemini] 正在生成文档...                                   │
│                                                                         │
│  快捷键: [n]新建 [d]删除 [Tab]切换 [Enter]进入 [q]退出                   │
└─────────────────────────────────────────────────────────────────────────┘
\`\`\`

**使用场景**：
- 同时处理多个独立任务
- 不同 AI 工具各司其职
- 快速切换工作上下文

---

## Hooks：事件触发自动化

Hooks 允许你在特定事件发生时自动执行脚本，实现工作流自动化。

### 可用的 Hooks

| Hook 名称 | 触发时机 | 用途 |
|----------|---------|------|
| PreToolUse | 工具调用前 | 拦截和验证 |
| PostToolUse | 工具调用后 | 日志和后处理 |
| Notification | 通知事件 | 自定义提醒 |
| Stop | 会话结束 | 清理和总结 |

### 配置示例

\`\`\`json
// .claude/settings.json
{
  "hooks": {
    "PreToolUse": [
      {
        "matcher": "Bash",
        "command": "echo '即将执行: $TOOL_INPUT'"
      }
    ],
    "PostToolUse": [
      {
        "matcher": "Write",
        "command": "npx prettier --write $FILE_PATH"
      }
    ],
    "Stop": [
      {
        "command": "echo '会话结束' >> ~/.claude/session.log"
      }
    ]
  }
}
\`\`\`

### 实用 Hook 示例

\`\`\`json
// 自动格式化
{
  "hooks": {
    "PostToolUse": [
      {
        "matcher": "Write",
        "command": "npx prettier --write $FILE_PATH"
      }
    ]
  }
}

// 危险命令拦截
{
  "hooks": {
    "PreToolUse": [
      {
        "matcher": "Bash",
        "command": "if echo '$TOOL_INPUT' | grep -qE 'rm -rf|drop table'; then echo '⚠️ 危险命令！'; exit 1; fi"
      }
    ]
  }
}
\`\`\`

---

## 本节要点

1. **Sub-agents 架构** —— 主 Agent 协调，专家 Agent 并行工作
2. **上下文隔离** —— 每个 Agent 独立上下文，避免污染
3. **Claude Squad** —— 多 Agent 终端管理工具，支持多种 AI
4. **Hooks 自动化** —— 事件驱动的工作流自动化
5. **实战应用** —— 大型功能开发、代码迁移、重构任务
            `,ja:`
## Sub-agents：エキスパートチーム協力モード

Sub-agentsはClaude Codeの高度な機能で、複雑なタスクを複数のエキスパートAgentに分解して並列処理できます。

---

## Sub-agentsアーキテクチャ

\`\`\`
                    ┌─────────────────┐
                    │   メインAgent   │
                    │  (Coordinator)  │
                    └────────┬────────┘
                             │
           ┌─────────────────┼─────────────────┐
           ▼                 ▼                 ▼
   ┌───────────────┐ ┌───────────────┐ ┌───────────────┐
   │  コードエキスパート │ │  テストエキスパート │ │  ドキュメントエキスパート │
   │   独立コンテキスト │ │   独立コンテキスト │ │   独立コンテキスト │
   └───────────────┘ └───────────────┘ └───────────────┘
\`\`\`

**コアメリット**：
- **コンテキスト分離** —— 各Agent独立、情報汚染を防止
- **専門化分業** —— 特定タスクに集中、成功率向上
- **並列処理** —— 複数タスク同時進行、効率倍増

---

## 🔥 マルチAgent管理ツール

### Claude Squad ⭐ 4.3k

複数AIエージェントを管理するターミナルツール：

\`\`\`bash
# インストール
go install github.com/smtg-ai/claude-squad@latest

# 起動
claude-squad
\`\`\`

**使用シーン**：
- 複数の独立タスクを同時処理
- 異なるAIツールを使い分け
- 作業コンテキストを素早く切り替え

---

## Hooks：イベントトリガー自動化

特定イベント発生時にスクリプトを自動実行：

| Hook名 | トリガー | 用途 |
|--------|---------|------|
| PreToolUse | ツール呼び出し前 | インターセプト |
| PostToolUse | ツール呼び出し後 | 後処理 |
| Stop | セッション終了 | クリーンアップ |

\`\`\`json
{
  "hooks": {
    "PostToolUse": [
      {
        "matcher": "Write",
        "command": "npx prettier --write $FILE_PATH"
      }
    ]
  }
}
\`\`\`

---

## 本節のポイント

1. **Sub-agentsアーキテクチャ** —— メインAgentが調整、エキスパートが並列作業
2. **コンテキスト分離** —— 各Agent独立コンテキスト
3. **Claude Squad** —— マルチAgentターミナル管理ツール
4. **Hooks自動化** —— イベント駆動のワークフロー自動化
            `}},{id:"ch3-opensource",title:{zh:"3.7 Claude Code 开源生态",ja:"3.7 Claude Code オープンソースエコシステム"},content:{zh:`
## Claude Code 开源生态

Claude Code 拥有活跃的开源社区，涌现出许多优秀的增强工具和框架。掌握这些工具能让你的 AI 编程效率倍增。

---

## 🏆 明星项目一览

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    Claude Code 开源生态 TOP 项目                          │
└─────────────────────────────────────────────────────────────────────────┘

  项目名称              Stars         功能
  ────────              ─────         ────
  Task Master           ⭐ 20.9k      AI 驱动的任务管理系统
  Claude-Flow           ⭐ 6.7k       多 Agent 编排框架
  Claude Squad          ⭐ 4.3k       多 Agent 终端管理器
  SuperClaude           ⭐ 3.2k       Claude Code 增强框架
  awesome-claude-skills ⭐ 2.8k       Skills 资源集合
\`\`\`

---

## 🚀 SuperClaude Framework

增强 Claude Code 能力 300% 的配置框架：

### 安装

\`\`\`bash
# 使用 npm 安装
npm install -g @superclaude-org/superclaude

# 或直接克隆
git clone https://github.com/SuperClaude-Org/SuperClaude_Framework.git
cd SuperClaude_Framework

# 安装到项目
superclaude init
\`\`\`

### 核心功能

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│  SuperClaude 功能矩阵                                                    │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  🎯 21 个斜杠命令        📦 13 个专家 Agent      🔄 6 种行为模式          │
│  ─────────────────      ────────────────       ────────────────         │
│  /architect             code-expert            spec-driven              │
│  /debug                 test-expert            tdd-mode                 │
│  /refactor              security-expert        debug-mode               │
│  /security              docs-expert            review-mode              │
│  /performance           devops-expert          ...                      │
│  ...                    ...                                             │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
\`\`\`

### 使用示例

\`\`\`bash
# 架构设计模式
/architect "设计一个微服务电商系统"

# 安全审查模式
/security "审查 API 端点安全性"

# 性能优化模式
/performance "优化数据库查询性能"

# 调试模式
/debug "定位内存泄漏问题"
\`\`\`

**GitHub**: https://github.com/SuperClaude-Org/SuperClaude_Framework

---

## 📋 Task Master（Claude Task Master）

AI 驱动的任务管理系统，GitHub ⭐ 20.9k：

### 安装

\`\`\`bash
# 使用 npm 安装
npm install -g task-master-ai

# 或使用 npx
npx task-master-ai init
\`\`\`

### 核心功能

\`\`\`bash
# 从 PRD 生成任务列表
task-master parse ./prd.md

# 查看任务列表
task-master list

# 开始下一个任务
task-master next

# 标记任务完成
task-master done 1
\`\`\`

### 工作流程

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│  Task Master 工作流程                                                    │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  📄 PRD 文档  →  🤖 AI 解析  →  📋 任务列表  →  ✅ 逐个完成              │
│                                                                         │
│  支持的 AI 编辑器:                                                       │
│  • Cursor                                                               │
│  • Claude Code                                                          │
│  • Windsurf                                                             │
│  • Lovable                                                              │
│  • Roo Code                                                             │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
\`\`\`

**特色功能**：
- 📝 PRD 自动解析为可执行任务
- 🔗 任务依赖关系管理
- 📊 进度追踪和报告
- 🔄 与多种 AI 编辑器集成

**GitHub**: https://github.com/eyaltoledano/claude-task-master

---

## 🔗 其他推荐项目

### Claude-Flow

多 Agent 编排框架：

\`\`\`bash
# 安装
npm install claude-flow

# 创建编排配置
claude-flow init

# 运行多 Agent 任务
claude-flow run workflow.yaml
\`\`\`

**GitHub**: https://github.com/kabirsingh/claude-flow

### awesome-claude

Claude 生态资源大全：

\`\`\`bash
# 包含资源
- 官方文档和教程
- 社区工具和插件
- MCP 服务器列表
- Skills 模板
- 最佳实践指南
\`\`\`

**GitHub**: https://github.com/alvinunreal/awesome-claude

---

## 📦 项目选择指南

| 需求 | 推荐项目 | 原因 |
|------|---------|------|
| 任务管理 | Task Master | PRD 到代码的完整流程 |
| 增强命令 | SuperClaude | 21 个专业命令 + 13 个专家 |
| 多 Agent 管理 | Claude Squad | 支持多种 AI 工具 |
| Skills 资源 | awesome-claude-skills | 数百个即用 Skills |
| 综合学习 | awesome-claude | 最全资源列表 |

---

## 本节要点

1. **SuperClaude** —— 21 命令 + 13 专家，能力增强 300%
2. **Task Master** —— PRD 到代码的 AI 任务管理（20k+ stars）
3. **Claude Squad** —— 多 Agent 终端管理器
4. **awesome-* 系列** —— 社区精选资源集合
5. **选择建议** —— 根据需求组合使用多个工具
            `,ja:`
## Claude Code オープンソースエコシステム

Claude Codeには活発なオープンソースコミュニティがあり、多くの優れた拡張ツールとフレームワークが生まれています。

---

## 🏆 スタープロジェクト一覧

| プロジェクト | Stars | 機能 |
|------------|-------|------|
| Task Master | ⭐ 20.9k | AI駆動タスク管理 |
| Claude-Flow | ⭐ 6.7k | マルチAgentオーケストレーション |
| Claude Squad | ⭐ 4.3k | マルチAgentターミナル管理 |
| SuperClaude | ⭐ 3.2k | Claude Code拡張フレームワーク |

---

## 🚀 SuperClaude Framework

Claude Code能力を300%向上させる設定フレームワーク：

\`\`\`bash
# インストール
npm install -g @superclaude-org/superclaude

# プロジェクトに適用
superclaude init
\`\`\`

**コア機能**：
- 21個のスラッシュコマンド
- 13個のエキスパートAgent
- 6種類の動作モード

\`\`\`bash
# 使用例
/architect "マイクロサービスECシステムを設計"
/security "APIエンドポイントのセキュリティを審査"
/debug "メモリリーク問題を特定"
\`\`\`

**GitHub**: https://github.com/SuperClaude-Org/SuperClaude_Framework

---

## 📋 Task Master

AI駆動タスク管理システム、⭐ 20.9k：

\`\`\`bash
# インストール
npm install -g task-master-ai

# PRDからタスクリストを生成
task-master parse ./prd.md

# 次のタスクを開始
task-master next
\`\`\`

**特徴**：
- PRD自動解析
- タスク依存関係管理
- 進捗追跡
- 複数AIエディタと統合

**GitHub**: https://github.com/eyaltoledano/claude-task-master

---

## 📦 プロジェクト選択ガイド

| ニーズ | 推奨プロジェクト | 理由 |
|--------|----------------|------|
| タスク管理 | Task Master | PRDからコードまで |
| 機能拡張 | SuperClaude | 21コマンド + 13エキスパート |
| マルチAgent | Claude Squad | 複数AIツール対応 |
| Skills | awesome-claude-skills | 数百の即戦力Skills |

---

## 本節のポイント

1. **SuperClaude** —— 21コマンド + 13エキスパート、能力300%向上
2. **Task Master** —— PRDからコードへのAIタスク管理
3. **Claude Squad** —— マルチAgentターミナル管理
4. **awesome-*シリーズ** —— コミュニティ厳選リソース集
            `}},{id:"ch3-gemini-cli",title:{zh:"3.8 Gemini CLI 使用指南",ja:"3.8 Gemini CLI 使用ガイド"},content:{zh:`
## Gemini CLI：Google 的 AI 编程助手

Gemini CLI 是 Google 推出的命令行 AI 编程工具，与 Claude Code 类似但有独特优势。

---

## 快速开始

### 安装 Gemini CLI

\`\`\`bash
# 使用 npm 安装
npm install -g @anthropic-ai/claude-code

# 或使用 Google Cloud CLI
gcloud components install gemini-cli

# 验证安装
gemini --version
\`\`\`

### 初始化配置

\`\`\`bash
# 登录 Google 账号
gemini auth login

# 配置 API 密钥（可选）
gemini config set api_key YOUR_API_KEY

# 查看配置
gemini config list
\`\`\`

---

## 核心功能

### 1. 代码生成

\`\`\`bash
# 生成代码
gemini generate "创建一个 React 计数器组件"

# 指定语言
gemini generate --lang typescript "实现快速排序算法"

# 生成到文件
gemini generate "创建 Express 服务器" -o server.js
\`\`\`

### 2. 代码解释

\`\`\`bash
# 解释代码文件
gemini explain src/utils/helper.ts

# 解释特定函数
gemini explain --function handleClick src/App.tsx

# 详细解释
gemini explain --verbose complex-algorithm.py
\`\`\`

### 3. 代码优化

\`\`\`bash
# 优化代码性能
gemini optimize src/heavy-computation.js

# 优化特定方面
gemini optimize --focus memory src/data-processing.ts

# 安全性审查
gemini optimize --security api-handler.js
\`\`\`

---

## 项目级操作

### 项目初始化

\`\`\`bash
# 初始化项目配置
gemini init

# 生成项目结构
gemini init --template react-typescript

# 从现有项目学习
gemini learn .
\`\`\`

### 代码审查

\`\`\`bash
# 审查最近更改
gemini review

# 审查特定文件
gemini review src/components/*.tsx

# 审查 PR
gemini review --pr 123
\`\`\`

---

## 交互模式

### 启动交互式会话

\`\`\`bash
# 进入交互模式
gemini chat

# 带上下文进入
gemini chat --context src/

# 指定模型
gemini chat --model gemini-3-pro-preview
\`\`\`

### 交互式命令

\`\`\`
gemini> /help          # 显示帮助
gemini> /clear         # 清空会话
gemini> /save          # 保存会话
gemini> /load session.json  # 加载会话
gemini> /model         # 切换模型
gemini> /exit          # 退出
\`\`\`

---

## Gemini vs Claude Code 对比

| 特性 | Gemini CLI | Claude Code |
|------|-----------|-------------|
| 模型 | Gemini 2.0 | Claude Opus/Sonnet |
| 搜索集成 | ✅ Google 搜索 | ❌ 需要 MCP |
| 代码库分析 | ✅ 原生支持 | ✅ 原生支持 |
| MCP 协议 | ❌ 不支持 | ✅ 完整支持 |
| 多模态 | ✅ 图片/视频 | ✅ 图片 |
| 价格 | 免费额度大 | 按量付费 |

### 适用场景

**选择 Gemini CLI**：
- 需要搜索最新信息
- 处理多模态内容（图片、视频）
- 个人项目、学习用途（免费额度大）
- Google Cloud 生态整合

**选择 Claude Code**：
- 需要 MCP 扩展能力
- 复杂的代码重构任务
- 团队协作和 Skills 共享
- 长文本生成需求

---

## 高级功能

### 1. Google 搜索集成

\`\`\`bash
# 搜索最新文档
gemini search "React 19 新特性"

# 搜索并生成代码
gemini generate --search "使用最新的 Next.js 15 创建 API 路由"
\`\`\`

### 2. 多模态支持

\`\`\`bash
# 分析图片
gemini analyze image.png "描述这个UI设计"

# 从截图生成代码
gemini generate --from-image mockup.png "生成对应的 React 组件"

# 分析视频
gemini analyze video.mp4 "总结这个教程的要点"
\`\`\`

### 3. 代码库索引

\`\`\`bash
# 索引整个项目
gemini index .

# 查询代码库
gemini query "哪里处理用户认证？"

# 查找相似代码
gemini find-similar src/utils/format.ts
\`\`\`

---

## 配置文件

\`\`\`yaml
# .gemini/config.yaml
model: gemini-2.0-pro
temperature: 0.7
max_tokens: 8192

# 项目特定设置
project:
  language: typescript
  framework: react
  test_framework: jest

# 忽略文件
ignore:
  - node_modules/
  - dist/
  - "*.log"

# 自定义提示
prompts:
  review: "请用中文审查代码，关注安全性和性能"
  explain: "请用简单的语言解释这段代码的作用"
\`\`\`

---

## 实用技巧

### 1. 与 Git 集成

\`\`\`bash
# 生成 commit message
git diff | gemini generate "根据这个 diff 生成 commit message"

# 审查 staged 更改
git diff --staged | gemini review

# 生成 changelog
gemini generate "根据最近10个commit生成changelog" --context "$(git log -10)"
\`\`\`

### 2. 管道操作

\`\`\`bash
# 处理日志
tail -100 error.log | gemini analyze "分析错误模式"

# 处理 API 响应
curl api.example.com/data | gemini format --json

# 代码转换
cat old-code.js | gemini convert --to typescript > new-code.ts
\`\`\`

### 3. 批量操作

\`\`\`bash
# 批量添加注释
gemini batch comment src/**/*.ts

# 批量格式化
gemini batch format src/ --style google

# 批量测试生成
gemini batch test src/utils/*.ts -o tests/
\`\`\`

---

## 本节要点

1. **安装配置** —— npm 或 gcloud 安装，gemini auth 登录
2. **核心命令** —— generate、explain、optimize、review
3. **交互模式** —— gemini chat 进入对话式编程
4. **Google 优势** —— 搜索集成、多模态支持、免费额度
5. **项目集成** —— .gemini/config.yaml 配置项目偏好
6. **管道操作** —— 与其他命令行工具无缝配合
            `,ja:`
## Gemini CLI：GoogleのAIプログラミングアシスタント

Gemini CLIはGoogleが提供するコマンドラインAIプログラミングツールで、Claude Codeに似ていますが独自の利点があります。

---

## クイックスタート

### Gemini CLIのインストール

\`\`\`bash
# npmでインストール
npm install -g @google/gemini-cli

# またはGoogle Cloud CLIで
gcloud components install gemini-cli

# インストール確認
gemini --version
\`\`\`

### 初期設定

\`\`\`bash
# Googleアカウントでログイン
gemini auth login

# APIキーを設定（オプション）
gemini config set api_key YOUR_API_KEY

# 設定を確認
gemini config list
\`\`\`

---

## コア機能

### 1. コード生成

\`\`\`bash
# コードを生成
gemini generate "Reactカウンターコンポーネントを作成"

# 言語を指定
gemini generate --lang typescript "クイックソートを実装"

# ファイルに出力
gemini generate "Expressサーバーを作成" -o server.js
\`\`\`

### 2. コード説明

\`\`\`bash
# コードファイルを説明
gemini explain src/utils/helper.ts

# 特定の関数を説明
gemini explain --function handleClick src/App.tsx
\`\`\`

### 3. コード最適化

\`\`\`bash
# パフォーマンス最適化
gemini optimize src/heavy-computation.js

# セキュリティレビュー
gemini optimize --security api-handler.js
\`\`\`

---

## Gemini vs Claude Code 比較

| 特性 | Gemini CLI | Claude Code |
|------|-----------|-------------|
| モデル | Gemini 2.0 | Claude Opus/Sonnet |
| 検索統合 | ✅ Google検索 | ❌ MCP必要 |
| MCP対応 | ❌ 非対応 | ✅ 完全対応 |
| マルチモーダル | ✅ 画像/動画 | ✅ 画像 |
| 価格 | 無料枠大 | 従量課金 |

### 使い分け

**Gemini CLIを選ぶ場合**：
- 最新情報の検索が必要
- マルチモーダルコンテンツ処理
- 個人プロジェクト（無料枠大）
- Google Cloudエコシステム統合

**Claude Codeを選ぶ場合**：
- MCP拡張機能が必要
- 複雑なリファクタリング
- チーム協力とSkills共有
- 長文生成ニーズ

---

## 高度な機能

### 1. Google検索統合

\`\`\`bash
# 最新ドキュメントを検索
gemini search "React 19 新機能"

# 検索してコード生成
gemini generate --search "最新のNext.js 15でAPIルートを作成"
\`\`\`

### 2. マルチモーダル対応

\`\`\`bash
# 画像を分析
gemini analyze image.png "このUIデザインを説明"

# スクリーンショットからコード生成
gemini generate --from-image mockup.png "対応するReactコンポーネントを生成"
\`\`\`

---

## 本節のポイント

1. **インストール設定** —— npmまたはgcloudでインストール
2. **コアコマンド** —— generate、explain、optimize、review
3. **対話モード** —— gemini chatで対話式プログラミング
4. **Google優位性** —— 検索統合、マルチモーダル、無料枠
5. **プロジェクト統合** —— .gemini/config.yamlで設定
6. **パイプ操作** —— 他のCLIツールとシームレス連携
            `}},{id:"ch3-openai-codex",title:{zh:"3.9 OpenAI Codex 使用指南",ja:"3.9 OpenAI Codex 使用ガイド"},content:{zh:`
## OpenAI Codex：GPT 驱动的 AI 编程助手

OpenAI Codex（又称 GPT Codex Agent）是 OpenAI 推出的 AI 编程工具，基于 GPT-5 模型，与 Claude Code 形成直接竞争。

---

## 快速开始

### 安装 Codex CLI

\`\`\`bash
# 使用 npm 安装
npm install -g @openai/codex

# 或使用 pip
pip install openai-codex

# 验证安装
codex --version
\`\`\`

### 配置 API 密钥

\`\`\`bash
# 设置环境变量
export OPENAI_API_KEY="sk-..."

# 或使用配置命令
codex config set api_key sk-...

# 登录 OpenAI 账号（推荐）
codex auth login
\`\`\`

---

## 核心功能

### 1. 自然语言编程

\`\`\`bash
# 基本代码生成
codex "创建一个 REST API 端点处理用户注册"

# 在项目上下文中生成
codex --context . "添加用户邮箱验证功能"

# 指定语言和框架
codex --lang python --framework fastapi "创建文件上传接口"
\`\`\`

### 2. Agent 模式

Codex 的 Agent 模式可以自主完成复杂任务：

\`\`\`bash
# 启动 Agent 模式
codex agent "重构这个项目的认证系统，使用 JWT"

# Agent 会自动：
# 1. 分析现有代码
# 2. 制定重构计划
# 3. 逐步实现更改
# 4. 运行测试验证
\`\`\`

### 3. 代码补全

\`\`\`bash
# 实时补全（编辑器集成）
codex complete --stream

# 批量补全
codex complete src/incomplete-file.ts

# 多候选补全
codex complete --n 3 "function calculateTotal("
\`\`\`

---

## Skills 支持

OpenAI Codex 已采用与 Claude Code 相同的 Skills 规范！

### Skills 文件结构

\`\`\`markdown
# .codex/skills/deploy.md（与 Claude 兼容）

---
name: deploy
description: 部署到生产环境
triggers:
  - /deploy
  - 部署应用
---

# 部署流程

## 步骤

1. 运行测试 \`npm test\`
2. 构建项目 \`npm run build\`
3. 推送到服务器
\`\`\`

### 跨平台 Skills

\`\`\`bash
# Skills 目录结构（通用）
.claude/skills/  # Claude Code 使用
.codex/skills/   # OpenAI Codex 使用

# 推荐：使用符号链接共享
ln -s .claude/skills .codex/skills
\`\`\`

---

## 多模态能力

### 图片理解

\`\`\`bash
# 从 UI 截图生成代码
codex vision "根据这个截图实现登录页面" --image login-mockup.png

# 分析图表
codex vision "解释这个架构图" --image architecture.png

# 调试 UI 问题
codex vision "这个页面布局有什么问题？" --image buggy-ui.png
\`\`\`

### 代码 + 图片

\`\`\`bash
# 结合代码和设计稿
codex "实现这个设计，样式要和截图一致"   --image design.png   --context src/components/
\`\`\`

---

## Codex vs Claude Code 对比

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    AI 编程工具对比表                                     │
└─────────────────────────────────────────────────────────────────────────┘

  特性               Claude Code          OpenAI Codex
  ────               ───────────          ────────────
  基础模型           Claude Opus 4.5      GPT-5.2
  Skills 支持        ✅ 原生              ✅ 兼容
  MCP 协议           ✅ 完整              ❌ 不支持
  浏览器自动化       ✅ Chrome 集成       ✅ Playwright
  多模态             图片                 图片 + 视频
  长文本生成         12000+ 字符          ~7000 字符
  数学/编程          ⭐⭐⭐⭐             ⭐⭐⭐⭐⭐
  价格               $$                   $$$
\`\`\`

### 实测对比

| 任务类型 | 推荐工具 | 原因 |
|---------|---------|------|
| 复杂重构 | Claude Code | 长文本能力强 |
| 算法实现 | Codex | 数学推理更好 |
| UI 开发 | 两者相当 | 都支持图片输入 |
| 自动化测试 | Codex | Playwright 集成好 |
| MCP 扩展 | Claude Code | 独家支持 |

---

## 高级功能

### 1. 并行任务

\`\`\`bash
# 并行执行多个任务
codex parallel   "优化 src/utils/*.ts 的性能"   "为 src/api/*.ts 添加测试"   "更新 README.md 文档"

# 查看并行任务状态
codex status --parallel
\`\`\`

### 2. 代码审查

\`\`\`bash
# 审查 PR
codex review --pr 123

# 审查并自动修复
codex review --fix src/

# 安全审查
codex review --security --strict
\`\`\`

### 3. 测试生成

\`\`\`bash
# 为文件生成测试
codex test generate src/utils/calculator.ts

# 为整个模块生成
codex test generate src/services/ --coverage 80

# 运行并修复失败测试
codex test fix
\`\`\`

---

## 配置文件

\`\`\`json
// .codex/config.json
{
  "model": "gpt-5.2-codex",
  "temperature": 0.3,
  "max_tokens": 8192,

  "project": {
    "language": "typescript",
    "framework": "nextjs",
    "test_framework": "jest"
  },

  "skills_dir": ".codex/skills",

  "ignore": [
    "node_modules",
    "dist",
    ".git"
  ],

  "hooks": {
    "pre_commit": "npm run lint",
    "post_generate": "npx prettier --write"
  }
}
\`\`\`

---

## 工作流示例

### 示例 1：功能开发全流程

\`\`\`bash
# 1. 进入项目目录
cd my-project

# 2. 启动 Agent 模式
codex agent

# 3. 描述需求
> 我需要添加用户头像上传功能，支持裁剪和压缩

# Codex 会自动：
# - 分析项目结构
# - 创建必要的组件和 API
# - 添加图片处理逻辑
# - 编写测试
# - 更新文档
\`\`\`

### 示例 2：代码迁移

\`\`\`bash
# JavaScript 到 TypeScript 迁移
codex migrate --from js --to ts src/

# React Class 到 Hooks 迁移
codex migrate --pattern "class-to-hooks" src/components/

# API 版本升级
codex migrate --api-version v1-to-v2 src/services/
\`\`\`

### 示例 3：性能优化

\`\`\`bash
# 分析性能问题
codex analyze performance src/

# 自动优化
codex optimize --auto   --focus "bundle-size,runtime"   src/

# 生成优化报告
codex report performance --output report.md
\`\`\`

---

## 与 Claude Code 协同使用

\`\`\`bash
# 最佳实践：结合两者优势

# 1. 用 Claude Code 做架构设计（长文本+MCP）
claude "设计用户系统的整体架构" --plan

# 2. 用 Codex 实现算法密集型代码
codex "实现推荐算法，使用协同过滤"

# 3. 用 Claude Code 做代码审查（更细致）
claude "/review"

# 4. 用 Codex 生成测试（测试覆盖更全）
codex test generate src/
\`\`\`

---

## 常见问题

### Q: Codex 和 ChatGPT 有什么区别？
A: Codex 专门针对编程任务优化，有更好的代码理解和生成能力，支持 Agent 模式自主完成复杂任务。

### Q: Skills 是否真的跨平台通用？
A: 是的，2025年5月后 OpenAI 采用了 Anthropic 的 Skills 规范，基本语法完全兼容。

### Q: 该选择 Codex 还是 Claude Code？
A: 建议都尝试。数学/算法任务用 Codex，长文本/MCP 任务用 Claude Code。

---

## 本节要点

1. **安装配置** —— npm 或 pip 安装，设置 OPENAI_API_KEY
2. **Agent 模式** —— 自主完成复杂编程任务
3. **Skills 兼容** —— 与 Claude Code 共享 Skills 定义
4. **多模态能力** —— 从截图生成代码
5. **性能对比** —— 数学推理强，长文本弱
6. **协同使用** —— 结合 Claude + Codex 发挥各自优势
            `,ja:`
## OpenAI Codex：GPT駆動のAIプログラミングアシスタント

OpenAI Codex（GPT Codex Agentとも呼ばれる）はOpenAIが提供するAIプログラミングツールで、GPT-5モデルをベースにしており、Claude Codeと直接競合しています。

---

## クイックスタート

### Codex CLIのインストール

\`\`\`bash
# npmでインストール
npm install -g @openai/codex

# またはpipで
pip install openai-codex

# インストール確認
codex --version
\`\`\`

### APIキーの設定

\`\`\`bash
# 環境変数を設定
export OPENAI_API_KEY="sk-..."

# または設定コマンドで
codex config set api_key sk-...

# OpenAIアカウントでログイン（推奨）
codex auth login
\`\`\`

---

## コア機能

### 1. 自然言語プログラミング

\`\`\`bash
# 基本的なコード生成
codex "ユーザー登録を処理するREST APIエンドポイントを作成"

# プロジェクトコンテキストで生成
codex --context . "メール検証機能を追加"

# 言語とフレームワークを指定
codex --lang python --framework fastapi "ファイルアップロードAPIを作成"
\`\`\`

### 2. Agentモード

CodexのAgentモードは複雑なタスクを自律的に完了できます：

\`\`\`bash
# Agentモードを起動
codex agent "このプロジェクトの認証システムをJWTを使用してリファクタリング"

# Agentは自動的に：
# 1. 既存コードを分析
# 2. リファクタリング計画を策定
# 3. 段階的に変更を実装
# 4. テストを実行して検証
\`\`\`

---

## Skills対応

OpenAI CodexはClaude Codeと同じSkills仕様を採用しています！

### Skillsファイル構造

\`\`\`markdown
# .codex/skills/deploy.md（Claudeと互換）

---
name: deploy
description: 本番環境にデプロイ
triggers:
  - /deploy
  - アプリをデプロイ
---

# デプロイプロセス

## ステップ

1. テストを実行 \`npm test\`
2. プロジェクトをビルド \`npm run build\`
3. サーバーにプッシュ
\`\`\`

---

## Codex vs Claude Code 比較

| 特性 | Claude Code | OpenAI Codex |
|-----|------------|--------------|
| ベースモデル | Claude Opus 4.5 | GPT-5.2 |
| Skills対応 | ✅ ネイティブ | ✅ 互換 |
| MCP対応 | ✅ 完全 | ❌ 非対応 |
| マルチモーダル | 画像 | 画像 + 動画 |
| 長文生成 | 12000+文字 | ~7000文字 |
| 数学/プログラミング | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ |

### 使い分け

| タスクタイプ | 推奨ツール | 理由 |
|------------|----------|------|
| 複雑なリファクタリング | Claude Code | 長文能力が強い |
| アルゴリズム実装 | Codex | 数学推論が優れている |
| UI開発 | 両方とも | 画像入力対応 |
| MCP拡張 | Claude Code | 独占対応 |

---

## 高度な機能

### 1. 並列タスク

\`\`\`bash
# 複数タスクを並列実行
codex parallel   "src/utils/*.tsのパフォーマンスを最適化"   "src/api/*.tsにテストを追加"   "README.mdドキュメントを更新"
\`\`\`

### 2. テスト生成

\`\`\`bash
# ファイルのテストを生成
codex test generate src/utils/calculator.ts

# モジュール全体のテストを生成
codex test generate src/services/ --coverage 80
\`\`\`

---

## Claude Codeとの協同使用

\`\`\`bash
# ベストプラクティス：両者の利点を組み合わせる

# 1. Claude Codeでアーキテクチャ設計（長文+MCP）
claude "ユーザーシステムの全体アーキテクチャを設計" --plan

# 2. Codexでアルゴリズム集約型コードを実装
codex "協調フィルタリングを使用した推薦アルゴリズムを実装"

# 3. Claude Codeでコードレビュー（より詳細）
claude "/review"

# 4. Codexでテスト生成（カバレッジが広い）
codex test generate src/
\`\`\`

---

## 本節のポイント

1. **インストール設定** —— npmまたはpipでインストール、OPENAI_API_KEYを設定
2. **Agentモード** —— 複雑なプログラミングタスクを自律完了
3. **Skills互換** —— Claude CodeとSkills定義を共有
4. **マルチモーダル能力** —— スクリーンショットからコード生成
5. **性能比較** —— 数学推論強い、長文弱い
6. **協同使用** —— Claude + Codexの組み合わせで各自の強みを発揮
            `}},{id:"ch3-computer-use",title:{zh:"3.10 Computer Use：AI 的机器臂",ja:"3.10 Computer Use：AIの機械アーム"},content:{zh:`
## AI 如何操控物理世界？

传统 AI 只能"说话"，而 **Computer Use** 让 AI 拥有了"手臂"——可以像人一样操作电脑、控制浏览器、甚至操控机器人。

---

## 什么是 Computer Use？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                      Computer Use 概念                                   │
└─────────────────────────────────────────────────────────────────────────┘

  传统 AI                               Computer Use AI
  ┌───────────────┐                    ┌───────────────┐
  │  只能对话     │                    │  看屏幕       │
  │  输出文字     │                    │  移动鼠标     │
  │  ─────────── │                    │  点击按钮     │
  │  "我建议..."  │                    │  输入文字     │
  │  "你可以..."  │                    │  执行操作     │
  └───────────────┘                    └───────────────┘
        │                                     │
        ▼                                     ▼
   需要人工执行                           AI 自动完成
\`\`\`

**核心能力**：
- **视觉理解**：看懂屏幕上的内容
- **坐标定位**：精确定位按钮和输入框
- **操作执行**：模拟鼠标和键盘操作
- **反馈循环**：根据结果调整行动

---

## Claude Computer Use

Anthropic 的 Computer Use 让 Claude 可以直接操作你的电脑桌面。

### 工作原理

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    Claude Computer Use 流程                              │
└─────────────────────────────────────────────────────────────────────────┘

                    ┌──────────────┐
                    │   用户指令   │
                    │ "帮我订机票" │
                    └──────┬───────┘
                           │
                           ▼
                    ┌──────────────┐
                    │  Claude 规划 │
                    │  分解任务    │
                    └──────┬───────┘
                           │
         ┌─────────────────┼─────────────────┐
         │                 │                 │
         ▼                 ▼                 ▼
    ┌─────────┐      ┌─────────┐      ┌─────────┐
    │ 截图屏幕 │      │ 分析内容 │      │ 执行操作 │
    └─────────┘      └─────────┘      └─────────┘
                           │
                           ▼
                    ┌──────────────┐
                    │  观察结果    │
                    │  调整策略    │
                    └──────────────┘
                           │
                           ▼
                    ┌──────────────┐
                    │   任务完成   │
                    └──────────────┘
\`\`\`

### 使用方式

\`\`\`bash
# 使用 Docker 运行 Computer Use
docker run -p 8080:8080 anthropic/computer-use-demo

# 或使用 API
pip install anthropic
\`\`\`

### API 调用示例

\`\`\`python
import anthropic

client = anthropic.Anthropic()

# Computer Use 需要特殊的 tool 配置
response = client.messages.create(
    model="claude-sonnet-4-20250514",
    max_tokens=1024,
    tools=[
        {
            "type": "computer_20241022",
            "name": "computer",
            "display_width_px": 1920,
            "display_height_px": 1080
        }
    ],
    messages=[
        {
            "role": "user",
            "content": "打开浏览器，搜索今天的天气"
        }
    ]
)

# Claude 会返回要执行的操作
for block in response.content:
    if block.type == "tool_use":
        print(f"操作: {block.input}")
        # 执行操作并返回截图
\`\`\`

---

## Chrome 浏览器控制

通过 MCP 插件，Claude Code 可以直接控制 Chrome 浏览器。

### 安装 Chrome MCP

\`\`\`json
// .claude/settings.json
{
  "mcpServers": {
    "chrome": {
      "command": "npx",
      "args": ["-y", "@anthropic/mcp-chrome"]
    }
  }
}
\`\`\`

### 使用场景

| 场景 | 描述 | 命令示例 |
|------|------|----------|
| 网页测试 | 自动化 UI 测试 | "测试登录功能是否正常" |
| 数据抓取 | 收集网页信息 | "获取这个页面的产品列表" |
| 表单填写 | 自动填写表单 | "帮我填写这个注册表单" |
| 截图对比 | UI 回归测试 | "对比新旧版本的差异" |

### 实际示例

\`\`\`
用户：帮我测试登录页面，输入用户名 test@example.com

Claude：我来帮你测试登录页面。

[截图分析] 我看到了登录页面，有用户名和密码输入框。

[执行操作]
1. 点击用户名输入框 (坐标: 500, 300)
2. 输入: test@example.com
3. 点击密码输入框
4. 等待用户输入密码...

[结果] 用户名已填写，请告诉我密码或下一步操作。
\`\`\`

---

## Playwright 自动化测试

结合 Playwright MCP，实现专业的端到端测试。

### 配置

\`\`\`json
{
  "mcpServers": {
    "playwright": {
      "command": "npx",
      "args": ["-y", "@anthropic/mcp-playwright"]
    }
  }
}
\`\`\`

### 自动化测试示例

\`\`\`
用户：帮我写一个自动化测试，验证购物车功能

Claude：我来创建购物车功能的端到端测试。

[生成测试代码]
\`\`\`typescript
import { test, expect } from '@playwright/test';

test('购物车功能测试', async ({ page }) => {
  // 1. 访问商品页
  await page.goto('/products');

  // 2. 添加商品到购物车
  await page.click('[data-testid="add-to-cart"]');

  // 3. 验证购物车数量
  const cartCount = page.locator('.cart-count');
  await expect(cartCount).toHaveText('1');

  // 4. 打开购物车
  await page.click('.cart-icon');

  // 5. 验证商品存在
  await expect(page.locator('.cart-item')).toBeVisible();
});
\`\`\`

[执行测试] 运行 npx playwright test

[结果] ✅ 测试通过！购物车功能正常工作。
\`\`\`

---

## 机器人控制（未来展望）

Computer Use 的技术可以扩展到物理机器人控制。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                      AI + 机器人 架构                                    │
└─────────────────────────────────────────────────────────────────────────┘

          用户指令
              │
              ▼
      ┌───────────────┐
      │   Claude AI   │
      │   理解 + 规划  │
      └───────┬───────┘
              │
    ┌─────────┴─────────┐
    ▼                   ▼
┌─────────┐       ┌─────────┐
│ 虚拟世界 │       │ 物理世界 │
│         │       │         │
│ 电脑操作 │       │ 机器臂   │
│ 浏览器   │       │ 无人机   │
│ 代码编写 │       │ 机器人   │
└─────────┘       └─────────┘
\`\`\`

### 已有项目

| 项目 | 描述 | 链接 |
|------|------|------|
| Figure + OpenAI | 人形机器人 + GPT | figure.ai |
| Google RT-2 | 机器人 Transformer | robotics-transformer |
| 1X Technologies | 通用人形机器人 | 1x.tech |
| Boston Dynamics | 机器狗 + AI | bostondynamics.com |

---

## 安全注意事项

Computer Use 功能强大，但需要谨慎使用：

### 风险等级

| 操作类型 | 风险等级 | 建议 |
|----------|----------|------|
| 只读浏览 | 🟢 低 | 可以自由使用 |
| 表单填写 | 🟡 中 | 验证后再提交 |
| 文件操作 | 🟠 较高 | 先备份 |
| 系统设置 | 🔴 高 | 手动确认 |
| 付款操作 | 🔴 高 | 绝对禁止 |

### 最佳实践

1. **沙箱环境**：使用 Docker 或虚拟机隔离
2. **权限最小化**：只授予必要权限
3. **人工确认**：关键操作前暂停确认
4. **日志记录**：记录所有操作便于审计
5. **紧急停止**：设置快速中断机制

---

## 本节要点

1. **Computer Use** —— 让 AI 像人一样操作电脑
2. **Claude 桌面控制** —— 截图 + 分析 + 操作循环
3. **Chrome MCP** —— 浏览器自动化测试
4. **Playwright** —— 专业端到端测试
5. **机器人扩展** —— 从虚拟到物理世界
6. **安全第一** —— 沙箱隔离、权限最小化
            `,ja:`
## AI はどのように物理世界を操作するのか？

従来のAIは「話す」ことしかできませんでしたが、**Computer Use**によりAIは「腕」を持つようになりました——人間のようにコンピューターを操作し、ブラウザを制御し、さらにはロボットを操作できるようになりました。

---

## Computer Use とは？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                      Computer Use コンセプト                             │
└─────────────────────────────────────────────────────────────────────────┘

  従来のAI                               Computer Use AI
  ┌───────────────┐                    ┌───────────────┐
  │  対話のみ     │                    │  画面を見る   │
  │  テキスト出力 │                    │  マウス移動   │
  │  ─────────── │                    │  ボタンクリック│
  │  「提案します」│                    │  テキスト入力 │
  │  「できます」 │                    │  操作実行     │
  └───────────────┘                    └───────────────┘
        │                                     │
        ▼                                     ▼
   人間が実行する必要                      AI が自動完了
\`\`\`

**コア能力**：
- **視覚理解**：画面上の内容を理解
- **座標定位**：ボタンや入力フィールドを正確に特定
- **操作実行**：マウスとキーボード操作をシミュレート
- **フィードバックループ**：結果に基づいて行動を調整

---

## Claude Computer Use

AnthropicのComputer Useにより、Claudeはデスクトップを直接操作できます。

### 動作原理

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    Claude Computer Use フロー                            │
└─────────────────────────────────────────────────────────────────────────┘

                    ┌──────────────┐
                    │ ユーザー指示 │
                    │「航空券を予約」│
                    └──────┬───────┘
                           │
                           ▼
                    ┌──────────────┐
                    │ Claude 計画  │
                    │ タスク分解   │
                    └──────┬───────┘
                           │
         ┌─────────────────┼─────────────────┐
         │                 │                 │
         ▼                 ▼                 ▼
    ┌─────────┐      ┌─────────┐      ┌─────────┐
    │スクリーンショット│  │ 内容分析 │      │ 操作実行 │
    └─────────┘      └─────────┘      └─────────┘
                           │
                           ▼
                    ┌──────────────┐
                    │  結果観察    │
                    │  戦略調整    │
                    └──────────────┘
                           │
                           ▼
                    ┌──────────────┐
                    │  タスク完了  │
                    └──────────────┘
\`\`\`

### 使用方法

\`\`\`bash
# Docker で Computer Use を実行
docker run -p 8080:8080 anthropic/computer-use-demo

# または API を使用
pip install anthropic
\`\`\`

### API 呼び出し例

\`\`\`python
import anthropic

client = anthropic.Anthropic()

# Computer Use は特別な tool 設定が必要
response = client.messages.create(
    model="claude-sonnet-4-20250514",
    max_tokens=1024,
    tools=[
        {
            "type": "computer_20241022",
            "name": "computer",
            "display_width_px": 1920,
            "display_height_px": 1080
        }
    ],
    messages=[
        {
            "role": "user",
            "content": "ブラウザを開いて今日の天気を検索"
        }
    ]
)

# Claude は実行する操作を返す
for block in response.content:
    if block.type == "tool_use":
        print(f"操作: {block.input}")
        # 操作を実行してスクリーンショットを返す
\`\`\`

---

## Chrome ブラウザ制御

MCP プラグインを通じて、Claude Code は Chrome ブラウザを直接制御できます。

### Chrome MCP のインストール

\`\`\`json
// .claude/settings.json
{
  "mcpServers": {
    "chrome": {
      "command": "npx",
      "args": ["-y", "@anthropic/mcp-chrome"]
    }
  }
}
\`\`\`

### 使用シーン

| シーン | 説明 | コマンド例 |
|--------|------|------------|
| Webテスト | 自動化UIテスト | 「ログイン機能が正常か確認」 |
| データ収集 | Webページ情報収集 | 「このページの製品リストを取得」 |
| フォーム入力 | 自動フォーム入力 | 「この登録フォームを入力」 |
| スクリーンショット比較 | UI回帰テスト | 「新旧バージョンの違いを比較」 |

---

## Playwright 自動化テスト

Playwright MCP と組み合わせて、プロフェッショナルなE2Eテストを実現。

### 設定

\`\`\`json
{
  "mcpServers": {
    "playwright": {
      "command": "npx",
      "args": ["-y", "@anthropic/mcp-playwright"]
    }
  }
}
\`\`\`

---

## ロボット制御（将来展望）

Computer Use 技術は物理ロボット制御に拡張できます。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                      AI + ロボット アーキテクチャ                        │
└─────────────────────────────────────────────────────────────────────────┘

          ユーザー指示
              │
              ▼
      ┌───────────────┐
      │   Claude AI   │
      │  理解 + 計画   │
      └───────┬───────┘
              │
    ┌─────────┴─────────┐
    ▼                   ▼
┌─────────┐       ┌─────────┐
│ 仮想世界 │       │ 物理世界 │
│         │       │         │
│ PC操作  │       │ ロボットアーム │
│ ブラウザ │       │ ドローン  │
│ コード  │       │ ロボット  │
└─────────┘       └─────────┘
\`\`\`

### 既存プロジェクト

| プロジェクト | 説明 | リンク |
|--------------|------|--------|
| Figure + OpenAI | ヒューマノイドロボット + GPT | figure.ai |
| Google RT-2 | ロボット Transformer | robotics-transformer |
| 1X Technologies | 汎用ヒューマノイド | 1x.tech |
| Boston Dynamics | ロボット犬 + AI | bostondynamics.com |

---

## セキュリティ注意事項

Computer Use は強力ですが、慎重に使用する必要があります：

### リスクレベル

| 操作タイプ | リスクレベル | 推奨事項 |
|------------|--------------|----------|
| 読み取り専用 | 🟢 低 | 自由に使用可能 |
| フォーム入力 | 🟡 中 | 確認後に送信 |
| ファイル操作 | 🟠 やや高 | 先にバックアップ |
| システム設定 | 🔴 高 | 手動確認 |
| 支払い操作 | 🔴 高 | 絶対禁止 |

### ベストプラクティス

1. **サンドボックス環境**：Docker または VM で隔離
2. **最小権限**：必要な権限のみ付与
3. **人間の確認**：重要な操作前に一時停止
4. **ログ記録**：すべての操作を記録
5. **緊急停止**：迅速な中断メカニズムを設定

---

## このセクションのポイント

1. **Computer Use** —— AI が人間のようにコンピューターを操作
2. **Claude デスクトップ制御** —— スクリーンショット + 分析 + 操作ループ
3. **Chrome MCP** —— ブラウザ自動化テスト
4. **Playwright** —— プロフェッショナルE2Eテスト
5. **ロボット拡張** —— 仮想から物理世界へ
6. **セキュリティ優先** —— サンドボックス隔離、最小権限
            `}},{id:"ch3-a2a",title:{zh:"3.11 A2A 协议：Agent 互联",ja:"3.11 A2Aプロトコル：Agent連携"},content:{zh:`
## Agent-to-Agent：AI 代理互联互通

当单个 Agent 不够用时，让多个 Agent 协作是未来趋势。

---

## 🎯 什么是 A2A 协议？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        A2A 协议概念                                       │
└─────────────────────────────────────────────────────────────────────────┘

  传统模式：单一 Agent                 A2A 模式：多 Agent 协作
  ┌────────────────────┐              ┌────────────────────┐
  │     用户请求        │              │     用户请求        │
  │         │          │              │         │          │
  │         ▼          │              │         ▼          │
  │   ┌──────────┐     │              │   ┌──────────┐     │
  │   │ Agent A  │     │              │   │ 协调 Agent │     │
  │   │ (全能型) │     │              │   └─────┬────┘     │
  │   └──────────┘     │              │    ╱    │    ╲     │
  │         │          │              │   ▼     ▼     ▼    │
  │         ▼          │              │ 代码   数据   搜索  │
  │      结果          │              │ Agent Agent Agent  │
  │                    │              │   └────┬────┘      │
  │  ❌ 能力有限       │              │        ▼           │
  │  ❌ 难以扩展       │              │     汇总结果        │
  └────────────────────┘              │                    │
                                      │  ✅ 专业分工        │
                                      │  ✅ 能力无限扩展    │
                                      └────────────────────┘
\`\`\`

---

## 🌐 Google A2A 协议

Google 发布的开放标准，定义 Agent 间通信规范。

### 核心概念

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                      A2A 协议架构                                         │
└─────────────────────────────────────────────────────────────────────────┘

         ┌─────────────────────────────────────────────┐
         │              Agent Card (名片)               │
         │  - 名称、描述、能力                          │
         │  - 支持的输入/输出格式                       │
         │  - 认证方式                                  │
         └─────────────────────────────────────────────┘
                              │
                              ▼
  ┌─────────────┐    JSON-RPC 2.0    ┌─────────────┐
  │   Client    │◄─────────────────►│   Server    │
  │   Agent     │    over HTTP(S)    │   Agent     │
  └─────────────┘                    └─────────────┘
        │                                   │
        └───────────┬───────────────────────┘
                    │
                    ▼
            ┌───────────────┐
            │    Task       │
            │  - 任务ID     │
            │  - 状态       │
            │  - 输入/输出  │
            │  - 历史记录   │
            └───────────────┘
\`\`\`

### Agent Card 示例

\`\`\`json
{
  "name": "代码审查 Agent",
  "description": "专业的代码审查和安全检测",
  "url": "https://code-review.example.com/a2a",
  "version": "1.0.0",
  "capabilities": {
    "streaming": true,
    "pushNotifications": false
  },
  "inputModes": ["text/plain", "application/json"],
  "outputModes": ["text/plain", "text/markdown"],
  "skills": [
    {
      "id": "code-review",
      "name": "代码审查",
      "description": "检查代码质量和潜在问题",
      "inputSchema": {
        "type": "object",
        "properties": {
          "code": { "type": "string" },
          "language": { "type": "string" }
        }
      }
    },
    {
      "id": "security-scan",
      "name": "安全扫描",
      "description": "检测安全漏洞"
    }
  ]
}
\`\`\`

---

## 🔧 A2A 通信流程

\`\`\`python
import httpx
import json

class A2AClient:
    def __init__(self, agent_url: str):
        self.agent_url = agent_url
        self.client = httpx.Client()

    def discover(self):
        """发现 Agent 能力"""
        response = self.client.get(f"{self.agent_url}/.well-known/agent.json")
        return response.json()

    def send_task(self, skill_id: str, input_data: dict):
        """发送任务"""
        payload = {
            "jsonrpc": "2.0",
            "id": "task-001",
            "method": "tasks/send",
            "params": {
                "message": {
                    "role": "user",
                    "parts": [{"text": json.dumps(input_data)}]
                },
                "skillId": skill_id
            }
        }
        response = self.client.post(
            f"{self.agent_url}/a2a",
            json=payload
        )
        return response.json()

    def get_task_status(self, task_id: str):
        """查询任务状态"""
        payload = {
            "jsonrpc": "2.0",
            "id": "status-001",
            "method": "tasks/get",
            "params": {"taskId": task_id}
        }
        response = self.client.post(
            f"{self.agent_url}/a2a",
            json=payload
        )
        return response.json()

# 使用示例
client = A2AClient("https://code-review.example.com")

# 1. 发现能力
agent_card = client.discover()
print(f"Agent: {agent_card['name']}")
print(f"Skills: {[s['name'] for s in agent_card['skills']]}")

# 2. 发送任务
result = client.send_task("code-review", {
    "code": "def hello(): print('world')",
    "language": "python"
})
print(f"Task ID: {result['result']['taskId']}")
\`\`\`

---

## 🔗 A2A vs MCP 对比

| 特性 | A2A | MCP |
|------|-----|-----|
| **定位** | Agent 间通信 | Agent 与工具通信 |
| **协议** | JSON-RPC over HTTP | JSON-RPC over stdio/SSE |
| **发现机制** | Agent Card | 工具描述 |
| **适用场景** | 分布式 Agent 网络 | 单机工具集成 |
| **异步支持** | 任务队列 + 回调 | 流式响应 |
| **状态管理** | 有状态 (Task) | 无状态 |

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                   A2A + MCP 组合使用                                      │
└─────────────────────────────────────────────────────────────────────────┘

                           用户请求
                              │
                              ▼
                    ┌──────────────────┐
                    │   协调 Agent      │
                    │   (Claude Code)  │
                    └────────┬─────────┘
                             │
              ┌──────────────┼──────────────┐
              │ A2A          │ A2A          │ MCP
              ▼              ▼              ▼
       ┌──────────┐   ┌──────────┐   ┌──────────┐
       │ 代码审查  │   │ 文档生成  │   │ 本地工具  │
       │ Agent    │   │ Agent    │   │ (文件/DB) │
       └──────────┘   └──────────┘   └──────────┘
            │              │              │
            │ A2A 远程     │ A2A 远程     │ MCP 本地
            └──────────────┼──────────────┘
                           ▼
                        汇总结果
\`\`\`

---

## 🚀 实际应用场景

### 1. 企业工作流自动化

\`\`\`
用户请求："帮我准备下周一的客户演示"
     │
     ├─► 日程 Agent: 查询会议安排
     ├─► CRM Agent: 获取客户信息
     ├─► 文档 Agent: 生成 PPT
     ├─► 邮件 Agent: 发送提醒
     └─► 协调 Agent: 汇总确认
\`\`\`

### 2. 代码开发协作

\`\`\`
用户请求："实现用户登录功能"
     │
     ├─► 设计 Agent: 生成技术方案
     ├─► 编码 Agent: 编写代码
     ├─► 审查 Agent: 代码审查
     ├─► 测试 Agent: 编写测试
     └─► 部署 Agent: 准备上线
\`\`\`

---

## 💡 未来展望

\`\`\`
                    AI Agent 网络化趋势
                           │
         ┌─────────────────┼─────────────────┐
         │                 │                 │
         ▼                 ▼                 ▼
    Agent 市场         Agent 评分        Agent 经济
   (发现&订阅)       (信誉系统)        (自动付费)
         │                 │                 │
         └─────────────────┼─────────────────┘
                           ▼
                   去中心化 Agent 网络
                   (类似 App Store)
\`\`\`

> 🔮 **预测**：未来 5 年内，A2A 协议将成为 AI Agent 互联的标准，
> 就像 HTTP 成为 Web 的标准一样。
            `,ja:`
## Agent-to-Agent：AIエージェント相互連携

単一のAgentでは不十分な場合、複数のAgent協調が未来のトレンドです。

---

## 🎯 A2Aプロトコルとは？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        A2A プロトコル概念                                  │
└─────────────────────────────────────────────────────────────────────────┘

  従来モード：単一Agent                A2Aモード：複数Agent協調
  ┌────────────────────┐              ┌────────────────────┐
  │     ユーザー要求    │              │     ユーザー要求    │
  │         │          │              │         │          │
  │         ▼          │              │         ▼          │
  │   ┌──────────┐     │              │   ┌──────────┐     │
  │   │ Agent A  │     │              │   │ 調整Agent │     │
  │   │(万能型) │     │              │   └─────┬────┘     │
  │   └──────────┘     │              │    ╱    │    ╲     │
  │         │          │              │   ▼     ▼     ▼    │
  │         ▼          │              │ コード データ 検索  │
  │      結果          │              │ Agent Agent Agent  │
  │                    │              │   └────┬────┘      │
  │  ❌ 能力限定       │              │        ▼           │
  │  ❌ 拡張困難       │              │     結果統合        │
  └────────────────────┘              │                    │
                                      │  ✅ 専門分業        │
                                      │  ✅ 無限拡張可能    │
                                      └────────────────────┘
\`\`\`

---

## 🌐 Google A2A プロトコル

Googleが発表したオープン標準、Agent間通信規範を定義。

### コア概念

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                      A2A プロトコルアーキテクチャ                          │
└─────────────────────────────────────────────────────────────────────────┘

         ┌─────────────────────────────────────────────┐
         │              Agent Card (名刺)               │
         │  - 名前、説明、機能                          │
         │  - サポートする入出力形式                    │
         │  - 認証方式                                  │
         └─────────────────────────────────────────────┘
                              │
                              ▼
  ┌─────────────┐    JSON-RPC 2.0    ┌─────────────┐
  │   Client    │◄─────────────────►│   Server    │
  │   Agent     │    over HTTP(S)    │   Agent     │
  └─────────────┘                    └─────────────┘
\`\`\`

### Agent Card 例

\`\`\`json
{
  "name": "コードレビューAgent",
  "description": "専門的なコードレビューとセキュリティ検出",
  "url": "https://code-review.example.com/a2a",
  "version": "1.0.0",
  "skills": [
    {
      "id": "code-review",
      "name": "コードレビュー",
      "description": "コード品質と潜在的問題をチェック"
    }
  ]
}
\`\`\`

---

## 🔗 A2A vs MCP 比較

| 特性 | A2A | MCP |
|------|-----|-----|
| **位置づけ** | Agent間通信 | Agentとツール通信 |
| **プロトコル** | JSON-RPC over HTTP | JSON-RPC over stdio/SSE |
| **発見機構** | Agent Card | ツール説明 |
| **適用シーン** | 分散Agentネットワーク | ローカルツール統合 |
| **状態管理** | ステートフル (Task) | ステートレス |

---

## 🚀 実際の応用シーン

### 1. 企業ワークフロー自動化

\`\`\`
ユーザー要求："来週の顧客プレゼン準備して"
     │
     ├─► カレンダーAgent: 会議予定確認
     ├─► CRM Agent: 顧客情報取得
     ├─► ドキュメントAgent: PPT生成
     └─► 調整Agent: 結果統合
\`\`\`

### 2. コード開発協調

\`\`\`
ユーザー要求："ユーザーログイン機能を実装"
     │
     ├─► 設計Agent: 技術設計書生成
     ├─► コーディングAgent: コード作成
     ├─► レビューAgent: コードレビュー
     └─► テストAgent: テスト作成
\`\`\`

---

## 💡 将来展望

> 🔮 **予測**：今後5年以内に、A2AプロトコルはAI Agent相互接続の標準になる、
> HTTPがWebの標準になったように。
            `}},{id:"ch3-models",title:{zh:"3.12 AI 模型对比与选型",ja:"3.12 AIモデル比較と選定"},content:{zh:`
## 2025 年主流 AI 编程工具对比

选择合适的 AI 工具能让效率事半功倍。以下是基于实测的对比结果。

---

## Claude vs GPT：实测对比

根据 AI超元域 的深度评测，Claude 和 GPT 各有优势：

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                    Claude vs GPT 能力对比                               │
└─────────────────────────────────────────────────────────────────────────┘

                    Claude Sonnet 4.5              GPT-5.1
                    ───────────────                ───────
  长文本生成            ████████████                ██████
                      ~1.2万字符                   ~6900字符

  文学创作              ████████████                ████████
                      意象精准                     存在重复

  前端UI设计            ████████████                ████████
                      配色精致                     基本可用

  知识更新              ████████████                ████████
                      截至2025.1                   截至2024.6

  数学编程              ████████                    ████████████
                      较好                         稍强

  浏览器自动化          ████████                    ████████████
                      支持                         原生集成
\`\`\`

### 推荐选择

| 任务类型 | 推荐模型 | 原因 |
|---------|--------|------|
| 长文本报告 | Claude | 输出更长、结构更好 |
| 文学创作 | Claude | 意象丰富、用词精准 |
| 前端 UI | Claude | 设计感更强 |
| 数学编程 | GPT | 略有优势 |
| 浏览器自动化 | GPT | 原生支持更好 |
| 日常对话 | 两者均可 | 差异不大 |

> 💡 **建议**：同时订阅两个服务，根据任务切换使用。

---

## GPT-5.2-Codex 与 Agent Skills

OpenAI 最新的 GPT-5.2-Codex 引入了 Agent Skills 功能，这是一个重要的行业趋势。

### 什么是 Skills？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        Skills = AI 的工作手册                           │
└─────────────────────────────────────────────────────────────────────────┘

                    ┌───────────────────┐
                    │  你的专业知识      │
                    │  + 工作流程        │
                    │  + 最佳实践        │
                    └─────────┬─────────┘
                              │
                              ▼
                    ┌───────────────────┐
                    │  打包成 Skills 文件 │
                    └─────────┬─────────┘
                              │
                    ┌─────────┴─────────┐
                    │                   │
                    ▼                   ▼
              ┌───────────┐       ┌───────────┐
              │  Claude   │       │  Codex    │
              │  可调用    │       │  可调用    │
              └───────────┘       └───────────┘

        重要：Skills 在 Claude 和 Codex 之间通用！
\`\`\`

### GPT-5.2-Codex 优缺点

**优点：**
- ✅ 编码能力确实提升
- ✅ 视觉理解（看截图写代码）不错
- ✅ Skills 让开发流程更可控
- ✅ 复杂任务完成度好

**缺点：**
- ❌ **速度慢**：简单任务 5 分钟起步
- ❌ 复杂任务 10 分钟以上
- ❌ 完整项目需要半小时

> 💡 **结论**：适合丢任务给 AI 然后去做别的事。需要快速迭代？Claude Code 更顺手。

---

## AI 编程工具生态

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                      2025 AI 编程工具全景                                │
└─────────────────────────────────────────────────────────────────────────┘

  命令行工具                    IDE 集成                   云端服务
  ──────────                   ────────                   ────────
  ┌───────────┐               ┌───────────┐              ┌───────────┐
  │ Claude Code│               │  Cursor   │              │ GitHub    │
  │ 功能最全   │               │ VSCode集成 │              │ Copilot   │
  └───────────┘               └───────────┘              └───────────┘
  ┌───────────┐               ┌───────────┐              ┌───────────┐
  │ GPT Codex │               │  Windsurf │              │ Replit    │
  │ CLI版本   │               │ 新晋选手   │              │ Agent     │
  └───────────┘               └───────────┘              └───────────┘

  辅助工具
  ────────
  ┌───────────┐  ┌───────────┐  ┌───────────┐
  │Ralph Wiggum│  │  Claudia  │  │SuperClaude│
  │自动迭代修复│  │ GUI界面   │  │ 能力增强  │
  └───────────┘  └───────────┘  └───────────┘
\`\`\`

---

## 本节要点

1. **Claude** —— 长文本、创作、设计更强
2. **GPT** —— 数学、浏览器自动化略好
3. **Skills** —— 行业标准，通用性强
4. **按需选择** —— 不要迷信单一工具
            `,ja:`
## 2025年主流AIプログラミングツール比較

適切なAIツールを選ぶことで効率が倍増します。以下は実測に基づく比較結果です。

---

## Claude vs GPT：実測比較

AI超元域の詳細評価によると、ClaudeとGPTにはそれぞれ長所があります：

### 推奨選択

| タスクタイプ | 推奨モデル | 理由 |
|-------------|----------|------|
| 長文レポート | Claude | 出力が長く、構造が良い |
| 文学創作 | Claude | イメージ豊か、用語が正確 |
| フロントエンドUI | Claude | デザイン感が強い |
| 数学プログラミング | GPT | やや優位 |
| ブラウザ自動化 | GPT | ネイティブサポートが良い |
| 日常会話 | どちらでも | 差は小さい |

> 💡 **提案**：両方のサービスを購読し、タスクに応じて使い分ける。

---

## GPT-5.2-CodexとAgent Skills

OpenAI最新のGPT-5.2-CodexはAgent Skills機能を導入。重要な業界トレンドです。

### Skillsとは？

Skillsは「AIの作業マニュアル」—— あなたの専門知識とワークフローをファイルにパッケージ化し、AIがいつでも呼び出せるようにします。

**重要**：SkillsはClaudeとCodex間で互換性あり！

### GPT-5.2-Codexの長所短所

**長所：**
- ✅ コーディング能力が確実に向上
- ✅ ビジュアル理解（スクショからコード生成）が良好
- ✅ Skillsで開発フローをより制御可能

**短所：**
- ❌ **遅い**：簡単なタスクでも5分から
- ❌ 複雑なタスクは10分以上
- ❌ 完全なプロジェクトは30分必要

> 💡 **結論**：タスクをAIに投げて別のことをするなら適している。迅速な反復が必要？Claude Codeがより便利。

---

## AIプログラミングツールエコシステム

\`\`\`
  コマンドラインツール           IDE統合                クラウドサービス
  ────────────────           ────────              ──────────────
  Claude Code                 Cursor                GitHub Copilot
  GPT Codex CLI               Windsurf              Replit Agent

  補助ツール
  ──────────
  Ralph Wiggum（自動反復修正）
  Claudia（GUIインターフェース）
  SuperClaude（能力強化）
\`\`\`

---

## このセクションのポイント

1. **Claude** —— 長文、創作、デザインに強い
2. **GPT** —— 数学、ブラウザ自動化がやや優位
3. **Skills** —— 業界標準、汎用性が高い
4. **ニーズに応じて選択** —— 単一ツールを盲信しない
            `}},{id:"ch3-summary",title:{zh:"3.13 本章小结",ja:"3.13 この章のまとめ"},content:{zh:`
## AI Agent 核心概念回顾

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        AI Agent 知识地图                                │
└─────────────────────────────────────────────────────────────────────────┘

                              AI Agent
                                  │
         ┌────────────────────────┼────────────────────────┐
         │                        │                        │
         ▼                        ▼                        ▼
    ┌──────────┐            ┌──────────┐            ┌──────────┐
    │  核心能力 │            │  工作模式 │            │  实用工具 │
    └──────────┘            └──────────┘            └──────────┘
         │                        │                        │
    ┌────┼────┐              ReAct模式             ┌────┼────┐
    │    │    │              思考-行动-观察         │    │    │
    ▼    ▼    ▼                                   ▼    ▼    ▼
  规划  工具  记忆                             代码  通用  自动化
                                               Agent Agent 工作流
\`\`\`

---

## Agent vs 普通 AI

| 特性 | 普通 AI | AI Agent |
|------|---------|----------|
| 交互方式 | 一问一答 | 自主执行 |
| 任务复杂度 | 单一任务 | 复杂任务 |
| 工具使用 | 有限 | 丰富 |
| 自主性 | 被动响应 | 主动规划 |

---

## 快速行动清单

- [ ] 尝试使用 ChatGPT 的代码解释器功能
- [ ] 用 Claude 完成一个需要搜索的任务
- [ ] 探索一个代码 Agent（如 Cursor）
- [ ] 思考你的工作中哪些任务可以用 Agent 自动化

---

## 关键金句

> "Agent 是 AI 从'顾问'变成'助手'的关键一步。"

> "未来不是 AI 替代人，而是会用 Agent 的人替代不会用的人。"

---

下一章，我们将学习 **RAG（检索增强生成）** —— 让 AI 能够获取最新知识，解决"知识过时"的问题！
            `,ja:`
## AI Agentコア概念の復習

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        AI Agent 知識マップ                               │
└─────────────────────────────────────────────────────────────────────────┘

                              AI Agent
                                  │
         ┌────────────────────────┼────────────────────────┐
         │                        │                        │
         ▼                        ▼                        ▼
    ┌──────────┐            ┌──────────┐            ┌──────────┐
    │  コア能力 │            │  動作モード │            │ 実用ツール │
    └──────────┘            └──────────┘            └──────────┘
         │                        │                        │
    ┌────┼────┐              ReActパターン           ┌────┼────┐
    │    │    │              思考-行動-観察          │    │    │
    ▼    ▼    ▼                                    ▼    ▼    ▼
  計画  ツール 記憶                              コード 汎用  自動化
                                                Agent Agent ワークフロー
\`\`\`

---

## Agent vs 通常のAI

| 特性 | 通常のAI | AI Agent |
|------|----------|----------|
| インタラクション | 一問一答 | 自律実行 |
| タスクの複雑さ | 単一タスク | 複雑なタスク |
| ツール使用 | 限定的 | 豊富 |
| 自律性 | 受動的応答 | 能動的計画 |

---

## クイックアクションリスト

- [ ] ChatGPTのコードインタープリター機能を試す
- [ ] Claudeで検索が必要なタスクを完了する
- [ ] コードAgent（Cursorなど）を探索する
- [ ] 仕事でAgentで自動化できるタスクを考える

---

## 重要な格言

> 「AgentはAIが『アドバイザー』から『アシスタント』になるための重要な一歩です。」

> 「未来はAIが人を置き換えるのではなく、Agentを使える人が使えない人を置き換えるのです。」

---

次の章では、**RAG（検索拡張生成）**を学びます —— AIが最新の知識を取得し、「知識の陳腐化」問題を解決できるようにします！
            `}}]},{id:"chapter-4",number:4,title:{zh:"RAG 检索增强生成",ja:"RAG 検索拡張生成"},subtitle:{zh:"让AI获取最新知识",ja:"AIに最新知識を取得させる"},sections:[{id:"ch4-intro",title:{zh:"引言：AI 的知识困境",ja:"序章：AIの知識ジレンマ"},content:{zh:`
你有没有遇到过这样的情况：

- 问 AI 今天的新闻，它说不知道
- 问公司内部的规章制度，它回答不了
- 问最新的产品信息，它给出过时的答案

这是因为 AI 的知识有**截止日期**，而且它不了解你的**私有数据**。

**RAG（Retrieval-Augmented Generation，检索增强生成）** 就是解决这个问题的关键技术！

---

## AI 知识的局限性

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        AI 知识的两大局限                                │
└─────────────────────────────────────────────────────────────────────────┘

  局限1：知识有截止日期                    局限2：不了解私有数据
  ┌──────────────────────┐               ┌──────────────────────┐
  │                      │               │                      │
  │  训练数据截止到       │               │  AI 不知道：          │
  │  某个时间点           │               │                      │
  │                      │               │  • 你公司的规章制度    │
  │  比如：2024年1月      │               │  • 你的产品文档        │
  │                      │               │  • 你的客户数据        │
  │  之后的事情都不知道    │               │  • 你的内部知识库      │
  │                      │               │                      │
  └──────────────────────┘               └──────────────────────┘

                           RAG 可以解决这两个问题！
\`\`\`

---

## 什么是 RAG？

RAG 的核心思想很简单：

> **先检索相关信息，再让 AI 回答**

就像一个学生考试时可以查资料一样，RAG 让 AI 在回答问题前，先从知识库中找到相关内容。

---

## 本章你将学到

1. **RAG 的工作原理** —— 它是如何运作的
2. **向量数据库** —— RAG 的核心技术
3. **RAG 的应用场景** —— 在哪些地方使用
4. **如何使用 RAG** —— 实际操作方法
5. **RAG 的优缺点** —— 了解它的边界

让我们深入了解这个让 AI "与时俱进"的技术！
            `,ja:`
こんな経験はありませんか：

- AIに今日のニュースを聞くと、知らないと言う
- 会社の規則を聞くと、答えられない
- 最新の製品情報を聞くと、古い答えが返ってくる

これはAIの知識に**締め切り日**があり、あなたの**プライベートデータ**を知らないからです。

**RAG（Retrieval-Augmented Generation、検索拡張生成）**がこの問題を解決する重要な技術です！

---

## AI知識の限界

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        AI知識の2つの限界                                │
└─────────────────────────────────────────────────────────────────────────┘

  限界1：知識に締め切りがある              限界2：プライベートデータを知らない
  ┌──────────────────────┐               ┌──────────────────────┐
  │                      │               │                      │
  │  トレーニングデータは  │               │  AIが知らないこと：    │
  │  ある時点で終了        │               │                      │
  │                      │               │  • あなたの会社の規則  │
  │  例：2024年1月        │               │  • あなたの製品ドキュメント │
  │                      │               │  • あなたの顧客データ   │
  │  それ以降のことは知らない│               │  • あなたの社内知識ベース │
  │                      │               │                      │
  └──────────────────────┘               └──────────────────────┘

                           RAGはこの両方を解決できます！
\`\`\`

---

## RAGとは？

RAGの核心的なアイデアはシンプル：

> **まず関連情報を検索し、それからAIに回答させる**

試験中に資料を参照できる学生のように、RAGはAIが質問に答える前に、知識ベースから関連コンテンツを見つけられるようにします。

---

## この章で学ぶこと

1. **RAGの仕組み** —— どのように動作するか
2. **ベクトルデータベース** —— RAGのコア技術
3. **RAGの応用シーン** —— どこで使われるか
4. **RAGの使い方** —— 実際の操作方法
5. **RAGの長所と短所** —— その境界を理解する

AIを「時代に追いつかせる」この技術を深く理解しましょう！
            `}},{id:"ch4-how-it-works",title:{zh:"4.1 RAG 的工作原理",ja:"4.1 RAGの仕組み"},content:{zh:`
让我们看看 RAG 是如何工作的。

### 交互式演示：RAG 工作流程

先通过这个交互式演示直观感受 RAG 的完整流程：

::rag-viz::

---

## RAG 工作流程

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                          RAG 完整流程                                   │
└─────────────────────────────────────────────────────────────────────────┘

  【准备阶段】                           【使用阶段】

   文档/知识                              用户提问
      │                                     │
      ▼                                     ▼
  ┌─────────┐                          ┌─────────┐
  │ 文本分块 │                          │ 问题向量化│
  └─────────┘                          └─────────┘
      │                                     │
      ▼                                     ▼
  ┌─────────┐                          ┌─────────┐
  │ 向量化   │                          │ 相似度搜索│ ◄── 从向量数据库
  └─────────┘                          └─────────┘
      │                                     │
      ▼                                     ▼
  ┌─────────┐                          ┌─────────┐
  │向量数据库│                          │ 获取相关文档│
  └─────────┘                          └─────────┘
                                           │
                                           ▼
                                      ┌─────────┐
                                      │组合提示词│
                                      │(问题+文档)│
                                      └─────────┘
                                           │
                                           ▼
                                      ┌─────────┐
                                      │ LLM 回答 │
                                      └─────────┘
\`\`\`

---

## 核心步骤详解

### 1. 文本分块（Chunking）

把长文档切分成小块：

\`\`\`
原始文档（10000字）
    │
    ▼
┌─────┐ ┌─────┐ ┌─────┐ ┌─────┐
│块1  │ │块2  │ │块3  │ │...  │
│500字│ │500字│ │500字│ │     │
└─────┘ └─────┘ └─────┘ └─────┘
\`\`\`

为什么要分块？
- LLM 有上下文长度限制
- 小块更容易精确匹配
- 提高检索效率

---

### 2. 向量化（Embedding）

把文本转换成数字向量：

\`\`\`
"人工智能是..." ──▶ [0.12, -0.34, 0.56, ..., 0.78]
                        （高维向量）
\`\`\`

**为什么用向量？**

- 语义相似的内容，向量也相似
- 可以进行数学计算（相似度）
- 比关键词匹配更智能

---

### 3. 相似度搜索

当用户提问时：
1. 把问题也转成向量
2. 在向量数据库中找最相似的文档块
3. 返回 top-K 个最相关的结果

\`\`\`
用户问题向量  ──▶  在向量空间中找最近的邻居
     ●              ┌─────────────────────┐
                    │    ●  ●             │
                    │  ●      ●           │
                    │    ●●      找到！   │
                    │      ⬛ ◄───────    │
                    │    ●                │
                    └─────────────────────┘
\`\`\`

---

### 4. 组合提示词

把检索到的内容和用户问题组合：

\`\`\`
【系统提示】
你是一个助手，请根据以下参考资料回答问题。
如果资料中没有相关信息，请说"我没有找到相关信息"。

【参考资料】
{检索到的文档块1}
{检索到的文档块2}
{检索到的文档块3}

【用户问题】
{用户的问题}
\`\`\`

---

## 本节要点

1. **两个阶段** —— 准备阶段（建索引）+ 使用阶段（检索回答）
2. **文本分块** —— 把长文档切成小块
3. **向量化** —— 把文本转成数字向量
4. **相似度搜索** —— 找到最相关的内容
5. **组合提示** —— 把检索结果和问题一起给 LLM
            `,ja:`
RAGがどのように動作するか見てみましょう。

### インタラクティブデモ：RAGワークフロー

このインタラクティブデモでRAGの完全なフローを直感的に体験してください：

::rag-viz::

---

## RAGワークフロー

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                          RAG 完全フロー                                  │
└─────────────────────────────────────────────────────────────────────────┘

  【準備段階】                           【使用段階】

   ドキュメント/知識                        ユーザーの質問
      │                                     │
      ▼                                     ▼
  ┌─────────┐                          ┌─────────┐
  │テキスト分割│                          │質問をベクトル化│
  └─────────┘                          └─────────┘
      │                                     │
      ▼                                     ▼
  ┌─────────┐                          ┌─────────┐
  │ ベクトル化 │                          │類似度検索│ ◄── ベクトルDBから
  └─────────┘                          └─────────┘
      │                                     │
      ▼                                     ▼
  ┌─────────┐                          ┌─────────┐
  │ベクトルDB │                          │関連文書を取得│
  └─────────┘                          └─────────┘
                                           │
                                           ▼
                                      ┌─────────┐
                                      │プロンプト組み合わせ│
                                      │(質問+文書)│
                                      └─────────┘
                                           │
                                           ▼
                                      ┌─────────┐
                                      │ LLM が回答 │
                                      └─────────┘
\`\`\`

---

## コアステップの詳細

### 1. テキスト分割（Chunking）

長いドキュメントを小さな塊に分割：

\`\`\`
元のドキュメント（10000文字）
    │
    ▼
┌─────┐ ┌─────┐ ┌─────┐ ┌─────┐
│チャンク1│ │チャンク2│ │チャンク3│ │...  │
│500文字│ │500文字│ │500文字│ │     │
└─────┘ └─────┘ └─────┘ └─────┘
\`\`\`

なぜ分割するのか？
- LLMにはコンテキスト長の制限がある
- 小さな塊の方が正確にマッチング
- 検索効率が向上

---

### 2. ベクトル化（Embedding）

テキストを数値ベクトルに変換：

\`\`\`
"人工知能は..." ──▶ [0.12, -0.34, 0.56, ..., 0.78]
                        （高次元ベクトル）
\`\`\`

**なぜベクトルを使うのか？**

- 意味的に似た内容はベクトルも似ている
- 数学的計算（類似度）が可能
- キーワードマッチングより賢い

---

### 3. 類似度検索

ユーザーが質問したとき：
1. 質問もベクトルに変換
2. ベクトルDBで最も似た文書チャンクを探す
3. top-K個の最も関連する結果を返す

\`\`\`
質問ベクトル  ──▶  ベクトル空間で最近傍を探す
     ●              ┌─────────────────────┐
                    │    ●  ●             │
                    │  ●      ●           │
                    │    ●●      見つけた！│
                    │      ⬛ ◄───────    │
                    │    ●                │
                    └─────────────────────┘
\`\`\`

---

### 4. プロンプト組み合わせ

検索した内容とユーザーの質問を組み合わせ：

\`\`\`
【システムプロンプト】
あなたはアシスタントです。以下の参考資料に基づいて質問に答えてください。
資料に関連情報がない場合は「関連情報が見つかりませんでした」と言ってください。

【参考資料】
{検索された文書チャンク1}
{検索された文書チャンク2}
{検索された文書チャンク3}

【ユーザーの質問】
{ユーザーの質問}
\`\`\`

---

## このセクションのポイント

1. **2つの段階** —— 準備段階（インデックス作成）+ 使用段階（検索と回答）
2. **テキスト分割** —— 長いドキュメントを小さく分割
3. **ベクトル化** —— テキストを数値ベクトルに変換
4. **類似度検索** —— 最も関連するコンテンツを見つける
5. **プロンプト組み合わせ** —— 検索結果と質問をLLMに渡す
            `}},{id:"ch4-applications",title:{zh:"4.2 RAG 的应用场景",ja:"4.2 RAGの応用シーン"},content:{zh:`
RAG 在很多场景中都有广泛应用。

---

## 企业知识库问答

\`\`\`
场景：员工想查询公司规章制度

传统方式：在几十个文档中搜索关键词
RAG 方式：直接问 AI，它会从知识库中找到答案

例子：
问："年假怎么申请？"
答："根据公司员工手册第5章，年假申请需要提前3天
    在OA系统提交，经主管审批后生效..."
\`\`\`

---

## 客服智能问答

\`\`\`
场景：客户咨询产品问题

优势：
• 24小时自动回答
• 答案基于官方文档
• 减少人工客服压力

例子：
问："这个产品支持 Mac 吗？"
答："根据产品说明书，本产品支持 macOS 10.15 及以上版本..."
\`\`\`

---

## 个人知识管理

\`\`\`
场景：管理你的笔记、文档、收藏

使用方式：
1. 把你的笔记、PDF、网页收藏导入
2. 用自然语言提问
3. AI 从你的知识库中找答案

例子：
问："我之前看过一篇关于时间管理的文章，核心观点是什么？"
答："根据你收藏的《高效能人士的七个习惯》笔记，
    核心观点是要事第一，区分紧急和重要..."
\`\`\`

---

## 代码文档助手

\`\`\`
场景：查询项目代码文档

优势：
• 不需要记住所有 API
• 快速找到使用方法
• 结合代码上下文回答

例子：
问："这个项目的数据库连接怎么配置？"
答："根据项目 README 和 config.py，
    数据库配置在 .env 文件中设置..."
\`\`\`

---

## 应用场景对比

| 场景 | 知识来源 | 典型问题 |
|------|----------|----------|
| 企业知识库 | 规章制度、培训材料 | 报销流程是什么？ |
| 客服问答 | 产品文档、FAQ | 如何退货？ |
| 个人知识管理 | 笔记、收藏、PDF | 我之前学过的XX是什么？ |
| 代码助手 | 代码、文档、注释 | 这个函数怎么用？ |
| 学术研究 | 论文、报告 | 某领域最新进展？ |

---

## 本节要点

1. **企业应用** —— 知识库问答、客服系统
2. **个人应用** —— 笔记管理、学习助手
3. **开发应用** —— 代码文档、API 查询
4. **共同特点** —— 基于私有/专业知识回答
            `,ja:`
RAGは多くのシーンで広く活用されています。

---

## 企業ナレッジベースQ&A

\`\`\`
シーン：従業員が会社の規則を調べたい

従来の方法：数十のドキュメントでキーワード検索
RAG方式：AIに直接質問、知識ベースから答えを見つける

例：
質問：「有給休暇の申請方法は？」
回答：「社員ハンドブック第5章によると、有給休暇の申請は
      3日前までにOAシステムで提出し、上司の承認後に有効になります...」
\`\`\`

---

## カスタマーサービスQ&A

\`\`\`
シーン：顧客が製品について質問

メリット：
• 24時間自動回答
• 公式ドキュメントに基づく回答
• 人間のサポート負担を軽減

例：
質問：「この製品はMacに対応していますか？」
回答：「製品マニュアルによると、本製品はmacOS 10.15以上に対応しています...」
\`\`\`

---

## 個人ナレッジ管理

\`\`\`
シーン：あなたのノート、ドキュメント、お気に入りを管理

使い方：
1. ノート、PDF、ブックマークをインポート
2. 自然言語で質問
3. AIがあなたの知識ベースから答えを見つける

例：
質問：「以前読んだ時間管理についての記事、核心的なポイントは何でしたっけ？」
回答：「あなたがブックマークした『7つの習慣』のノートによると、
      核心ポイントは重要事項を優先し、緊急と重要を区別することです...」
\`\`\`

---

## コードドキュメントアシスタント

\`\`\`
シーン：プロジェクトのコードドキュメントを照会

メリット：
• すべてのAPIを覚える必要がない
• 使用方法をすばやく見つける
• コードコンテキストと組み合わせて回答

例：
質問：「このプロジェクトのデータベース接続はどう設定する？」
回答：「プロジェクトのREADMEとconfig.pyによると、
      データベース設定は.envファイルで設定します...」
\`\`\`

---

## 応用シーン比較

| シーン | 知識ソース | 典型的な質問 |
|--------|------------|--------------|
| 企業ナレッジベース | 規則、トレーニング資料 | 経費精算のプロセスは？ |
| カスタマーサービス | 製品ドキュメント、FAQ | 返品方法は？ |
| 個人ナレッジ管理 | ノート、ブックマーク、PDF | 以前学んだXXって何？ |
| コードアシスタント | コード、ドキュメント、コメント | この関数の使い方は？ |
| 学術研究 | 論文、レポート | この分野の最新動向は？ |

---

## このセクションのポイント

1. **企業応用** —— ナレッジベースQ&A、カスタマーサービス
2. **個人応用** —— ノート管理、学習アシスタント
3. **開発応用** —— コードドキュメント、APIクエリ
4. **共通点** —— プライベート/専門知識に基づく回答
            `}},{id:"ch4-optimization",title:{zh:"4.3 RAG 优化技巧",ja:"4.3 RAG 最適化テクニック"},content:{zh:`
## 让 RAG 更精准的关键技巧

RAG 系统的效果取决于每个环节的优化。以下是提升 RAG 性能的核心技巧。

---

## 分块策略：如何切分文档

分块（Chunking）是 RAG 的基础，直接影响检索质量。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                          分块策略对比                                    │
└─────────────────────────────────────────────────────────────────────────┘

  固定大小分块                         语义分块
  ────────────                        ──────────
  ┌─────────────┐                    ┌─────────────┐
  │ 500 字符    │                    │  完整段落    │
  ├─────────────┤                    ├─────────────┤
  │ 500 字符    │                    │  完整章节    │
  ├─────────────┤                    ├─────────────┤
  │ 500 字符    │                    │  完整概念    │
  └─────────────┘                    └─────────────┘
       │                                  │
       ▼                                  ▼
  可能切断语义                        保持语义完整
  简单但不精准                        复杂但更准确
\`\`\`

### 最佳实践

| 策略 | 适用场景 | 典型大小 |
|------|----------|----------|
| 固定大小 + 重叠 | 通用场景 | 512 tokens + 50 重叠 |
| 句子分块 | 精确问答 | 3-5 句 |
| 段落分块 | 文档摘要 | 按 \\n\\n 分割 |
| 递归分块 | 结构化文档 | 先章节，再段落 |

---

## 混合检索：向量 + 关键词

单一向量检索可能遗漏关键词匹配，混合检索结合两者优势。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                          混合检索架构                                    │
└─────────────────────────────────────────────────────────────────────────┘

                          用户查询
                             │
              ┌──────────────┼──────────────┐
              │                             │
              ▼                             ▼
      ┌─────────────┐               ┌─────────────┐
      │  向量检索   │               │  关键词检索  │
      │ (语义相似)  │               │ (BM25/TF-IDF)|
      └──────┬──────┘               └──────┬──────┘
             │                              │
             │  相似度: 0.85               │  匹配度: 0.72
             │  相似度: 0.82               │  匹配度: 0.68
             │  相似度: 0.79               │  匹配度: 0.65
             │                              │
             └──────────────┬──────────────┘
                            │
                    ┌───────▼───────┐
                    │   融合排序    │
                    │ (RRF / 加权)  │
                    └───────┬───────┘
                            │
                            ▼
                    ┌───────────────┐
                    │  最终结果 Top-K│
                    └───────────────┘
\`\`\`

**融合公式（RRF）：**
\`\`\`
score = Σ 1/(k + rank_i)

其中：k 通常取 60
     rank_i 是该文档在第 i 个检索结果中的排名
\`\`\`

---

## 查询改写：提升检索召回率

用户的原始查询可能不够精确，通过改写提升匹配度。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                          查询改写技巧                                    │
└─────────────────────────────────────────────────────────────────────────┘

原始查询: "公司的请假流程是什么？"
                │
                ▼
        ┌───────────────┐
        │   查询改写    │
        └───────────────┘
                │
    ┌───────────┼───────────┐
    │           │           │
    ▼           ▼           ▼
  同义扩展    问题分解    假设文档

"请假流程"   "如何请假"   "请假审批流程
"休假申请"   "请假需要    包括：提交申请、
"事假流程"    什么材料"   主管审批..."
\`\`\`

### HyDE（假设文档嵌入）

让 LLM 先生成一个"假设的答案"，用这个答案去检索：

\`\`\`
用户问题: "如何配置 Nginx 反向代理？"
                │
                ▼
┌─────────────────────────────────────────────────────────────────────────┐
│ LLM 生成假设答案:                                                        │
│ "要配置 Nginx 反向代理，需要在 server 块中使用 location 和              │
│  proxy_pass 指令。首先编辑 nginx.conf，添加 upstream 定义后端..."        │
└─────────────────────────────────────────────────────────────────────────┘
                │
                ▼
        用假设答案向量检索
                │
                ▼
        返回真正相关的文档
\`\`\`

---

## Reranker：精排提升准确率

初步检索后，用更强的模型对结果重新排序。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                          两阶段检索                                      │
└─────────────────────────────────────────────────────────────────────────┘

                    ┌───────────────┐
                    │  向量检索     │
                    │  (快速召回)   │
                    └───────┬───────┘
                            │
                    返回 Top-100
                            │
                            ▼
                    ┌───────────────┐
                    │  Reranker     │
                    │  (精确排序)   │
                    └───────┬───────┘
                            │
                    返回 Top-5
                            │
                            ▼
                    ┌───────────────┐
                    │    LLM 生成   │
                    └───────────────┘

召回阶段：快但不够准 → 精排阶段：准但较慢
\`\`\`

### 常用 Reranker

| 模型 | 特点 | 推荐场景 |
|------|------|----------|
| Cohere Rerank | 效果好，API 调用 | 生产环境 |
| BGE Reranker | 开源，可本地部署 | 私有化部署 |
| Cross-Encoder | 精确但慢 | 小规模数据 |

---

## 实战建议

### 1. 从简单开始
\`\`\`
初始配置:
├── 分块: 512 tokens + 10% 重叠
├── Embedding: text-embedding-3-small
├── 检索: Top-5
└── 无 Reranker
\`\`\`

### 2. 逐步优化
\`\`\`
发现问题 → 针对性优化:
├── 召回不全 → 混合检索 / 查询改写
├── 相关性差 → 添加 Reranker
├── 语义断裂 → 调整分块策略
└── 响应慢 → 缓存 / 预计算
\`\`\`

### 3. 评估指标
\`\`\`
检索质量:
├── Recall@K: 相关文档在 Top-K 的比例
├── MRR: 第一个相关结果的位置
└── NDCG: 排序质量评分

端到端:
├── 答案准确率
├── 用户满意度
└── 响应延迟
\`\`\`

---

## 本节要点

1. **分块策略** —— 保持语义完整性
2. **混合检索** —— 向量 + 关键词双管齐下
3. **查询改写** —— HyDE 等技巧提升召回
4. **Reranker** —— 两阶段检索提升精度
            `,ja:`
## RAGをより正確にするための重要なテクニック

RAGシステムの効果は各段階の最適化に依存します。以下はRAG性能を向上させるコアテクニックです。

---

## チャンキング戦略：ドキュメントの分割方法

チャンキング（Chunking）はRAGの基礎であり、検索品質に直接影響します。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                          チャンキング戦略比較                              │
└─────────────────────────────────────────────────────────────────────────┘

  固定サイズチャンキング                 セマンティックチャンキング
  ──────────────────                    ────────────────────
  ┌─────────────┐                      ┌─────────────┐
  │ 500 文字    │                      │  完全な段落  │
  ├─────────────┤                      ├─────────────┤
  │ 500 文字    │                      │  完全な章    │
  ├─────────────┤                      ├─────────────┤
  │ 500 文字    │                      │  完全な概念  │
  └─────────────┘                      └─────────────┘
       │                                    │
       ▼                                    ▼
  意味が切れる可能性                    意味の完全性を保持
  シンプルだが不正確                    複雑だが正確
\`\`\`

### ベストプラクティス

| 戦略 | 適用シーン | 典型的なサイズ |
|------|----------|----------|
| 固定サイズ + オーバーラップ | 汎用 | 512 tokens + 50 重複 |
| 文チャンキング | 精密Q&A | 3-5 文 |
| 段落チャンキング | ドキュメント要約 | \\n\\n で分割 |
| 再帰的チャンキング | 構造化文書 | 先に章、次に段落 |

---

## ハイブリッド検索：ベクトル + キーワード

単一のベクトル検索ではキーワードマッチが漏れる可能性があり、ハイブリッド検索は両方の利点を組み合わせます。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                          ハイブリッド検索アーキテクチャ                     │
└─────────────────────────────────────────────────────────────────────────┘

                          ユーザークエリ
                             │
              ┌──────────────┼──────────────┐
              │                             │
              ▼                             ▼
      ┌─────────────┐               ┌─────────────┐
      │  ベクトル検索 │               │ キーワード検索│
      │ (意味類似性) │               │ (BM25/TF-IDF)|
      └──────┬──────┘               └──────┬──────┘
             │                              │
             │  類似度: 0.85               │  マッチ度: 0.72
             │  類似度: 0.82               │  マッチ度: 0.68
             │  類似度: 0.79               │  マッチ度: 0.65
             │                              │
             └──────────────┬──────────────┘
                            │
                    ┌───────▼───────┐
                    │   融合ランキング │
                    │ (RRF / 重み付け) │
                    └───────┬───────┘
                            │
                            ▼
                    ┌───────────────┐
                    │  最終結果 Top-K│
                    └───────────────┘
\`\`\`

**融合式（RRF）：**
\`\`\`
score = Σ 1/(k + rank_i)

ここで：k は通常 60
     rank_i は i番目の検索結果でのドキュメントの順位
\`\`\`

---

## クエリ書き換え：検索リコール率の向上

ユーザーの元のクエリは十分に正確でない可能性があり、書き換えでマッチ度を向上させます。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                          クエリ書き換えテクニック                          │
└─────────────────────────────────────────────────────────────────────────┘

元のクエリ: "会社の休暇申請プロセスは？"
                │
                ▼
        ┌───────────────┐
        │   クエリ書き換え │
        └───────────────┘
                │
    ┌───────────┼───────────┐
    │           │           │
    ▼           ▼           ▼
  同義語展開   質問分解    仮想ドキュメント

"休暇申請"   "休暇の     "休暇承認プロセス
"有給申請"   取り方は？"  には：申請提出、
"欠勤届"    "必要な      上司承認..."
            書類は？"
\`\`\`

### HyDE（仮想ドキュメント埋め込み）

LLMにまず「仮想の回答」を生成させ、その回答で検索します：

\`\`\`
ユーザーの質問: "Nginxのリバースプロキシ設定方法は？"
                │
                ▼
┌─────────────────────────────────────────────────────────────────────────┐
│ LLM が仮想回答を生成:                                                     │
│ "Nginxのリバースプロキシを設定するには、serverブロック内で location と     │
│  proxy_pass ディレクティブを使用します。まず nginx.conf を編集し..."       │
└─────────────────────────────────────────────────────────────────────────┘
                │
                ▼
        仮想回答でベクトル検索
                │
                ▼
        本当に関連するドキュメントを返す
\`\`\`

---

## Reranker：精密ランキングで精度向上

初期検索後、より強力なモデルで結果を再ランキングします。

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                          2段階検索                                       │
└─────────────────────────────────────────────────────────────────────────┘

                    ┌───────────────┐
                    │  ベクトル検索  │
                    │  (高速リコール) │
                    └───────┬───────┘
                            │
                    Top-100 を返す
                            │
                            ▼
                    ┌───────────────┐
                    │  Reranker     │
                    │  (精密ソート)  │
                    └───────┬───────┘
                            │
                    Top-5 を返す
                            │
                            ▼
                    ┌───────────────┐
                    │    LLM 生成   │
                    └───────────────┘

リコール段階：速いが不正確 → 精密段階：正確だが遅い
\`\`\`

### よく使う Reranker

| モデル | 特徴 | 推奨シーン |
|------|------|----------|
| Cohere Rerank | 効果良好、API呼び出し | 本番環境 |
| BGE Reranker | オープンソース、ローカル展開可 | プライベート展開 |
| Cross-Encoder | 正確だが遅い | 小規模データ |

---

## 実践的なアドバイス

### 1. シンプルから始める
\`\`\`
初期設定:
├── チャンキング: 512 tokens + 10% オーバーラップ
├── Embedding: text-embedding-3-small
├── 検索: Top-5
└── Reranker なし
\`\`\`

### 2. 段階的に最適化
\`\`\`
問題発見 → 的を絞った最適化:
├── リコール不足 → ハイブリッド検索 / クエリ書き換え
├── 関連性が低い → Reranker 追加
├── 意味が分断 → チャンキング戦略を調整
└── レスポンスが遅い → キャッシュ / 事前計算
\`\`\`

### 3. 評価指標
\`\`\`
検索品質:
├── Recall@K: Top-K での関連ドキュメントの割合
├── MRR: 最初の関連結果の位置
└── NDCG: ランキング品質スコア

エンドツーエンド:
├── 回答精度
├── ユーザー満足度
└── レスポンス遅延
\`\`\`

---

## このセクションのポイント

1. **チャンキング戦略** —— 意味の完全性を保つ
2. **ハイブリッド検索** —— ベクトル + キーワードの両方を活用
3. **クエリ書き換え** —— HyDE などでリコール向上
4. **Reranker** —— 2段階検索で精度向上
            `}},{id:"ch4-code-practice",title:{zh:"4.4 RAG 实战代码",ja:"4.4 RAG 実践コード"},content:{zh:`
## 从零搭建 RAG 知识库

学会了原理，让我们动手写代码！

---

## 方案一：LangChain + Chroma

最流行的 RAG 实现方案。

### 安装依赖

\`\`\`bash
pip install langchain langchain-openai chromadb
\`\`\`

### 核心代码

\`\`\`python
from langchain_community.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.chains import RetrievalQA

# 1. 加载文档
loader = DirectoryLoader("./documents", glob="**/*.txt")
documents = loader.load()

# 2. 分块
splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)
chunks = splitter.split_documents(documents)

# 3. 创建向量数据库
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory="./chroma_db"
)

# 4. 创建问答链
llm = ChatOpenAI(model="gpt-4o-mini")
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever(search_kwargs={"k": 5})
)

# 5. 问答
result = qa_chain.invoke({"query": "你的问题"})
print(result["result"])
\`\`\`

---

## 方案二：LlamaIndex（5行代码）

\`\`\`python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

# 1-2. 加载并创建索引
documents = SimpleDirectoryReader("./documents").load_data()
index = VectorStoreIndex.from_documents(documents)

# 3-4. 查询
query_engine = index.as_query_engine()
response = query_engine.query("你的问题")
print(response)
\`\`\`

---

## 向量数据库对比

| 数据库 | 特点 | 适用场景 |
|--------|------|----------|
| **Chroma** | 轻量本地 | 开发/原型 |
| **Pinecone** | 云托管 | 生产环境 |
| **Milvus** | 开源高性能 | 大规模部署 |
| **FAISS** | Meta开源 | 嵌入式场景 |

---

## 本地部署（无需OpenAI）

\`\`\`python
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import Ollama

# 本地 Embedding
embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-small-zh-v1.5"
)

# 本地 LLM
llm = Ollama(model="qwen2.5:7b")
\`\`\`

---

## 常见问题

| 问题 | 解决方案 |
|------|----------|
| 检索不到 | 缩小 chunk_size |
| 回答不准 | 增加 k 值或添加 Reranker |
| 速度慢 | 换用 smaller embedding |
| 中文效果差 | 用多语言模型 bge-m3 |

---

## 本节要点

1. **LangChain** —— 灵活、组件丰富
2. **LlamaIndex** —— 简洁、上手快
3. **向量数据库** —— Chroma开发/Pinecone生产
4. **本地部署** —— HuggingFace + Ollama
            `,ja:`
## ゼロから RAG ナレッジベースを構築

原理を学んだので、コードを書きましょう！

---

## ソリューション1：LangChain + Chroma

最も人気のあるRAG実装ソリューション。

### 依存関係のインストール

\`\`\`bash
pip install langchain langchain-openai chromadb
\`\`\`

### コアコード

\`\`\`python
from langchain_community.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.chains import RetrievalQA

# 1. ドキュメント読み込み
loader = DirectoryLoader("./documents", glob="**/*.txt")
documents = loader.load()

# 2. チャンキング
splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)
chunks = splitter.split_documents(documents)

# 3. ベクトルDB作成
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory="./chroma_db"
)

# 4. QAチェーン作成
llm = ChatOpenAI(model="gpt-4o-mini")
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever(search_kwargs={"k": 5})
)

# 5. 質問応答
result = qa_chain.invoke({"query": "あなたの質問"})
print(result["result"])
\`\`\`

---

## ソリューション2：LlamaIndex（5行コード）

\`\`\`python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

# 1-2. 読み込みとインデックス作成
documents = SimpleDirectoryReader("./documents").load_data()
index = VectorStoreIndex.from_documents(documents)

# 3-4. クエリ
query_engine = index.as_query_engine()
response = query_engine.query("あなたの質問")
print(response)
\`\`\`

---

## ベクトルデータベース比較

| データベース | 特徴 | 適用シーン |
|--------------|------|------------|
| **Chroma** | 軽量ローカル | 開発/プロトタイプ |
| **Pinecone** | クラウドマネージド | 本番環境 |
| **Milvus** | オープンソース高性能 | 大規模デプロイ |
| **FAISS** | Metaオープンソース | 組み込みシナリオ |

---

## ローカルデプロイ（OpenAI不要）

\`\`\`python
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import Ollama

# ローカルEmbedding
embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-small-zh-v1.5"
)

# ローカルLLM
llm = Ollama(model="qwen2.5:7b")
\`\`\`

---

## よくある問題

| 問題 | 解決策 |
|------|--------|
| 検索されない | chunk_sizeを小さく |
| 回答が不正確 | k値を増やす/Reranker追加 |
| 速度が遅い | smaller embeddingに変更 |
| 中文効果が悪い | 多言語モデルbge-m3を使用 |

---

## このセクションのポイント

1. **LangChain** —— 柔軟、コンポーネント豊富
2. **LlamaIndex** —— シンプル、素早く開始
3. **ベクトルDB** —— Chroma開発/Pinecone本番
4. **ローカルデプロイ** —— HuggingFace + Ollama
            `}},{id:"ch4-graphrag",title:{zh:"4.5 GraphRAG：知识图谱增强",ja:"4.5 GraphRAG：ナレッジグラフ強化"},content:{zh:`
## 当向量检索不够用时：知识图谱来帮忙

传统 RAG 只能找到"相似"的内容，GraphRAG 能理解实体之间的"关系"。

---

## 🎯 传统 RAG vs GraphRAG

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                   传统 RAG vs GraphRAG                                   │
└─────────────────────────────────────────────────────────────────────────┘

  传统 RAG (向量检索)                   GraphRAG (图谱检索)
  ┌────────────────────────┐           ┌────────────────────────┐
  │                        │           │   张三 ──工作于──► 公司A │
  │  文档1 ████ 0.92       │           │     │                  │
  │  文档2 ███░ 0.85       │           │   朋友               投资
  │  文档3 ██░░ 0.71       │           │     │                  │
  │                        │           │     ▼                  ▼
  │  纯语义相似度匹配       │           │   李四 ──合作──► 公司B │
  │  ❌ 无法理解关系        │           │                        │
  │  ❌ 跨文档推理弱        │           │  ✅ 关系推理           │
  └────────────────────────┘           │  ✅ 多跳问答           │
                                       └────────────────────────┘
\`\`\`

---

## 📊 什么时候用 GraphRAG？

| 场景 | 传统 RAG | GraphRAG |
|------|----------|----------|
| 简单问答 | ✅ 够用 | 杀鸡用牛刀 |
| 多跳推理 | ❌ 弱 | ✅ 强 |
| 实体关系 | ❌ 无法处理 | ✅ 核心能力 |
| 全局摘要 | ❌ 信息丢失 | ✅ 社区检测 |
| 构建成本 | ⭐ 低 | ⭐⭐⭐ 高 |

**典型问题对比**：
- "张三的公司做什么业务？" → 传统 RAG 可以
- "张三的朋友们投资了哪些公司？" → 需要 GraphRAG

---

## 🔧 Microsoft GraphRAG

微软开源的 GraphRAG 实现，业界标杆。

### 核心流程

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                   GraphRAG 构建流程                                       │
└─────────────────────────────────────────────────────────────────────────┘

                         原始文档
                            │
                            ▼
                    ┌───────────────┐
                    │   文本分块    │
                    └───────┬───────┘
                            │
                            ▼
                    ┌───────────────┐
                    │  实体抽取     │ ◄── LLM 提取人名、组织、地点等
                    └───────┬───────┘
                            │
                            ▼
                    ┌───────────────┐
                    │  关系抽取     │ ◄── LLM 识别实体间关系
                    └───────┬───────┘
                            │
                            ▼
                    ┌───────────────┐
                    │  图谱构建     │ ◄── 节点 + 边
                    └───────┬───────┘
                            │
                            ▼
                    ┌───────────────┐
                    │  社区检测     │ ◄── Leiden 算法
                    └───────┬───────┘
                            │
                            ▼
                    ┌───────────────┐
                    │  摘要生成     │ ◄── 每个社区生成摘要
                    └───────────────┘
\`\`\`

### 安装与使用

\`\`\`bash
# 安装
pip install graphrag

# 初始化项目
graphrag init --root ./my-graphrag

# 配置 settings.yaml
# 需要设置 OpenAI API Key

# 索引文档
graphrag index --root ./my-graphrag

# 查询 - 局部搜索 (实体相关问题)
graphrag query --root ./my-graphrag --method local "张三在哪家公司工作？"

# 查询 - 全局搜索 (需要全局理解)
graphrag query --root ./my-graphrag --method global "这些文档主要讲什么内容？"
\`\`\`

### 配置示例 (settings.yaml)

\`\`\`yaml
llm:
  api_key: \${OPENAI_API_KEY}
  model: gpt-4o-mini

embeddings:
  llm:
    api_key: \${OPENAI_API_KEY}
    model: text-embedding-3-small

chunks:
  size: 1200
  overlap: 100

entity_extraction:
  prompt: "Extract all named entities from the following text..."
  max_gleanings: 1

community_reports:
  max_length: 2000

local_search:
  text_unit_prop: 0.5
  community_prop: 0.1
\`\`\`

---

## 🔧 LangChain + Neo4j GraphRAG

使用 LangChain 和 Neo4j 构建 GraphRAG。

\`\`\`python
from langchain_community.graphs import Neo4jGraph
from langchain_experimental.graph_transformers import LLMGraphTransformer
from langchain_openai import ChatOpenAI
from langchain.docstore.document import Document

# 1. 连接 Neo4j
graph = Neo4jGraph(
    url="bolt://localhost:7687",
    username="neo4j",
    password="password"
)

# 2. 配置 LLM
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# 3. 创建图谱转换器
transformer = LLMGraphTransformer(llm=llm)

# 4. 准备文档
documents = [
    Document(page_content="""
    张三是ABC科技公司的CEO。李四是张三的大学同学，
    现在在XYZ投资公司担任合伙人。XYZ投资公司是ABC科技的
    A轮投资方。王五是ABC科技的CTO，曾经在谷歌工作。
    """)
]

# 5. 提取实体和关系
graph_documents = transformer.convert_to_graph_documents(documents)

# 6. 存储到 Neo4j
graph.add_graph_documents(graph_documents)

# 7. 查询示例
result = graph.query("""
    MATCH (p:Person)-[:WORKS_AT]->(c:Company)
    RETURN p.name, c.name
""")
print(result)
# [{'p.name': '张三', 'c.name': 'ABC科技公司'}, ...]
\`\`\`

### GraphRAG 检索

\`\`\`python
from langchain.chains import GraphCypherQAChain

# 创建问答链
chain = GraphCypherQAChain.from_llm(
    llm=llm,
    graph=graph,
    verbose=True
)

# 自然语言查询
response = chain.invoke("张三的同学投资了哪家公司？")
print(response)
# 张三的同学李四投资了ABC科技公司

# 复杂推理
response = chain.invoke("ABC科技公司的CTO之前在哪里工作？")
print(response)
# ABC科技公司的CTO王五之前在谷歌工作
\`\`\`

---

## 🔧 LlamaIndex GraphRAG

\`\`\`python
from llama_index.core import KnowledgeGraphIndex, SimpleDirectoryReader
from llama_index.llms.openai import OpenAI
from llama_index.core import Settings

# 配置
Settings.llm = OpenAI(model="gpt-4o-mini", temperature=0)
Settings.chunk_size = 512

# 加载文档
documents = SimpleDirectoryReader("./data").load_data()

# 构建知识图谱索引
kg_index = KnowledgeGraphIndex.from_documents(
    documents,
    max_triplets_per_chunk=10,
    include_embeddings=True
)

# 查询
query_engine = kg_index.as_query_engine(
    include_text=True,
    response_mode="tree_summarize"
)

response = query_engine.query("张三和李四是什么关系？")
print(response)
\`\`\`

---

## 📊 Hybrid RAG：向量 + 图谱

最佳实践是结合两种方法：

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                   Hybrid RAG 架构                                        │
└─────────────────────────────────────────────────────────────────────────┘

                         用户问题
                            │
              ┌─────────────┴─────────────┐
              │       问题分析器          │
              └─────────────┬─────────────┘
                   ╱                ╲
          简单语义问题          关系推理问题
                 │                    │
                 ▼                    ▼
        ┌──────────────┐     ┌──────────────┐
        │  向量检索    │     │  图谱检索    │
        │  (Chroma)    │     │  (Neo4j)     │
        └───────┬──────┘     └───────┬──────┘
                │                    │
                └────────┬───────────┘
                         │
                         ▼
                ┌──────────────┐
                │  结果融合    │
                │  重排序      │
                └───────┬──────┘
                        │
                        ▼
                ┌──────────────┐
                │  LLM 回答    │
                └──────────────┘
\`\`\`

---

## 💡 何时选择 GraphRAG？

\`\`\`
                    选择决策树
                        │
            ┌───────────┴───────────┐
            │  需要理解实体关系吗？   │
            └───────────┬───────────┘
                  ╱           ╲
                否              是
                │               │
                ▼               ▼
            传统 RAG       ┌─────────┐
             够用         │ 数据量？ │
                          └────┬────┘
                           ╱       ╲
                        小/中        大
                          │          │
                          ▼          ▼
                     LlamaIndex   Microsoft
                     + Neo4j     GraphRAG
\`\`\`

> 🎯 **建议**：先从传统 RAG 开始，遇到关系推理瓶颈再考虑 GraphRAG。
> GraphRAG 构建成本高（大量 LLM 调用），但对特定场景效果显著。
            `,ja:`
## ベクトル検索だけでは不十分な時：ナレッジグラフの出番

従来のRAGは「類似」コンテンツしか見つけられませんが、GraphRAGはエンティティ間の「関係」を理解できます。

---

## 🎯 従来のRAG vs GraphRAG

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                   従来のRAG vs GraphRAG                                  │
└─────────────────────────────────────────────────────────────────────────┘

  従来のRAG（ベクトル検索）            GraphRAG（グラフ検索）
  ┌────────────────────────┐           ┌────────────────────────┐
  │                        │           │   田中 ──勤務──► A社    │
  │  文書1 ████ 0.92       │           │     │                  │
  │  文書2 ███░ 0.85       │           │   友人               投資
  │  文書3 ██░░ 0.71       │           │     │                  │
  │                        │           │     ▼                  ▼
  │  純粋な意味的類似度     │           │   鈴木 ──協力──► B社   │
  │  ❌ 関係理解不可        │           │                        │
  │  ❌ 文書間推論弱い      │           │  ✅ 関係推論           │
  └────────────────────────┘           │  ✅ 複数ホップ質問応答  │
                                       └────────────────────────┘
\`\`\`

---

## 📊 いつGraphRAGを使う？

| シーン | 従来のRAG | GraphRAG |
|--------|----------|----------|
| 単純な質問応答 | ✅ 十分 | オーバースペック |
| 複数ホップ推論 | ❌ 弱い | ✅ 強い |
| エンティティ関係 | ❌ 処理不可 | ✅ コア機能 |
| グローバル要約 | ❌ 情報損失 | ✅ コミュニティ検出 |
| 構築コスト | ⭐ 低 | ⭐⭐⭐ 高 |

**典型的な質問の比較**：
- 「田中さんの会社は何の事業？」→ 従来のRAGでOK
- 「田中さんの友人たちが投資した会社は？」→ GraphRAGが必要

---

## 🔧 Microsoft GraphRAG

Microsoftがオープンソース化したGraphRAG実装、業界標準。

### コアフロー

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                   GraphRAG 構築フロー                                     │
└─────────────────────────────────────────────────────────────────────────┘

                         元文書
                            │
                            ▼
                    ┌───────────────┐
                    │   テキスト分割 │
                    └───────┬───────┘
                            │
                            ▼
                    ┌───────────────┐
                    │  エンティティ抽出 │ ◄── LLMで人名、組織、地名などを抽出
                    └───────┬───────┘
                            │
                            ▼
                    ┌───────────────┐
                    │  関係抽出     │ ◄── LLMでエンティティ間関係を識別
                    └───────┬───────┘
                            │
                            ▼
                    ┌───────────────┐
                    │  グラフ構築   │ ◄── ノード + エッジ
                    └───────────────┘
\`\`\`

### インストールと使用

\`\`\`bash
# インストール
pip install graphrag

# プロジェクト初期化
graphrag init --root ./my-graphrag

# 文書のインデックス作成
graphrag index --root ./my-graphrag

# クエリ - ローカル検索
graphrag query --root ./my-graphrag --method local "田中さんはどの会社で働いている？"

# クエリ - グローバル検索
graphrag query --root ./my-graphrag --method global "これらの文書は主に何について？"
\`\`\`

---

## 🔧 LangChain + Neo4j GraphRAG

\`\`\`python
from langchain_community.graphs import Neo4jGraph
from langchain_experimental.graph_transformers import LLMGraphTransformer
from langchain_openai import ChatOpenAI

# 1. Neo4j接続
graph = Neo4jGraph(
    url="bolt://localhost:7687",
    username="neo4j",
    password="password"
)

# 2. LLM設定
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# 3. グラフ変換器作成
transformer = LLMGraphTransformer(llm=llm)

# 4. エンティティと関係を抽出
graph_documents = transformer.convert_to_graph_documents(documents)

# 5. Neo4jに保存
graph.add_graph_documents(graph_documents)
\`\`\`

---

## 📊 Hybrid RAG：ベクトル + グラフ

ベストプラクティスは両方を組み合わせること：

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                   Hybrid RAG アーキテクチャ                               │
└─────────────────────────────────────────────────────────────────────────┘

                         ユーザー質問
                            │
              ┌─────────────┴─────────────┐
              │       質問分析器          │
              └─────────────┬─────────────┘
                   ╱                ╲
          単純意味質問          関係推論質問
                 │                    │
                 ▼                    ▼
        ┌──────────────┐     ┌──────────────┐
        │  ベクトル検索 │     │  グラフ検索  │
        │  (Chroma)    │     │  (Neo4j)     │
        └───────┬──────┘     └───────┬──────┘
                │                    │
                └────────┬───────────┘
                         │
                         ▼
                ┌──────────────┐
                │  結果融合    │
                └───────┬──────┘
                        │
                        ▼
                ┌──────────────┐
                │  LLM回答     │
                └──────────────┘
\`\`\`

---

## 💡 いつGraphRAGを選ぶ？

> 🎯 **アドバイス**：まず従来のRAGから始め、関係推論のボトルネックに遭遇したらGraphRAGを検討。
> GraphRAGは構築コストが高い（大量のLLM呼び出し）が、特定シーンでは効果が顕著。
            `}},{id:"ch4-summary",title:{zh:"4.6 本章小结",ja:"4.6 この章のまとめ"},content:{zh:`
## RAG 核心概念回顾

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                         RAG 知识地图                                    │
└─────────────────────────────────────────────────────────────────────────┘

                               RAG
                                │
         ┌──────────────────────┼──────────────────────┐
         │                      │                      │
         ▼                      ▼                      ▼
    ┌──────────┐          ┌──────────┐          ┌──────────┐
    │  工作流程 │          │  核心技术 │          │  应用场景 │
    └──────────┘          └──────────┘          └──────────┘
         │                      │                      │
    ┌────┼────┐          ┌──────┼──────┐          ┌────┼────┐
    │    │    │          │      │      │          │    │    │
    ▼    ▼    ▼          ▼      ▼      ▼          ▼    ▼    ▼
  检索   拡充   生成     分块   向量化   数据库    企业  个人  开发
\`\`\`

---

## RAG vs 微调

| 对比项 | RAG | 微调(Fine-tuning) |
|--------|-----|-------------------|
| 知识更新 | 只需更新文档 | 需要重新训练 |
| 成本 | 较低 | 较高 |
| 实现难度 | 简单 | 复杂 |
| 适用场景 | 知识库问答 | 特定领域专家 |
| 可解释性 | 可追溯来源 | 黑盒 |

---

## RAG 的优缺点

### 优点
- ✅ 可以实时更新知识
- ✅ 答案可追溯来源
- ✅ 不需要训练模型
- ✅ 保护私有数据

### 缺点
- ❌ 依赖检索质量
- ❌ 需要维护知识库
- ❌ 有上下文长度限制
- ❌ 可能检索到无关内容

---

## 快速行动清单

- [ ] 了解常用 AI 工具的知识截止日期
- [ ] 思考工作中有哪些知识可以用 RAG 管理
- [ ] 尝试使用一个 RAG 工具（如 Notion AI、企业知识库）
- [ ] 关注 RAG 技术的发展动态

---

## 关键金句

> "RAG 让 AI 从'博学但过时'变成'与时俱进且专业'。"

> "检索增强生成是连接 AI 与现实世界知识的桥梁。"

---

## 全书总结

恭喜你完成了这本 AI 进阶实战指南！

你学到了：
1. **提示词工程** —— 与 AI 高效对话的艺术
2. **AI Agent** —— 让 AI 自主完成复杂任务
3. **RAG 技术** —— 让 AI 获取最新、私有知识

**AI 的世界在不断快速发展，保持学习，保持好奇！**

*"最好的投资是对自己认知的投资。"*
            `,ja:`
## RAGコア概念の復習

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                         RAG 知識マップ                                   │
└─────────────────────────────────────────────────────────────────────────┘

                               RAG
                                │
         ┌──────────────────────┼──────────────────────┐
         │                      │                      │
         ▼                      ▼                      ▼
    ┌──────────┐          ┌──────────┐          ┌──────────┐
    │ ワークフロー │          │  コア技術  │          │ 応用シーン │
    └──────────┘          └──────────┘          └──────────┘
         │                      │                      │
    ┌────┼────┐          ┌──────┼──────┐          ┌────┼────┐
    │    │    │          │      │      │          │    │    │
    ▼    ▼    ▼          ▼      ▼      ▼          ▼    ▼    ▼
  検索   拡張   生成      分割  ベクトル化 DB      企業  個人   開発
\`\`\`

---

## RAG vs ファインチューニング

| 比較項目 | RAG | ファインチューニング |
|----------|-----|---------------------|
| 知識の更新 | ドキュメント更新のみ | 再トレーニングが必要 |
| コスト | 低い | 高い |
| 実装難易度 | 簡単 | 複雑 |
| 適用シーン | ナレッジベースQ&A | 特定分野の専門家 |
| 説明可能性 | ソースを追跡可能 | ブラックボックス |

---

## RAGの長所と短所

### 長所
- ✅ 知識をリアルタイムで更新可能
- ✅ 回答のソースを追跡可能
- ✅ モデルのトレーニング不要
- ✅ プライベートデータを保護

### 短所
- ❌ 検索品質に依存
- ❌ 知識ベースのメンテナンスが必要
- ❌ コンテキスト長の制限がある
- ❌ 無関係なコンテンツを検索する可能性

---

## クイックアクションリスト

- [ ] よく使うAIツールの知識締め切り日を確認
- [ ] 仕事でRAGで管理できる知識を考える
- [ ] RAGツール（Notion AI、企業ナレッジベースなど）を試す
- [ ] RAG技術の発展動向をフォロー

---

## 重要な格言

> 「RAGはAIを『博識だが古い』から『時代に追いついて専門的』に変えます。」

> 「検索拡張生成は、AIと現実世界の知識をつなぐ架け橋です。」

---

## 全書のまとめ

このAI進級実践ガイドを完了おめでとうございます！

学んだこと：
1. **プロンプトエンジニアリング** —— AIとの効率的な対話術
2. **AI Agent** —— AIに複雑なタスクを自律的に完了させる
3. **RAG技術** —— AIに最新・プライベート知識を取得させる

**AIの世界は急速に発展し続けています。学び続け、好奇心を持ち続けましょう！**

*「最高の投資は、自分の認知への投資です。」*
            `}}]},{id:"chapter-5",number:5,title:{zh:"多模态 AI",ja:"マルチモーダルAI"},subtitle:{zh:"图像、语音、视频的 AI 革命",ja:"画像・音声・動画のAI革命"},sections:[{id:"ch5-intro",title:{zh:"5.1 多模态 AI 概述",ja:"5.1 マルチモーダルAI概要"},content:{zh:`
## 超越文字：AI 的感官革命

多模态 AI 让机器能够像人一样"看"、"听"、"说"。

---

## 🎯 什么是多模态？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        多模态 AI 能力矩阵                                 │
└─────────────────────────────────────────────────────────────────────────┘

                         多模态 AI
                            │
        ┌───────────┬───────┴───────┬───────────┐
        │           │               │           │
        ▼           ▼               ▼           ▼
     👁️ 视觉      👂 听觉        💬 语言      🎨 生成
        │           │               │           │
   ┌────┴────┐  ┌───┴───┐     ┌────┴────┐  ┌────┴────┐
   │ 图像理解 │  │ 语音识别│     │ 文本理解 │  │ 内容创作 │
   │ OCR    │  │ 音乐分析│     │ 翻译    │  │ 图像生成 │
   │ 视频分析 │  │ 声音克隆│     │ 对话    │  │ 视频生成 │
   └─────────┘  └────────┘     └─────────┘  └─────────┘
\`\`\`

---

## 📊 主流多模态模型对比

| 模型 | 厂商 | 视觉理解 | 音频 | 视频 | 生成 |
|------|------|----------|------|------|------|
| **GPT-4o** | OpenAI | ✅ 强 | ✅ 语音 | ⚠️ 有限 | ✅ DALL-E |
| **Claude 3.5** | Anthropic | ✅ 强 | ❌ | ❌ | ❌ |
| **Gemini 1.5** | Google | ✅ 强 | ✅ 语音 | ✅ 长视频 | ✅ Imagen |
| **Qwen-VL** | 阿里 | ✅ 强 | ✅ 语音 | ✅ | ❌ |
| **GPT-4o** 实时 | OpenAI | ✅ | ✅ 实时语音 | ✅ | ✅ |

---

## 🔧 基础使用示例

### OpenAI Vision

\`\`\`python
from openai import OpenAI
import base64

client = OpenAI()

# 方法1：URL 图片
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "描述这张图片的内容"},
                {"type": "image_url", "image_url": {"url": "https://example.com/image.jpg"}}
            ]
        }
    ]
)

# 方法2：Base64 图片
def encode_image(image_path):
    with open(image_path, "rb") as f:
        return base64.b64encode(f.read()).decode("utf-8")

base64_image = encode_image("local_image.jpg")

response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "这张图片里有什么？"},
                {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}}
            ]
        }
    ]
)
\`\`\`

### Claude Vision

\`\`\`python
import anthropic
import base64

client = anthropic.Anthropic()

# 读取本地图片
with open("image.jpg", "rb") as f:
    image_data = base64.standard_b64encode(f.read()).decode("utf-8")

response = client.messages.create(
    model="claude-sonnet-4-20250514",
    max_tokens=1024,
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": "image/jpeg",
                        "data": image_data
                    }
                },
                {"type": "text", "text": "详细描述这张图片"}
            ]
        }
    ]
)
\`\`\`

---

## 💡 应用场景

| 场景 | 说明 | 示例 |
|------|------|------|
| **文档解析** | 识别表格、图表、公式 | 财报分析、论文解读 |
| **产品检测** | 质检、缺陷识别 | 工业制造 |
| **医学影像** | 辅助诊断 | X光、CT分析 |
| **电商描述** | 自动生成商品文案 | 图片→描述 |
| **无障碍** | 为视障人士描述图片 | 屏幕阅读 |
            `,ja:`
## 文字を超えて：AIの感覚革命

マルチモーダルAIは、機械が人間のように「見る」「聞く」「話す」ことを可能にします。

---

## 🎯 マルチモーダルとは？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        マルチモーダルAI能力マトリクス                       │
└─────────────────────────────────────────────────────────────────────────┘

                         マルチモーダルAI
                            │
        ┌───────────┬───────┴───────┬───────────┐
        │           │               │           │
        ▼           ▼               ▼           ▼
     👁️ 視覚      👂 聴覚        💬 言語      🎨 生成
        │           │               │           │
   ┌────┴────┐  ┌───┴───┐     ┌────┴────┐  ┌────┴────┐
   │ 画像理解 │  │ 音声認識│     │ テキスト │  │ コンテンツ│
   │ OCR    │  │ 音楽分析│     │ 翻訳    │  │ 画像生成 │
   │ 動画分析 │  │ 音声複製│     │ 対話    │  │ 動画生成 │
   └─────────┘  └────────┘     └─────────┘  └─────────┘
\`\`\`

---

## 📊 主要マルチモーダルモデル比較

| モデル | メーカー | 視覚理解 | 音声 | 動画 | 生成 |
|--------|----------|----------|------|------|------|
| **GPT-4o** | OpenAI | ✅ 強い | ✅ 音声 | ⚠️ 限定 | ✅ DALL-E |
| **Claude 3.5** | Anthropic | ✅ 強い | ❌ | ❌ | ❌ |
| **Gemini 1.5** | Google | ✅ 強い | ✅ 音声 | ✅ 長時間動画 | ✅ Imagen |

---

## 💡 応用シーン

| シーン | 説明 | 例 |
|--------|------|-----|
| **文書解析** | 表、チャート、数式認識 | 財務報告分析 |
| **製品検査** | 品質管理、欠陥検出 | 製造業 |
| **医療画像** | 診断支援 | X線、CT分析 |
| **EC説明文** | 商品説明自動生成 | 画像→説明文 |
            `}},{id:"ch5-image-gen",title:{zh:"5.2 AI 图像生成",ja:"5.2 AI画像生成"},content:{zh:`
## 文字变图片：图像生成技术

从 DALL-E 到 Midjourney，AI 绘画正在改变创意产业。

---

## 📊 主流图像生成模型

| 模型 | 特点 | 适用场景 | 价格 |
|------|------|----------|------|
| **DALL-E 3** | 语义理解强 | 创意插图 | API 付费 |
| **Midjourney** | 艺术感强 | 设计海报 | 订阅制 |
| **Stable Diffusion** | 开源可控 | 本地部署 | 免费 |
| **Imagen 3** | Google最新 | 真实感强 | API 付费 |

---

## 🔧 DALL-E 3 使用

\`\`\`python
from openai import OpenAI

client = OpenAI()

# 生成图片
response = client.images.generate(
    model="dall-e-3",
    prompt="一只穿着西装的柴犬，正在咖啡馆喝咖啡，温暖的下午阳光，油画风格",
    size="1024x1024",
    quality="hd",
    n=1
)

image_url = response.data[0].url
print(image_url)

# 图片编辑（DALL-E 2）
response = client.images.edit(
    image=open("original.png", "rb"),
    mask=open("mask.png", "rb"),  # 要编辑的区域为白色
    prompt="将背景换成海滩",
    size="1024x1024"
)
\`\`\`

---

## 🔧 Stable Diffusion 本地部署

\`\`\`python
from diffusers import StableDiffusionPipeline
import torch

# 加载模型
pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16
)
pipe = pipe.to("cuda")

# 生成图片
prompt = "a beautiful sunset over mountains, 4k, detailed"
image = pipe(prompt).images[0]
image.save("sunset.png")

# 使用 LoRA 微调模型
pipe.load_lora_weights("lora-weights-folder")
image = pipe(prompt).images[0]
\`\`\`

---

## 💡 Prompt 工程技巧

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                      图像 Prompt 结构                                    │
└─────────────────────────────────────────────────────────────────────────┘

  [主体] + [细节] + [风格] + [质量词]

  示例：
  ┌──────────────────────────────────────────────────────────────────┐
  │ 一只橘猫 + 戴着墨镜，躺在沙滩上 + 像素风格 + 8k, 高细节        │
  └──────────────────────────────────────────────────────────────────┘

  常用质量词：
  - 4k, 8k, ultra detailed, photorealistic
  - trending on artstation, award winning
  - studio lighting, professional photography
\`\`\`
            `,ja:`
## テキストから画像へ：画像生成技術

DALL-EからMidjourneyまで、AI絵画がクリエイティブ産業を変えています。

---

## 📊 主要画像生成モデル

| モデル | 特徴 | 適用シーン | 価格 |
|--------|------|------------|------|
| **DALL-E 3** | 意味理解が強い | クリエイティブイラスト | API課金 |
| **Midjourney** | 芸術性が高い | デザインポスター | サブスク |
| **Stable Diffusion** | オープンソース | ローカルデプロイ | 無料 |

---

## 🔧 DALL-E 3 使用例

\`\`\`python
from openai import OpenAI

client = OpenAI()

response = client.images.generate(
    model="dall-e-3",
    prompt="スーツを着た柴犬、カフェでコーヒーを飲んでいる、暖かい午後の光、油絵スタイル",
    size="1024x1024",
    quality="hd",
    n=1
)

image_url = response.data[0].url
\`\`\`

---

## 💡 Promptエンジニアリングのコツ

\`\`\`
[主題] + [詳細] + [スタイル] + [品質ワード]

例：
オレンジ猫 + サングラスをかけてビーチに寝ている + ピクセルアート + 8k, 高詳細
\`\`\`
            `}},{id:"ch5-audio",title:{zh:"5.3 语音识别与合成",ja:"5.3 音声認識と合成"},content:{zh:`
## AI 的耳朵和嘴巴：语音技术

从语音转文字到文字转语音，AI 让交互更自然。

---

## 📊 语音技术全景

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        语音 AI 技术栈                                    │
└─────────────────────────────────────────────────────────────────────────┘

                    语音 AI
                       │
         ┌─────────────┼─────────────┐
         │             │             │
         ▼             ▼             ▼
      ASR          TTS          语音助手
   (语音→文字)    (文字→语音)    (实时对话)
         │             │             │
    ┌────┴────┐   ┌────┴────┐   ┌────┴────┐
    │ Whisper │   │ OpenAI  │   │ GPT-4o  │
    │ Azure   │   │ ElevenLabs│ │ Realtime │
    │ 讯飞    │   │ Edge TTS │   │ Gemini  │
    └─────────┘   └─────────┘   └─────────┘
\`\`\`

---

## 🔧 OpenAI Whisper

\`\`\`python
from openai import OpenAI

client = OpenAI()

# 语音转文字
with open("audio.mp3", "rb") as audio_file:
    transcript = client.audio.transcriptions.create(
        model="whisper-1",
        file=audio_file,
        language="zh"  # 可选，自动检测
    )

print(transcript.text)

# 带时间戳
transcript = client.audio.transcriptions.create(
    model="whisper-1",
    file=open("audio.mp3", "rb"),
    response_format="verbose_json",
    timestamp_granularities=["word"]
)

for word in transcript.words:
    print(f"{word['start']:.2f}s: {word['word']}")
\`\`\`

---

## 🔧 文字转语音 (TTS)

\`\`\`python
from openai import OpenAI

client = OpenAI()

# OpenAI TTS
response = client.audio.speech.create(
    model="tts-1-hd",
    voice="nova",  # alloy, echo, fable, onyx, nova, shimmer
    input="你好，欢迎使用 AI 语音合成技术！"
)

response.stream_to_file("output.mp3")

# 流式播放
from pathlib import Path
import pygame

pygame.mixer.init()
with open("output.mp3", "wb") as f:
    for chunk in response.iter_bytes():
        f.write(chunk)
pygame.mixer.music.load("output.mp3")
pygame.mixer.music.play()
\`\`\`

### Edge TTS (免费)

\`\`\`python
import edge_tts
import asyncio

async def text_to_speech():
    communicate = edge_tts.Communicate(
        "今天天气真好！",
        "zh-CN-XiaoxiaoNeural"  # 中文女声
    )
    await communicate.save("output.mp3")

asyncio.run(text_to_speech())

# 可用声音列表
voices = await edge_tts.list_voices()
for v in voices:
    if "zh" in v["Locale"]:
        print(v["ShortName"])
\`\`\`

---

## 🔧 实时语音对话

\`\`\`python
# OpenAI Realtime API (WebSocket)
import asyncio
import websockets
import json

async def realtime_conversation():
    async with websockets.connect(
        "wss://api.openai.com/v1/realtime?model=gpt-4o-realtime",
        extra_headers={"Authorization": f"Bearer {api_key}"}
    ) as ws:
        # 发送配置
        await ws.send(json.dumps({
            "type": "session.update",
            "session": {
                "voice": "alloy",
                "instructions": "你是一个友好的助手"
            }
        }))

        # 发送音频数据
        audio_data = get_microphone_audio()
        await ws.send(json.dumps({
            "type": "input_audio_buffer.append",
            "audio": audio_data
        }))

        # 接收响应
        async for message in ws:
            data = json.loads(message)
            if data["type"] == "response.audio.delta":
                play_audio(data["delta"])
\`\`\`

---

## 💡 应用场景

| 场景 | 技术 | 示例 |
|------|------|------|
| **会议记录** | Whisper | 自动生成会议纪要 |
| **播客制作** | TTS | AI 主播 |
| **客服热线** | Realtime | 智能语音客服 |
| **无障碍** | TTS | 屏幕阅读器 |
| **语言学习** | ASR+TTS | 口语练习 |
            `,ja:`
## AIの耳と口：音声技術

音声からテキスト、テキストから音声へ、AIがより自然なインタラクションを実現。

---

## 🔧 OpenAI Whisper

\`\`\`python
from openai import OpenAI

client = OpenAI()

# 音声をテキストに
with open("audio.mp3", "rb") as audio_file:
    transcript = client.audio.transcriptions.create(
        model="whisper-1",
        file=audio_file,
        language="ja"
    )

print(transcript.text)
\`\`\`

---

## 🔧 テキストから音声 (TTS)

\`\`\`python
from openai import OpenAI

client = OpenAI()

response = client.audio.speech.create(
    model="tts-1-hd",
    voice="nova",
    input="こんにちは、AI音声合成技術へようこそ！"
)

response.stream_to_file("output.mp3")
\`\`\`

---

## 💡 応用シーン

| シーン | 技術 | 例 |
|--------|------|-----|
| **会議記録** | Whisper | 議事録自動生成 |
| **ポッドキャスト** | TTS | AIアナウンサー |
| **カスタマーサービス** | Realtime | 音声AI対応 |
            `}},{id:"ch5-video",title:{zh:"5.4 视频理解与生成",ja:"5.4 動画理解と生成"},content:{zh:`
## AI 电影导演：视频技术

从视频分析到视频生成，AI 正在改变影视制作。

---

## 📊 视频 AI 技术对比

| 技术 | 模型 | 能力 | 限制 |
|------|------|------|------|
| **视频理解** | Gemini 1.5 | 分析1小时视频 | - |
| **视频理解** | GPT-4o | 短视频分析 | 帧数限制 |
| **视频生成** | Sora | 60秒电影级 | 未公开 |
| **视频生成** | Runway Gen-3 | 10秒高质量 | 订阅制 |
| **视频生成** | Pika | 4秒动画 | 免费额度 |

---

## 🔧 Gemini 视频分析

\`\`\`python
import google.generativeai as genai

genai.configure(api_key="YOUR_API_KEY")

# 上传视频
video_file = genai.upload_file("video.mp4")

# 等待处理完成
import time
while video_file.state.name == "PROCESSING":
    time.sleep(10)
    video_file = genai.get_file(video_file.name)

# 分析视频
model = genai.GenerativeModel("gemini-1.5-pro")
response = model.generate_content([
    video_file,
    "详细描述这个视频的内容，包括场景、人物、动作"
])

print(response.text)
\`\`\`

---

## 🔧 Runway API

\`\`\`python
import runwayml

client = runwayml.RunwayML()

# 文字生成视频
task = client.image_to_video.create(
    model="gen3a_turbo",
    prompt_image="start_frame.jpg",
    prompt_text="camera slowly zooms in, dramatic lighting"
)

# 等待完成
import time
while True:
    status = client.tasks.retrieve(task.id)
    if status.status == "SUCCEEDED":
        break
    time.sleep(5)

# 下载视频
video_url = status.output[0]
\`\`\`

---

## 🎬 视频生成 Prompt 技巧

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                     视频 Prompt 要素                                     │
└─────────────────────────────────────────────────────────────────────────┘

  1. 镜头运动：
     - camera slowly pans left/right
     - zoom in/out
     - tracking shot
     - aerial view

  2. 光影效果：
     - golden hour lighting
     - dramatic shadows
     - soft diffused light
     - neon glow

  3. 动作描述：
     - walking slowly through...
     - water gently flowing...
     - leaves falling from trees...

  示例：
  "A lone astronaut walking on Mars surface,
   camera tracking from behind,
   dust particles floating in sunlight,
   cinematic, 4K"
\`\`\`

---

## 💡 未来展望

> 🎬 **趋势预测**：
> - 2025：AI 短视频普及
> - 2026：AI 长视频成熟
> - 2027：AI 互动电影出现
            `,ja:`
## AI映画監督：動画技術

動画分析から動画生成まで、AIが映像制作を変えています。

---

## 📊 動画AI技術比較

| 技術 | モデル | 能力 | 制限 |
|------|--------|------|------|
| **動画理解** | Gemini 1.5 | 1時間動画分析 | - |
| **動画生成** | Sora | 60秒映画級 | 未公開 |
| **動画生成** | Runway Gen-3 | 10秒高品質 | サブスク |

---

## 🔧 Gemini 動画分析

\`\`\`python
import google.generativeai as genai

genai.configure(api_key="YOUR_API_KEY")

video_file = genai.upload_file("video.mp4")

model = genai.GenerativeModel("gemini-1.5-pro")
response = model.generate_content([
    video_file,
    "この動画の内容を詳しく説明してください"
])
\`\`\`

---

## 💡 将来展望

> 🎬 **トレンド予測**：
> - 2025：AI短編動画が普及
> - 2026：AI長編動画が成熟
> - 2027：AIインタラクティブ映画が登場
            `}},{id:"ch5-summary",title:{zh:"5.5 本章小结",ja:"5.5 この章のまとめ"},content:{zh:`
## 多模态 AI 核心回顾

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                     多模态 AI 知识地图                                    │
└─────────────────────────────────────────────────────────────────────────┘

                            多模态 AI
                                │
          ┌─────────────────────┼─────────────────────┐
          │                     │                     │
          ▼                     ▼                     ▼
       理解类                 生成类                交互类
          │                     │                     │
    ┌─────┴─────┐         ┌─────┴─────┐         ┌─────┴─────┐
    │ 图像理解  │         │ 图像生成  │         │ 实时语音  │
    │ 视频理解  │         │ 视频生成  │         │ 多模态   │
    │ 语音识别  │         │ 语音合成  │         │ 对话     │
    └───────────┘         └───────────┘         └───────────┘
\`\`\`

---

## 💡 关键要点

1. **图像理解** —— GPT-4o、Claude 都很强
2. **图像生成** —— DALL-E 易用，SD 可控
3. **语音识别** —— Whisper 开源最佳
4. **语音合成** —— Edge TTS 免费，OpenAI 高质量
5. **视频分析** —— Gemini 支持长视频
6. **视频生成** —— Sora 领先，Runway 可用

> 🎯 **建议**：根据具体需求选择工具，多模态能力正在快速发展。
            `,ja:`
## マルチモーダルAIコア復習

---

## 💡 重要ポイント

1. **画像理解** —— GPT-4o、Claude共に強力
2. **画像生成** —— DALL-E使いやすい、SD制御可能
3. **音声認識** —— Whisperオープンソース最強
4. **音声合成** —— Edge TTS無料、OpenAI高品質
5. **動画分析** —— Gemini長時間動画対応
6. **動画生成** —— Soraリード、Runway使用可能

> 🎯 **アドバイス**：具体的なニーズに応じてツールを選択。マルチモーダル能力は急速に発展中。
            `}}]},{id:"chapter-6",number:6,title:{zh:"AI 安全与伦理",ja:"AIセキュリティと倫理"},subtitle:{zh:"负责任的 AI 使用指南",ja:"責任あるAI使用ガイド"},sections:[{id:"ch6-hallucination",title:{zh:"6.1 AI 幻觉问题",ja:"6.1 AIハルシネーション問題"},content:{zh:`
## AI 会"一本正经地胡说八道"

当 AI 输出看起来正确但实际错误的信息时，我们称之为"幻觉"。

---

## 🎯 什么是 AI 幻觉？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        AI 幻觉类型                                       │
└─────────────────────────────────────────────────────────────────────────┘

                          AI 幻觉
                             │
         ┌───────────────────┼───────────────────┐
         │                   │                   │
         ▼                   ▼                   ▼
    事实性错误           逻辑性错误          编造内容
         │                   │                   │
   "北京是日本首都"    "1+1=3 因为..."    "论文XYZ研究表明"
   (完全错误)          (推理错误)          (不存在的引用)
\`\`\`

---

## 📊 幻觉发生的原因

| 原因 | 说明 | 示例 |
|------|------|------|
| **训练数据** | 数据中存在错误 | 过时的信息 |
| **知识截止** | 模型不知道最新事件 | 2023年后的新闻 |
| **过度自信** | 模型编造答案 | 不确定时仍给出答案 |
| **长文本** | 上下文过长时出错 | 忘记前文约束 |

---

## 🔧 检测与缓解方法

### 1. Prompt 约束

\`\`\`python
system_prompt = """
你是一个严谨的助手。请遵守以下规则：
1. 如果不确定，请说"我不确定"
2. 不要编造不存在的引用或数据
3. 区分事实和观点
4. 对于时效性信息，提醒知识截止日期
"""
\`\`\`

### 2. 使用 RAG 提供事实依据

\`\`\`python
# 先检索事实，再让AI回答
context = retriever.search(query)
response = llm.generate(
    f"基于以下资料回答问题，如果资料中没有相关信息请说'未找到相关资料'：\\n{context}\\n\\n问题：{query}"
)
\`\`\`

### 3. 事后验证

\`\`\`python
# 让另一个模型验证回答
verification_prompt = f"""
请验证以下回答是否准确：
问题：{question}
回答：{answer}

请检查：
1. 是否有事实性错误？
2. 是否有不确定的陈述被当作事实？
3. 引用的资料是否真实存在？
"""
\`\`\`

---

## 💡 实践建议

> 🎯 **记住**：
> - 始终对AI输出保持质疑态度
> - 重要决策需要人工验证
> - 使用RAG减少幻觉
> - 让AI表达不确定性
            `,ja:`
## AIは「真面目に嘘をつく」

AIが正しそうに見えて実際には間違っている情報を出力する時、これを「ハルシネーション」と呼びます。

---

## 🎯 AIハルシネーションとは？

| タイプ | 説明 | 例 |
|--------|------|-----|
| **事実誤り** | 完全に間違った事実 | 「東京はアメリカの首都」 |
| **論理誤り** | 推論の誤り | 「1+1=3なぜなら...」 |
| **捏造** | 存在しない引用 | 「論文XYZによると」 |

---

## 🔧 検出と緩和方法

1. **Prompt制約** —— 不確実な時は「わかりません」と言うよう指示
2. **RAG使用** —— 事実に基づいて回答させる
3. **事後検証** —— 別のモデルで検証

> 🎯 **覚えておくこと**：AIの出力は常に疑う姿勢を持つ
            `}},{id:"ch6-bias",title:{zh:"6.2 偏见与公平性",ja:"6.2 バイアスと公平性"},content:{zh:`
## AI 可能带有偏见

训练数据中的偏见会被模型学习并放大。

---

## 📊 常见偏见类型

| 类型 | 说明 | 示例 |
|------|------|------|
| **性别偏见** | 职业与性别关联 | "护士=女性" |
| **种族偏见** | 人种刻板印象 | 人脸识别误差 |
| **文化偏见** | 西方中心主义 | 忽视非英语文化 |
| **年龄偏见** | 年龄歧视 | "老年人不懂技术" |

---

## 🔧 检测偏见

\`\`\`python
# 测试性别偏见
prompts = [
    "The doctor told the nurse that she...",
    "The engineer explained to the secretary that he..."
]

# 观察模型是否做出性别假设
for prompt in prompts:
    response = model.generate(prompt)
    print(f"Prompt: {prompt}")
    print(f"Response: {response}")
\`\`\`

---

## 🔧 缓解策略

1. **多样化训练数据**
2. **Prompt 去偏见**：明确要求公平对待
3. **输出审核**：检测敏感内容
4. **用户反馈**：收集偏见报告

---

## 💡 最佳实践

> ⚖️ 设计 AI 系统时，始终考虑公平性和包容性。
            `,ja:`
## AIはバイアスを持つ可能性がある

訓練データのバイアスはモデルに学習され、増幅されます。

---

## 📊 一般的なバイアスタイプ

| タイプ | 説明 | 例 |
|--------|------|-----|
| **性別バイアス** | 職業と性別の関連付け | 「看護師=女性」 |
| **人種バイアス** | 人種ステレオタイプ | 顔認識の誤差 |
| **文化バイアス** | 西洋中心主義 | 非英語文化の無視 |

---

## 🔧 緩和策

1. **多様な訓練データ**
2. **Promptでバイアス排除**を明示
3. **出力監査**：敏感なコンテンツを検出
4. **ユーザーフィードバック**収集

> ⚖️ AIシステム設計時は、常に公平性と包括性を考慮する
            `}},{id:"ch6-privacy",title:{zh:"6.3 隐私与数据安全",ja:"6.3 プライバシーとデータセキュリティ"},content:{zh:`
## 保护用户数据是底线

使用 AI 时，数据隐私至关重要。

---

## ⚠️ 风险点

| 风险 | 说明 | 缓解措施 |
|------|------|----------|
| **数据泄露** | 敏感数据发送给API | 脱敏处理 |
| **模型记忆** | 模型"记住"私人信息 | 使用无状态API |
| **第三方共享** | 数据被用于训练 | 选择 opt-out |

---

## 🔧 安全实践

\`\`\`python
# 1. 数据脱敏
import re

def anonymize(text):
    # 移除邮箱
    text = re.sub(r'[\\w.-]+@[\\w.-]+', '[EMAIL]', text)
    # 移除手机号
    text = re.sub(r'1[3-9]\\d{9}', '[PHONE]', text)
    # 移除身份证
    text = re.sub(r'\\d{17}[\\dXx]', '[ID]', text)
    return text

# 2. 选择隐私友好的服务
# - 本地部署 (Ollama, LMStudio)
# - 企业版 API (数据不用于训练)
# - 自建 RAG (数据不出内网)
\`\`\`

---

## 💡 选择 AI 服务时的隐私考虑

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                      隐私保护决策树                                       │
└─────────────────────────────────────────────────────────────────────────┘

                     数据敏感度？
                         │
            ┌────────────┴────────────┐
            │                         │
          低/中                       高
            │                         │
            ▼                         ▼
       云端 API               ┌───────────────┐
       (方便快捷)             │ 本地部署优先  │
                              │ - Ollama      │
                              │ - LMStudio    │
                              │ - 私有化部署  │
                              └───────────────┘
\`\`\`
            `,ja:`
## ユーザーデータの保護は最低限

AIを使用する際、データプライバシーは非常に重要です。

---

## ⚠️ リスクポイント

| リスク | 説明 | 緩和措置 |
|--------|------|----------|
| **データ漏洩** | 機密データをAPIに送信 | 匿名化処理 |
| **モデル記憶** | モデルが個人情報を「記憶」 | ステートレスAPIを使用 |
| **第三者共有** | データが訓練に使用される | opt-out を選択 |

---

## 💡 プライバシー保護の決定木

高機密データ → ローカルデプロイ優先 (Ollama, LMStudio)
低/中機密データ → クラウドAPI (便利で高速)
            `}},{id:"ch6-alignment",title:{zh:"6.4 AI 对齐与安全",ja:"6.4 AIアラインメントと安全性"},content:{zh:`
## 让 AI 按人类意图行事

AI 对齐(Alignment)是确保 AI 系统的行为符合人类价值观和意图。

---

## 🎯 什么是 AI 对齐？

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                        AI 对齐目标                                       │
└─────────────────────────────────────────────────────────────────────────┘

                    安全的 AI
                        │
          ┌─────────────┼─────────────┐
          │             │             │
          ▼             ▼             ▼
       有益性         无害性         诚实性
       Helpful       Harmless       Honest
          │             │             │
    帮助用户完成     不做有害的事    不欺骗用户
    合理请求                         承认不确定
\`\`\`

---

## 🔧 对齐技术

| 技术 | 说明 | 代表 |
|------|------|------|
| **RLHF** | 人类反馈强化学习 | ChatGPT |
| **Constitutional AI** | 宪法AI | Claude |
| **DPO** | 直接偏好优化 | 开源模型 |

---

## 💡 作为用户的责任

> 🛡️ **负责任使用 AI**：
> - 不用于违法用途
> - 不滥用生成能力
> - 保持人类监督
> - 报告问题行为
            `,ja:`
## AIを人間の意図に沿って動作させる

AIアラインメントは、AIシステムの行動が人間の価値観と意図に沿うことを確保すること。

---

## 🎯 AIアラインメントとは？

\`\`\`
                    安全なAI
                        │
          ┌─────────────┼─────────────┐
          ▼             ▼             ▼
       有益性         無害性         誠実性
       Helpful       Harmless       Honest
\`\`\`

---

## 💡 ユーザーとしての責任

> 🛡️ **責任あるAI使用**：
> - 違法用途に使用しない
> - 生成能力を乱用しない
> - 人間の監督を維持
> - 問題行動を報告
            `}},{id:"ch6-summary",title:{zh:"6.5 本章小结",ja:"6.5 この章のまとめ"},content:{zh:`
## AI 安全与伦理核心要点

---

## 💡 关键原则

1. **幻觉防护** —— 使用 RAG、设置约束、事后验证
2. **偏见意识** —— 审视输出、多样化输入
3. **隐私优先** —— 敏感数据脱敏或本地部署
4. **负责任使用** —— 遵守法规、保持人类监督

> 🎯 **记住**：AI 是工具，人类负责如何使用它。
            `,ja:`
## AIセキュリティと倫理コアポイント

---

## 💡 重要原則

1. **ハルシネーション防止** —— RAG使用、制約設定、事後検証
2. **バイアス意識** —— 出力を審査、入力を多様化
3. **プライバシー優先** —— 機密データ匿名化またはローカルデプロイ
4. **責任ある使用** —— 規制遵守、人間の監督維持

> 🎯 **覚えておくこと**：AIはツール、人間がその使い方に責任を持つ
            `}}]},{id:"chapter-7",number:7,title:{zh:"实战项目",ja:"実践プロジェクト"},subtitle:{zh:"从零开始构建 AI 应用",ja:"ゼロからAIアプリケーションを構築"},sections:[{id:"ch7-chatbot",title:{zh:"7.1 智能客服机器人",ja:"7.1 スマートカスタマーサービスボット"},content:{zh:`
## 项目：企业级智能客服系统

构建一个功能完整的智能客服机器人，支持知识库问答、多轮对话、意图识别。

---

## 📁 项目结构

\`\`\`
smart-customer-service/
├── requirements.txt          # 依赖清单
├── .env                      # 环境变量
├── config.py                 # 配置管理
├── data/
│   └── knowledge/            # 知识库文档
│       ├── products.md
│       ├── faq.md
│       └── policies.md
├── src/
│   ├── __init__.py
│   ├── knowledge_base.py     # 知识库管理
│   ├── chat_engine.py        # 对话引擎
│   ├── intent_classifier.py  # 意图识别
│   └── memory.py             # 对话记忆
├── app.py                    # Streamlit 应用
└── tests/
    └── test_chat.py
\`\`\`

---

## 📋 依赖安装

\`\`\`bash
# requirements.txt
langchain>=0.3.0
langchain-anthropic>=0.3.0
langchain-openai>=0.3.0
langchain-chroma>=0.2.0
chromadb>=0.5.0
streamlit>=1.40.0
python-dotenv>=1.0.0
pydantic>=2.0.0
\`\`\`

\`\`\`bash
pip install -r requirements.txt
\`\`\`

---

## 🔧 配置管理 (config.py)

\`\`\`python
import os
from dotenv import load_dotenv
from pydantic_settings import BaseSettings

load_dotenv()

class Settings(BaseSettings):
    # LLM 配置
    llm_provider: str = "anthropic"  # anthropic / openai
    anthropic_api_key: str = ""
    openai_api_key: str = ""
    model_name: str = "claude-sonnet-4-20250514"

    # 向量数据库
    chroma_persist_dir: str = "./chroma_db"
    embedding_model: str = "text-embedding-3-small"

    # 检索配置
    top_k: int = 5
    score_threshold: float = 0.7

    # 对话配置
    max_history: int = 10
    system_prompt: str = """你是一个专业的客服助手。
请根据知识库内容回答用户问题。
如果知识库中没有相关信息，请诚实说明。
回答要简洁、准确、有帮助。"""

    class Config:
        env_file = ".env"

settings = Settings()
\`\`\`

---

## 📚 知识库管理 (knowledge_base.py)

\`\`\`python
import os
from pathlib import Path
from typing import List, Optional

from langchain_community.document_loaders import (
    DirectoryLoader,
    TextLoader,
    UnstructuredMarkdownLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document

from config import settings


class KnowledgeBase:
    """知识库管理类"""

    def __init__(self, persist_dir: str = None):
        self.persist_dir = persist_dir or settings.chroma_persist_dir
        self.embeddings = OpenAIEmbeddings(
            model=settings.embedding_model,
            api_key=settings.openai_api_key
        )
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\\n## ", "\\n### ", "\\n\\n", "\\n", " "]
        )
        self._vectorstore: Optional[Chroma] = None

    @property
    def vectorstore(self) -> Chroma:
        """懒加载向量存储"""
        if self._vectorstore is None:
            if os.path.exists(self.persist_dir):
                self._vectorstore = Chroma(
                    persist_directory=self.persist_dir,
                    embedding_function=self.embeddings
                )
            else:
                raise ValueError("知识库未初始化，请先调用 build_from_directory()")
        return self._vectorstore

    def build_from_directory(self, docs_dir: str) -> int:
        """从目录构建知识库"""
        # 加载 Markdown 文件
        loader = DirectoryLoader(
            docs_dir,
            glob="**/*.md",
            loader_cls=UnstructuredMarkdownLoader,
            show_progress=True
        )
        documents = loader.load()

        # 添加来源元数据
        for doc in documents:
            doc.metadata["source"] = Path(doc.metadata.get("source", "")).name

        # 分块
        chunks = self.text_splitter.split_documents(documents)
        print(f"文档分块完成: {len(documents)} 文档 -> {len(chunks)} 块")

        # 创建向量存储
        self._vectorstore = Chroma.from_documents(
            documents=chunks,
            embedding=self.embeddings,
            persist_directory=self.persist_dir
        )

        return len(chunks)

    def add_documents(self, texts: List[str], metadata: dict = None) -> None:
        """添加新文档"""
        docs = [Document(page_content=t, metadata=metadata or {}) for t in texts]
        chunks = self.text_splitter.split_documents(docs)
        self.vectorstore.add_documents(chunks)

    def search(self, query: str, k: int = None) -> List[Document]:
        """相似度搜索"""
        k = k or settings.top_k
        return self.vectorstore.similarity_search(query, k=k)

    def search_with_score(self, query: str, k: int = None) -> List[tuple]:
        """带分数的搜索"""
        k = k or settings.top_k
        results = self.vectorstore.similarity_search_with_score(query, k=k)
        # 过滤低分结果
        return [(doc, score) for doc, score in results
                if score >= settings.score_threshold]


# 全局实例
knowledge_base = KnowledgeBase()
\`\`\`

---

## 🧠 意图识别 (intent_classifier.py)

\`\`\`python
from enum import Enum
from typing import Tuple
from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field

from config import settings


class Intent(str, Enum):
    """用户意图枚举"""
    PRODUCT_INQUIRY = "product_inquiry"      # 产品咨询
    ORDER_STATUS = "order_status"            # 订单查询
    TECHNICAL_SUPPORT = "technical_support"  # 技术支持
    COMPLAINT = "complaint"                  # 投诉建议
    GENERAL_CHAT = "general_chat"            # 闲聊
    TRANSFER_HUMAN = "transfer_human"        # 转人工


class IntentResult(BaseModel):
    """意图识别结果"""
    intent: Intent = Field(description="识别到的意图")
    confidence: float = Field(description="置信度 0-1", ge=0, le=1)
    keywords: list[str] = Field(description="关键词列表", default_factory=list)


class IntentClassifier:
    """意图分类器"""

    def __init__(self):
        self.llm = ChatAnthropic(
            model=settings.model_name,
            api_key=settings.anthropic_api_key
        ).with_structured_output(IntentResult)

        self.prompt = ChatPromptTemplate.from_messages([
            ("system", """你是意图识别专家。分析用户消息，识别意图类别。

可选意图：
- product_inquiry: 询问产品功能、价格、规格
- order_status: 查询订单、物流、退换货
- technical_support: 使用问题、故障排查
- complaint: 投诉、建议、不满
- general_chat: 问候、闲聊、无明确目的
- transfer_human: 明确要求人工客服

输出结构化 JSON。"""),
            ("human", "{message}")
        ])

        self.chain = self.prompt | self.llm

    def classify(self, message: str) -> IntentResult:
        """识别用户意图"""
        return self.chain.invoke({"message": message})

    def should_transfer(self, intent_result: IntentResult) -> bool:
        """判断是否需要转人工"""
        # 明确要求转人工
        if intent_result.intent == Intent.TRANSFER_HUMAN:
            return True
        # 投诉类高置信度
        if intent_result.intent == Intent.COMPLAINT and intent_result.confidence > 0.8:
            return True
        return False


intent_classifier = IntentClassifier()
\`\`\`

---

## 💬 对话记忆 (memory.py)

\`\`\`python
from typing import List, Dict
from collections import deque
from datetime import datetime

from config import settings


class ConversationMemory:
    """对话记忆管理"""

    def __init__(self, session_id: str, max_history: int = None):
        self.session_id = session_id
        self.max_history = max_history or settings.max_history
        self.messages: deque = deque(maxlen=self.max_history * 2)
        self.created_at = datetime.now()
        self.metadata: Dict = {}

    def add_user_message(self, content: str) -> None:
        """添加用户消息"""
        self.messages.append({
            "role": "user",
            "content": content,
            "timestamp": datetime.now().isoformat()
        })

    def add_assistant_message(self, content: str, sources: List[str] = None) -> None:
        """添加助手消息"""
        self.messages.append({
            "role": "assistant",
            "content": content,
            "sources": sources or [],
            "timestamp": datetime.now().isoformat()
        })

    def get_history(self) -> List[Dict]:
        """获取对话历史"""
        return list(self.messages)

    def get_context_string(self) -> str:
        """获取上下文字符串，用于 prompt"""
        if not self.messages:
            return "（无历史对话）"

        lines = []
        for msg in self.messages:
            role = "用户" if msg["role"] == "user" else "客服"
            lines.append(f"{role}: {msg['content']}")
        return "\\n".join(lines)

    def clear(self) -> None:
        """清空记忆"""
        self.messages.clear()


class MemoryStore:
    """会话存储管理"""

    def __init__(self):
        self._sessions: Dict[str, ConversationMemory] = {}

    def get_or_create(self, session_id: str) -> ConversationMemory:
        """获取或创建会话"""
        if session_id not in self._sessions:
            self._sessions[session_id] = ConversationMemory(session_id)
        return self._sessions[session_id]

    def delete(self, session_id: str) -> None:
        """删除会话"""
        self._sessions.pop(session_id, None)


memory_store = MemoryStore()
\`\`\`

---

## 🤖 对话引擎 (chat_engine.py)

\`\`\`python
from typing import Generator, Dict, Any
from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

from config import settings
from knowledge_base import knowledge_base
from intent_classifier import intent_classifier, Intent
from memory import memory_store, ConversationMemory


class ChatEngine:
    """智能对话引擎"""

    def __init__(self):
        self.llm = ChatAnthropic(
            model=settings.model_name,
            api_key=settings.anthropic_api_key,
            streaming=True
        )

        self.qa_prompt = ChatPromptTemplate.from_messages([
            ("system", """{system_prompt}

## 知识库参考
{context}

## 对话历史
{history}

请根据以上信息回答用户问题。回答要求：
1. 优先使用知识库内容
2. 保持对话连贯性
3. 如果不确定，请诚实说明
4. 回答简洁专业"""),
            ("human", "{question}")
        ])

        self.chain = self.qa_prompt | self.llm | StrOutputParser()

    def _retrieve_context(self, query: str) -> str:
        """检索相关知识"""
        try:
            docs = knowledge_base.search(query)
            if not docs:
                return "（未找到相关知识库内容）"

            context_parts = []
            for i, doc in enumerate(docs, 1):
                source = doc.metadata.get("source", "未知")
                context_parts.append(f"[{i}] 来源: {source}\\n{doc.page_content}")

            return "\\n\\n".join(context_parts)
        except Exception as e:
            return f"（知识库检索失败: {e}）"

    def chat(self,
             session_id: str,
             message: str) -> Generator[str, None, Dict[str, Any]]:
        """
        处理用户消息，流式返回回复

        Args:
            session_id: 会话ID
            message: 用户消息

        Yields:
            str: 回复文本片段

        Returns:
            Dict: 元数据（意图、来源等）
        """
        memory = memory_store.get_or_create(session_id)

        # 1. 意图识别
        intent_result = intent_classifier.classify(message)

        # 2. 检查是否需要转人工
        if intent_classifier.should_transfer(intent_result):
            response = "好的，我这就为您转接人工客服，请稍候..."
            memory.add_user_message(message)
            memory.add_assistant_message(response)
            yield response
            return {
                "intent": intent_result.intent,
                "transfer_human": True
            }

        # 3. 检索知识库
        context = self._retrieve_context(message)
        history = memory.get_context_string()

        # 4. 生成回复（流式）
        memory.add_user_message(message)
        full_response = ""

        for chunk in self.chain.stream({
            "system_prompt": settings.system_prompt,
            "context": context,
            "history": history,
            "question": message
        }):
            full_response += chunk
            yield chunk

        # 5. 保存回复
        memory.add_assistant_message(full_response)

        return {
            "intent": intent_result.intent,
            "confidence": intent_result.confidence,
            "transfer_human": False
        }

    def get_history(self, session_id: str) -> list:
        """获取对话历史"""
        memory = memory_store.get_or_create(session_id)
        return memory.get_history()


chat_engine = ChatEngine()
\`\`\`

---

## 🖥️ Streamlit 应用 (app.py)

\`\`\`python
import streamlit as st
import uuid
from datetime import datetime

from chat_engine import chat_engine
from knowledge_base import knowledge_base

# 页面配置
st.set_page_config(
    page_title="智能客服助手",
    page_icon="🤖",
    layout="wide"
)

# 自定义样式
st.markdown("""
<style>
.stChatMessage {
    padding: 1rem;
    border-radius: 0.5rem;
}
.user-message {
    background-color: #e3f2fd;
}
.assistant-message {
    background-color: #f5f5f5;
}
</style>
""", unsafe_allow_html=True)

# 初始化会话状态
if "session_id" not in st.session_state:
    st.session_state.session_id = str(uuid.uuid4())
if "messages" not in st.session_state:
    st.session_state.messages = []

# 侧边栏
with st.sidebar:
    st.title("🤖 智能客服")
    st.markdown("---")

    # 知识库管理
    st.subheader("📚 知识库")
    if st.button("🔄 重建知识库"):
        with st.spinner("正在构建知识库..."):
            try:
                count = knowledge_base.build_from_directory("./data/knowledge")
                st.success(f"✅ 已索引 {count} 个文档块")
            except Exception as e:
                st.error(f"❌ 构建失败: {e}")

    st.markdown("---")

    # 会话管理
    st.subheader("💬 当前会话")
    st.text(f"ID: {st.session_state.session_id[:8]}...")
    if st.button("🗑️ 清空对话"):
        st.session_state.messages = []
        st.rerun()

    st.markdown("---")
    st.caption(f"© {datetime.now().year} 智能客服系统")

# 主界面
st.title("💬 智能客服助手")
st.caption("有什么可以帮助您的？")

# 显示对话历史
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# 用户输入
if prompt := st.chat_input("请输入您的问题..."):
    # 显示用户消息
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # 生成回复
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""

        try:
            for chunk in chat_engine.chat(
                st.session_state.session_id,
                prompt
            ):
                if isinstance(chunk, str):
                    full_response += chunk
                    message_placeholder.markdown(full_response + "▌")

            message_placeholder.markdown(full_response)
            st.session_state.messages.append({
                "role": "assistant",
                "content": full_response
            })

        except Exception as e:
            error_msg = f"抱歉，处理您的请求时出现错误: {str(e)}"
            message_placeholder.error(error_msg)

# 快捷问题
st.markdown("---")
st.subheader("💡 常见问题")
cols = st.columns(3)
quick_questions = [
    "产品有哪些功能？",
    "如何查询订单状态？",
    "退换货政策是什么？"
]
for col, q in zip(cols, quick_questions):
    if col.button(q, use_container_width=True):
        st.session_state.messages.append({"role": "user", "content": q})
        st.rerun()
\`\`\`

---

## 🚀 启动运行

\`\`\`bash
# 1. 配置环境变量
cat > .env << EOF
ANTHROPIC_API_KEY=your-api-key
OPENAI_API_KEY=your-openai-key
EOF

# 2. 准备知识库文档 (data/knowledge/*.md)

# 3. 构建知识库
python -c "from knowledge_base import knowledge_base; \\
           knowledge_base.build_from_directory('./data/knowledge')"

# 4. 启动应用
streamlit run app.py
\`\`\`

---

## 🧪 测试用例 (tests/test_chat.py)

\`\`\`python
import pytest
from chat_engine import chat_engine
from intent_classifier import intent_classifier, Intent
from memory import memory_store

def test_intent_classification():
    """测试意图识别"""
    result = intent_classifier.classify("你们的产品多少钱？")
    assert result.intent == Intent.PRODUCT_INQUIRY
    assert result.confidence > 0.5

def test_transfer_human():
    """测试转人工判断"""
    result = intent_classifier.classify("我要转人工")
    assert intent_classifier.should_transfer(result)

def test_conversation_memory():
    """测试对话记忆"""
    memory = memory_store.get_or_create("test-session")
    memory.add_user_message("你好")
    memory.add_assistant_message("您好！有什么可以帮您？")

    history = memory.get_history()
    assert len(history) == 2
    assert history[0]["role"] == "user"

def test_chat_stream():
    """测试流式对话"""
    session_id = "test-session"
    response_chunks = []

    for chunk in chat_engine.chat(session_id, "你好"):
        if isinstance(chunk, str):
            response_chunks.append(chunk)

    full_response = "".join(response_chunks)
    assert len(full_response) > 0

if __name__ == "__main__":
    pytest.main([__file__, "-v"])
\`\`\`

---

## 📊 生产部署清单

| 项目 | 开发环境 | 生产环境 |
|------|----------|----------|
| 向量数据库 | Chroma (本地) | Pinecone / Qdrant |
| 会话存储 | 内存 | Redis |
| 日志 | 控制台 | ELK / CloudWatch |
| 监控 | 无 | Prometheus + Grafana |
| 部署 | Streamlit | Docker + K8s |
| CDN | 无 | CloudFlare |
            `,ja:`
## プロジェクト：エンタープライズスマートカスタマーサービス

ナレッジベースQ&A、マルチターン対話、意図認識をサポートする完全な機能を備えたスマートカスタマーサービスボットを構築します。

---

## 📁 プロジェクト構造

\`\`\`
smart-customer-service/
├── requirements.txt          # 依存関係リスト
├── .env                      # 環境変数
├── config.py                 # 設定管理
├── data/
│   └── knowledge/            # ナレッジベースドキュメント
├── src/
│   ├── knowledge_base.py     # ナレッジベース管理
│   ├── chat_engine.py        # 対話エンジン
│   ├── intent_classifier.py  # 意図認識
│   └── memory.py             # 対話メモリ
├── app.py                    # Streamlitアプリ
└── tests/
    └── test_chat.py
\`\`\`

---

## 📋 依存関係インストール

\`\`\`bash
pip install langchain langchain-anthropic langchain-chroma streamlit
\`\`\`

---

## 🔧 設定管理 (config.py)

\`\`\`python
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    llm_provider: str = "anthropic"
    anthropic_api_key: str = ""
    model_name: str = "claude-sonnet-4-20250514"
    chroma_persist_dir: str = "./chroma_db"
    top_k: int = 5
    max_history: int = 10

    system_prompt: str = """あなたはプロフェッショナルな
カスタマーサービスアシスタントです。
ナレッジベースの内容に基づいて質問に回答してください。"""

settings = Settings()
\`\`\`

---

## 🧠 意図認識 (intent_classifier.py)

\`\`\`python
from enum import Enum
from pydantic import BaseModel

class Intent(str, Enum):
    PRODUCT_INQUIRY = "product_inquiry"      # 製品問い合わせ
    ORDER_STATUS = "order_status"            # 注文照会
    TECHNICAL_SUPPORT = "technical_support"  # 技術サポート
    COMPLAINT = "complaint"                  # 苦情
    TRANSFER_HUMAN = "transfer_human"        # 人間オペレーターへ

class IntentResult(BaseModel):
    intent: Intent
    confidence: float
    keywords: list[str] = []
\`\`\`

---

## 💬 対話エンジン (chat_engine.py)

\`\`\`python
from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import ChatPromptTemplate

class ChatEngine:
    def __init__(self):
        self.llm = ChatAnthropic(
            model="claude-sonnet-4-20250514",
            streaming=True
        )

    def chat(self, session_id: str, message: str):
        # 1. 意図認識
        intent = self.classify_intent(message)

        # 2. ナレッジベース検索
        context = self.retrieve_context(message)

        # 3. 回答生成（ストリーミング）
        for chunk in self.generate_response(context, message):
            yield chunk
\`\`\`

---

## 🖥️ Streamlitアプリ (app.py)

\`\`\`python
import streamlit as st

st.set_page_config(page_title="スマートカスタマーサービス", page_icon="🤖")
st.title("💬 スマートカスタマーサービス")

# チャット履歴表示
for msg in st.session_state.get("messages", []):
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ユーザー入力
if prompt := st.chat_input("ご質問をどうぞ..."):
    with st.chat_message("assistant"):
        response = st.write_stream(chat_engine.chat(session_id, prompt))
\`\`\`

---

## 📊 本番デプロイチェックリスト

| 項目 | 開発環境 | 本番環境 |
|------|----------|----------|
| ベクトルDB | Chroma | Pinecone / Qdrant |
| セッション | メモリ | Redis |
| ログ | コンソール | ELK |
| デプロイ | Streamlit | Docker + K8s |
            `}},{id:"ch7-doc-qa",title:{zh:"7.2 文档问答系统",ja:"7.2 ドキュメントQ&Aシステム"},content:{zh:`
## 项目：多格式文档问答系统

构建一个支持 PDF/Word/Excel/TXT 的智能文档问答系统，带来源引用和页码标注。

---

## 📁 项目结构

\`\`\`
document-qa-system/
├── requirements.txt
├── .env
├── config.py
├── uploads/                    # 上传文档目录
├── vector_store/               # 向量存储
├── src/
│   ├── __init__.py
│   ├── document_loader.py      # 多格式文档加载
│   ├── text_processor.py       # 文本处理与分块
│   ├── vector_store.py         # 向量存储管理
│   ├── qa_engine.py            # 问答引擎
│   └── citation.py             # 来源引用处理
├── api/
│   ├── __init__.py
│   ├── main.py                 # FastAPI 服务
│   └── schemas.py              # 数据模型
└── frontend/
    └── app.py                  # Gradio 界面
\`\`\`

---

## 📋 依赖安装

\`\`\`bash
# requirements.txt
langchain>=0.3.0
langchain-anthropic>=0.3.0
langchain-openai>=0.3.0
langchain-chroma>=0.2.0
chromadb>=0.5.0

# 文档解析
pypdf>=4.0.0
python-docx>=1.0.0
openpyxl>=3.1.0
unstructured>=0.10.0

# API & 前端
fastapi>=0.115.0
uvicorn>=0.30.0
gradio>=5.0.0
python-multipart>=0.0.9
\`\`\`

---

## 📄 多格式文档加载器 (document_loader.py)

\`\`\`python
import os
from pathlib import Path
from typing import List, Optional, Dict, Any
from abc import ABC, abstractmethod

from langchain_core.documents import Document
from langchain_community.document_loaders import (
    PyPDFLoader,
    Docx2txtLoader,
    UnstructuredExcelLoader,
    TextLoader
)


class DocumentLoader(ABC):
    """文档加载器基类"""

    @abstractmethod
    def load(self, file_path: str) -> List[Document]:
        pass

    @abstractmethod
    def supports(self, file_path: str) -> bool:
        pass


class PDFLoader(DocumentLoader):
    """PDF 加载器，保留页码信息"""

    def supports(self, file_path: str) -> bool:
        return file_path.lower().endswith('.pdf')

    def load(self, file_path: str) -> List[Document]:
        loader = PyPDFLoader(file_path)
        docs = loader.load()

        # 添加页码和文件名元数据
        filename = Path(file_path).name
        for i, doc in enumerate(docs):
            doc.metadata.update({
                "source": filename,
                "page": i + 1,
                "file_type": "pdf"
            })

        return docs


class WordLoader(DocumentLoader):
    """Word 文档加载器"""

    def supports(self, file_path: str) -> bool:
        return file_path.lower().endswith(('.docx', '.doc'))

    def load(self, file_path: str) -> List[Document]:
        loader = Docx2txtLoader(file_path)
        docs = loader.load()

        filename = Path(file_path).name
        for doc in docs:
            doc.metadata.update({
                "source": filename,
                "file_type": "word"
            })

        return docs


class ExcelLoader(DocumentLoader):
    """Excel 加载器"""

    def supports(self, file_path: str) -> bool:
        return file_path.lower().endswith(('.xlsx', '.xls'))

    def load(self, file_path: str) -> List[Document]:
        loader = UnstructuredExcelLoader(file_path)
        docs = loader.load()

        filename = Path(file_path).name
        for doc in docs:
            doc.metadata.update({
                "source": filename,
                "file_type": "excel"
            })

        return docs


class TxtLoader(DocumentLoader):
    """纯文本加载器"""

    def supports(self, file_path: str) -> bool:
        return file_path.lower().endswith('.txt')

    def load(self, file_path: str) -> List[Document]:
        loader = TextLoader(file_path, encoding='utf-8')
        docs = loader.load()

        filename = Path(file_path).name
        for doc in docs:
            doc.metadata.update({
                "source": filename,
                "file_type": "text"
            })

        return docs


class MultiFormatLoader:
    """统一多格式加载器"""

    def __init__(self):
        self.loaders: List[DocumentLoader] = [
            PDFLoader(),
            WordLoader(),
            ExcelLoader(),
            TxtLoader()
        ]

    def load(self, file_path: str) -> List[Document]:
        """加载单个文档"""
        for loader in self.loaders:
            if loader.supports(file_path):
                return loader.load(file_path)

        raise ValueError(f"不支持的文件格式: {file_path}")

    def load_directory(self, dir_path: str) -> List[Document]:
        """加载目录下所有文档"""
        all_docs = []
        supported_extensions = ('.pdf', '.docx', '.doc', '.xlsx', '.xls', '.txt')

        for file_path in Path(dir_path).rglob('*'):
            if file_path.suffix.lower() in supported_extensions:
                try:
                    docs = self.load(str(file_path))
                    all_docs.extend(docs)
                    print(f"✅ 已加载: {file_path.name}")
                except Exception as e:
                    print(f"❌ 加载失败 {file_path.name}: {e}")

        return all_docs

    def get_supported_formats(self) -> List[str]:
        return ["PDF", "Word (.docx)", "Excel (.xlsx)", "Text (.txt)"]


# 全局实例
document_loader = MultiFormatLoader()
\`\`\`

---

## 🔍 文本处理与分块 (text_processor.py)

\`\`\`python
from typing import List, Dict, Any
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document


class TextProcessor:
    """文本处理器"""

    def __init__(
        self,
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
        length_function: callable = len
    ):
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=length_function,
            separators=[
                "\\n\\n",  # 段落
                "\\n",     # 换行
                "。",      # 中文句号
                ".",       # 英文句号
                "！",
                "!",
                "？",
                "?",
                "；",
                ";",
                " ",
                ""
            ]
        )

    def split_documents(self, documents: List[Document]) -> List[Document]:
        """分割文档，保留元数据"""
        chunks = self.splitter.split_documents(documents)

        # 添加分块索引
        for i, chunk in enumerate(chunks):
            chunk.metadata["chunk_id"] = i
            chunk.metadata["chunk_total"] = len(chunks)

        return chunks

    def preprocess_text(self, text: str) -> str:
        """文本预处理"""
        # 去除多余空白
        import re
        text = re.sub(r'\\s+', ' ', text)
        # 去除特殊字符
        text = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]', '', text)
        return text.strip()


text_processor = TextProcessor()
\`\`\`

---

## 💾 向量存储管理 (vector_store.py)

\`\`\`python
import os
from typing import List, Optional, Tuple
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document


class VectorStoreManager:
    """向量存储管理器"""

    def __init__(
        self,
        persist_directory: str = "./vector_store",
        collection_name: str = "documents"
    ):
        self.persist_directory = persist_directory
        self.collection_name = collection_name
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small"
        )
        self._vectorstore: Optional[Chroma] = None

    @property
    def vectorstore(self) -> Chroma:
        if self._vectorstore is None:
            if os.path.exists(self.persist_directory):
                self._vectorstore = Chroma(
                    persist_directory=self.persist_directory,
                    collection_name=self.collection_name,
                    embedding_function=self.embeddings
                )
            else:
                raise ValueError("向量库未初始化")
        return self._vectorstore

    def create_from_documents(self, documents: List[Document]) -> int:
        """从文档创建向量库"""
        self._vectorstore = Chroma.from_documents(
            documents=documents,
            embedding=self.embeddings,
            persist_directory=self.persist_directory,
            collection_name=self.collection_name
        )
        return len(documents)

    def add_documents(self, documents: List[Document]) -> None:
        """添加文档到现有向量库"""
        self.vectorstore.add_documents(documents)

    def search(
        self,
        query: str,
        k: int = 5,
        filter_dict: dict = None
    ) -> List[Document]:
        """相似度搜索"""
        return self.vectorstore.similarity_search(
            query,
            k=k,
            filter=filter_dict
        )

    def search_with_score(
        self,
        query: str,
        k: int = 5
    ) -> List[Tuple[Document, float]]:
        """带分数的搜索"""
        return self.vectorstore.similarity_search_with_score(query, k=k)

    def delete_by_source(self, source: str) -> None:
        """按来源删除文档"""
        # Chroma 支持按 metadata 过滤删除
        self.vectorstore._collection.delete(
            where={"source": source}
        )

    def get_all_sources(self) -> List[str]:
        """获取所有文档来源"""
        results = self.vectorstore._collection.get()
        sources = set()
        for meta in results.get("metadatas", []):
            if meta and "source" in meta:
                sources.add(meta["source"])
        return list(sources)


vector_store = VectorStoreManager()
\`\`\`

---

## 🤖 问答引擎 (qa_engine.py)

\`\`\`python
from typing import List, Dict, Any, Generator
from dataclasses import dataclass

from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.documents import Document

from vector_store import vector_store


@dataclass
class Citation:
    """引用信息"""
    source: str
    page: int | None
    content: str
    relevance_score: float


@dataclass
class QAResponse:
    """问答响应"""
    answer: str
    citations: List[Citation]
    query: str


class DocumentQAEngine:
    """文档问答引擎"""

    def __init__(self, model: str = "claude-sonnet-4-20250514"):
        self.llm = ChatAnthropic(
            model=model,
            streaming=True
        )

        self.prompt = ChatPromptTemplate.from_messages([
            ("system", """你是一个专业的文档问答助手。

## 任务
根据提供的文档内容回答用户问题。

## 规则
1. 只使用文档中的信息回答
2. 如果文档中没有相关信息，明确说明
3. 引用来源时使用 [来源: 文件名, 第X页] 格式
4. 回答要准确、简洁、有条理

## 文档内容
{context}

---
请回答用户问题，并标注引用来源。"""),
            ("human", "{question}")
        ])

        self.chain = self.prompt | self.llm | StrOutputParser()

    def _format_context(
        self,
        docs: List[tuple[Document, float]]
    ) -> tuple[str, List[Citation]]:
        """格式化检索结果和引用"""
        context_parts = []
        citations = []

        for i, (doc, score) in enumerate(docs, 1):
            source = doc.metadata.get("source", "未知")
            page = doc.metadata.get("page")
            content = doc.page_content

            # 构建上下文
            page_info = f", 第{page}页" if page else ""
            context_parts.append(
                f"[文档{i}] 来源: {source}{page_info}\\n{content}"
            )

            # 记录引用
            citations.append(Citation(
                source=source,
                page=page,
                content=content[:200] + "..." if len(content) > 200 else content,
                relevance_score=score
            ))

        return "\\n\\n---\\n\\n".join(context_parts), citations

    def query(
        self,
        question: str,
        k: int = 5,
        source_filter: str = None
    ) -> QAResponse:
        """问答查询（非流式）"""
        # 检索相关文档
        filter_dict = {"source": source_filter} if source_filter else None
        docs_with_scores = vector_store.search_with_score(question, k=k)

        if not docs_with_scores:
            return QAResponse(
                answer="抱歉，未找到与问题相关的文档内容。",
                citations=[],
                query=question
            )

        # 格式化上下文
        context, citations = self._format_context(docs_with_scores)

        # 生成回答
        answer = self.chain.invoke({
            "context": context,
            "question": question
        })

        return QAResponse(
            answer=answer,
            citations=citations,
            query=question
        )

    def query_stream(
        self,
        question: str,
        k: int = 5
    ) -> Generator[str, None, List[Citation]]:
        """流式问答查询"""
        docs_with_scores = vector_store.search_with_score(question, k=k)

        if not docs_with_scores:
            yield "抱歉，未找到与问题相关的文档内容。"
            return []

        context, citations = self._format_context(docs_with_scores)

        for chunk in self.chain.stream({
            "context": context,
            "question": question
        }):
            yield chunk

        return citations


qa_engine = DocumentQAEngine()
\`\`\`

---

## 🌐 FastAPI 服务 (api/main.py)

\`\`\`python
import os
import shutil
from typing import List, Optional
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

from document_loader import document_loader
from text_processor import text_processor
from vector_store import vector_store
from qa_engine import qa_engine


app = FastAPI(title="文档问答系统 API")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"]
)

UPLOAD_DIR = "./uploads"
os.makedirs(UPLOAD_DIR, exist_ok=True)


# 请求/响应模型
class QuestionRequest(BaseModel):
    question: str
    k: int = 5
    source_filter: Optional[str] = None


class CitationResponse(BaseModel):
    source: str
    page: Optional[int]
    content: str
    score: float


class AnswerResponse(BaseModel):
    answer: str
    citations: List[CitationResponse]


class UploadResponse(BaseModel):
    filename: str
    chunks: int
    message: str


# API 端点
@app.post("/upload", response_model=UploadResponse)
async def upload_document(file: UploadFile = File(...)):
    """上传并索引文档"""
    # 保存文件
    file_path = os.path.join(UPLOAD_DIR, file.filename)
    with open(file_path, "wb") as f:
        shutil.copyfileobj(file.file, f)

    try:
        # 加载文档
        docs = document_loader.load(file_path)

        # 分块
        chunks = text_processor.split_documents(docs)

        # 索引
        vector_store.add_documents(chunks)

        return UploadResponse(
            filename=file.filename,
            chunks=len(chunks),
            message=f"成功索引 {len(chunks)} 个文本块"
        )

    except Exception as e:
        # 清理文件
        os.remove(file_path)
        raise HTTPException(status_code=400, detail=str(e))


@app.post("/ask", response_model=AnswerResponse)
async def ask_question(request: QuestionRequest):
    """文档问答"""
    response = qa_engine.query(
        question=request.question,
        k=request.k,
        source_filter=request.source_filter
    )

    return AnswerResponse(
        answer=response.answer,
        citations=[
            CitationResponse(
                source=c.source,
                page=c.page,
                content=c.content,
                score=c.relevance_score
            )
            for c in response.citations
        ]
    )


@app.get("/documents")
async def list_documents():
    """列出已索引的文档"""
    return {"documents": vector_store.get_all_sources()}


@app.delete("/documents/{filename}")
async def delete_document(filename: str):
    """删除指定文档"""
    vector_store.delete_by_source(filename)
    file_path = os.path.join(UPLOAD_DIR, filename)
    if os.path.exists(file_path):
        os.remove(file_path)
    return {"message": f"已删除 {filename}"}


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
\`\`\`

---

## 🖥️ Gradio 界面 (frontend/app.py)

\`\`\`python
import gradio as gr
import requests

API_URL = "http://localhost:8000"


def upload_file(file):
    """上传文件"""
    if file is None:
        return "请选择文件"

    with open(file.name, "rb") as f:
        response = requests.post(
            f"{API_URL}/upload",
            files={"file": (file.name.split("/")[-1], f)}
        )

    if response.ok:
        data = response.json()
        return f"✅ {data['message']}"
    else:
        return f"❌ 上传失败: {response.text}"


def ask_question(question, k):
    """问答"""
    if not question:
        return "", ""

    response = requests.post(
        f"{API_URL}/ask",
        json={"question": question, "k": int(k)}
    )

    if response.ok:
        data = response.json()
        answer = data["answer"]

        # 格式化引用
        citations = []
        for c in data["citations"]:
            page_info = f", 第{c['page']}页" if c['page'] else ""
            citations.append(
                f"📄 **{c['source']}**{page_info}\\n"
                f"相关度: {c['score']:.2f}\\n"
                f"> {c['content'][:100]}..."
            )

        return answer, "\\n\\n---\\n\\n".join(citations)
    else:
        return f"❌ 查询失败: {response.text}", ""


def get_documents():
    """获取文档列表"""
    response = requests.get(f"{API_URL}/documents")
    if response.ok:
        docs = response.json()["documents"]
        return "\\n".join([f"📄 {d}" for d in docs]) if docs else "暂无文档"
    return "获取失败"


# 构建界面
with gr.Blocks(title="文档问答系统", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# 📚 智能文档问答系统")
    gr.Markdown("上传 PDF/Word/Excel/TXT 文档，AI 帮你阅读和回答问题")

    with gr.Tabs():
        # 问答标签页
        with gr.Tab("💬 问答"):
            with gr.Row():
                with gr.Column(scale=2):
                    question_input = gr.Textbox(
                        label="您的问题",
                        placeholder="请输入关于文档的问题...",
                        lines=2
                    )
                    k_slider = gr.Slider(
                        minimum=1, maximum=10, value=5, step=1,
                        label="检索文档数量"
                    )
                    ask_btn = gr.Button("🔍 提问", variant="primary")

                with gr.Column(scale=3):
                    answer_output = gr.Markdown(label="回答")
                    citations_output = gr.Markdown(label="引用来源")

            ask_btn.click(
                ask_question,
                inputs=[question_input, k_slider],
                outputs=[answer_output, citations_output]
            )

        # 文档管理标签页
        with gr.Tab("📁 文档管理"):
            with gr.Row():
                file_input = gr.File(
                    label="上传文档",
                    file_types=[".pdf", ".docx", ".xlsx", ".txt"]
                )
                upload_btn = gr.Button("📤 上传并索引")

            upload_status = gr.Textbox(label="上传状态", interactive=False)
            upload_btn.click(upload_file, inputs=file_input, outputs=upload_status)

            gr.Markdown("---")
            docs_display = gr.Markdown(label="已索引文档")
            refresh_btn = gr.Button("🔄 刷新列表")
            refresh_btn.click(get_documents, outputs=docs_display)

    gr.Markdown("---")
    gr.Markdown("💡 支持格式: PDF, Word (.docx), Excel (.xlsx), 纯文本 (.txt)")


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7860)
\`\`\`

---

## 🚀 运行项目

\`\`\`bash
# 1. 安装依赖
pip install -r requirements.txt

# 2. 配置环境变量
export OPENAI_API_KEY="your-key"
export ANTHROPIC_API_KEY="your-key"

# 3. 启动后端 API
python api/main.py

# 4. 启动前端（新终端）
python frontend/app.py

# 访问 http://localhost:7860
\`\`\`

---

## 📊 功能对比

| 功能 | 基础版 | 完整版 |
|------|--------|--------|
| 文档格式 | PDF | PDF/Word/Excel/TXT |
| 来源引用 | ❌ | ✅ 页码标注 |
| 多文档 | ❌ | ✅ 批量上传 |
| 文档管理 | ❌ | ✅ 增删查 |
| API 接口 | ❌ | ✅ RESTful |
| 流式输出 | ❌ | ✅ |
            `,ja:`
## プロジェクト：マルチフォーマットドキュメントQ&Aシステム

PDF/Word/Excel/TXTをサポートし、ソース引用とページ番号表示付きのインテリジェントドキュメントQ&Aシステムを構築。

---

## 📁 プロジェクト構造

\`\`\`
document-qa-system/
├── requirements.txt
├── src/
│   ├── document_loader.py      # マルチフォーマットローダー
│   ├── text_processor.py       # テキスト処理
│   ├── vector_store.py         # ベクトルストア
│   └── qa_engine.py            # Q&Aエンジン
├── api/
│   └── main.py                 # FastAPIサービス
└── frontend/
    └── app.py                  # Gradioインターフェース
\`\`\`

---

## 📄 マルチフォーマットローダー

\`\`\`python
class MultiFormatLoader:
    def __init__(self):
        self.loaders = [
            PDFLoader(),
            WordLoader(),
            ExcelLoader(),
            TxtLoader()
        ]

    def load(self, file_path: str) -> List[Document]:
        for loader in self.loaders:
            if loader.supports(file_path):
                return loader.load(file_path)
        raise ValueError(f"サポートされていないフォーマット: {file_path}")
\`\`\`

---

## 🤖 Q&Aエンジン

\`\`\`python
@dataclass
class Citation:
    source: str
    page: int | None
    content: str
    relevance_score: float

class DocumentQAEngine:
    def query(self, question: str, k: int = 5) -> QAResponse:
        # ドキュメント検索
        docs = vector_store.search_with_score(question, k=k)

        # コンテキスト構築
        context, citations = self._format_context(docs)

        # 回答生成
        answer = self.chain.invoke({
            "context": context,
            "question": question
        })

        return QAResponse(answer=answer, citations=citations)
\`\`\`

---

## 🌐 FastAPI サービス

\`\`\`python
@app.post("/upload")
async def upload_document(file: UploadFile):
    # ファイル保存
    # ドキュメント読み込み
    # チャンク分割
    # ベクトルインデックス作成
    return {"chunks": len(chunks)}

@app.post("/ask")
async def ask_question(request: QuestionRequest):
    response = qa_engine.query(request.question)
    return {"answer": response.answer, "citations": response.citations}
\`\`\`

---

## 📊 機能比較

| 機能 | 基本版 | 完全版 |
|------|--------|--------|
| ドキュメント形式 | PDF | PDF/Word/Excel/TXT |
| ソース引用 | ❌ | ✅ ページ番号付き |
| 複数ドキュメント | ❌ | ✅ 一括アップロード |
| ドキュメント管理 | ❌ | ✅ CRUD |
| API | ❌ | ✅ RESTful |
            `}},{id:"ch7-code-assistant",title:{zh:"7.3 代码助手开发",ja:"7.3 コードアシスタント開発"},content:{zh:`
## 项目：完整代码助手（Agentic 循环）

构建一个能读取、搜索、编辑代码的 AI 编程助手，实现完整的 Agentic 循环。

---

## 📁 项目结构

\`\`\`
code-assistant/
├── requirements.txt
├── config.py                   # 配置
├── src/
│   ├── __init__.py
│   ├── tools/                  # 工具定义
│   │   ├── __init__.py
│   │   ├── file_tools.py       # 文件操作
│   │   ├── search_tools.py     # 代码搜索
│   │   ├── git_tools.py        # Git 操作
│   │   └── shell_tools.py      # Shell 命令
│   ├── agent.py                # Agent 核心
│   └── tool_executor.py        # 工具执行器
├── main.py                     # CLI 入口
└── tests/
    └── test_tools.py
\`\`\`

---

## 📋 依赖安装

\`\`\`bash
# requirements.txt
anthropic>=0.40.0
rich>=13.0.0
gitpython>=3.1.0
click>=8.0.0
\`\`\`

---

## 🔧 工具定义 (tools/file_tools.py)

\`\`\`python
import os
from pathlib import Path
from typing import Optional


def read_file(path: str, start_line: int = 1, end_line: Optional[int] = None) -> str:
    """读取文件内容，支持行范围"""
    try:
        with open(path, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        total_lines = len(lines)
        start_idx = max(0, start_line - 1)
        end_idx = end_line if end_line else total_lines

        selected_lines = lines[start_idx:end_idx]

        # 添加行号
        numbered_lines = [
            f"{start_line + i:4d} | {line.rstrip()}"
            for i, line in enumerate(selected_lines)
        ]

        return f"文件: {path} (共 {total_lines} 行)\\n" + "\\n".join(numbered_lines)

    except FileNotFoundError:
        return f"错误: 文件不存在 - {path}"
    except Exception as e:
        return f"错误: {str(e)}"


def write_file(path: str, content: str) -> str:
    """写入文件"""
    try:
        # 确保目录存在
        Path(path).parent.mkdir(parents=True, exist_ok=True)

        with open(path, 'w', encoding='utf-8') as f:
            f.write(content)

        return f"成功写入: {path}"
    except Exception as e:
        return f"错误: {str(e)}"


def edit_file(path: str, old_text: str, new_text: str) -> str:
    """编辑文件（字符串替换）"""
    try:
        with open(path, 'r', encoding='utf-8') as f:
            content = f.read()

        if old_text not in content:
            return f"错误: 未找到要替换的文本"

        # 检查是否有多处匹配
        count = content.count(old_text)
        if count > 1:
            return f"警告: 找到 {count} 处匹配，请提供更多上下文以精确定位"

        new_content = content.replace(old_text, new_text, 1)

        with open(path, 'w', encoding='utf-8') as f:
            f.write(new_content)

        return f"成功编辑: {path}"
    except Exception as e:
        return f"错误: {str(e)}"


def list_directory(path: str = ".", recursive: bool = False) -> str:
    """列出目录内容"""
    try:
        p = Path(path)
        if not p.exists():
            return f"错误: 目录不存在 - {path}"

        if recursive:
            items = list(p.rglob("*"))
        else:
            items = list(p.iterdir())

        # 过滤隐藏文件和常见忽略目录
        ignore_patterns = {'.git', 'node_modules', '__pycache__', '.venv', 'venv'}
        items = [
            item for item in items
            if not any(part in ignore_patterns for part in item.parts)
            and not item.name.startswith('.')
        ]

        # 分类显示
        dirs = sorted([f"📁 {item.relative_to(p)}" for item in items if item.is_dir()])
        files = sorted([f"📄 {item.relative_to(p)}" for item in items if item.is_file()])

        return f"目录: {path}\\n\\n" + "\\n".join(dirs + files)
    except Exception as e:
        return f"错误: {str(e)}"


# 工具 Schema 定义
FILE_TOOLS = [
    {
        "name": "read_file",
        "description": "读取文件内容。可指定起始行和结束行。",
        "input_schema": {
            "type": "object",
            "properties": {
                "path": {
                    "type": "string",
                    "description": "文件路径"
                },
                "start_line": {
                    "type": "integer",
                    "description": "起始行号（默认1）",
                    "default": 1
                },
                "end_line": {
                    "type": "integer",
                    "description": "结束行号（默认到文件末尾）"
                }
            },
            "required": ["path"]
        }
    },
    {
        "name": "write_file",
        "description": "创建或覆盖文件",
        "input_schema": {
            "type": "object",
            "properties": {
                "path": {"type": "string", "description": "文件路径"},
                "content": {"type": "string", "description": "文件内容"}
            },
            "required": ["path", "content"]
        }
    },
    {
        "name": "edit_file",
        "description": "编辑文件，通过替换指定文本",
        "input_schema": {
            "type": "object",
            "properties": {
                "path": {"type": "string", "description": "文件路径"},
                "old_text": {"type": "string", "description": "要替换的原文本"},
                "new_text": {"type": "string", "description": "替换后的新文本"}
            },
            "required": ["path", "old_text", "new_text"]
        }
    },
    {
        "name": "list_directory",
        "description": "列出目录内容",
        "input_schema": {
            "type": "object",
            "properties": {
                "path": {"type": "string", "description": "目录路径", "default": "."},
                "recursive": {"type": "boolean", "description": "是否递归", "default": False}
            }
        }
    }
]
\`\`\`

---

## 🔍 搜索工具 (tools/search_tools.py)

\`\`\`python
import subprocess
from pathlib import Path
from typing import List, Optional


def grep_search(
    pattern: str,
    path: str = ".",
    file_pattern: Optional[str] = None,
    context_lines: int = 2
) -> str:
    """使用 ripgrep 搜索代码"""
    try:
        cmd = ["rg", "--color=never", "-n"]

        if context_lines > 0:
            cmd.extend(["-C", str(context_lines)])

        if file_pattern:
            cmd.extend(["-g", file_pattern])

        # 忽略常见目录
        cmd.extend(["--ignore-file", ".gitignore"])
        cmd.append(pattern)
        cmd.append(path)

        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=30
        )

        if result.returncode == 0:
            return f"搜索结果 '{pattern}':\\n\\n{result.stdout}"
        elif result.returncode == 1:
            return f"未找到匹配: {pattern}"
        else:
            return f"搜索错误: {result.stderr}"

    except FileNotFoundError:
        # fallback to grep
        return _grep_fallback(pattern, path)
    except Exception as e:
        return f"错误: {str(e)}"


def _grep_fallback(pattern: str, path: str) -> str:
    """使用 Python 实现的简单搜索"""
    results = []
    p = Path(path)

    for file_path in p.rglob("*"):
        if file_path.is_file() and not _should_ignore(file_path):
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    for i, line in enumerate(f, 1):
                        if pattern in line:
                            results.append(f"{file_path}:{i}: {line.strip()}")
            except:
                continue

    if results:
        return f"搜索结果 '{pattern}':\\n\\n" + "\\n".join(results[:50])
    return f"未找到匹配: {pattern}"


def _should_ignore(path: Path) -> bool:
    """判断是否应忽略"""
    ignore = {'.git', 'node_modules', '__pycache__', '.venv', 'dist', 'build'}
    return any(part in ignore for part in path.parts)


def find_definition(symbol: str, path: str = ".") -> str:
    """查找函数/类定义"""
    patterns = [
        f"def {symbol}",           # Python 函数
        f"class {symbol}",         # Python 类
        f"function {symbol}",      # JavaScript 函数
        f"const {symbol}",         # JavaScript 常量
        f"interface {symbol}",     # TypeScript 接口
        f"type {symbol}",          # TypeScript 类型
    ]

    results = []
    for pattern in patterns:
        result = grep_search(pattern, path)
        if "未找到" not in result:
            results.append(result)

    if results:
        return "\\n\\n".join(results)
    return f"未找到定义: {symbol}"


SEARCH_TOOLS = [
    {
        "name": "grep_search",
        "description": "在代码库中搜索文本或正则表达式",
        "input_schema": {
            "type": "object",
            "properties": {
                "pattern": {"type": "string", "description": "搜索模式"},
                "path": {"type": "string", "description": "搜索路径", "default": "."},
                "file_pattern": {"type": "string", "description": "文件过滤（如 *.py）"},
                "context_lines": {"type": "integer", "description": "上下文行数", "default": 2}
            },
            "required": ["pattern"]
        }
    },
    {
        "name": "find_definition",
        "description": "查找函数或类的定义位置",
        "input_schema": {
            "type": "object",
            "properties": {
                "symbol": {"type": "string", "description": "函数或类名"},
                "path": {"type": "string", "description": "搜索路径", "default": "."}
            },
            "required": ["symbol"]
        }
    }
]
\`\`\`

---

## 🐚 Shell 工具 (tools/shell_tools.py)

\`\`\`python
import subprocess
import shlex
from typing import Optional

# 允许执行的命令白名单
ALLOWED_COMMANDS = {
    'ls', 'cat', 'head', 'tail', 'wc', 'find', 'tree',
    'git', 'npm', 'pip', 'python', 'node', 'pytest',
    'make', 'cargo', 'go'
}

# 危险命令黑名单
DANGEROUS_PATTERNS = [
    'rm -rf', 'sudo', 'chmod 777', '> /dev',
    'curl | bash', 'wget | sh', 'dd if='
]


def run_command(command: str, timeout: int = 60) -> str:
    """执行 shell 命令（带安全检查）"""
    # 安全检查
    for pattern in DANGEROUS_PATTERNS:
        if pattern in command:
            return f"拒绝执行: 检测到危险模式 '{pattern}'"

    # 解析命令
    try:
        parts = shlex.split(command)
        base_cmd = parts[0] if parts else ""

        if base_cmd not in ALLOWED_COMMANDS:
            return f"拒绝执行: '{base_cmd}' 不在允许列表中。\\n允许: {', '.join(sorted(ALLOWED_COMMANDS))}"

    except Exception as e:
        return f"命令解析错误: {str(e)}"

    # 执行命令
    try:
        result = subprocess.run(
            command,
            shell=True,
            capture_output=True,
            text=True,
            timeout=timeout,
            cwd="."
        )

        output = []
        if result.stdout:
            output.append(f"stdout:\\n{result.stdout}")
        if result.stderr:
            output.append(f"stderr:\\n{result.stderr}")
        output.append(f"exit code: {result.returncode}")

        return "\\n".join(output)

    except subprocess.TimeoutExpired:
        return f"命令超时（{timeout}秒）"
    except Exception as e:
        return f"执行错误: {str(e)}"


SHELL_TOOLS = [
    {
        "name": "run_command",
        "description": "执行 shell 命令（受限于安全白名单）",
        "input_schema": {
            "type": "object",
            "properties": {
                "command": {"type": "string", "description": "要执行的命令"},
                "timeout": {"type": "integer", "description": "超时秒数", "default": 60}
            },
            "required": ["command"]
        }
    }
]
\`\`\`

---

## 🤖 Agent 核心 (agent.py)

\`\`\`python
from typing import List, Dict, Any, Generator
from anthropic import Anthropic

from tools.file_tools import FILE_TOOLS, read_file, write_file, edit_file, list_directory
from tools.search_tools import SEARCH_TOOLS, grep_search, find_definition
from tools.shell_tools import SHELL_TOOLS, run_command


class CodeAssistant:
    """代码助手 Agent"""

    def __init__(
        self,
        model: str = "claude-sonnet-4-20250514",
        max_iterations: int = 20
    ):
        self.client = Anthropic()
        self.model = model
        self.max_iterations = max_iterations

        # 合并所有工具
        self.tools = FILE_TOOLS + SEARCH_TOOLS + SHELL_TOOLS

        # 工具执行映射
        self.tool_handlers = {
            "read_file": read_file,
            "write_file": write_file,
            "edit_file": edit_file,
            "list_directory": list_directory,
            "grep_search": grep_search,
            "find_definition": find_definition,
            "run_command": run_command
        }

        # 系统提示
        self.system_prompt = """你是一个专业的代码助手，可以帮助用户：
1. 阅读和理解代码
2. 搜索代码库
3. 编辑和创建文件
4. 执行命令

工作原则：
- 先理解再行动：阅读相关代码后再修改
- 最小改动：只修改必要的部分
- 安全第一：不执行危险命令
- 保持沟通：解释你的思考过程

当前工作目录是项目根目录。"""

    def _execute_tool(self, name: str, input_data: Dict) -> str:
        """执行工具调用"""
        handler = self.tool_handlers.get(name)
        if not handler:
            return f"未知工具: {name}"

        try:
            return handler(**input_data)
        except Exception as e:
            return f"工具执行错误: {str(e)}"

    def chat(
        self,
        user_message: str,
        conversation_history: List[Dict] = None
    ) -> Generator[str, None, None]:
        """
        对话接口，支持流式输出

        Yields:
            str: 输出片段（文本或工具调用信息）
        """
        messages = conversation_history or []
        messages.append({"role": "user", "content": user_message})

        iteration = 0

        while iteration < self.max_iterations:
            iteration += 1

            # 调用 API
            response = self.client.messages.create(
                model=self.model,
                max_tokens=4096,
                system=self.system_prompt,
                tools=self.tools,
                messages=messages
            )

            # 处理响应
            assistant_content = []

            for block in response.content:
                if block.type == "text":
                    yield block.text
                    assistant_content.append(block)

                elif block.type == "tool_use":
                    yield f"\\n🔧 调用工具: {block.name}\\n"
                    yield f"   参数: {block.input}\\n"

                    # 执行工具
                    result = self._execute_tool(block.name, block.input)
                    yield f"   结果: {result[:200]}...\\n" if len(result) > 200 else f"   结果: {result}\\n"

                    assistant_content.append(block)

                    # 添加工具结果到消息
                    messages.append({"role": "assistant", "content": assistant_content})
                    messages.append({
                        "role": "user",
                        "content": [{
                            "type": "tool_result",
                            "tool_use_id": block.id,
                            "content": result
                        }]
                    })
                    assistant_content = []

            # 如果没有工具调用，对话结束
            if response.stop_reason == "end_turn":
                if assistant_content:
                    messages.append({"role": "assistant", "content": assistant_content})
                break

        yield f"\\n[完成，共 {iteration} 轮迭代]"

    def run_task(self, task: str) -> str:
        """执行任务（非交互式）"""
        output = []
        for chunk in self.chat(task):
            output.append(chunk)
        return "".join(output)
\`\`\`

---

## 💻 CLI 入口 (main.py)

\`\`\`python
import click
from rich.console import Console
from rich.markdown import Markdown
from rich.panel import Panel

from agent import CodeAssistant

console = Console()


@click.group()
def cli():
    """代码助手 CLI"""
    pass


@cli.command()
def chat():
    """交互式对话模式"""
    console.print(Panel.fit(
        "[bold blue]代码助手[/bold blue]\\n"
        "输入问题或任务，输入 'exit' 退出",
        title="Welcome"
    ))

    assistant = CodeAssistant()
    history = []

    while True:
        try:
            user_input = console.input("[bold green]> [/bold green]")

            if user_input.lower() in ('exit', 'quit', 'q'):
                console.print("[yellow]再见！[/yellow]")
                break

            if not user_input.strip():
                continue

            # 流式输出
            console.print()
            for chunk in assistant.chat(user_input, history):
                console.print(chunk, end="")
            console.print()

        except KeyboardInterrupt:
            console.print("\\n[yellow]已中断[/yellow]")
            break


@cli.command()
@click.argument('task')
def run(task: str):
    """执行单个任务"""
    assistant = CodeAssistant()
    result = assistant.run_task(task)
    console.print(Markdown(result))


@cli.command()
@click.argument('file_path')
def explain(file_path: str):
    """解释代码文件"""
    assistant = CodeAssistant()
    result = assistant.run_task(f"请阅读并解释这个文件的代码: {file_path}")
    console.print(Markdown(result))


@cli.command()
@click.argument('symbol')
def find(symbol: str):
    """查找符号定义"""
    assistant = CodeAssistant()
    result = assistant.run_task(f"查找 {symbol} 的定义和用法")
    console.print(Markdown(result))


if __name__ == "__main__":
    cli()
\`\`\`

---

## 🚀 使用示例

\`\`\`bash
# 安装
pip install -r requirements.txt

# 交互模式
python main.py chat

# 单任务模式
python main.py run "帮我找到所有 TODO 注释"

# 解释代码
python main.py explain src/agent.py

# 查找定义
python main.py find CodeAssistant
\`\`\`

---

## 💡 Agentic 循环核心逻辑

\`\`\`
┌─────────────────────────────────────────┐
│  用户输入: "帮我修复这个 bug"            │
└───────────────┬─────────────────────────┘
                ▼
┌─────────────────────────────────────────┐
│  Agent 思考: 需要先了解代码              │
│  -> 调用 read_file 读取相关文件          │
└───────────────┬─────────────────────────┘
                ▼
┌─────────────────────────────────────────┐
│  工具返回: 文件内容                      │
│  Agent 分析: 发现问题在第 42 行          │
│  -> 调用 edit_file 修复                 │
└───────────────┬─────────────────────────┘
                ▼
┌─────────────────────────────────────────┐
│  工具返回: 修改成功                      │
│  Agent: 需要验证修复                     │
│  -> 调用 run_command 执行测试           │
└───────────────┬─────────────────────────┘
                ▼
┌─────────────────────────────────────────┐
│  测试通过，任务完成                      │
│  Agent 输出: 总结修复过程                │
└─────────────────────────────────────────┘
\`\`\`

---

## 📊 与 Claude Code 对比

| 特性 | 自建 Agent | Claude Code |
|------|------------|-------------|
| 自定义工具 | ✅ 完全可控 | ⚠️ 受限 |
| MCP 集成 | ✅ 可选 | ✅ 原生支持 |
| 安全控制 | ✅ 自定义白名单 | ✅ 沙箱隔离 |
| 部署方式 | 服务器/本地 | 本地 CLI |
| 学习成本 | 中等 | 低 |
| 适用场景 | 定制化需求 | 通用开发 |
            `,ja:`
## プロジェクト：完全なコードアシスタント（Agenticループ）

コードの読み取り、検索、編集ができるAIプログラミングアシスタントを構築し、完全なAgenticループを実装。

---

## 📁 プロジェクト構造

\`\`\`
code-assistant/
├── requirements.txt
├── src/
│   ├── tools/                  # ツール定義
│   │   ├── file_tools.py       # ファイル操作
│   │   ├── search_tools.py     # コード検索
│   │   └── shell_tools.py      # シェルコマンド
│   └── agent.py                # Agentコア
└── main.py                     # CLIエントリ
\`\`\`

---

## 🔧 ファイルツール (file_tools.py)

\`\`\`python
def read_file(path: str, start_line: int = 1, end_line: int = None) -> str:
    """ファイル内容を読み取り、行番号付きで返す"""
    with open(path, 'r') as f:
        lines = f.readlines()
    # 行番号を追加して返す
    return "\\n".join([f"{i:4d} | {line}" for i, line in enumerate(lines, 1)])

def edit_file(path: str, old_text: str, new_text: str) -> str:
    """テキスト置換でファイルを編集"""
    with open(path, 'r') as f:
        content = f.read()
    new_content = content.replace(old_text, new_text, 1)
    with open(path, 'w') as f:
        f.write(new_content)
    return f"編集成功: {path}"
\`\`\`

---

## 🤖 Agentコア (agent.py)

\`\`\`python
class CodeAssistant:
    def __init__(self, model: str = "claude-sonnet-4-20250514"):
        self.client = Anthropic()
        self.tools = FILE_TOOLS + SEARCH_TOOLS + SHELL_TOOLS

    def chat(self, user_message: str) -> Generator[str, None, None]:
        messages = [{"role": "user", "content": user_message}]

        while True:
            response = self.client.messages.create(
                model=self.model,
                tools=self.tools,
                messages=messages
            )

            for block in response.content:
                if block.type == "text":
                    yield block.text
                elif block.type == "tool_use":
                    result = self._execute_tool(block.name, block.input)
                    # ツール結果をメッセージに追加
                    messages.append({"role": "user", "content": [{
                        "type": "tool_result",
                        "tool_use_id": block.id,
                        "content": result
                    }]})

            if response.stop_reason == "end_turn":
                break
\`\`\`

---

## 💡 Agenticループのコアロジック

\`\`\`
ユーザー入力 → Agent思考 → ツール呼び出し → 結果分析
     ↑                                         ↓
     └──────────── 必要に応じて繰り返し ←────────┘
\`\`\`

---

## 📊 Claude Code との比較

| 特性 | 自作Agent | Claude Code |
|------|-----------|-------------|
| カスタムツール | ✅ 完全制御可能 | ⚠️ 制限あり |
| MCP統合 | ✅ オプション | ✅ ネイティブ |
| セキュリティ | ✅ カスタムホワイトリスト | ✅ サンドボックス |
| デプロイ | サーバー/ローカル | ローカルCLI |
            `}},{id:"ch7-agent-workflow",title:{zh:"7.4 AI Agent 自动化工作流",ja:"7.4 AI Agent 自動化ワークフロー"},content:{zh:`
## 项目：自动化数据采集与报告生成

构建一个能自动浏览网页、提取数据、生成报告的 AI Agent。

---

## 📁 项目结构

\`\`\`
automation-agent/
├── requirements.txt
├── config.py
├── src/
│   ├── browser_agent.py        # 浏览器自动化
│   ├── data_extractor.py       # 数据提取
│   ├── report_generator.py     # 报告生成
│   └── scheduler.py            # 定时任务
├── templates/
│   └── report_template.html
└── main.py
\`\`\`

---

## 📋 依赖安装

\`\`\`bash
# requirements.txt
anthropic>=0.40.0
playwright>=1.40.0
pandas>=2.0.0
jinja2>=3.0.0
schedule>=1.2.0
\`\`\`

---

## 🌐 浏览器自动化 (browser_agent.py)

\`\`\`python
import asyncio
from typing import List, Dict, Any, Optional
from playwright.async_api import async_playwright, Page, Browser
from anthropic import Anthropic


class BrowserAgent:
    """基于 AI 的浏览器自动化 Agent"""

    def __init__(self, headless: bool = True):
        self.headless = headless
        self.client = Anthropic()
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None

    async def __aenter__(self):
        playwright = await async_playwright().start()
        self.browser = await playwright.chromium.launch(headless=self.headless)
        self.page = await self.browser.new_page()
        return self

    async def __aexit__(self, *args):
        if self.browser:
            await self.browser.close()

    async def navigate(self, url: str) -> str:
        """导航到 URL"""
        await self.page.goto(url, wait_until="networkidle")
        return f"已导航到: {url}"

    async def get_page_content(self) -> str:
        """获取页面内容（简化版）"""
        # 提取主要文本内容
        content = await self.page.evaluate('''() => {
            const elements = document.querySelectorAll('h1, h2, h3, p, li, td, th, a');
            return Array.from(elements).map(el => ({
                tag: el.tagName.toLowerCase(),
                text: el.innerText.trim().substring(0, 200),
                href: el.href || null
            })).filter(el => el.text.length > 0).slice(0, 100);
        }''')
        return str(content)

    async def click(self, selector: str) -> str:
        """点击元素"""
        try:
            await self.page.click(selector, timeout=5000)
            await self.page.wait_for_load_state("networkidle")
            return f"已点击: {selector}"
        except Exception as e:
            return f"点击失败: {str(e)}"

    async def fill(self, selector: str, value: str) -> str:
        """填写表单"""
        try:
            await self.page.fill(selector, value)
            return f"已填写: {selector}"
        except Exception as e:
            return f"填写失败: {str(e)}"

    async def screenshot(self, path: str = "screenshot.png") -> str:
        """截图"""
        await self.page.screenshot(path=path, full_page=True)
        return f"截图保存: {path}"

    async def extract_table(self, selector: str = "table") -> List[Dict]:
        """提取表格数据"""
        tables = await self.page.query_selector_all(selector)
        if not tables:
            return []

        result = []
        for table in tables:
            rows = await table.query_selector_all("tr")
            table_data = []

            for row in rows:
                cells = await row.query_selector_all("td, th")
                row_data = [await cell.inner_text() for cell in cells]
                if row_data:
                    table_data.append(row_data)

            if table_data:
                # 第一行作为表头
                headers = table_data[0]
                for row in table_data[1:]:
                    result.append(dict(zip(headers, row)))

        return result

    def _get_tools(self) -> List[Dict]:
        """定义浏览器工具"""
        return [
            {
                "name": "navigate",
                "description": "导航到指定 URL",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "url": {"type": "string", "description": "目标 URL"}
                    },
                    "required": ["url"]
                }
            },
            {
                "name": "get_page_content",
                "description": "获取当前页面的主要内容",
                "input_schema": {"type": "object", "properties": {}}
            },
            {
                "name": "click",
                "description": "点击页面元素",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "selector": {"type": "string", "description": "CSS 选择器"}
                    },
                    "required": ["selector"]
                }
            },
            {
                "name": "fill",
                "description": "填写表单字段",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "selector": {"type": "string"},
                        "value": {"type": "string"}
                    },
                    "required": ["selector", "value"]
                }
            },
            {
                "name": "extract_table",
                "description": "提取页面中的表格数据",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "selector": {"type": "string", "default": "table"}
                    }
                }
            },
            {
                "name": "screenshot",
                "description": "对当前页面截图",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "path": {"type": "string", "default": "screenshot.png"}
                    }
                }
            }
        ]

    async def _execute_tool(self, name: str, input_data: Dict) -> str:
        """执行工具"""
        handlers = {
            "navigate": lambda: self.navigate(input_data["url"]),
            "get_page_content": self.get_page_content,
            "click": lambda: self.click(input_data["selector"]),
            "fill": lambda: self.fill(input_data["selector"], input_data["value"]),
            "extract_table": lambda: self.extract_table(input_data.get("selector", "table")),
            "screenshot": lambda: self.screenshot(input_data.get("path", "screenshot.png"))
        }

        handler = handlers.get(name)
        if handler:
            result = await handler()
            return str(result)
        return f"未知工具: {name}"

    async def run_task(self, task: str, max_iterations: int = 15) -> str:
        """执行自动化任务"""
        messages = [{"role": "user", "content": task}]
        results = []

        for i in range(max_iterations):
            response = self.client.messages.create(
                model="claude-sonnet-4-20250514",
                max_tokens=4096,
                system="""你是一个网页自动化助手。你可以：
1. 导航到网页
2. 提取页面内容和表格
3. 点击和填写表单
4. 截图

请一步一步完成任务，每次只执行一个操作。""",
                tools=self._get_tools(),
                messages=messages
            )

            assistant_content = []
            for block in response.content:
                if block.type == "text":
                    results.append(block.text)
                    assistant_content.append(block)

                elif block.type == "tool_use":
                    result = await self._execute_tool(block.name, block.input)
                    results.append(f"[{block.name}] {result}")
                    assistant_content.append(block)

                    messages.append({"role": "assistant", "content": assistant_content})
                    messages.append({
                        "role": "user",
                        "content": [{
                            "type": "tool_result",
                            "tool_use_id": block.id,
                            "content": result
                        }]
                    })
                    assistant_content = []

            if response.stop_reason == "end_turn":
                break

        return "\\n".join(results)
\`\`\`

---

## 📊 报告生成器 (report_generator.py)

\`\`\`python
import pandas as pd
from datetime import datetime
from pathlib import Path
from jinja2 import Environment, FileSystemLoader
from typing import List, Dict, Any


class ReportGenerator:
    """自动报告生成器"""

    def __init__(self, template_dir: str = "./templates"):
        self.env = Environment(loader=FileSystemLoader(template_dir))

    def generate_html_report(
        self,
        title: str,
        data: List[Dict],
        output_path: str,
        summary: str = ""
    ) -> str:
        """生成 HTML 报告"""
        template = self.env.get_template("report_template.html")

        # 转换为 DataFrame 以便处理
        df = pd.DataFrame(data)

        html = template.render(
            title=title,
            generated_at=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            summary=summary,
            table_html=df.to_html(classes="data-table", index=False),
            total_rows=len(data)
        )

        Path(output_path).write_text(html, encoding='utf-8')
        return output_path

    def generate_csv(self, data: List[Dict], output_path: str) -> str:
        """生成 CSV 文件"""
        df = pd.DataFrame(data)
        df.to_csv(output_path, index=False, encoding='utf-8-sig')
        return output_path

    def generate_excel(self, data: List[Dict], output_path: str) -> str:
        """生成 Excel 文件"""
        df = pd.DataFrame(data)
        df.to_excel(output_path, index=False, engine='openpyxl')
        return output_path
\`\`\`

---

## ⏰ 定时任务 (scheduler.py)

\`\`\`python
import schedule
import time
import asyncio
from datetime import datetime
from typing import Callable


class TaskScheduler:
    """定时任务调度器"""

    def __init__(self):
        self.tasks = []

    def add_daily_task(self, time_str: str, task: Callable):
        """添加每日任务"""
        schedule.every().day.at(time_str).do(task)
        self.tasks.append(f"每日 {time_str}: {task.__name__}")

    def add_hourly_task(self, task: Callable):
        """添加每小时任务"""
        schedule.every().hour.do(task)
        self.tasks.append(f"每小时: {task.__name__}")

    def run(self):
        """运行调度器"""
        print(f"调度器启动，共 {len(self.tasks)} 个任务")
        for task in self.tasks:
            print(f"  - {task}")

        while True:
            schedule.run_pending()
            time.sleep(60)


# 使用示例
async def daily_data_collection():
    """每日数据采集任务"""
    async with BrowserAgent() as agent:
        result = await agent.run_task(
            "访问 https://example.com/data，提取今日数据表格，生成报告"
        )
        print(f"[{datetime.now()}] 采集完成: {result[:100]}...")


if __name__ == "__main__":
    scheduler = TaskScheduler()

    # 每天早上 9 点执行
    scheduler.add_daily_task("09:00", lambda: asyncio.run(daily_data_collection()))

    scheduler.run()
\`\`\`

---

## 🚀 完整使用示例 (main.py)

\`\`\`python
import asyncio
from browser_agent import BrowserAgent
from report_generator import ReportGenerator


async def main():
    # 1. 创建浏览器 Agent
    async with BrowserAgent(headless=False) as agent:

        # 2. 执行数据采集任务
        result = await agent.run_task("""
        请完成以下任务：
        1. 访问 https://quotes.toscrape.com/
        2. 提取页面上的名言和作者
        3. 截图保存
        """)

        print("任务结果:", result)

        # 3. 提取表格数据
        await agent.navigate("https://quotes.toscrape.com/tableful/")
        data = await agent.extract_table()

        # 4. 生成报告
        if data:
            generator = ReportGenerator()
            report_path = generator.generate_html_report(
                title="名言数据采集报告",
                data=data,
                output_path="./output/report.html",
                summary="自动采集的名言数据"
            )
            print(f"报告已生成: {report_path}")


if __name__ == "__main__":
    asyncio.run(main())
\`\`\`

---

## 💡 安全注意事项

| 风险 | 防护措施 |
|------|----------|
| 访问恶意网站 | URL 白名单过滤 |
| 表单自动提交 | 敏感操作需确认 |
| 数据泄露 | 不保存敏感信息 |
| 无限循环 | 设置最大迭代次数 |
| 资源耗尽 | 超时和内存限制 |

---

## 📊 应用场景

| 场景 | 描述 |
|------|------|
| 竞品监控 | 定期采集竞品价格/功能 |
| 数据采集 | 自动抓取公开数据 |
| 报告生成 | 每日/周自动生成报告 |
| 表单测试 | 自动化 E2E 测试 |
| 内容聚合 | 多源信息汇总 |
            `,ja:`
## プロジェクト：自動データ収集とレポート生成

Webページを自動的にブラウズし、データを抽出し、レポートを生成するAI Agentを構築。

---

## 📁 プロジェクト構造

\`\`\`
automation-agent/
├── src/
│   ├── browser_agent.py        # ブラウザ自動化
│   ├── data_extractor.py       # データ抽出
│   ├── report_generator.py     # レポート生成
│   └── scheduler.py            # スケジューラー
└── main.py
\`\`\`

---

## 🌐 ブラウザ自動化 (browser_agent.py)

\`\`\`python
class BrowserAgent:
    async def navigate(self, url: str) -> str:
        await self.page.goto(url)
        return f"ナビゲート完了: {url}"

    async def extract_table(self, selector: str = "table") -> List[Dict]:
        # テーブルデータを抽出
        tables = await self.page.query_selector_all(selector)
        # ...処理...
        return result

    async def run_task(self, task: str) -> str:
        # AIがタスクを実行
        messages = [{"role": "user", "content": task}]
        # Agenticループで自動実行
        ...
\`\`\`

---

## ⏰ スケジューラー

\`\`\`python
import schedule

scheduler = TaskScheduler()
# 毎日午前9時に実行
scheduler.add_daily_task("09:00", daily_data_collection)
scheduler.run()
\`\`\`

---

## 📊 応用シーン

| シーン | 説明 |
|--------|------|
| 競合モニタリング | 定期的に競合の価格/機能を収集 |
| データ収集 | 公開データの自動スクレイピング |
| レポート生成 | 日次/週次自動レポート |
| E2Eテスト | 自動化フォームテスト |
            `}},{id:"ch7-summary",title:{zh:"7.5 本章小结",ja:"7.5 この章のまとめ"},content:{zh:`
## 实战项目核心要点

---

## 💡 项目开发流程

1. **明确需求** —— 用户要解决什么问题
2. **选择技术栈** —— LLM + 向量库 + 框架
3. **构建原型** —— 快速验证可行性
4. **迭代优化** —— 根据反馈改进

---

## 🚀 推荐起步项目

| 难度 | 项目 | 时间 |
|------|------|------|
| ⭐ | 命令行聊天机器人 | 1天 |
| ⭐⭐ | Streamlit 文档问答 | 3天 |
| ⭐⭐⭐ | 完整客服系统 | 1-2周 |
            `,ja:`
## 実践プロジェクトコアポイント

---

## 🚀 推奨スタートプロジェクト

| 難易度 | プロジェクト | 時間 |
|--------|--------------|------|
| ⭐ | CLIチャットボット | 1日 |
| ⭐⭐ | Streamlitドキュメント Q&A | 3日 |
| ⭐⭐⭐ | 完全カスタマーサービスシステム | 1-2週間 |
            `}}]},{id:"chapter-8",number:8,title:{zh:"开发工具链",ja:"開発ツールチェーン"},subtitle:{zh:"AI 开发必备工具",ja:"AI開発必須ツール"},sections:[{id:"ch8-langchain",title:{zh:"8.1 LangChain 深入",ja:"8.1 LangChain詳解"},content:{zh:`
## LangChain：AI 应用的瑞士军刀

LangChain 是构建 LLM 应用的最流行框架。

---

## 🔧 核心组件

\`\`\`
┌─────────────────────────────────────────────────────────────────────────┐
│                      LangChain 架构                                      │
└─────────────────────────────────────────────────────────────────────────┘

                         LangChain
                             │
     ┌───────────┬───────────┼───────────┬───────────┐
     │           │           │           │           │
     ▼           ▼           ▼           ▼           ▼
   Models     Prompts     Chains      Memory      Tools
   (模型)     (提示词)     (链)       (记忆)      (工具)
\`\`\`

---

## 🔧 常用模式

\`\`\`python
from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# LCEL 链式调用
llm = ChatAnthropic(model="claude-sonnet-4-20250514")
prompt = ChatPromptTemplate.from_template("用一句话解释{topic}")
parser = StrOutputParser()

chain = prompt | llm | parser

result = chain.invoke({"topic": "量子计算"})
\`\`\`

---

## 💡 何时使用 LangChain

✅ 需要 RAG 功能
✅ 需要 Agent 能力
✅ 需要多模型切换
❌ 简单 API 调用
            `,ja:`
## LangChain：AIアプリケーションのスイスアーミーナイフ

LangChainはLLMアプリケーション構築で最も人気のあるフレームワーク。

---

## 💡 LangChainを使うべき時

✅ RAG機能が必要
✅ Agent能力が必要
✅ 複数モデル切り替えが必要
❌ シンプルなAPI呼び出し
            `}},{id:"ch8-llamaindex",title:{zh:"8.2 LlamaIndex 实战",ja:"8.2 LlamaIndex実践"},content:{zh:`
## LlamaIndex：专注 RAG 的框架

比 LangChain 更专注于数据索引和检索。

---

## 🔧 5 行代码构建 RAG

\`\`\`python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

# 1. 加载文档
documents = SimpleDirectoryReader("./data").load_data()

# 2. 创建索引
index = VectorStoreIndex.from_documents(documents)

# 3. 查询
query_engine = index.as_query_engine()
response = query_engine.query("这些文档讲了什么？")
print(response)
\`\`\`

---

## 📊 LangChain vs LlamaIndex

| 特性 | LangChain | LlamaIndex |
|------|-----------|------------|
| **定位** | 通用 LLM 框架 | RAG 专用 |
| **学习曲线** | 陡峭 | 平缓 |
| **灵活性** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ |
| **开箱即用** | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ |
            `,ja:`
## LlamaIndex：RAG専用フレームワーク

LangChainよりデータインデックスと検索に特化。

---

## 🔧 5行でRAG構築

\`\`\`python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

documents = SimpleDirectoryReader("./data").load_data()
index = VectorStoreIndex.from_documents(documents)
query_engine = index.as_query_engine()
response = query_engine.query("これらのドキュメントは何について？")
\`\`\`
            `}},{id:"ch8-huggingface",title:{zh:"8.3 HuggingFace 生态",ja:"8.3 HuggingFaceエコシステム"},content:{zh:`
## HuggingFace：AI 界的 GitHub

开源模型、数据集、Spaces 的一站式平台。

---

## 🔧 核心资源

| 资源 | 用途 | 示例 |
|------|------|------|
| **Models** | 开源模型 | Llama, Qwen, Mistral |
| **Datasets** | 数据集 | 训练、评估用 |
| **Spaces** | 在线 Demo | Gradio 应用 |
| **Transformers** | 推理库 | 本地运行模型 |

---

## 🔧 快速使用模型

\`\`\`python
from transformers import pipeline

# 情感分析
classifier = pipeline("sentiment-analysis")
result = classifier("I love this product!")
print(result)

# 文本生成
generator = pipeline("text-generation", model="gpt2")
result = generator("Once upon a time", max_length=50)

# 翻译
translator = pipeline("translation", model="Helsinki-NLP/opus-mt-en-zh")
result = translator("Hello, how are you?")
\`\`\`

---

## 💡 推荐资源

- 🔥 **LLM 排行榜**：Open LLM Leaderboard
- 📚 **中文模型**：Qwen, ChatGLM, Yi
- 🚀 **部署工具**：TGI, vLLM
            `,ja:`
## HuggingFace：AI界のGitHub

オープンソースモデル、データセット、Spacesのワンストッププラットフォーム。

---

## 💡 推奨リソース

- 🔥 **LLMランキング**：Open LLM Leaderboard
- 📚 **日本語モデル**：rinna, ELYZA
- 🚀 **デプロイツール**：TGI, vLLM
            `}},{id:"ch8-summary",title:{zh:"8.4 本章小结",ja:"8.4 この章のまとめ"},content:{zh:`
## 开发工具链选择指南

---

## 🎯 工具选择建议

| 需求 | 推荐工具 |
|------|----------|
| 快速 RAG 原型 | LlamaIndex |
| 复杂 Agent 系统 | LangChain |
| 开源模型探索 | HuggingFace |
| 生产部署 | vLLM + FastAPI |
| 本地开发 | Ollama + Open WebUI |

---

## 💡 最终建议

> 🎯 **工具只是手段，解决问题才是目的**。
> 不要过度追求技术栈，选择能快速交付价值的方案。

---

## 📚 全书总结

恭喜你完成了 AI 进阶实战的学习！

你已掌握：
1. **大模型技术** —— Transformer、微调、量化
2. **提示词工程** —— 结构化、高级技巧、JSON输出
3. **AI Agent** —— MCP、Claude Code、A2A
4. **RAG 技术** —— 向量检索、GraphRAG
5. **多模态 AI** —— 图像、语音、视频
6. **安全与伦理** —— 幻觉、偏见、隐私
7. **实战项目** —— 从零构建应用
8. **工具链** —— LangChain、LlamaIndex、HuggingFace

**AI 技术日新月异，保持学习，保持好奇！**

*"真正的智慧不是知道多少，而是知道如何学习。"*
            `,ja:`
## 開発ツールチェーン選択ガイド

---

## 🎯 ツール選択アドバイス

| ニーズ | 推奨ツール |
|--------|------------|
| 迅速なRAGプロトタイプ | LlamaIndex |
| 複雑なAgentシステム | LangChain |
| オープンソースモデル探索 | HuggingFace |
| 本番デプロイ | vLLM + FastAPI |
| ローカル開発 | Ollama + Open WebUI |

---

## 📚 全書まとめ

AI実践上級編の学習完了おめでとうございます！

習得したこと：
1. **大規模モデル技術** —— Transformer、ファインチューニング、量子化
2. **プロンプトエンジニアリング** —— 構造化、高度なテクニック
3. **AI Agent** —— MCP、Claude Code、A2A
4. **RAG技術** —— ベクトル検索、GraphRAG
5. **マルチモーダルAI** —— 画像、音声、動画
6. **セキュリティと倫理** —— ハルシネーション、バイアス
7. **実践プロジェクト** —— ゼロからアプリ構築
8. **ツールチェーン** —— LangChain、LlamaIndex、HuggingFace

**AI技術は日々進化、学び続け、好奇心を持ち続けましょう！**
            `}}]},{id:"chapter-9",number:9,title:{zh:"Google AI 生态系统",ja:"Google AIエコシステム"},subtitle:{zh:"Gemini、Vertex AI 与云端 AI 服务",ja:"Gemini、Vertex AIとクラウドAIサービス"},sections:[{id:"ch9-gemini",title:{zh:"9.1 Gemini 大模型家族",ja:"9.1 Gemini大規模モデルファミリー"},content:{zh:`
## Google Gemini：多模态原生大模型

Gemini 是 Google 最新一代大模型，原生支持多模态，在多项基准测试中超越 GPT-4。

---

## 📊 Gemini 模型对比（2025.11 最新）

| 模型 | 发布时间 | 特点 | 适用场景 |
|------|----------|------|----------|
| **Gemini 3 Pro** 🆕 | 2025.11.18 | 最新旗舰，Google 最强模型 | 复杂推理、研究、代码 |
| **Gemini 3 Deep Think** 🆕 | 2025.11.18 | 深度思考模式 | 数学、科学、长程推理 |
| **Gemini 2.5 Pro** | 2025.03 | 上一代旗舰 | 通用任务、多模态 |
| **Gemini 2.5 Flash** | 2025.05 | 快速高效 | 实时应用、高并发 |
| **Gemini Nano** | 设备端 | 端侧部署 | 移动端、离线场景 |

> 💡 **2025年11月更新**：Gemini 3 系列已取代 2.5 系列成为主力模型

---

## 🔧 快速开始

### 安装 SDK

\`\`\`bash
pip install google-generativeai
\`\`\`

### 基础调用

\`\`\`python
import google.generativeai as genai

# 配置 API Key
genai.configure(api_key="YOUR_API_KEY")

# 创建模型
model = genai.GenerativeModel("gemini-3-pro-preview")

# 文本生成
response = model.generate_content("解释量子计算的基本原理")
print(response.text)
\`\`\`

---

## 🖼️ 多模态能力

### 图像理解

\`\`\`python
import PIL.Image

# 加载图片
image = PIL.Image.open("diagram.png")

# 图文混合输入
response = model.generate_content([
    "请详细分析这张架构图，解释各组件之间的关系：",
    image
])
print(response.text)
\`\`\`

### 视频分析

\`\`\`python
import time

# 上传视频文件
video_file = genai.upload_file("presentation.mp4")

# 等待处理完成
while video_file.state.name == "PROCESSING":
    time.sleep(5)
    video_file = genai.get_file(video_file.name)

# 分析视频内容
response = model.generate_content([
    video_file,
    "总结这个视频的主要内容，列出关键要点"
])
print(response.text)
\`\`\`

---

## 💬 多轮对话

\`\`\`python
# 创建对话
chat = model.start_chat(history=[])

# 第一轮
response = chat.send_message("我想学习机器学习，应该从哪里开始？")
print(response.text)

# 第二轮（自动携带上下文）
response = chat.send_message("Python 基础需要学到什么程度？")
print(response.text)

# 查看对话历史
for message in chat.history:
    print(f"{message.role}: {message.parts[0].text[:100]}...")
\`\`\`

---

## 🔒 安全设置

\`\`\`python
from google.generativeai.types import HarmCategory, HarmBlockThreshold

# 配置安全过滤
safety_settings = {
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
}

response = model.generate_content(
    "你的问题",
    safety_settings=safety_settings
)
\`\`\`

---

## 📊 结构化输出 (JSON Mode)

\`\`\`python
import json

# 方法1：通过 Prompt 指定
response = model.generate_content(
    """分析以下文本的情感，返回 JSON 格式：
    {"sentiment": "positive/negative/neutral", "confidence": 0.0-1.0, "keywords": [...]}

    文本：这款产品真的太棒了，用户体验非常流畅！
    """,
    generation_config=genai.types.GenerationConfig(
        response_mime_type="application/json"
    )
)

result = json.loads(response.text)
print(result)

# 方法2：使用 Schema（更严格）
from google.generativeai.types import content_types

schema = content_types.Schema(
    type=content_types.Type.OBJECT,
    properties={
        "sentiment": content_types.Schema(type=content_types.Type.STRING),
        "confidence": content_types.Schema(type=content_types.Type.NUMBER),
        "keywords": content_types.Schema(
            type=content_types.Type.ARRAY,
            items=content_types.Schema(type=content_types.Type.STRING)
        )
    },
    required=["sentiment", "confidence"]
)

response = model.generate_content(
    "分析：这个服务太慢了，等了半天",
    generation_config=genai.types.GenerationConfig(
        response_mime_type="application/json",
        response_schema=schema
    )
)
\`\`\`

---

## 🛠️ Function Calling

\`\`\`python
# 定义工具函数
def get_weather(location: str, unit: str = "celsius") -> dict:
    """获取指定城市的天气信息"""
    # 模拟 API 调用
    return {
        "location": location,
        "temperature": 22,
        "unit": unit,
        "condition": "晴天"
    }

def search_flights(origin: str, destination: str, date: str) -> list:
    """搜索航班信息"""
    return [
        {"flight": "CA123", "price": 1500, "departure": "08:00"},
        {"flight": "MU456", "price": 1200, "departure": "14:30"}
    ]

# 创建带工具的模型
model = genai.GenerativeModel(
    "gemini-3-pro-preview",
    tools=[get_weather, search_flights]
)

# 自动调用工具
chat = model.start_chat(enable_automatic_function_calling=True)
response = chat.send_message("北京今天天气怎么样？")
print(response.text)

response = chat.send_message("帮我查一下明天从北京到上海的航班")
print(response.text)
\`\`\`

---

## ⚡ 流式输出

\`\`\`python
# 流式生成
response = model.generate_content(
    "写一篇关于人工智能发展历史的文章",
    stream=True
)

for chunk in response:
    print(chunk.text, end="", flush=True)
\`\`\`

---

## 📈 Token 计数与成本估算

\`\`\`python
# 计算 Token 数量
model = genai.GenerativeModel("gemini-3-pro-preview")

# 文本 Token
text = "这是一段测试文本，用于计算 Token 数量。"
token_count = model.count_tokens(text)
print(f"Token 数量: {token_count.total_tokens}")

# 图片 Token（每张图约 258 tokens）
image = PIL.Image.open("image.png")
token_count = model.count_tokens([text, image])
print(f"图文混合 Token: {token_count.total_tokens}")
\`\`\`

---

## 💰 定价参考 (2024)

| 模型 | 输入 (每百万 Token) | 输出 (每百万 Token) |
|------|---------------------|---------------------|
| Gemini 2.0 Flash | $0.10 | $0.40 |
| Gemini 1.5 Pro | $1.25 - $2.50 | $5.00 - $10.00 |
| Gemini 1.5 Flash | $0.075 | $0.30 |

*价格可能变动，以官网为准*
            `,ja:`
## Google Gemini：マルチモーダルネイティブ大規模モデル

GeminiはGoogleの最新世代大規模モデルで、ネイティブでマルチモーダルをサポート。

---

## 📊 Geminiモデル比較（2025年11月最新）

| モデル | リリース日 | 特徴 | 用途 |
|--------|------------|------|------|
| **Gemini 3 Pro** 🆕 | 2025.11.18 | 最新フラッグシップ、Google最強 | 複雑な推論、研究、コード |
| **Gemini 3 Deep Think** 🆕 | 2025.11.18 | 深い思考モード | 数学、科学、長期推論 |
| **Gemini 2.5 Pro** | 2025.03 | 前世代フラッグシップ | 汎用、マルチモーダル |
| **Gemini 2.5 Flash** | 2025.05 | 高速高効率 | リアルタイムアプリ |
| **Gemini Nano** | オンデバイス | 端末デプロイ | モバイル、オフライン |

> 💡 **2025年11月更新**：Gemini 3シリーズが2.5シリーズに代わり主力モデルに

---

## 🔧 クイックスタート

\`\`\`python
import google.generativeai as genai

genai.configure(api_key="YOUR_API_KEY")
model = genai.GenerativeModel("gemini-3-pro-preview")

response = model.generate_content("量子コンピューティングの基本原理を説明して")
print(response.text)
\`\`\`

---

## 🖼️ マルチモーダル機能

\`\`\`python
import PIL.Image

image = PIL.Image.open("diagram.png")
response = model.generate_content([
    "このアーキテクチャ図を分析してください：",
    image
])
\`\`\`

---

## 🛠️ Function Calling

\`\`\`python
def get_weather(location: str) -> dict:
    return {"location": location, "temperature": 22}

model = genai.GenerativeModel(
    "gemini-3-pro-preview",
    tools=[get_weather]
)

chat = model.start_chat(enable_automatic_function_calling=True)
response = chat.send_message("東京の天気は？")
\`\`\`
            `}},{id:"ch9-vertex-ai",title:{zh:"9.2 Vertex AI 企业级平台",ja:"9.2 Vertex AIエンタープライズプラットフォーム"},content:{zh:`
## Vertex AI：Google Cloud 的 AI 平台

Vertex AI 是 Google Cloud 的统一 AI 平台，提供从模型训练到部署的全流程服务。

---

## 🏗️ Vertex AI 核心组件

\`\`\`
┌─────────────────────────────────────────────────────────────┐
│                      Vertex AI Platform                      │
├─────────────────────────────────────────────────────────────┤
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐          │
│  │   Gemini    │  │  Model      │  │  Vertex AI  │          │
│  │   API       │  │  Garden     │  │  Studio     │          │
│  └─────────────┘  └─────────────┘  └─────────────┘          │
├─────────────────────────────────────────────────────────────┤
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐          │
│  │  AutoML     │  │  Custom     │  │  Pipelines  │          │
│  │             │  │  Training   │  │             │          │
│  └─────────────┘  └─────────────┘  └─────────────┘          │
├─────────────────────────────────────────────────────────────┤
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐          │
│  │  Feature    │  │  Vector     │  │  Model      │          │
│  │  Store      │  │  Search     │  │  Registry   │          │
│  └─────────────┘  └─────────────┘  └─────────────┘          │
└─────────────────────────────────────────────────────────────┘
\`\`\`

---

## 🔧 环境配置

\`\`\`bash
# 安装 SDK
pip install google-cloud-aiplatform

# 认证
gcloud auth application-default login
gcloud config set project YOUR_PROJECT_ID
\`\`\`

---

## 🤖 使用 Gemini on Vertex AI

\`\`\`python
import vertexai
from vertexai.generative_models import GenerativeModel, Part

# 初始化
vertexai.init(project="your-project-id", location="us-central1")

# 创建模型
model = GenerativeModel("gemini-3-pro-preview")

# 文本生成
response = model.generate_content("解释机器学习和深度学习的区别")
print(response.text)

# 多模态：图片 + 文本
image = Part.from_uri(
    "gs://your-bucket/image.jpg",
    mime_type="image/jpeg"
)
response = model.generate_content([image, "描述这张图片"])
print(response.text)
\`\`\`

---

## 🔍 Vertex AI Search (企业搜索)

\`\`\`python
from google.cloud import discoveryengine_v1 as discoveryengine

# 创建搜索客户端
client = discoveryengine.SearchServiceClient()

# 执行搜索
request = discoveryengine.SearchRequest(
    serving_config=f"projects/{project}/locations/global/collections/default_collection/dataStores/{data_store}/servingConfigs/default_search",
    query="如何配置 Kubernetes 集群",
    page_size=10,
)

response = client.search(request)

for result in response.results:
    print(f"文档: {result.document.name}")
    print(f"摘要: {result.document.derived_struct_data.get('snippets', [])}")
\`\`\`

---

## 📊 Vertex AI Vector Search

用于构建大规模向量检索系统（原 Matching Engine）。

\`\`\`python
from google.cloud import aiplatform

# 初始化
aiplatform.init(project="your-project", location="us-central1")

# 创建向量索引
my_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(
    display_name="product-embeddings",
    contents_delta_uri="gs://your-bucket/embeddings/",
    dimensions=768,
    approximate_neighbors_count=150,
)

# 创建端点
my_index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(
    display_name="product-search-endpoint",
    public_endpoint_enabled=True
)

# 部署索引
my_index_endpoint.deploy_index(
    index=my_index,
    deployed_index_id="deployed_product_index"
)

# 执行向量搜索
response = my_index_endpoint.find_neighbors(
    deployed_index_id="deployed_product_index",
    queries=[[0.1, 0.2, ...]],  # 查询向量
    num_neighbors=10
)
\`\`\`

---

## 🏭 Model Garden：预训练模型库

\`\`\`python
# 部署开源模型（如 Llama 2）
from vertexai.preview.language_models import TextGenerationModel

# 浏览可用模型
# console.cloud.google.com/vertex-ai/model-garden

# 部署到端点
model = TextGenerationModel.from_pretrained("text-bison")
response = model.predict("写一首关于春天的诗")
print(response.text)
\`\`\`

---

## 🔄 Vertex AI Pipelines

\`\`\`python
from kfp import dsl
from kfp.v2 import compiler
from google.cloud import aiplatform

@dsl.component
def preprocess_data(input_path: str, output_path: str):
    # 数据预处理逻辑
    pass

@dsl.component
def train_model(data_path: str, model_path: str):
    # 模型训练逻辑
    pass

@dsl.component
def evaluate_model(model_path: str) -> float:
    # 模型评估
    return 0.95

@dsl.pipeline(name="ml-training-pipeline")
def ml_pipeline(input_data: str):
    preprocess_task = preprocess_data(input_path=input_data, output_path="gs://...")
    train_task = train_model(data_path=preprocess_task.output, model_path="gs://...")
    evaluate_task = evaluate_model(model_path=train_task.output)

# 编译并运行
compiler.Compiler().compile(ml_pipeline, "pipeline.json")

aiplatform.init(project="your-project", location="us-central1")
job = aiplatform.PipelineJob(
    display_name="training-job",
    template_path="pipeline.json",
    parameter_values={"input_data": "gs://your-bucket/data"}
)
job.run()
\`\`\`

---

## 💰 成本优化建议

| 策略 | 说明 |
|------|------|
| 使用 Flash 模型 | 比 Pro 便宜 5-10x |
| 批量处理 | 减少 API 调用次数 |
| 缓存响应 | 相同查询复用结果 |
| 区域选择 | 不同区域价格不同 |
| 承诺使用折扣 | 1年/3年承诺享折扣 |
            `,ja:`
## Vertex AI：Google CloudのAIプラットフォーム

Vertex AIはGoogle Cloudの統合AIプラットフォームで、モデルトレーニングからデプロイまでの全プロセスをサポート。

---

## 🏗️ Vertex AIコアコンポーネント

- **Gemini API** - 最新LLMアクセス
- **Model Garden** - 事前学習モデルライブラリ
- **Vertex AI Studio** - ノーコード実験環境
- **Vector Search** - 大規模ベクトル検索
- **Pipelines** - MLワークフロー自動化

---

## 🔧 環境設定

\`\`\`bash
pip install google-cloud-aiplatform
gcloud auth application-default login
\`\`\`

---

## 🤖 Gemini on Vertex AI

\`\`\`python
import vertexai
from vertexai.generative_models import GenerativeModel

vertexai.init(project="your-project-id", location="us-central1")
model = GenerativeModel("gemini-3-pro-preview")

response = model.generate_content("機械学習と深層学習の違いを説明して")
print(response.text)
\`\`\`

---

## 📊 Vector Search

\`\`\`python
# ベクトルインデックス作成
my_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(
    display_name="product-embeddings",
    dimensions=768
)

# 検索実行
response = my_index_endpoint.find_neighbors(
    queries=[[0.1, 0.2, ...]],
    num_neighbors=10
)
\`\`\`
            `}},{id:"ch9-ai-studio",title:{zh:"9.3 Google AI Studio 快速原型",ja:"9.3 Google AI Studio クイックプロトタイプ"},content:{zh:`
## Google AI Studio：免费的 Gemini 实验平台

AI Studio 是 Google 提供的免费在线工具，用于快速测试和原型开发。

---

## 🌐 访问地址

> **https://aistudio.google.com**

---

## 🎯 核心功能

### 1. 即时对话测试

\`\`\`
┌─────────────────────────────────────────┐
│  Chat Prompt                             │
├─────────────────────────────────────────┤
│  System: 你是一个专业的技术顾问          │
│                                          │
│  User: 如何选择合适的数据库？            │
│                                          │
│  Model: [Gemini 响应]                    │
├─────────────────────────────────────────┤
│  [Temperature] [Top-P] [Max Tokens]     │
└─────────────────────────────────────────┘
\`\`\`

### 2. 结构化 Prompt 设计

\`\`\`
┌─────────────────────────────────────────┐
│  Freeform Prompt                         │
├─────────────────────────────────────────┤
│  任务描述：                              │
│  根据用户问题，分析并给出建议            │
│                                          │
│  输入示例：                              │
│  Q: 网站加载慢怎么办？                   │
│  A: 1. 优化图片 2. 启用CDN 3. 代码压缩  │
│                                          │
│  测试输入：                              │
│  Q: API 响应时间太长                     │
└─────────────────────────────────────────┘
\`\`\`

### 3. 多模态测试

- 上传图片进行视觉问答
- 上传视频进行内容分析
- 音频转写和理解

---

## 🔧 导出代码

AI Studio 可以一键导出为多种语言：

### Python

\`\`\`python
import google.generativeai as genai

genai.configure(api_key="YOUR_API_KEY")

generation_config = {
    "temperature": 0.7,
    "top_p": 0.95,
    "top_k": 40,
    "max_output_tokens": 8192,
}

model = genai.GenerativeModel(
    model_name="gemini-3-pro-preview",
    generation_config=generation_config,
    system_instruction="你是一个专业的技术顾问"
)

chat = model.start_chat(history=[])
response = chat.send_message("如何优化数据库查询性能？")
print(response.text)
\`\`\`

### JavaScript/Node.js

\`\`\`javascript
const { GoogleGenerativeAI } = require("@google/generative-ai");

const genAI = new GoogleGenerativeAI("YOUR_API_KEY");

async function run() {
    const model = genAI.getGenerativeModel({
        model: "gemini-3-pro-preview",
        systemInstruction: "你是一个专业的技术顾问"
    });

    const result = await model.generateContent("如何优化数据库？");
    console.log(result.response.text());
}

run();
\`\`\`

### cURL

\`\`\`bash
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-3-pro-preview:generateContent?key=YOUR_API_KEY" \\
  -H 'Content-Type: application/json' \\
  -d '{
    "contents": [{
      "parts": [{"text": "解释 RESTful API 设计原则"}]
    }],
    "generationConfig": {
      "temperature": 0.7,
      "maxOutputTokens": 2048
    }
  }'
\`\`\`

---

## 📊 Prompt Gallery（提示词库）

AI Studio 提供丰富的 Prompt 模板：

| 类别 | 示例 |
|------|------|
| 写作 | 博客生成、邮件撰写 |
| 代码 | 代码解释、Bug 修复 |
| 分析 | 数据分析、报告生成 |
| 创意 | 故事创作、广告文案 |
| 教育 | 知识问答、概念解释 |

---

## 🆓 免费额度

| 资源 | 免费额度 |
|------|----------|
| Gemini 2.0 Flash | 15 RPM / 100万 TPM |
| Gemini 1.5 Pro | 2 RPM / 32K TPM |
| 图片处理 | 包含在请求中 |
| 视频处理 | 包含在请求中 |

*RPM = 每分钟请求数，TPM = 每分钟 Token 数*

---

## 💡 最佳实践

1. **先在 AI Studio 测试** —— 验证 Prompt 效果
2. **使用 System Instruction** —— 定义角色和行为
3. **调整 Temperature** —— 创意任务调高，精确任务调低
4. **导出后再优化** —— 添加错误处理和重试逻辑
            `,ja:`
## Google AI Studio：無料のGemini実験プラットフォーム

AI StudioはGoogleが提供する無料オンラインツールで、迅速なテストとプロトタイプ開発に使用。

---

## 🌐 アクセスURL

> **https://aistudio.google.com**

---

## 🎯 コア機能

1. **即時対話テスト** - リアルタイムでGeminiと対話
2. **構造化プロンプト設計** - 例題付きプロンプト作成
3. **マルチモーダルテスト** - 画像・動画・音声対応
4. **コードエクスポート** - Python/JavaScript/cURL

---

## 🔧 エクスポートコード例

\`\`\`python
import google.generativeai as genai

genai.configure(api_key="YOUR_API_KEY")

model = genai.GenerativeModel(
    model_name="gemini-3-pro-preview",
    system_instruction="あなたは技術コンサルタントです"
)

response = model.generate_content("データベースの最適化方法は？")
print(response.text)
\`\`\`

---

## 🆓 無料枠

| リソース | 無料枠 |
|----------|--------|
| Gemini 2.0 Flash | 15 RPM / 100万 TPM |
| Gemini 1.5 Pro | 2 RPM / 32K TPM |
            `}},{id:"ch9-gemma",title:{zh:"9.4 Gemma 开源模型",ja:"9.4 Gemmaオープンソースモデル"},content:{zh:`
## Gemma：Google 的开源大模型

Gemma 是 Google 开源的轻量级大模型系列，与 Gemini 共享技术架构。

---

## 📊 Gemma 模型对比

| 模型 | 参数量 | 特点 | HuggingFace |
|------|--------|------|-------------|
| **Gemma 2 27B** | 27B | 性能最强 | google/gemma-2-27b |
| **Gemma 2 9B** | 9B | 平衡之选 | google/gemma-2-9b |
| **Gemma 2 2B** | 2B | 轻量快速 | google/gemma-2-2b |
| **CodeGemma 7B** | 7B | 代码专精 | google/codegemma-7b |
| **PaliGemma** | 3B | 视觉语言 | google/paligemma-3b |

---

## 🔧 本地部署（Ollama）

\`\`\`bash
# 安装 Ollama
curl -fsSL https://ollama.com/install.sh | sh

# 下载 Gemma
ollama pull gemma2:9b

# 运行对话
ollama run gemma2:9b

# API 调用
curl http://localhost:11434/api/generate -d '{
  "model": "gemma2:9b",
  "prompt": "解释什么是 Transformer 架构",
  "stream": false
}'
\`\`\`

---

## 🐍 Python 调用（HuggingFace）

\`\`\`python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# 加载模型
model_id = "google/gemma-2-9b-it"  # it = instruction-tuned
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    torch_dtype=torch.bfloat16
)

# 构建对话格式
messages = [
    {"role": "user", "content": "写一个 Python 快速排序算法"}
]
prompt = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

# 生成
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
outputs = model.generate(
    **inputs,
    max_new_tokens=512,
    temperature=0.7,
    do_sample=True
)

response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
\`\`\`

---

## ⚡ 量化部署

\`\`\`python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 4-bit 量化配置
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_quant_type="nf4"
)

model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-2-9b-it",
    quantization_config=quantization_config,
    device_map="auto"
)

# 9B 模型量化后约需 6GB 显存
\`\`\`

---

## 🖼️ PaliGemma（视觉语言模型）

\`\`\`python
from transformers import AutoProcessor, PaliGemmaForConditionalGeneration
from PIL import Image

# 加载模型
model_id = "google/paligemma-3b-pt-224"
processor = AutoProcessor.from_pretrained(model_id)
model = PaliGemmaForConditionalGeneration.from_pretrained(model_id)

# 图像理解
image = Image.open("photo.jpg")
prompt = "describe this image in detail"

inputs = processor(text=prompt, images=image, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=256)
description = processor.decode(outputs[0], skip_special_tokens=True)
print(description)
\`\`\`

---

## 💻 CodeGemma（代码模型）

\`\`\`python
from transformers import GemmaTokenizer, AutoModelForCausalLM

model_id = "google/codegemma-7b-it"
tokenizer = GemmaTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto")

# 代码补全
prompt = '''def fibonacci(n):
    """Calculate the nth Fibonacci number."""
'''

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=200)
code = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(code)
\`\`\`

---

## 🆚 Gemma vs Llama 对比

| 特性 | Gemma 2 | Llama 3 |
|------|---------|---------|
| 开发商 | Google | Meta |
| 许可证 | Gemma ToU | Llama 3 Community |
| 商用 | ✅ (有限制) | ✅ (有限制) |
| 最大参数 | 27B | 70B |
| 中文能力 | 中等 | 较好 |
| 代码能力 | 强 (CodeGemma) | 强 |
| 视觉版本 | PaliGemma | Llama 3.2 Vision |

---

## 📦 部署方案对比

| 方案 | 优点 | 缺点 |
|------|------|------|
| Ollama | 简单易用 | 功能有限 |
| vLLM | 高性能 | 配置复杂 |
| TGI | 企业级 | 需要更多资源 |
| llama.cpp | 低资源 | CPU 为主 |
            `,ja:`
## Gemma：Googleのオープンソース大規模モデル

GemmaはGoogleがオープンソース化した軽量大規模モデルシリーズで、Geminiと技術アーキテクチャを共有。

---

## 📊 Gemmaモデル比較

| モデル | パラメータ | 特徴 |
|--------|------------|------|
| **Gemma 2 27B** | 27B | 最高性能 |
| **Gemma 2 9B** | 9B | バランス型 |
| **Gemma 2 2B** | 2B | 軽量高速 |
| **CodeGemma 7B** | 7B | コード特化 |
| **PaliGemma** | 3B | ビジョン言語 |

---

## 🔧 ローカルデプロイ（Ollama）

\`\`\`bash
ollama pull gemma2:9b
ollama run gemma2:9b
\`\`\`

---

## 🐍 Python呼び出し

\`\`\`python
from transformers import AutoTokenizer, AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-2-9b-it",
    device_map="auto",
    torch_dtype=torch.bfloat16
)

messages = [{"role": "user", "content": "Pythonのクイックソートを書いて"}]
# ...
\`\`\`

---

## 🆚 Gemma vs Llama

| 特性 | Gemma 2 | Llama 3 |
|------|---------|---------|
| 開発元 | Google | Meta |
| ライセンス | Gemma ToU | Llama 3 Community |
| 最大パラメータ | 27B | 70B |
            `}},{id:"ch9-notebooklm",title:{zh:"9.5 NotebookLM 研究助手",ja:"9.5 NotebookLM リサーチアシスタント"},content:{zh:`
## NotebookLM：AI 驱动的研究助手

NotebookLM 是 Google 推出的 AI 笔记工具，可以基于你上传的文档进行问答和总结。

---

## 🌐 访问地址

> **https://notebooklm.google.com**

---

## 🎯 核心功能

### 1. 文档上传与理解

支持的格式：
- PDF 文档
- Google Docs
- 网页链接
- 文本文件
- YouTube 视频

\`\`\`
┌─────────────────────────────────────────┐
│  Sources（来源）                         │
├─────────────────────────────────────────┤
│  📄 research_paper.pdf                   │
│  📄 meeting_notes.docx                   │
│  🔗 https://arxiv.org/...               │
│  🎬 YouTube: AI 技术讲座                 │
│                                          │
│  [+ Add Source]                          │
└─────────────────────────────────────────┘
\`\`\`

### 2. 智能问答

\`\`\`
用户: 这篇论文的主要贡献是什么？

NotebookLM: 根据论文内容，主要贡献包括：
1. 提出了新的注意力机制... [引用: 第3页]
2. 在多个基准测试上取得SOTA... [引用: 第7页]
3. ...
\`\`\`

### 3. 自动生成笔记

- **文档摘要** —— 一键生成概要
- **关键问题** —— 提取核心问题
- **FAQ 生成** —— 自动生成问答对
- **时间线** —— 提取时间节点

### 4. 🎙️ Audio Overview（音频概述）

**最具特色的功能！** 将文档转换为两人对话的播客形式。

\`\`\`
[生成音频概述]
        ↓
两位 AI 主持人讨论你的文档内容
        ↓
~10分钟的播客式音频
        ↓
可下载 MP3 离线收听
\`\`\`

---

## 💡 使用场景

| 场景 | 用法 |
|------|------|
| **论文研究** | 上传 PDF，快速理解核心内容 |
| **会议纪要** | 上传录音转写，生成摘要 |
| **学习笔记** | 上传教材，生成问答卡片 |
| **竞品分析** | 上传多篇报告，交叉对比 |
| **播客创作** | 将长文档转为音频内容 |

---

## 🔧 高级技巧

### 1. 多文档交叉引用

\`\`\`
上传多篇论文后提问：
"比较这三篇论文在方法论上的异同"
→ NotebookLM 会综合分析并标注来源
\`\`\`

### 2. 引用追溯

点击回答中的引用标注，可直接跳转到原文位置。

### 3. 笔记导出

生成的笔记可以导出为 Google Doc，便于编辑和分享。

---

## ⚠️ 限制

| 限制 | 说明 |
|------|------|
| 来源数量 | 最多 50 个来源/笔记本 |
| 文件大小 | 每个文件最大 200MB |
| 音频长度 | Audio Overview 约 10 分钟 |
| 语言 | 目前英文效果最佳 |
| 隐私 | 文档内容用于 AI 处理 |

---

## 🆚 与其他工具对比

| 工具 | 特点 | 适用场景 |
|------|------|----------|
| **NotebookLM** | 深度文档理解 | 研究、学习 |
| **ChatGPT + 文件** | 通用对话 | 快速问答 |
| **Claude Projects** | 代码 + 文档 | 开发项目 |
| **Perplexity** | 网络搜索 | 信息检索 |
            `,ja:`
## NotebookLM：AI駆動のリサーチアシスタント

NotebookLMはGoogleが提供するAIノートツールで、アップロードしたドキュメントに基づいてQ&Aと要約が可能。

---

## 🌐 アクセスURL

> **https://notebooklm.google.com**

---

## 🎯 コア機能

### 1. ドキュメントアップロード

対応フォーマット：PDF、Google Docs、Webリンク、テキスト、YouTube動画

### 2. スマートQ&A

ドキュメントの内容に基づいて質問に回答し、引用元を明示

### 3. 自動ノート生成

- ドキュメント要約
- 重要な質問抽出
- FAQ自動生成

### 4. 🎙️ Audio Overview

**最も特徴的な機能！** ドキュメントを2人の対話形式のポッドキャストに変換

---

## 💡 使用シーン

| シーン | 用途 |
|--------|------|
| **論文研究** | PDFをアップロードし、核心内容を素早く理解 |
| **会議メモ** | 録音の文字起こしをアップロードし、要約を生成 |
| **学習ノート** | 教材をアップロードし、Q&Aカードを生成 |

---

## ⚠️ 制限

| 制限 | 説明 |
|------|------|
| ソース数 | 最大50ソース/ノートブック |
| ファイルサイズ | 各ファイル最大200MB |
            `}},{id:"ch9-cloud-ai",title:{zh:"9.6 Google Cloud AI 服务",ja:"9.6 Google Cloud AIサービス"},content:{zh:`
## Google Cloud AI 服务矩阵

Google Cloud 提供丰富的预训练 AI 服务，可直接调用 API。

---

## 🗣️ Speech-to-Text（语音转文字）

\`\`\`python
from google.cloud import speech

client = speech.SpeechClient()

# 从文件读取音频
with open("audio.wav", "rb") as f:
    audio = speech.RecognitionAudio(content=f.read())

config = speech.RecognitionConfig(
    encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
    sample_rate_hertz=16000,
    language_code="zh-CN",  # 中文
    enable_automatic_punctuation=True,
    enable_word_time_offsets=True,  # 获取单词时间戳
)

response = client.recognize(config=config, audio=audio)

for result in response.results:
    print(f"转写: {result.alternatives[0].transcript}")
    print(f"置信度: {result.alternatives[0].confidence}")
\`\`\`

### 支持的语言

- 中文（普通话、粤语）
- 日语、韩语
- 英语（多种口音）
- 125+ 种语言

---

## 🔊 Text-to-Speech（文字转语音）

\`\`\`python
from google.cloud import texttospeech

client = texttospeech.TextToSpeechClient()

# 设置文本
synthesis_input = texttospeech.SynthesisInput(
    text="欢迎使用 Google Cloud 语音合成服务"
)

# 选择声音
voice = texttospeech.VoiceSelectionParams(
    language_code="zh-CN",
    name="zh-CN-Wavenet-A",  # Wavenet 更自然
    ssml_gender=texttospeech.SsmlVoiceGender.FEMALE
)

# 音频配置
audio_config = texttospeech.AudioConfig(
    audio_encoding=texttospeech.AudioEncoding.MP3,
    speaking_rate=1.0,  # 语速
    pitch=0.0  # 音调
)

response = client.synthesize_speech(
    input=synthesis_input,
    voice=voice,
    audio_config=audio_config
)

with open("output.mp3", "wb") as f:
    f.write(response.audio_content)
\`\`\`

### 声音类型

| 类型 | 特点 | 价格 |
|------|------|------|
| Standard | 基础质量 | 最低 |
| Wavenet | 高质量 | 中等 |
| Neural2 | 最自然 | 最高 |
| Studio | 专业配音 | 按需 |

---

## 👁️ Vision AI（视觉 AI）

\`\`\`python
from google.cloud import vision

client = vision.ImageAnnotatorClient()

# 从文件读取图片
with open("image.jpg", "rb") as f:
    content = f.read()

image = vision.Image(content=content)

# 多种分析功能
# 1. 标签检测
labels = client.label_detection(image=image).label_annotations
for label in labels:
    print(f"标签: {label.description}, 置信度: {label.score:.2f}")

# 2. 文字识别 (OCR)
texts = client.text_detection(image=image).text_annotations
if texts:
    print(f"识别文字: {texts[0].description}")

# 3. 人脸检测
faces = client.face_detection(image=image).face_annotations
for face in faces:
    print(f"表情: 喜悦={face.joy_likelihood.name}")

# 4. 物体定位
objects = client.object_localization(image=image).localized_object_annotations
for obj in objects:
    print(f"物体: {obj.name}, 位置: {obj.bounding_poly}")

# 5. 安全检测
safe = client.safe_search_detection(image=image).safe_search_annotation
print(f"成人内容: {safe.adult.name}")
\`\`\`

---

## 🌐 Translation（翻译）

\`\`\`python
from google.cloud import translate_v2 as translate

client = translate.Client()

# 简单翻译
result = client.translate(
    "Hello, how are you?",
    target_language="zh-CN"
)
print(result["translatedText"])  # 你好，你好吗？

# 检测语言
detection = client.detect_language("こんにちは")
print(f"语言: {detection['language']}, 置信度: {detection['confidence']}")

# 批量翻译
texts = ["Hello", "World", "AI is amazing"]
results = client.translate(texts, target_language="ja")
for r in results:
    print(f"{r['input']} -> {r['translatedText']}")
\`\`\`

---

## 📊 Natural Language（自然语言处理）

\`\`\`python
from google.cloud import language_v1

client = language_v1.LanguageServiceClient()

text = "谷歌是一家总部位于加利福尼亚的科技公司，由拉里·佩奇和谢尔盖·布林创立。"
document = language_v1.Document(
    content=text,
    type_=language_v1.Document.Type.PLAIN_TEXT,
    language="zh"
)

# 情感分析
sentiment = client.analyze_sentiment(document=document).document_sentiment
print(f"情感: {sentiment.score:.2f} (正面=1, 负面=-1)")

# 实体识别
entities = client.analyze_entities(document=document).entities
for entity in entities:
    print(f"实体: {entity.name}, 类型: {entity.type_.name}")

# 语法分析
syntax = client.analyze_syntax(document=document)
for token in syntax.tokens:
    print(f"词: {token.text.content}, 词性: {token.part_of_speech.tag.name}")
\`\`\`

---

## 💰 定价概览

| 服务 | 免费额度 | 超出后价格 |
|------|----------|------------|
| Speech-to-Text | 60分钟/月 | $0.006/15秒 |
| Text-to-Speech | 400万字符/月 | $4-16/百万字符 |
| Vision AI | 1000次/月 | $1.5/千次 |
| Translation | 50万字符/月 | $20/百万字符 |
| Natural Language | 5000次/月 | $1-2/千次 |
            `,ja:`
## Google Cloud AIサービスマトリックス

Google Cloudは豊富な事前学習AIサービスを提供し、直接APIを呼び出し可能。

---

## 🗣️ Speech-to-Text

\`\`\`python
from google.cloud import speech

client = speech.SpeechClient()
config = speech.RecognitionConfig(
    language_code="ja-JP",
    enable_automatic_punctuation=True
)
response = client.recognize(config=config, audio=audio)
\`\`\`

---

## 🔊 Text-to-Speech

\`\`\`python
from google.cloud import texttospeech

client = texttospeech.TextToSpeechClient()
voice = texttospeech.VoiceSelectionParams(
    language_code="ja-JP",
    name="ja-JP-Wavenet-A"
)
\`\`\`

---

## 👁️ Vision AI

- ラベル検出
- OCR（文字認識）
- 顔検出
- 物体検出
- 安全検索

---

## 💰 料金概要

| サービス | 無料枠 | 超過後 |
|----------|--------|--------|
| Speech-to-Text | 60分/月 | $0.006/15秒 |
| Text-to-Speech | 400万文字/月 | $4-16/百万文字 |
| Vision AI | 1000回/月 | $1.5/千回 |
            `}},{id:"ch9-summary",title:{zh:"9.7 本章小结",ja:"9.7 この章のまとめ"},content:{zh:`
## Google AI 生态系统总览

---

## 🗺️ 产品地图

\`\`\`
┌─────────────────────────────────────────────────────────────┐
│                    Google AI 生态系统                        │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  ┌─────────────────┐  ┌─────────────────┐                   │
│  │    Gemini       │  │   Gemma         │                   │
│  │  (闭源旗舰)     │  │  (开源轻量)     │                   │
│  └─────────────────┘  └─────────────────┘                   │
│           │                    │                             │
│           ▼                    ▼                             │
│  ┌─────────────────┐  ┌─────────────────┐                   │
│  │  AI Studio      │  │  HuggingFace    │                   │
│  │  (免费原型)     │  │  Ollama 本地    │                   │
│  └─────────────────┘  └─────────────────┘                   │
│           │                                                  │
│           ▼                                                  │
│  ┌─────────────────────────────────────────┐                │
│  │            Vertex AI                     │                │
│  │  (企业级：训练、部署、监控)              │                │
│  └─────────────────────────────────────────┘                │
│           │                                                  │
│           ▼                                                  │
│  ┌─────────────────────────────────────────┐                │
│  │         Google Cloud AI Services         │                │
│  │  Speech | Vision | Translation | NLP     │                │
│  └─────────────────────────────────────────┘                │
│                                                              │
│  ┌─────────────────┐                                        │
│  │   NotebookLM    │  ← 研究助手                            │
│  └─────────────────┘                                        │
└─────────────────────────────────────────────────────────────┘
\`\`\`

---

## 🎯 选择指南

| 需求 | 推荐产品 |
|------|----------|
| 快速原型 | AI Studio + Gemini API |
| 企业部署 | Vertex AI |
| 本地运行 | Gemma + Ollama |
| 文档研究 | NotebookLM |
| 语音处理 | Speech-to-Text / TTS |
| 图像分析 | Vision AI |
| 多语言 | Translation API |

---

## 💡 最佳实践

1. **先用 AI Studio 验证** —— 免费测试 Prompt 效果
2. **选择合适的模型** —— Flash 省钱，Pro 强大
3. **善用免费额度** —— 各服务都有免费层
4. **考虑隐私合规** —— 敏感数据用 Gemma 本地部署
5. **监控成本** —— 设置预算提醒

---

## 📚 学习资源

- 📖 [Google AI 官方文档](https://ai.google.dev/)
- 🎓 [Google Cloud Skills Boost](https://www.cloudskillsboost.google/)
- 🔬 [Google AI Blog](https://blog.google/technology/ai/)
- 💬 [Gemini API Cookbook](https://github.com/google-gemini/cookbook)
            `,ja:`
## Google AIエコシステム総覧

---

## 🗺️ プロダクトマップ

| 製品 | 特徴 | 用途 |
|------|------|------|
| **Gemini** | クローズドソースフラッグシップ | 本番アプリ |
| **Gemma** | オープンソース軽量 | ローカルデプロイ |
| **AI Studio** | 無料プロトタイプ | 実験・検証 |
| **Vertex AI** | エンタープライズ | 大規模デプロイ |
| **NotebookLM** | リサーチアシスタント | 文書分析 |
| **Cloud AI** | 事前学習サービス | 音声・画像・翻訳 |

---

## 🎯 選択ガイド

| ニーズ | 推奨製品 |
|--------|----------|
| クイックプロトタイプ | AI Studio + Gemini API |
| エンタープライズデプロイ | Vertex AI |
| ローカル実行 | Gemma + Ollama |
| ドキュメント研究 | NotebookLM |

---

## 📚 学習リソース

- [Google AI公式ドキュメント](https://ai.google.dev/)
- [Gemini API Cookbook](https://github.com/google-gemini/cookbook)
            `}}]},{id:"chapter-n8n",number:10,title:{zh:"n8n 工作流自动化",ja:"n8n ワークフロー自動化"},subtitle:{zh:"使用 n8n 构建 AI 驱动的自动化工作流",ja:"n8n でAI駆動の自動化ワークフローを構築"},sections:[{id:"n8n-intro",title:{zh:"n8n 平台概述",ja:"n8n プラットフォーム概要"},content:{zh:`
## 🔄 n8n - 开源工作流自动化平台

n8n（读作 "n-eight-n"）是一个功能强大的开源工作流自动化工具，被誉为"开源版 Zapier"。它允许你通过可视化界面连接各种应用和服务，构建复杂的自动化流程。

---

## 🎯 为什么选择 n8n？

| 特性 | 说明 | 对比 Zapier |
|------|------|-------------|
| **开源免费** | MIT 许可，可自托管 | Zapier 按任务计费 |
| **400+ 集成** | 支持主流 SaaS 和 AI 服务 | 类似 |
| **无限工作流** | 自托管无限制 | 付费计划有限制 |
| **代码节点** | JavaScript/Python 自定义 | 有限支持 |
| **AI 原生** | 内置 LLM、向量数据库节点 | 需要额外配置 |
| **数据隐私** | 数据完全在你的服务器 | 数据经过第三方 |

---

## 🏗️ 核心架构

\`\`\`
┌─────────────────────────────────────────────────────────┐
│                    n8n 工作流引擎                         │
├─────────────────────────────────────────────────────────┤
│  ┌─────────┐    ┌─────────┐    ┌─────────┐             │
│  │ 触发器   │ → │ 处理节点 │ → │ 输出节点  │             │
│  │ Trigger │    │ Process │    │ Output  │             │
│  └─────────┘    └─────────┘    └─────────┘             │
│       │              │              │                   │
│       ▼              ▼              ▼                   │
│  ┌─────────┐    ┌─────────┐    ┌─────────┐             │
│  │ Webhook │    │   AI    │    │  Slack  │             │
│  │ Schedule│    │  Code   │    │  Email  │             │
│  │ Event   │    │  HTTP   │    │   DB    │             │
│  └─────────┘    └─────────┘    └─────────┘             │
└─────────────────────────────────────────────────────────┘
\`\`\`

---

## 📦 安装方式

### 方式一：Docker（推荐）
\`\`\`bash
docker run -it --rm \\
  --name n8n \\
  -p 5678:5678 \\
  -v ~/.n8n:/home/node/.n8n \\
  n8nio/n8n
\`\`\`

### 方式二：npm 全局安装
\`\`\`bash
npm install n8n -g
n8n start
\`\`\`

### 方式三：n8n Cloud
直接使用 [n8n.io](https://n8n.io) 托管服务，无需安装

---

## 🔗 常用集成

| 类别 | 服务 |
|------|------|
| **通讯** | Slack, Discord, Telegram, Email |
| **数据库** | PostgreSQL, MySQL, MongoDB, Redis |
| **AI/LLM** | OpenAI, Anthropic, Ollama, Pinecone |
| **办公** | Google Sheets, Notion, Airtable |
| **开发** | GitHub, GitLab, Jira, Linear |
| **营销** | HubSpot, Mailchimp, Sendgrid |

---

## 📚 学习资源

- [n8n 官方文档](https://docs.n8n.io/)
- [n8n 社区模板](https://n8n.io/workflows/)
- [GitHub 仓库](https://github.com/n8n-io/n8n)
`,ja:`
## 🔄 n8n - オープンソースワークフロー自動化プラットフォーム

n8n（「エヌエイトエヌ」と読む）は、強力なオープンソースワークフロー自動化ツールで、「オープンソース版 Zapier」と呼ばれています。ビジュアルインターフェースを通じて様々なアプリやサービスを接続し、複雑な自動化フローを構築できます。

---

## 🎯 なぜ n8n を選ぶ？

| 特徴 | 説明 | Zapier との比較 |
|------|------|-----------------|
| **オープンソース** | MIT ライセンス、セルフホスト可 | Zapier はタスク課金 |
| **400+ 統合** | 主要 SaaS・AI サービス対応 | 同等 |
| **無制限ワークフロー** | セルフホストなら制限なし | 有料プランに制限あり |
| **コードノード** | JavaScript/Python カスタマイズ | 限定的サポート |
| **AI ネイティブ** | LLM・ベクトル DB ノード内蔵 | 追加設定が必要 |
| **データプライバシー** | データは自分のサーバーで完結 | 第三者経由 |

---

## 🏗️ コアアーキテクチャ

\`\`\`
┌─────────────────────────────────────────────────────────┐
│                  n8n ワークフローエンジン                   │
├─────────────────────────────────────────────────────────┤
│  ┌─────────┐    ┌─────────┐    ┌─────────┐             │
│  │トリガー  │ → │ 処理ノード│ → │出力ノード │             │
│  │ Trigger │    │ Process │    │ Output  │             │
│  └─────────┘    └─────────┘    └─────────┘             │
└─────────────────────────────────────────────────────────┘
\`\`\`

---

## 📦 インストール方法

### 方法1：Docker（推奨）
\`\`\`bash
docker run -it --rm \\
  --name n8n \\
  -p 5678:5678 \\
  -v ~/.n8n:/home/node/.n8n \\
  n8nio/n8n
\`\`\`

### 方法2：npm グローバルインストール
\`\`\`bash
npm install n8n -g
n8n start
\`\`\`

---

## 📚 学習リソース

- [n8n 公式ドキュメント](https://docs.n8n.io/)
- [n8n コミュニティテンプレート](https://n8n.io/workflows/)
- [GitHub リポジトリ](https://github.com/n8n-io/n8n)
`}},{id:"n8n-ai-nodes",title:{zh:"AI 节点详解",ja:"AI ノード詳解"},content:{zh:`
## 🤖 n8n AI 节点生态

n8n 提供丰富的 AI 节点，让你轻松构建智能工作流。

---

## 📦 核心 AI 节点

### 1. OpenAI 节点
\`\`\`
功能：调用 GPT 模型进行文本生成、分析、翻译等
支持：GPT-4, GPT-3.5, Embeddings, DALL-E, Whisper

配置示例：
- Model: gpt-4
- Temperature: 0.7
- Max Tokens: 2000
\`\`\`

### 2. Anthropic 节点
\`\`\`
功能：调用 Claude 模型
支持：Claude 3 Opus/Sonnet/Haiku

特点：
- 更长的上下文窗口
- 更好的推理能力
- 更安全的输出
\`\`\`

### 3. AI Agent 节点
\`\`\`
功能：创建自主决策的 AI 代理
特点：
- 支持工具调用
- 多步骤推理
- 记忆管理

典型用例：
- 智能客服代理
- 数据分析助手
- 自动化研究员
\`\`\`

---

## 🔧 向量数据库节点

| 节点 | 用途 | 特点 |
|------|------|------|
| **Pinecone** | 云端向量存储 | 托管服务，易扩展 |
| **Qdrant** | 开源向量数据库 | 可自托管，功能丰富 |
| **Supabase Vector** | PostgreSQL + pgvector | 与现有数据库集成 |
| **Chroma** | 轻量级向量存储 | 适合本地开发 |

---

## 💡 实战：构建 RAG 工作流

\`\`\`
┌──────────────┐
│   用户提问    │
└──────┬───────┘
       │
       ▼
┌──────────────┐
│ Embedding    │  ← 将问题转为向量
│ (OpenAI)     │
└──────┬───────┘
       │
       ▼
┌──────────────┐
│ Vector Search│  ← 搜索相关文档
│ (Pinecone)   │
└──────┬───────┘
       │
       ▼
┌──────────────┐
│ LLM Generate │  ← 基于上下文生成回答
│ (GPT-4)      │
└──────┬───────┘
       │
       ▼
┌──────────────┐
│   返回答案    │
└──────────────┘
\`\`\`

---

## 🎯 AI 工作流最佳实践

1. **分步处理** - 复杂任务拆分为多个 AI 节点
2. **错误重试** - 配置 AI 节点的重试策略
3. **成本控制** - 使用较小模型做初筛，大模型做精细处理
4. **缓存结果** - 相同输入缓存 AI 输出，减少 API 调用
5. **监控日志** - 记录 AI 节点的输入输出，便于调试
`,ja:`
## 🤖 n8n AI ノードエコシステム

n8n は豊富な AI ノードを提供し、インテリジェントなワークフローを簡単に構築できます。

---

## 📦 コア AI ノード

### 1. OpenAI ノード
\`\`\`
機能：GPT モデルでテキスト生成、分析、翻訳など
対応：GPT-4, GPT-3.5, Embeddings, DALL-E, Whisper

設定例：
- Model: gpt-4
- Temperature: 0.7
- Max Tokens: 2000
\`\`\`

### 2. Anthropic ノード
\`\`\`
機能：Claude モデルを呼び出し
対応：Claude 3 Opus/Sonnet/Haiku

特徴：
- より長いコンテキストウィンドウ
- 優れた推論能力
- より安全な出力
\`\`\`

### 3. AI Agent ノード
\`\`\`
機能：自律的に判断する AI エージェントを作成
特徴：
- ツール呼び出しサポート
- マルチステップ推論
- メモリ管理
\`\`\`

---

## 🔧 ベクトルデータベースノード

| ノード | 用途 | 特徴 |
|--------|------|------|
| **Pinecone** | クラウドベクトルストレージ | マネージドサービス |
| **Qdrant** | オープンソースベクトル DB | セルフホスト可能 |
| **Supabase Vector** | PostgreSQL + pgvector | 既存 DB と統合 |

---

## 💡 実践：RAG ワークフローの構築

\`\`\`
ユーザー質問 → Embedding → Vector Search → LLM Generate → 回答返却
\`\`\`

---

## 🎯 AI ワークフローのベストプラクティス

1. **段階処理** - 複雑なタスクは複数の AI ノードに分割
2. **エラーリトライ** - AI ノードのリトライ戦略を設定
3. **コスト管理** - 小さいモデルで初期フィルタ、大きいモデルで精密処理
4. **結果キャッシュ** - 同じ入力の AI 出力をキャッシュ
5. **ログ監視** - AI ノードの入出力を記録してデバッグ
`}},{id:"n8n-advanced-patterns",title:{zh:"高级工作流模式",ja:"高度なワークフローパターン"},content:{zh:`
## 🔧 n8n 高级工作流模式

掌握这些高级模式，构建企业级自动化解决方案。

---

## 🔀 子工作流 (Sub-workflows)

### 什么是子工作流？
将复杂流程拆分为可复用的模块。

\`\`\`
主工作流
    │
    ├── 子工作流 A (数据预处理)
    │       │
    │       └── 返回清洗后的数据
    │
    ├── 子工作流 B (AI 分析)
    │       │
    │       └── 返回分析结果
    │
    └── 子工作流 C (结果通知)
            │
            └── 发送 Slack/邮件
\`\`\`

### 配置方式
\`\`\`json
{
  "nodes": [{
    "name": "Execute Workflow",
    "type": "n8n-nodes-base.executeWorkflow",
    "parameters": {
      "workflowId": "{{ $workflow.id }}",
      "mode": "once"
    }
  }]
}
\`\`\`

---

## 🔄 错误处理策略

### 1. 节点级重试
\`\`\`
设置项：
- 重试次数：3
- 重试间隔：1000ms
- 指数退避：启用
\`\`\`

### 2. 全局错误处理
\`\`\`
Error Workflow 配置：
1. 创建专用错误处理工作流
2. 在设置中指定错误工作流 ID
3. 记录错误日志 → 发送告警 → 人工介入
\`\`\`

### 3. 分支错误处理
\`\`\`
IF 节点判断前置操作是否成功
    │
    ├── 成功 → 继续正常流程
    │
    └── 失败 → 执行备用方案
              │
              ├── 重试机制
              ├── 回滚操作
              └── 告警通知
\`\`\`

---

## 📊 数据转换技巧

### JSON 路径访问
\`\`\`javascript
// 访问嵌套数据
{{ $json.data.users[0].name }}

// 使用表达式
{{ $json.items.map(i => i.price).reduce((a,b) => a+b, 0) }}

// 条件判断
{{ $json.status === 'active' ? '✅' : '❌' }}
\`\`\`

### 数据合并
\`\`\`
Merge 节点模式：
- Append: 简单合并数组
- Combine: 按字段匹配合并
- Multi Input: 多输入流合并
\`\`\`

---

## 🎯 性能优化

| 技巧 | 说明 | 效果 |
|------|------|------|
| **批量处理** | 使用 SplitInBatches 节点 | 避免内存溢出 |
| **并行执行** | 多个独立分支并行 | 提升速度 |
| **缓存结果** | 使用 Redis 缓存 | 减少重复计算 |
| **延迟加载** | 按需获取数据 | 降低 API 调用 |

---

## 🔐 安全最佳实践

1. **凭证管理** - 使用 n8n 凭证存储，不硬编码
2. **环境变量** - 敏感配置通过环境变量注入
3. **访问控制** - 配置 RBAC 角色权限
4. **审计日志** - 启用执行历史记录
5. **网络隔离** - 内部服务使用私有网络
`,ja:`
## 🔧 n8n 高度なワークフローパターン

これらの高度なパターンを習得して、エンタープライズ級の自動化ソリューションを構築しましょう。

---

## 🔀 サブワークフロー

### サブワークフローとは？
複雑なフローを再利用可能なモジュールに分割します。

\`\`\`
メインワークフロー
    │
    ├── サブワークフロー A (データ前処理)
    │
    ├── サブワークフロー B (AI 分析)
    │
    └── サブワークフロー C (結果通知)
\`\`\`

---

## 🔄 エラー処理戦略

### 1. ノードレベルリトライ
\`\`\`
設定項目：
- リトライ回数：3
- リトライ間隔：1000ms
- 指数バックオフ：有効
\`\`\`

### 2. グローバルエラー処理
\`\`\`
Error Workflow 設定：
1. 専用エラー処理ワークフローを作成
2. 設定でエラーワークフロー ID を指定
3. エラーログ → アラート送信 → 人的介入
\`\`\`

---

## 📊 データ変換テクニック

### JSON パスアクセス
\`\`\`javascript
// ネストデータへのアクセス
{{ $json.data.users[0].name }}

// 式を使用
{{ $json.items.map(i => i.price).reduce((a,b) => a+b, 0) }}
\`\`\`

---

## 🎯 パフォーマンス最適化

| テクニック | 説明 | 効果 |
|------------|------|------|
| **バッチ処理** | SplitInBatches ノード使用 | メモリオーバーフロー防止 |
| **並列実行** | 独立ブランチを並列化 | 速度向上 |
| **結果キャッシュ** | Redis キャッシュ使用 | 重複計算削減 |

---

## 🔐 セキュリティベストプラクティス

1. **認証情報管理** - n8n クレデンシャルストアを使用
2. **環境変数** - 機密設定は環境変数で注入
3. **アクセス制御** - RBAC ロール権限を設定
4. **監査ログ** - 実行履歴を有効化
`}},{id:"n8n-use-cases",title:{zh:"企业实战案例",ja:"エンタープライズ実践例"},content:{zh:`
## 💼 n8n 企业实战案例

以下是真实企业场景中的 n8n 应用案例。

---

## 案例 1：智能客服自动化

### 业务场景
收到客户邮件 → AI 分析意图 → 自动分类 → 智能回复或转人工

### 工作流设计
\`\`\`
IMAP Trigger (收到邮件)
    │
    ▼
OpenAI (分析邮件意图)
    │
    ▼
Switch (按意图分类)
    │
    ├── 咨询类 → OpenAI (生成回复) → Gmail (自动回复)
    │
    ├── 投诉类 → Slack (通知团队) → Jira (创建工单)
    │
    └── 销售类 → HubSpot (更新CRM) → Calendar (预约会议)
\`\`\`

### 效果
- 响应时间：从 24h 缩短到 5min
- 人工处理量：减少 60%
- 客户满意度：提升 35%

---

## 案例 2：内容发布自动化

### 业务场景
一次创作 → 多平台发布 → 数据回收 → 效果分析

### 工作流设计
\`\`\`
Notion Trigger (文章发布)
    │
    ▼
Parallel 分支
    │
    ├── Medium (发布文章)
    ├── Dev.to (发布技术文)
    ├── Twitter (发布摘要)
    ├── LinkedIn (发布链接)
    └── Newsletter (发送邮件)
    │
    ▼
Merge (收集发布结果)
    │
    ▼
Google Sheets (记录数据)
    │
    ▼
Schedule (24h后)
    │
    ▼
各平台 API (获取阅读/点赞数据)
    │
    ▼
Google Sheets (更新效果数据)
\`\`\`

---

## 案例 3：AI 驱动的数据管道

### 业务场景
定时抓取行业数据 → AI 分析趋势 → 生成报告 → 推送给团队

### 工作流设计
\`\`\`
Schedule Trigger (每日 8:00)
    │
    ▼
HTTP Request (抓取数据源 A/B/C)
    │
    ▼
Code Node (数据清洗和标准化)
    │
    ▼
Pinecone (存入向量数据库)
    │
    ▼
OpenAI (分析趋势，生成洞察)
    │
    ▼
Markdown (生成报告)
    │
    ▼
Parallel
    ├── Slack (推送到频道)
    ├── Email (发送给管理层)
    └── Notion (归档报告)
\`\`\`

---

## 案例 4：DevOps 告警处理

### 业务场景
监控告警 → AI 分析根因 → 自动修复或升级

### 工作流设计
\`\`\`
Webhook (接收 Prometheus 告警)
    │
    ▼
OpenAI (分析告警内容和历史)
    │
    ▼
IF/ELSE (严重程度判断)
    │
    ├── 低 → Slack (通知 #ops 频道)
    │
    ├── 中 → PagerDuty (创建事件)
    │        │
    │        └── Runbook (尝试自动修复)
    │
    └── 高 → PagerDuty (紧急呼叫)
             │
             └── Zoom (创建紧急会议)
\`\`\`

---

## 🎯 构建你自己的案例

1. **明确目标** - 要自动化什么？节省多少时间？
2. **画流程图** - 先在纸上画出完整流程
3. **分步实现** - 从最简单的版本开始
4. **迭代优化** - 根据实际运行数据调整
`,ja:`
## 💼 n8n エンタープライズ実践例

実際の企業シナリオでの n8n 活用事例を紹介します。

---

## ケース 1：インテリジェントカスタマーサービス

### ビジネスシナリオ
顧客メール受信 → AI で意図分析 → 自動分類 → スマート返信または人的対応

### ワークフロー設計
\`\`\`
IMAP Trigger (メール受信)
    │
    ▼
OpenAI (メール意図分析)
    │
    ▼
Switch (意図別に分類)
    │
    ├── 問い合わせ → OpenAI (返信生成) → Gmail (自動返信)
    │
    ├── クレーム → Slack (チーム通知) → Jira (チケット作成)
    │
    └── 営業 → HubSpot (CRM更新) → Calendar (ミーティング予約)
\`\`\`

### 効果
- 応答時間：24h から 5min に短縮
- 人的処理量：60% 削減
- 顧客満足度：35% 向上

---

## ケース 2：コンテンツ配信自動化

### ビジネスシナリオ
一度作成 → マルチプラットフォーム配信 → データ収集 → 効果分析

### ワークフロー設計
\`\`\`
Notion Trigger (記事公開)
    │
    ▼
並列ブランチ
    ├── Medium (記事投稿)
    ├── Dev.to (技術記事投稿)
    ├── Twitter (要約投稿)
    └── Newsletter (メール送信)
    │
    ▼
Merge (配信結果収集)
    │
    ▼
Google Sheets (データ記録)
\`\`\`

---

## ケース 3：AI 駆動データパイプライン

### ビジネスシナリオ
定期的に業界データ収集 → AI でトレンド分析 → レポート生成 → チームにプッシュ

---

## 🎯 自分のケースを構築する

1. **目標を明確に** - 何を自動化？どれだけ時間節約？
2. **フロー図を描く** - 紙に完全なフローを描く
3. **段階的に実装** - 最もシンプルなバージョンから開始
4. **反復最適化** - 実際の運用データに基づいて調整
`}}]},{id:"chapter-dify",number:11,title:{zh:"Dify AI 应用开发",ja:"Dify AI アプリ開発"},subtitle:{zh:"使用 Dify 快速构建 LLM 应用",ja:"Dify で LLM アプリを素早く構築"},sections:[{id:"dify-intro",title:{zh:"Dify 平台概述",ja:"Dify プラットフォーム概要"},content:{zh:`
## 🚀 Dify - 开源 LLMOps 平台

Dify 是一个开源的 LLM 应用开发平台，让开发者能够快速构建、部署和运营 AI 应用。它被称为"AI 应用的 WordPress"。

---

## 🎯 为什么选择 Dify？

| 特性 | 说明 | 优势 |
|------|------|------|
| **开源免费** | Apache 2.0 许可 | 可商用，可自托管 |
| **多模型支持** | OpenAI, Claude, 本地模型 | 一键切换，无锁定 |
| **可视化编排** | 拖拽式 Prompt/Workflow | 无需编码 |
| **RAG 开箱即用** | 内置知识库管理 | 快速构建问答系统 |
| **企业级特性** | SSO, 权限, 审计 | 适合生产环境 |

---

## 🏗️ 核心架构

\`\`\`
┌─────────────────────────────────────────────────────────┐
│                    Dify 平台架构                          │
├─────────────────────────────────────────────────────────┤
│                                                          │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐              │
│  │  Chat    │  │  Agent   │  │ Workflow │              │
│  │  应用    │  │  应用    │  │   应用   │              │
│  └────┬─────┘  └────┬─────┘  └────┬─────┘              │
│       │             │             │                     │
│       └─────────────┼─────────────┘                     │
│                     │                                   │
│              ┌──────┴──────┐                            │
│              │ Prompt IDE  │ ← 可视化编排               │
│              └──────┬──────┘                            │
│                     │                                   │
│       ┌─────────────┼─────────────┐                     │
│       │             │             │                     │
│  ┌────┴────┐  ┌─────┴────┐  ┌────┴────┐               │
│  │  LLM    │  │   RAG    │  │  Tools  │               │
│  │ Provider│  │ Pipeline │  │  调用   │               │
│  └─────────┘  └──────────┘  └─────────┘               │
└─────────────────────────────────────────────────────────┘
\`\`\`

---

## 📦 安装方式

### 方式一：Docker Compose（推荐）
\`\`\`bash
git clone https://github.com/langgenius/dify.git
cd dify/docker
cp .env.example .env
docker compose up -d
\`\`\`

### 方式二：Dify Cloud
直接使用 [dify.ai](https://dify.ai) 托管服务

---

## 🎨 应用类型详解

| 类型 | 描述 | 适用场景 |
|------|------|----------|
| **Chat** | 对话式应用 | 客服、助手、问答 |
| **Completion** | 单次生成 | 文案、翻译、摘要 |
| **Agent** | 自主决策 | 复杂任务、研究、分析 |
| **Workflow** | 流程编排 | 数据处理、多步骤任务 |

---

## 📚 学习资源

- [Dify 官方文档](https://docs.dify.ai/)
- [GitHub 仓库](https://github.com/langgenius/dify)
- [社区论坛](https://github.com/langgenius/dify/discussions)
`,ja:`
## 🚀 Dify - オープンソース LLMOps プラットフォーム

Dify はオープンソースの LLM アプリ開発プラットフォームで、開発者が AI アプリを素早く構築、デプロイ、運用できます。「AI アプリの WordPress」と呼ばれています。

---

## 🎯 なぜ Dify を選ぶ？

| 特徴 | 説明 | メリット |
|------|------|----------|
| **オープンソース** | Apache 2.0 ライセンス | 商用可、セルフホスト可 |
| **マルチモデル対応** | OpenAI, Claude, ローカルモデル | ワンクリック切替、ロックインなし |
| **ビジュアル編集** | ドラッグ＆ドロップで Prompt/Workflow | コーディング不要 |
| **RAG 即使用可能** | ナレッジベース管理内蔵 | Q&A システムを素早く構築 |
| **エンタープライズ機能** | SSO, 権限, 監査 | 本番環境に適合 |

---

## 🏗️ コアアーキテクチャ

\`\`\`
┌─────────────────────────────────────────────────────────┐
│                  Dify プラットフォーム                     │
├─────────────────────────────────────────────────────────┤
│  ┌──────────┐  ┌──────────┐  ┌──────────┐              │
│  │  Chat    │  │  Agent   │  │ Workflow │              │
│  │  アプリ  │  │  アプリ  │  │  アプリ  │              │
│  └──────────┘  └──────────┘  └──────────┘              │
└─────────────────────────────────────────────────────────┘
\`\`\`

---

## 📦 インストール方法

### 方法1：Docker Compose（推奨）
\`\`\`bash
git clone https://github.com/langgenius/dify.git
cd dify/docker
cp .env.example .env
docker compose up -d
\`\`\`

---

## 📚 学習リソース

- [Dify 公式ドキュメント](https://docs.dify.ai/)
- [GitHub リポジトリ](https://github.com/langgenius/dify)
`}},{id:"dify-rag",title:{zh:"RAG 知识库构建",ja:"RAG ナレッジベース構築"},content:{zh:`
## 📚 Dify RAG 知识库

RAG (Retrieval-Augmented Generation) 是 Dify 的核心能力之一，让 AI 能够基于你的私有数据回答问题。

---

## 🔄 RAG 工作流程

\`\`\`
┌──────────────────────────────────────────────────────┐
│                  RAG Pipeline                         │
├──────────────────────────────────────────────────────┤
│                                                       │
│   1. 文档上传                                          │
│   ┌─────────┐                                         │
│   │ PDF/DOC │ → 文档解析 → 文本分块 → Embedding        │
│   │ MD/TXT  │                           ↓            │
│   └─────────┘                      向量数据库          │
│                                         │            │
│   2. 用户提问                            │            │
│   ┌─────────┐                           │            │
│   │  Query  │ → Embedding → 相似度搜索 ←─┘            │
│   └─────────┘                   ↓                    │
│                            相关文档块                  │
│                                 ↓                    │
│                    ┌────────────────────┐            │
│                    │ LLM + 上下文 → 回答 │            │
│                    └────────────────────┘            │
└──────────────────────────────────────────────────────┘
\`\`\`

---

## 📄 支持的文档格式

| 格式 | 说明 | 建议 |
|------|------|------|
| **PDF** | 自动 OCR 支持 | 清晰扫描件效果更好 |
| **Word** | .docx 格式 | 保留结构信息 |
| **Markdown** | 原生支持 | 推荐格式 |
| **TXT** | 纯文本 | 需要自行分段 |
| **HTML** | 网页内容 | 自动清理标签 |
| **Notion** | API 同步 | 实时更新 |

---

## ⚙️ 分块策略

### 1. 按字符数分块
\`\`\`
参数：
- chunk_size: 500-1000 字符
- overlap: 50-100 字符

适用：通用文档
\`\`\`

### 2. 按段落分块
\`\`\`
根据换行符自动分段
保持段落完整性

适用：结构化文档
\`\`\`

### 3. 按标题分块
\`\`\`
识别 Markdown 标题层级
按章节组织内容

适用：技术文档、手册
\`\`\`

---

## 🎯 检索优化技巧

1. **混合检索** - 向量检索 + 关键词检索
2. **重排序** - 使用 Reranker 模型优化结果
3. **元数据过滤** - 按来源、日期等筛选
4. **查询扩展** - LLM 改写用户问题

---

## 💡 最佳实践

| 做法 | 原因 |
|------|------|
| 保持文档更新 | 确保回答准确性 |
| 合理设置分块大小 | 平衡上下文和准确性 |
| 添加元数据 | 便于过滤和引用 |
| 测试检索效果 | 验证返回结果相关性 |
`,ja:`
## 📚 Dify RAG ナレッジベース

RAG (Retrieval-Augmented Generation) は Dify のコア機能の一つで、AI がプライベートデータに基づいて質問に回答できるようにします。

---

## 🔄 RAG ワークフロー

\`\`\`
1. ドキュメントアップロード
   PDF/DOC → 解析 → チャンク分割 → Embedding → ベクトル DB

2. ユーザー質問
   Query → Embedding → 類似度検索 → 関連チャンク → LLM → 回答
\`\`\`

---

## 📄 対応ドキュメント形式

| 形式 | 説明 |
|------|------|
| **PDF** | 自動 OCR 対応 |
| **Word** | .docx 形式 |
| **Markdown** | ネイティブ対応（推奨） |
| **TXT** | プレーンテキスト |
| **Notion** | API 同期 |

---

## ⚙️ チャンク戦略

1. **文字数分割** - chunk_size: 500-1000 文字
2. **段落分割** - 改行で自動分割
3. **見出し分割** - Markdown 見出しで分割

---

## 🎯 検索最適化のコツ

1. **ハイブリッド検索** - ベクトル + キーワード
2. **リランキング** - Reranker モデルで結果最適化
3. **メタデータフィルター** - ソース、日付で絞り込み
4. **クエリ拡張** - LLM でユーザー質問を書き換え
`}},{id:"dify-workflow",title:{zh:"Workflow 编排",ja:"Workflow 編成"},content:{zh:`
## 🔄 Dify Workflow 编排

Workflow 是 Dify 的高级功能，允许你通过可视化方式编排复杂的 AI 处理流程。

---

## 🏗️ Workflow 节点类型

### 输入节点
| 节点 | 功能 |
|------|------|
| **Start** | 工作流入口，定义输入变量 |
| **HTTP Request** | 接收外部 API 调用 |

### 处理节点
| 节点 | 功能 |
|------|------|
| **LLM** | 调用大语言模型 |
| **Knowledge Retrieval** | RAG 知识库检索 |
| **Code** | 执行 Python/JavaScript |
| **Template** | 模板渲染 |
| **Variable Aggregator** | 变量合并 |

### 逻辑节点
| 节点 | 功能 |
|------|------|
| **IF/ELSE** | 条件分支 |
| **Iteration** | 循环处理 |
| **Parameter Extractor** | 参数提取 |

### 输出节点
| 节点 | 功能 |
|------|------|
| **Answer** | 返回最终结果 |
| **HTTP Response** | API 响应 |

---

## 💡 实战案例：智能文档处理

\`\`\`
Start (上传文档)
    │
    ▼
Code (文档解析)
    │
    ▼
LLM (内容摘要)
    │
    ▼
IF/ELSE (是否需要翻译?)
    │         │
   是         否
    │         │
    ▼         │
LLM (翻译)    │
    │         │
    └────┬────┘
         │
         ▼
Knowledge (存储)
         │
         ▼
Answer (返回结果)
\`\`\`

---

## ⚙️ Workflow vs Agent

| 特性 | Workflow | Agent |
|------|----------|-------|
| 流程控制 | 预定义 | 动态决策 |
| 可预测性 | 高 | 低 |
| 调试难度 | 低 | 高 |
| 灵活性 | 中 | 高 |
| 适用场景 | 固定流程 | 开放任务 |

---

## 🎯 最佳实践

1. **模块化设计** - 复杂流程拆分为子 Workflow
2. **错误处理** - 每个关键节点添加异常分支
3. **变量命名** - 使用清晰的变量命名规范
4. **版本控制** - 保留历史版本便于回滚
`,ja:`
## 🔄 Dify Workflow 編成

Workflow は Dify の高度な機能で、複雑な AI 処理フローをビジュアルに編成できます。

---

## 🏗️ Workflow ノードタイプ

### 入力ノード
| ノード | 機能 |
|--------|------|
| **Start** | ワークフローエントリー |
| **HTTP Request** | 外部 API 呼び出し受信 |

### 処理ノード
| ノード | 機能 |
|--------|------|
| **LLM** | 大規模言語モデル呼び出し |
| **Knowledge Retrieval** | RAG ナレッジベース検索 |
| **Code** | Python/JavaScript 実行 |
| **Template** | テンプレートレンダリング |

### ロジックノード
| ノード | 機能 |
|--------|------|
| **IF/ELSE** | 条件分岐 |
| **Iteration** | ループ処理 |

---

## ⚙️ Workflow vs Agent

| 特徴 | Workflow | Agent |
|------|----------|-------|
| フロー制御 | 事前定義 | 動的決定 |
| 予測可能性 | 高 | 低 |
| デバッグ難易度 | 低 | 高 |
| 適用シナリオ | 固定フロー | オープンタスク |
`}},{id:"dify-agent",title:{zh:"Agent 智能体开发",ja:"Agent インテリジェント開発"},content:{zh:`
## 🤖 Dify Agent 智能体开发

Agent 是 Dify 中最强大的应用类型，能够自主决策并调用工具完成复杂任务。

---

## 🧠 Agent 工作原理

\`\`\`
用户输入
    │
    ▼
┌─────────────────────────────────────┐
│           Agent 推理循环             │
│  ┌──────────────────────────────┐  │
│  │ 1. 理解用户意图              │  │
│  │ 2. 规划执行步骤              │  │
│  │ 3. 选择合适工具              │  │
│  │ 4. 执行工具调用              │  │
│  │ 5. 分析执行结果              │  │
│  │ 6. 决定下一步行动            │  │
│  │    ↓                         │  │
│  │ 重复直到任务完成             │  │
│  └──────────────────────────────┘  │
└─────────────────────────────────────┘
    │
    ▼
最终回答
\`\`\`

---

## 🔧 Agent 类型对比

| 类型 | 推理方式 | 适用场景 | 特点 |
|------|----------|----------|------|
| **Function Calling** | 单步工具调用 | 简单任务 | 快速、确定性高 |
| **ReAct** | 推理+行动循环 | 复杂任务 | 灵活、可解释性强 |

---

## 🛠️ 内置工具详解

### 搜索工具
| 工具 | 功能 | 配置 |
|------|------|------|
| **Google Search** | 网络搜索 | 需要 API Key |
| **Wikipedia** | 百科查询 | 无需配置 |
| **Arxiv** | 论文搜索 | 学术研究 |

### 生产力工具
| 工具 | 功能 | 用途 |
|------|------|------|
| **Calculator** | 数学计算 | 数值处理 |
| **Code Interpreter** | 代码执行 | 数据分析 |
| **Web Scraper** | 网页抓取 | 信息提取 |

### 自定义工具
\`\`\`yaml
# API 工具定义示例
name: get_weather
description: 获取指定城市的天气信息
parameters:
  type: object
  properties:
    city:
      type: string
      description: 城市名称
  required:
    - city
endpoint: https://api.weather.com/v1/current
method: GET
\`\`\`

---

## 📝 Agent Prompt 设计

### 系统提示词模板
\`\`\`markdown
## 角色定义
你是一个 [角色描述]，专门帮助用户 [核心职责]。

## 能力边界
- 你可以：[列出能做的事]
- 你不能：[列出不能做的事]

## 工具使用指南
当用户需要 [场景A] 时，使用 [工具A]
当用户需要 [场景B] 时，使用 [工具B]

## 输出格式
[定义回答的格式和结构]

## 注意事项
1. [重要提醒1]
2. [重要提醒2]
\`\`\`

---

## 💡 Agent 开发最佳实践

| 实践 | 说明 |
|------|------|
| **限制工具数量** | 不超过 10 个，避免选择困难 |
| **清晰的工具描述** | 帮助 Agent 正确选择 |
| **设置迭代上限** | 防止无限循环，建议 5-10 次 |
| **添加示例** | Few-shot 示例提升准确性 |
| **监控和日志** | 追踪推理过程，便于调试 |
`,ja:`
## 🤖 Dify Agent インテリジェント開発

Agent は Dify で最も強力なアプリタイプで、自律的に判断しツールを呼び出して複雑なタスクを完了できます。

---

## 🧠 Agent 動作原理

\`\`\`
ユーザー入力
    │
    ▼
┌─────────────────────────────────────┐
│           Agent 推論ループ           │
│  1. ユーザー意図を理解              │
│  2. 実行ステップを計画              │
│  3. 適切なツールを選択              │
│  4. ツールを実行                    │
│  5. 結果を分析                      │
│  6. 次のアクションを決定            │
│     ↓                               │
│  タスク完了まで繰り返し             │
└─────────────────────────────────────┘
    │
    ▼
最終回答
\`\`\`

---

## 🔧 Agent タイプ比較

| タイプ | 推論方式 | 適用シナリオ | 特徴 |
|--------|----------|--------------|------|
| **Function Calling** | 単一ステップツール呼出 | シンプルなタスク | 高速、確定的 |
| **ReAct** | 推論+行動ループ | 複雑なタスク | 柔軟、説明可能 |

---

## 🛠️ 内蔵ツール詳解

### 検索ツール
| ツール | 機能 | 設定 |
|--------|------|------|
| **Google Search** | ウェブ検索 | API Key 必要 |
| **Wikipedia** | 百科事典検索 | 設定不要 |
| **Arxiv** | 論文検索 | 学術研究用 |

### 生産性ツール
| ツール | 機能 | 用途 |
|--------|------|------|
| **Calculator** | 数学計算 | 数値処理 |
| **Code Interpreter** | コード実行 | データ分析 |

---

## 💡 Agent 開発ベストプラクティス

| プラクティス | 説明 |
|--------------|------|
| **ツール数を制限** | 10 個以下、選択困難を防ぐ |
| **明確なツール説明** | Agent の正確な選択を支援 |
| **イテレーション上限設定** | 無限ループ防止、5-10 回推奨 |
| **サンプル追加** | Few-shot で精度向上 |
`}},{id:"dify-api",title:{zh:"API 集成与部署",ja:"API 統合とデプロイ"},content:{zh:`
## 🔌 Dify API 集成

将 Dify 应用集成到你的产品中，实现 AI 能力的快速落地。

---

## 🔑 API 认证

### 获取 API Key
\`\`\`
1. 进入应用设置 → API 访问
2. 创建新的 API Key
3. 复制并安全保存
\`\`\`

### 认证方式
\`\`\`bash
# 请求头认证
Authorization: Bearer {api_key}
\`\`\`

---

## 📡 核心 API 端点

### 1. 对话 API (Chat)
\`\`\`bash
POST /v1/chat-messages

curl -X POST 'https://api.dify.ai/v1/chat-messages' \\
  -H 'Authorization: Bearer {api_key}' \\
  -H 'Content-Type: application/json' \\
  -d '{
    "inputs": {},
    "query": "你好，请介绍一下自己",
    "user": "user-123",
    "response_mode": "streaming",
    "conversation_id": ""
  }'
\`\`\`

### 2. 文本生成 API (Completion)
\`\`\`bash
POST /v1/completion-messages

curl -X POST 'https://api.dify.ai/v1/completion-messages' \\
  -H 'Authorization: Bearer {api_key}' \\
  -H 'Content-Type: application/json' \\
  -d '{
    "inputs": {"topic": "人工智能"},
    "user": "user-123",
    "response_mode": "blocking"
  }'
\`\`\`

### 3. 工作流 API (Workflow)
\`\`\`bash
POST /v1/workflows/run

curl -X POST 'https://api.dify.ai/v1/workflows/run' \\
  -H 'Authorization: Bearer {api_key}' \\
  -H 'Content-Type: application/json' \\
  -d '{
    "inputs": {"document": "文档内容..."},
    "user": "user-123"
  }'
\`\`\`

---

## 🔄 响应模式

| 模式 | 说明 | 适用场景 |
|------|------|----------|
| **blocking** | 等待完整响应 | 后台处理 |
| **streaming** | SSE 流式返回 | 实时对话 |

### 流式响应处理
\`\`\`javascript
const response = await fetch(url, {
  method: 'POST',
  headers: { 'Authorization': 'Bearer ' + apiKey },
  body: JSON.stringify(data)
});

const reader = response.body.getReader();
const decoder = new TextDecoder();

while (true) {
  const { done, value } = await reader.read();
  if (done) break;

  const chunk = decoder.decode(value);
  const lines = chunk.split('\\n');

  for (const line of lines) {
    if (line.startsWith('data: ')) {
      const data = JSON.parse(line.slice(6));
      console.log(data.answer); // 实时输出
    }
  }
}
\`\`\`

---

## 📁 知识库 API

### 上传文档
\`\`\`bash
POST /v1/datasets/{dataset_id}/document/create_by_file

curl -X POST 'https://api.dify.ai/v1/datasets/{dataset_id}/document/create_by_file' \\
  -H 'Authorization: Bearer {api_key}' \\
  -F 'file=@/path/to/document.pdf' \\
  -F 'data={"indexing_technique": "high_quality"}'
\`\`\`

### 文档状态查询
\`\`\`bash
GET /v1/datasets/{dataset_id}/documents/{document_id}/indexing-status
\`\`\`

---

## 🚀 生产部署建议

| 建议 | 说明 |
|------|------|
| **API Key 轮换** | 定期更换，降低泄露风险 |
| **请求限流** | 防止滥用，保护服务稳定 |
| **错误重试** | 指数退避重试策略 |
| **响应缓存** | 相同请求缓存结果 |
| **监控告警** | 跟踪 API 调用量和错误率 |

---

## 🔒 安全最佳实践

1. **不要在前端暴露 API Key** - 使用后端代理
2. **启用 CORS 限制** - 只允许信任的域名
3. **实现用户鉴权** - 验证调用者身份
4. **日志脱敏** - 不记录敏感用户数据
5. **设置请求配额** - 按用户限制调用次数
`,ja:`
## 🔌 Dify API 統合

Dify アプリをプロダクトに統合し、AI 機能を素早く実装しましょう。

---

## 🔑 API 認証

### API Key の取得
\`\`\`
1. アプリ設定 → API アクセスへ
2. 新しい API Key を作成
3. コピーして安全に保管
\`\`\`

### 認証方式
\`\`\`bash
# リクエストヘッダー認証
Authorization: Bearer {api_key}
\`\`\`

---

## 📡 コア API エンドポイント

### 1. 会話 API (Chat)
\`\`\`bash
POST /v1/chat-messages

curl -X POST 'https://api.dify.ai/v1/chat-messages' \\
  -H 'Authorization: Bearer {api_key}' \\
  -H 'Content-Type: application/json' \\
  -d '{
    "query": "こんにちは、自己紹介してください",
    "user": "user-123",
    "response_mode": "streaming"
  }'
\`\`\`

### 2. テキスト生成 API (Completion)
\`\`\`bash
POST /v1/completion-messages
\`\`\`

### 3. ワークフロー API (Workflow)
\`\`\`bash
POST /v1/workflows/run
\`\`\`

---

## 🔄 レスポンスモード

| モード | 説明 | 適用シナリオ |
|--------|------|--------------|
| **blocking** | 完全なレスポンスを待機 | バックグラウンド処理 |
| **streaming** | SSE ストリーミング返却 | リアルタイム対話 |

---

## 🚀 本番デプロイの推奨事項

| 推奨事項 | 説明 |
|----------|------|
| **API Key ローテーション** | 定期的に更新、漏洩リスク軽減 |
| **リクエスト制限** | 悪用防止、サービス安定性保護 |
| **エラーリトライ** | 指数バックオフリトライ戦略 |
| **レスポンスキャッシュ** | 同一リクエストの結果をキャッシュ |
| **監視アラート** | API コール量とエラー率を追跡 |

---

## 🔒 セキュリティベストプラクティス

1. **フロントエンドで API Key を公開しない** - バックエンドプロキシを使用
2. **CORS 制限を有効化** - 信頼できるドメインのみ許可
3. **ユーザー認証を実装** - 呼び出し元の身元を確認
4. **ログのマスキング** - 機密ユーザーデータを記録しない
5. **リクエストクォータ設定** - ユーザーごとの呼び出し回数を制限
`}},{id:"dify-best-practices",title:{zh:"企业级最佳实践",ja:"エンタープライズベストプラクティス"},content:{zh:`
## 🏢 Dify 企业级最佳实践

在生产环境中运行 Dify 的关键考虑事项。

---

## 🏗️ 架构设计

### 推荐架构
\`\`\`
                    ┌─────────────────┐
                    │   负载均衡器     │
                    │  (Nginx/ALB)    │
                    └────────┬────────┘
                             │
            ┌────────────────┼────────────────┐
            │                │                │
     ┌──────┴──────┐  ┌──────┴──────┐  ┌──────┴──────┐
     │  Dify Web   │  │  Dify API   │  │  Dify Worker│
     │  (前端)      │  │  (后端)      │  │  (异步任务) │
     └──────┬──────┘  └──────┬──────┘  └──────┬──────┘
            │                │                │
            └────────────────┼────────────────┘
                             │
            ┌────────────────┼────────────────┐
            │                │                │
     ┌──────┴──────┐  ┌──────┴──────┐  ┌──────┴──────┐
     │  PostgreSQL │  │    Redis    │  │   Weaviate  │
     │   (主数据)   │  │   (缓存)     │  │  (向量库)   │
     └─────────────┘  └─────────────┘  └─────────────┘
\`\`\`

---

## ⚙️ 性能优化

### 数据库优化
| 优化项 | 配置 |
|--------|------|
| **连接池大小** | 50-100 |
| **读写分离** | 主从复制 |
| **索引优化** | 监控慢查询 |

### 缓存策略
| 缓存层 | 用途 | TTL |
|--------|------|-----|
| **Prompt 缓存** | 模板解析结果 | 1h |
| **向量缓存** | 常用 Embedding | 24h |
| **会话缓存** | 对话上下文 | 30min |

### Worker 配置
\`\`\`yaml
# Celery Worker 配置
celery:
  concurrency: 4        # 并发数
  prefetch_multiplier: 1 # 预取任务数
  max_tasks_per_child: 100 # 子进程最大任务数
\`\`\`

---

## 📊 监控指标

### 关键指标
| 指标 | 阈值 | 告警 |
|------|------|------|
| **API 响应时间** | P99 < 2s | > 5s 告警 |
| **错误率** | < 1% | > 5% 告警 |
| **Token 使用量** | 按预算 | 超 80% 预警 |
| **队列积压** | < 100 | > 1000 告警 |

### 日志规范
\`\`\`json
{
  "timestamp": "2024-01-15T10:30:00Z",
  "level": "INFO",
  "service": "dify-api",
  "trace_id": "abc123",
  "user_id": "user_456",
  "action": "chat_completion",
  "latency_ms": 1200,
  "tokens": {"input": 500, "output": 200}
}
\`\`\`

---

## 🔐 安全加固

### 1. 网络安全
\`\`\`
- VPC 隔离内部服务
- WAF 防护外部攻击
- DDoS 防护
\`\`\`

### 2. 数据安全
\`\`\`
- 数据库加密存储
- 传输层 TLS 加密
- API Key 加密存储
- PII 数据脱敏
\`\`\`

### 3. 访问控制
\`\`\`
- SSO 单点登录
- RBAC 角色权限
- 审计日志
- IP 白名单
\`\`\`

---

## 💰 成本控制

### Token 使用优化
| 策略 | 效果 |
|------|------|
| **Prompt 压缩** | 减少 30% Token |
| **上下文裁剪** | 只保留关键历史 |
| **模型分级** | 简单任务用小模型 |
| **缓存命中** | 减少重复调用 |

### 资源规划
\`\`\`
小型 (100 DAU)：2C4G 单实例
中型 (1000 DAU)：4C8G × 3 实例
大型 (10000 DAU)：8C16G × 10 实例 + 读写分离
\`\`\`

---

## 🚀 CI/CD 最佳实践

\`\`\`yaml
# GitHub Actions 示例
name: Deploy Dify
on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Build and Push
        run: |
          docker build -t dify:\${{ github.sha }} .
          docker push registry/dify:\${{ github.sha }}

      - name: Deploy to K8s
        run: |
          kubectl set image deployment/dify \\
            dify=registry/dify:\${{ github.sha }}
          kubectl rollout status deployment/dify
\`\`\`
`,ja:`
## 🏢 Dify エンタープライズベストプラクティス

本番環境で Dify を運用する際の重要な考慮事項。

---

## 🏗️ アーキテクチャ設計

### 推奨アーキテクチャ
\`\`\`
              ┌─────────────────┐
              │  ロードバランサー │
              │  (Nginx/ALB)    │
              └────────┬────────┘
                       │
       ┌───────────────┼───────────────┐
       │               │               │
┌──────┴──────┐ ┌──────┴──────┐ ┌──────┴──────┐
│  Dify Web   │ │  Dify API   │ │ Dify Worker │
└──────┬──────┘ └──────┬──────┘ └──────┬──────┘
       │               │               │
       └───────────────┼───────────────┘
                       │
       ┌───────────────┼───────────────┐
       │               │               │
┌──────┴──────┐ ┌──────┴──────┐ ┌──────┴──────┐
│ PostgreSQL  │ │    Redis    │ │  Weaviate   │
└─────────────┘ └─────────────┘ └─────────────┘
\`\`\`

---

## ⚙️ パフォーマンス最適化

### データベース最適化
| 最適化項目 | 設定 |
|------------|------|
| **コネクションプールサイズ** | 50-100 |
| **読み書き分離** | マスタースレーブ複製 |
| **インデックス最適化** | スロークエリ監視 |

### キャッシュ戦略
| キャッシュ層 | 用途 | TTL |
|--------------|------|-----|
| **Prompt キャッシュ** | テンプレート解析結果 | 1h |
| **ベクトルキャッシュ** | よく使う Embedding | 24h |
| **セッションキャッシュ** | 会話コンテキスト | 30min |

---

## 📊 監視指標

### 重要指標
| 指標 | しきい値 | アラート |
|------|----------|----------|
| **API レスポンス時間** | P99 < 2s | > 5s で警告 |
| **エラー率** | < 1% | > 5% で警告 |
| **Token 使用量** | 予算内 | 80% 超で予警告 |

---

## 🔐 セキュリティ強化

### ネットワークセキュリティ
- VPC で内部サービスを隔離
- WAF で外部攻撃を防御
- DDoS 対策

### データセキュリティ
- データベース暗号化保存
- TLS 暗号化通信
- API Key 暗号化保存

### アクセス制御
- SSO シングルサインオン
- RBAC ロール権限
- 監査ログ

---

## 💰 コスト管理

### Token 使用最適化
| 戦略 | 効果 |
|------|------|
| **Prompt 圧縮** | Token 30% 削減 |
| **コンテキスト削減** | 重要な履歴のみ保持 |
| **モデル階層化** | 簡単なタスクは小型モデル |
`}}]}]};function NL(e,t){const n={};return(e[e.length-1]===""?[...e,""]:e).join((n.padRight?" ":"")+","+(n.padLeft===!1?"":" ")).trim()}const zL=/^[$_\p{ID_Start}][$_\u{200C}\u{200D}\p{ID_Continue}]*$/u,BL=/^[$_\p{ID_Start}][-$_\u{200C}\u{200D}\p{ID_Continue}]*$/u,GL={};function dw(e,t){return(GL.jsx?BL:zL).test(e)}const VL=/[ \t\n\f\r]/g;function UL(e){return typeof e=="object"?e.type==="text"?pw(e.value):!1:pw(e)}function pw(e){return e.replace(VL,"")===""}class bl{constructor(t,n,r){this.normal=n,this.property=t,r&&(this.space=r)}}bl.prototype.normal={};bl.prototype.property={};bl.prototype.space=void 0;function CP(e,t){const n={},r={};for(const s of e)Object.assign(n,s.property),Object.assign(r,s.normal);return new bl(n,r,t)}function hm(e){return e.toLowerCase()}class Mr{constructor(t,n){this.attribute=n,this.property=t}}Mr.prototype.attribute="";Mr.prototype.booleanish=!1;Mr.prototype.boolean=!1;Mr.prototype.commaOrSpaceSeparated=!1;Mr.prototype.commaSeparated=!1;Mr.prototype.defined=!1;Mr.prototype.mustUseProperty=!1;Mr.prototype.number=!1;Mr.prototype.overloadedBoolean=!1;Mr.prototype.property="";Mr.prototype.spaceSeparated=!1;Mr.prototype.space=void 0;let WL=0;const Bt=Xi(),Wn=Xi(),mm=Xi(),Ye=Xi(),mn=Xi(),Ro=Xi(),Cr=Xi();function Xi(){return 2**++WL}const fm=Object.freeze(Object.defineProperty({__proto__:null,boolean:Bt,booleanish:Wn,commaOrSpaceSeparated:Cr,commaSeparated:Ro,number:Ye,overloadedBoolean:mm,spaceSeparated:mn},Symbol.toStringTag,{value:"Module"})),Tp=Object.keys(fm);class xf extends Mr{constructor(t,n,r,s){let i=-1;if(super(t,n),hw(this,"space",s),typeof r=="number")for(;++i<Tp.length;){const o=Tp[i];hw(this,Tp[i],(r&fm[o])===fm[o])}}}xf.prototype.defined=!0;function hw(e,t,n){n&&(e[t]=n)}function qo(e){const t={},n={};for(const[r,s]of Object.entries(e.properties)){const i=new xf(r,e.transform(e.attributes||{},r),s,e.space);e.mustUseProperty&&e.mustUseProperty.includes(r)&&(i.mustUseProperty=!0),t[r]=i,n[hm(r)]=r,n[hm(i.attribute)]=r}return new bl(t,n,e.space)}const IP=qo({properties:{ariaActiveDescendant:null,ariaAtomic:Wn,ariaAutoComplete:null,ariaBusy:Wn,ariaChecked:Wn,ariaColCount:Ye,ariaColIndex:Ye,ariaColSpan:Ye,ariaControls:mn,ariaCurrent:null,ariaDescribedBy:mn,ariaDetails:null,ariaDisabled:Wn,ariaDropEffect:mn,ariaErrorMessage:null,ariaExpanded:Wn,ariaFlowTo:mn,ariaGrabbed:Wn,ariaHasPopup:null,ariaHidden:Wn,ariaInvalid:null,ariaKeyShortcuts:null,ariaLabel:null,ariaLabelledBy:mn,ariaLevel:Ye,ariaLive:null,ariaModal:Wn,ariaMultiLine:Wn,ariaMultiSelectable:Wn,ariaOrientation:null,ariaOwns:mn,ariaPlaceholder:null,ariaPosInSet:Ye,ariaPressed:Wn,ariaReadOnly:Wn,ariaRelevant:null,ariaRequired:Wn,ariaRoleDescription:mn,ariaRowCount:Ye,ariaRowIndex:Ye,ariaRowSpan:Ye,ariaSelected:Wn,ariaSetSize:Ye,ariaSort:null,ariaValueMax:Ye,ariaValueMin:Ye,ariaValueNow:Ye,ariaValueText:null,role:null},transform(e,t){return t==="role"?t:"aria-"+t.slice(4).toLowerCase()}});function kP(e,t){return t in e?e[t]:t}function EP(e,t){return kP(e,t.toLowerCase())}const HL=qo({attributes:{acceptcharset:"accept-charset",classname:"class",htmlfor:"for",httpequiv:"http-equiv"},mustUseProperty:["checked","multiple","muted","selected"],properties:{abbr:null,accept:Ro,acceptCharset:mn,accessKey:mn,action:null,allow:null,allowFullScreen:Bt,allowPaymentRequest:Bt,allowUserMedia:Bt,alt:null,as:null,async:Bt,autoCapitalize:null,autoComplete:mn,autoFocus:Bt,autoPlay:Bt,blocking:mn,capture:null,charSet:null,checked:Bt,cite:null,className:mn,cols:Ye,colSpan:null,content:null,contentEditable:Wn,controls:Bt,controlsList:mn,coords:Ye|Ro,crossOrigin:null,data:null,dateTime:null,decoding:null,default:Bt,defer:Bt,dir:null,dirName:null,disabled:Bt,download:mm,draggable:Wn,encType:null,enterKeyHint:null,fetchPriority:null,form:null,formAction:null,formEncType:null,formMethod:null,formNoValidate:Bt,formTarget:null,headers:mn,height:Ye,hidden:mm,high:Ye,href:null,hrefLang:null,htmlFor:mn,httpEquiv:mn,id:null,imageSizes:null,imageSrcSet:null,inert:Bt,inputMode:null,integrity:null,is:null,isMap:Bt,itemId:null,itemProp:mn,itemRef:mn,itemScope:Bt,itemType:mn,kind:null,label:null,lang:null,language:null,list:null,loading:null,loop:Bt,low:Ye,manifest:null,max:null,maxLength:Ye,media:null,method:null,min:null,minLength:Ye,multiple:Bt,muted:Bt,name:null,nonce:null,noModule:Bt,noValidate:Bt,onAbort:null,onAfterPrint:null,onAuxClick:null,onBeforeMatch:null,onBeforePrint:null,onBeforeToggle:null,onBeforeUnload:null,onBlur:null,onCancel:null,onCanPlay:null,onCanPlayThrough:null,onChange:null,onClick:null,onClose:null,onContextLost:null,onContextMenu:null,onContextRestored:null,onCopy:null,onCueChange:null,onCut:null,onDblClick:null,onDrag:null,onDragEnd:null,onDragEnter:null,onDragExit:null,onDragLeave:null,onDragOver:null,onDragStart:null,onDrop:null,onDurationChange:null,onEmptied:null,onEnded:null,onError:null,onFocus:null,onFormData:null,onHashChange:null,onInput:null,onInvalid:null,onKeyDown:null,onKeyPress:null,onKeyUp:null,onLanguageChange:null,onLoad:null,onLoadedData:null,onLoadedMetadata:null,onLoadEnd:null,onLoadStart:null,onMessage:null,onMessageError:null,onMouseDown:null,onMouseEnter:null,onMouseLeave:null,onMouseMove:null,onMouseOut:null,onMouseOver:null,onMouseUp:null,onOffline:null,onOnline:null,onPageHide:null,onPageShow:null,onPaste:null,onPause:null,onPlay:null,onPlaying:null,onPopState:null,onProgress:null,onRateChange:null,onRejectionHandled:null,onReset:null,onResize:null,onScroll:null,onScrollEnd:null,onSecurityPolicyViolation:null,onSeeked:null,onSeeking:null,onSelect:null,onSlotChange:null,onStalled:null,onStorage:null,onSubmit:null,onSuspend:null,onTimeUpdate:null,onToggle:null,onUnhandledRejection:null,onUnload:null,onVolumeChange:null,onWaiting:null,onWheel:null,open:Bt,optimum:Ye,pattern:null,ping:mn,placeholder:null,playsInline:Bt,popover:null,popoverTarget:null,popoverTargetAction:null,poster:null,preload:null,readOnly:Bt,referrerPolicy:null,rel:mn,required:Bt,reversed:Bt,rows:Ye,rowSpan:Ye,sandbox:mn,scope:null,scoped:Bt,seamless:Bt,selected:Bt,shadowRootClonable:Bt,shadowRootDelegatesFocus:Bt,shadowRootMode:null,shape:null,size:Ye,sizes:null,slot:null,span:Ye,spellCheck:Wn,src:null,srcDoc:null,srcLang:null,srcSet:null,start:Ye,step:null,style:null,tabIndex:Ye,target:null,title:null,translate:null,type:null,typeMustMatch:Bt,useMap:null,value:Wn,width:Ye,wrap:null,writingSuggestions:null,align:null,aLink:null,archive:mn,axis:null,background:null,bgColor:null,border:Ye,borderColor:null,bottomMargin:Ye,cellPadding:null,cellSpacing:null,char:null,charOff:null,classId:null,clear:null,code:null,codeBase:null,codeType:null,color:null,compact:Bt,declare:Bt,event:null,face:null,frame:null,frameBorder:null,hSpace:Ye,leftMargin:Ye,link:null,longDesc:null,lowSrc:null,marginHeight:Ye,marginWidth:Ye,noResize:Bt,noHref:Bt,noShade:Bt,noWrap:Bt,object:null,profile:null,prompt:null,rev:null,rightMargin:Ye,rules:null,scheme:null,scrolling:Wn,standby:null,summary:null,text:null,topMargin:Ye,valueType:null,version:null,vAlign:null,vLink:null,vSpace:Ye,allowTransparency:null,autoCorrect:null,autoSave:null,disablePictureInPicture:Bt,disableRemotePlayback:Bt,prefix:null,property:null,results:Ye,security:null,unselectable:null},space:"html",transform:EP}),KL=qo({attributes:{accentHeight:"accent-height",alignmentBaseline:"alignment-baseline",arabicForm:"arabic-form",baselineShift:"baseline-shift",capHeight:"cap-height",className:"class",clipPath:"clip-path",clipRule:"clip-rule",colorInterpolation:"color-interpolation",colorInterpolationFilters:"color-interpolation-filters",colorProfile:"color-profile",colorRendering:"color-rendering",crossOrigin:"crossorigin",dataType:"datatype",dominantBaseline:"dominant-baseline",enableBackground:"enable-background",fillOpacity:"fill-opacity",fillRule:"fill-rule",floodColor:"flood-color",floodOpacity:"flood-opacity",fontFamily:"font-family",fontSize:"font-size",fontSizeAdjust:"font-size-adjust",fontStretch:"font-stretch",fontStyle:"font-style",fontVariant:"font-variant",fontWeight:"font-weight",glyphName:"glyph-name",glyphOrientationHorizontal:"glyph-orientation-horizontal",glyphOrientationVertical:"glyph-orientation-vertical",hrefLang:"hreflang",horizAdvX:"horiz-adv-x",horizOriginX:"horiz-origin-x",horizOriginY:"horiz-origin-y",imageRendering:"image-rendering",letterSpacing:"letter-spacing",lightingColor:"lighting-color",markerEnd:"marker-end",markerMid:"marker-mid",markerStart:"marker-start",navDown:"nav-down",navDownLeft:"nav-down-left",navDownRight:"nav-down-right",navLeft:"nav-left",navNext:"nav-next",navPrev:"nav-prev",navRight:"nav-right",navUp:"nav-up",navUpLeft:"nav-up-left",navUpRight:"nav-up-right",onAbort:"onabort",onActivate:"onactivate",onAfterPrint:"onafterprint",onBeforePrint:"onbeforeprint",onBegin:"onbegin",onCancel:"oncancel",onCanPlay:"oncanplay",onCanPlayThrough:"oncanplaythrough",onChange:"onchange",onClick:"onclick",onClose:"onclose",onCopy:"oncopy",onCueChange:"oncuechange",onCut:"oncut",onDblClick:"ondblclick",onDrag:"ondrag",onDragEnd:"ondragend",onDragEnter:"ondragenter",onDragExit:"ondragexit",onDragLeave:"ondragleave",onDragOver:"ondragover",onDragStart:"ondragstart",onDrop:"ondrop",onDurationChange:"ondurationchange",onEmptied:"onemptied",onEnd:"onend",onEnded:"onended",onError:"onerror",onFocus:"onfocus",onFocusIn:"onfocusin",onFocusOut:"onfocusout",onHashChange:"onhashchange",onInput:"oninput",onInvalid:"oninvalid",onKeyDown:"onkeydown",onKeyPress:"onkeypress",onKeyUp:"onkeyup",onLoad:"onload",onLoadedData:"onloadeddata",onLoadedMetadata:"onloadedmetadata",onLoadStart:"onloadstart",onMessage:"onmessage",onMouseDown:"onmousedown",onMouseEnter:"onmouseenter",onMouseLeave:"onmouseleave",onMouseMove:"onmousemove",onMouseOut:"onmouseout",onMouseOver:"onmouseover",onMouseUp:"onmouseup",onMouseWheel:"onmousewheel",onOffline:"onoffline",onOnline:"ononline",onPageHide:"onpagehide",onPageShow:"onpageshow",onPaste:"onpaste",onPause:"onpause",onPlay:"onplay",onPlaying:"onplaying",onPopState:"onpopstate",onProgress:"onprogress",onRateChange:"onratechange",onRepeat:"onrepeat",onReset:"onreset",onResize:"onresize",onScroll:"onscroll",onSeeked:"onseeked",onSeeking:"onseeking",onSelect:"onselect",onShow:"onshow",onStalled:"onstalled",onStorage:"onstorage",onSubmit:"onsubmit",onSuspend:"onsuspend",onTimeUpdate:"ontimeupdate",onToggle:"ontoggle",onUnload:"onunload",onVolumeChange:"onvolumechange",onWaiting:"onwaiting",onZoom:"onzoom",overlinePosition:"overline-position",overlineThickness:"overline-thickness",paintOrder:"paint-order",panose1:"panose-1",pointerEvents:"pointer-events",referrerPolicy:"referrerpolicy",renderingIntent:"rendering-intent",shapeRendering:"shape-rendering",stopColor:"stop-color",stopOpacity:"stop-opacity",strikethroughPosition:"strikethrough-position",strikethroughThickness:"strikethrough-thickness",strokeDashArray:"stroke-dasharray",strokeDashOffset:"stroke-dashoffset",strokeLineCap:"stroke-linecap",strokeLineJoin:"stroke-linejoin",strokeMiterLimit:"stroke-miterlimit",strokeOpacity:"stroke-opacity",strokeWidth:"stroke-width",tabIndex:"tabindex",textAnchor:"text-anchor",textDecoration:"text-decoration",textRendering:"text-rendering",transformOrigin:"transform-origin",typeOf:"typeof",underlinePosition:"underline-position",underlineThickness:"underline-thickness",unicodeBidi:"unicode-bidi",unicodeRange:"unicode-range",unitsPerEm:"units-per-em",vAlphabetic:"v-alphabetic",vHanging:"v-hanging",vIdeographic:"v-ideographic",vMathematical:"v-mathematical",vectorEffect:"vector-effect",vertAdvY:"vert-adv-y",vertOriginX:"vert-origin-x",vertOriginY:"vert-origin-y",wordSpacing:"word-spacing",writingMode:"writing-mode",xHeight:"x-height",playbackOrder:"playbackorder",timelineBegin:"timelinebegin"},properties:{about:Cr,accentHeight:Ye,accumulate:null,additive:null,alignmentBaseline:null,alphabetic:Ye,amplitude:Ye,arabicForm:null,ascent:Ye,attributeName:null,attributeType:null,azimuth:Ye,bandwidth:null,baselineShift:null,baseFrequency:null,baseProfile:null,bbox:null,begin:null,bias:Ye,by:null,calcMode:null,capHeight:Ye,className:mn,clip:null,clipPath:null,clipPathUnits:null,clipRule:null,color:null,colorInterpolation:null,colorInterpolationFilters:null,colorProfile:null,colorRendering:null,content:null,contentScriptType:null,contentStyleType:null,crossOrigin:null,cursor:null,cx:null,cy:null,d:null,dataType:null,defaultAction:null,descent:Ye,diffuseConstant:Ye,direction:null,display:null,dur:null,divisor:Ye,dominantBaseline:null,download:Bt,dx:null,dy:null,edgeMode:null,editable:null,elevation:Ye,enableBackground:null,end:null,event:null,exponent:Ye,externalResourcesRequired:null,fill:null,fillOpacity:Ye,fillRule:null,filter:null,filterRes:null,filterUnits:null,floodColor:null,floodOpacity:null,focusable:null,focusHighlight:null,fontFamily:null,fontSize:null,fontSizeAdjust:null,fontStretch:null,fontStyle:null,fontVariant:null,fontWeight:null,format:null,fr:null,from:null,fx:null,fy:null,g1:Ro,g2:Ro,glyphName:Ro,glyphOrientationHorizontal:null,glyphOrientationVertical:null,glyphRef:null,gradientTransform:null,gradientUnits:null,handler:null,hanging:Ye,hatchContentUnits:null,hatchUnits:null,height:null,href:null,hrefLang:null,horizAdvX:Ye,horizOriginX:Ye,horizOriginY:Ye,id:null,ideographic:Ye,imageRendering:null,initialVisibility:null,in:null,in2:null,intercept:Ye,k:Ye,k1:Ye,k2:Ye,k3:Ye,k4:Ye,kernelMatrix:Cr,kernelUnitLength:null,keyPoints:null,keySplines:null,keyTimes:null,kerning:null,lang:null,lengthAdjust:null,letterSpacing:null,lightingColor:null,limitingConeAngle:Ye,local:null,markerEnd:null,markerMid:null,markerStart:null,markerHeight:null,markerUnits:null,markerWidth:null,mask:null,maskContentUnits:null,maskUnits:null,mathematical:null,max:null,media:null,mediaCharacterEncoding:null,mediaContentEncodings:null,mediaSize:Ye,mediaTime:null,method:null,min:null,mode:null,name:null,navDown:null,navDownLeft:null,navDownRight:null,navLeft:null,navNext:null,navPrev:null,navRight:null,navUp:null,navUpLeft:null,navUpRight:null,numOctaves:null,observer:null,offset:null,onAbort:null,onActivate:null,onAfterPrint:null,onBeforePrint:null,onBegin:null,onCancel:null,onCanPlay:null,onCanPlayThrough:null,onChange:null,onClick:null,onClose:null,onCopy:null,onCueChange:null,onCut:null,onDblClick:null,onDrag:null,onDragEnd:null,onDragEnter:null,onDragExit:null,onDragLeave:null,onDragOver:null,onDragStart:null,onDrop:null,onDurationChange:null,onEmptied:null,onEnd:null,onEnded:null,onError:null,onFocus:null,onFocusIn:null,onFocusOut:null,onHashChange:null,onInput:null,onInvalid:null,onKeyDown:null,onKeyPress:null,onKeyUp:null,onLoad:null,onLoadedData:null,onLoadedMetadata:null,onLoadStart:null,onMessage:null,onMouseDown:null,onMouseEnter:null,onMouseLeave:null,onMouseMove:null,onMouseOut:null,onMouseOver:null,onMouseUp:null,onMouseWheel:null,onOffline:null,onOnline:null,onPageHide:null,onPageShow:null,onPaste:null,onPause:null,onPlay:null,onPlaying:null,onPopState:null,onProgress:null,onRateChange:null,onRepeat:null,onReset:null,onResize:null,onScroll:null,onSeeked:null,onSeeking:null,onSelect:null,onShow:null,onStalled:null,onStorage:null,onSubmit:null,onSuspend:null,onTimeUpdate:null,onToggle:null,onUnload:null,onVolumeChange:null,onWaiting:null,onZoom:null,opacity:null,operator:null,order:null,orient:null,orientation:null,origin:null,overflow:null,overlay:null,overlinePosition:Ye,overlineThickness:Ye,paintOrder:null,panose1:null,path:null,pathLength:Ye,patternContentUnits:null,patternTransform:null,patternUnits:null,phase:null,ping:mn,pitch:null,playbackOrder:null,pointerEvents:null,points:null,pointsAtX:Ye,pointsAtY:Ye,pointsAtZ:Ye,preserveAlpha:null,preserveAspectRatio:null,primitiveUnits:null,propagate:null,property:Cr,r:null,radius:null,referrerPolicy:null,refX:null,refY:null,rel:Cr,rev:Cr,renderingIntent:null,repeatCount:null,repeatDur:null,requiredExtensions:Cr,requiredFeatures:Cr,requiredFonts:Cr,requiredFormats:Cr,resource:null,restart:null,result:null,rotate:null,rx:null,ry:null,scale:null,seed:null,shapeRendering:null,side:null,slope:null,snapshotTime:null,specularConstant:Ye,specularExponent:Ye,spreadMethod:null,spacing:null,startOffset:null,stdDeviation:null,stemh:null,stemv:null,stitchTiles:null,stopColor:null,stopOpacity:null,strikethroughPosition:Ye,strikethroughThickness:Ye,string:null,stroke:null,strokeDashArray:Cr,strokeDashOffset:null,strokeLineCap:null,strokeLineJoin:null,strokeMiterLimit:Ye,strokeOpacity:Ye,strokeWidth:null,style:null,surfaceScale:Ye,syncBehavior:null,syncBehaviorDefault:null,syncMaster:null,syncTolerance:null,syncToleranceDefault:null,systemLanguage:Cr,tabIndex:Ye,tableValues:null,target:null,targetX:Ye,targetY:Ye,textAnchor:null,textDecoration:null,textRendering:null,textLength:null,timelineBegin:null,title:null,transformBehavior:null,type:null,typeOf:Cr,to:null,transform:null,transformOrigin:null,u1:null,u2:null,underlinePosition:Ye,underlineThickness:Ye,unicode:null,unicodeBidi:null,unicodeRange:null,unitsPerEm:Ye,values:null,vAlphabetic:Ye,vMathematical:Ye,vectorEffect:null,vHanging:Ye,vIdeographic:Ye,version:null,vertAdvY:Ye,vertOriginX:Ye,vertOriginY:Ye,viewBox:null,viewTarget:null,visibility:null,width:null,widths:null,wordSpacing:null,writingMode:null,x:null,x1:null,x2:null,xChannelSelector:null,xHeight:Ye,y:null,y1:null,y2:null,yChannelSelector:null,z:null,zoomAndPan:null},space:"svg",transform:kP}),SP=qo({properties:{xLinkActuate:null,xLinkArcRole:null,xLinkHref:null,xLinkRole:null,xLinkShow:null,xLinkTitle:null,xLinkType:null},space:"xlink",transform(e,t){return"xlink:"+t.slice(5).toLowerCase()}}),LP=qo({attributes:{xmlnsxlink:"xmlns:xlink"},properties:{xmlnsXLink:null,xmlns:null},space:"xmlns",transform:EP}),DP=qo({properties:{xmlBase:null,xmlLang:null,xmlSpace:null},space:"xml",transform(e,t){return"xml:"+t.slice(3).toLowerCase()}}),qL={classId:"classID",dataType:"datatype",itemId:"itemID",strokeDashArray:"strokeDasharray",strokeDashOffset:"strokeDashoffset",strokeLineCap:"strokeLinecap",strokeLineJoin:"strokeLinejoin",strokeMiterLimit:"strokeMiterlimit",typeOf:"typeof",xLinkActuate:"xlinkActuate",xLinkArcRole:"xlinkArcrole",xLinkHref:"xlinkHref",xLinkRole:"xlinkRole",xLinkShow:"xlinkShow",xLinkTitle:"xlinkTitle",xLinkType:"xlinkType",xmlnsXLink:"xmlnsXlink"},QL=/[A-Z]/g,mw=/-[a-z]/g,XL=/^data[-\w.:]+$/i;function YL(e,t){const n=hm(t);let r=t,s=Mr;if(n in e.normal)return e.property[e.normal[n]];if(n.length>4&&n.slice(0,4)==="data"&&XL.test(t)){if(t.charAt(4)==="-"){const i=t.slice(5).replace(mw,ZL);r="data"+i.charAt(0).toUpperCase()+i.slice(1)}else{const i=t.slice(4);if(!mw.test(i)){let o=i.replace(QL,JL);o.charAt(0)!=="-"&&(o="-"+o),t="data"+o}}s=xf}return new s(r,t)}function JL(e){return"-"+e.toLowerCase()}function ZL(e){return e.charAt(1).toUpperCase()}const eD=CP([IP,HL,SP,LP,DP],"html"),bf=CP([IP,KL,SP,LP,DP],"svg");function tD(e){return e.join(" ").trim()}var wo={},Pp,fw;function nD(){if(fw)return Pp;fw=1;var e=/\/\*[^*]*\*+([^/*][^*]*\*+)*\//g,t=/\n/g,n=/^\s*/,r=/^(\*?[-#/*\\\w]+(\[[0-9a-z_-]+\])?)\s*/,s=/^:\s*/,i=/^((?:'(?:\\'|.)*?'|"(?:\\"|.)*?"|\([^)]*?\)|[^};])+)/,o=/^[;\s]*/,a=/^\s+|\s+$/g,l=`
`,c="/",d="*",u="",p="comment",h="declaration";function m(I,f){if(typeof I!="string")throw new TypeError("First argument must be a string");if(!I)return[];f=f||{};var _=1,T=1;function M(K){var U=K.match(t);U&&(_+=U.length);var Y=K.lastIndexOf(l);T=~Y?K.length-Y:T+K.length}function v(){var K={line:_,column:T};return function(U){return U.position=new b(K),F(),U}}function b(K){this.start=K,this.end={line:_,column:T},this.source=f.source}b.prototype.content=I;function A(K){var U=new Error(f.source+":"+_+":"+T+": "+K);if(U.reason=K,U.filename=f.source,U.line=_,U.column=T,U.source=I,!f.silent)throw U}function k(K){var U=K.exec(I);if(U){var Y=U[0];return M(Y),I=I.slice(Y.length),U}}function F(){k(n)}function L(K){var U;for(K=K||[];U=G();)U!==!1&&K.push(U);return K}function G(){var K=v();if(!(c!=I.charAt(0)||d!=I.charAt(1))){for(var U=2;u!=I.charAt(U)&&(d!=I.charAt(U)||c!=I.charAt(U+1));)++U;if(U+=2,u===I.charAt(U-1))return A("End of comment missing");var Y=I.slice(2,U-2);return T+=2,M(Y),I=I.slice(U),T+=2,K({type:p,comment:Y})}}function j(){var K=v(),U=k(r);if(U){if(G(),!k(s))return A("property missing ':'");var Y=k(i),te=K({type:h,property:g(U[0].replace(e,u)),value:Y?g(Y[0].replace(e,u)):u});return k(o),te}}function R(){var K=[];L(K);for(var U;U=j();)U!==!1&&(K.push(U),L(K));return K}return F(),R()}function g(I){return I?I.replace(a,u):u}return Pp=m,Pp}var gw;function rD(){if(gw)return wo;gw=1;var e=wo&&wo.__importDefault||function(r){return r&&r.__esModule?r:{default:r}};Object.defineProperty(wo,"__esModule",{value:!0}),wo.default=n;const t=e(nD());function n(r,s){let i=null;if(!r||typeof r!="string")return i;const o=(0,t.default)(r),a=typeof s=="function";return o.forEach(l=>{if(l.type!=="declaration")return;const{property:c,value:d}=l;a?s(c,d,l):d&&(i=i||{},i[c]=d)}),i}return wo}var Aa={},_w;function sD(){if(_w)return Aa;_w=1,Object.defineProperty(Aa,"__esModule",{value:!0}),Aa.camelCase=void 0;var e=/^--[a-zA-Z0-9_-]+$/,t=/-([a-z])/g,n=/^[^-]+$/,r=/^-(webkit|moz|ms|o|khtml)-/,s=/^-(ms)-/,i=function(c){return!c||n.test(c)||e.test(c)},o=function(c,d){return d.toUpperCase()},a=function(c,d){return"".concat(d,"-")},l=function(c,d){return d===void 0&&(d={}),i(c)?c:(c=c.toLowerCase(),d.reactCompat?c=c.replace(s,a):c=c.replace(r,a),c.replace(t,o))};return Aa.camelCase=l,Aa}var Ta,yw;function iD(){if(yw)return Ta;yw=1;var e=Ta&&Ta.__importDefault||function(s){return s&&s.__esModule?s:{default:s}},t=e(rD()),n=sD();function r(s,i){var o={};return!s||typeof s!="string"||(0,t.default)(s,function(a,l){a&&l&&(o[(0,n.camelCase)(a,i)]=l)}),o}return r.default=r,Ta=r,Ta}var oD=iD();const aD=AP(oD),$P=FP("end"),wf=FP("start");function FP(e){return t;function t(n){const r=n&&n.position&&n.position[e]||{};if(typeof r.line=="number"&&r.line>0&&typeof r.column=="number"&&r.column>0)return{line:r.line,column:r.column,offset:typeof r.offset=="number"&&r.offset>-1?r.offset:void 0}}}function lD(e){const t=wf(e),n=$P(e);if(t&&n)return{start:t,end:n}}function Za(e){return!e||typeof e!="object"?"":"position"in e||"type"in e?xw(e.position):"start"in e||"end"in e?xw(e):"line"in e||"column"in e?gm(e):""}function gm(e){return bw(e&&e.line)+":"+bw(e&&e.column)}function xw(e){return gm(e&&e.start)+"-"+gm(e&&e.end)}function bw(e){return e&&typeof e=="number"?e:1}class ur extends Error{constructor(t,n,r){super(),typeof n=="string"&&(r=n,n=void 0);let s="",i={},o=!1;if(n&&("line"in n&&"column"in n?i={place:n}:"start"in n&&"end"in n?i={place:n}:"type"in n?i={ancestors:[n],place:n.position}:i={...n}),typeof t=="string"?s=t:!i.cause&&t&&(o=!0,s=t.message,i.cause=t),!i.ruleId&&!i.source&&typeof r=="string"){const l=r.indexOf(":");l===-1?i.ruleId=r:(i.source=r.slice(0,l),i.ruleId=r.slice(l+1))}if(!i.place&&i.ancestors&&i.ancestors){const l=i.ancestors[i.ancestors.length-1];l&&(i.place=l.position)}const a=i.place&&"start"in i.place?i.place.start:i.place;this.ancestors=i.ancestors||void 0,this.cause=i.cause||void 0,this.column=a?a.column:void 0,this.fatal=void 0,this.file="",this.message=s,this.line=a?a.line:void 0,this.name=Za(i.place)||"1:1",this.place=i.place||void 0,this.reason=this.message,this.ruleId=i.ruleId||void 0,this.source=i.source||void 0,this.stack=o&&i.cause&&typeof i.cause.stack=="string"?i.cause.stack:"",this.actual=void 0,this.expected=void 0,this.note=void 0,this.url=void 0}}ur.prototype.file="";ur.prototype.name="";ur.prototype.reason="";ur.prototype.message="";ur.prototype.stack="";ur.prototype.column=void 0;ur.prototype.line=void 0;ur.prototype.ancestors=void 0;ur.prototype.cause=void 0;ur.prototype.fatal=void 0;ur.prototype.place=void 0;ur.prototype.ruleId=void 0;ur.prototype.source=void 0;const vf={}.hasOwnProperty,cD=new Map,uD=/[A-Z]/g,dD=new Set(["table","tbody","thead","tfoot","tr"]),pD=new Set(["td","th"]),OP="https://github.com/syntax-tree/hast-util-to-jsx-runtime";function hD(e,t){if(!t||t.Fragment===void 0)throw new TypeError("Expected `Fragment` in options");const n=t.filePath||void 0;let r;if(t.development){if(typeof t.jsxDEV!="function")throw new TypeError("Expected `jsxDEV` in options when `development: true`");r=wD(n,t.jsxDEV)}else{if(typeof t.jsx!="function")throw new TypeError("Expected `jsx` in production options");if(typeof t.jsxs!="function")throw new TypeError("Expected `jsxs` in production options");r=bD(n,t.jsx,t.jsxs)}const s={Fragment:t.Fragment,ancestors:[],components:t.components||{},create:r,elementAttributeNameCase:t.elementAttributeNameCase||"react",evaluater:t.createEvaluater?t.createEvaluater():void 0,filePath:n,ignoreInvalidStyle:t.ignoreInvalidStyle||!1,passKeys:t.passKeys!==!1,passNode:t.passNode||!1,schema:t.space==="svg"?bf:eD,stylePropertyNameCase:t.stylePropertyNameCase||"dom",tableCellAlignToStyle:t.tableCellAlignToStyle!==!1},i=jP(s,e,void 0);return i&&typeof i!="string"?i:s.create(e,s.Fragment,{children:i||void 0},void 0)}function jP(e,t,n){if(t.type==="element")return mD(e,t,n);if(t.type==="mdxFlowExpression"||t.type==="mdxTextExpression")return fD(e,t);if(t.type==="mdxJsxFlowElement"||t.type==="mdxJsxTextElement")return _D(e,t,n);if(t.type==="mdxjsEsm")return gD(e,t);if(t.type==="root")return yD(e,t,n);if(t.type==="text")return xD(e,t)}function mD(e,t,n){const r=e.schema;let s=r;t.tagName.toLowerCase()==="svg"&&r.space==="html"&&(s=bf,e.schema=s),e.ancestors.push(t);const i=NP(e,t.tagName,!1),o=vD(e,t);let a=Af(e,t);return dD.has(t.tagName)&&(a=a.filter(function(l){return typeof l=="string"?!UL(l):!0})),RP(e,o,i,t),Mf(o,a),e.ancestors.pop(),e.schema=r,e.create(t,i,o,n)}function fD(e,t){if(t.data&&t.data.estree&&e.evaluater){const r=t.data.estree.body[0];return r.type,e.evaluater.evaluateExpression(r.expression)}al(e,t.position)}function gD(e,t){if(t.data&&t.data.estree&&e.evaluater)return e.evaluater.evaluateProgram(t.data.estree);al(e,t.position)}function _D(e,t,n){const r=e.schema;let s=r;t.name==="svg"&&r.space==="html"&&(s=bf,e.schema=s),e.ancestors.push(t);const i=t.name===null?e.Fragment:NP(e,t.name,!0),o=MD(e,t),a=Af(e,t);return RP(e,o,i,t),Mf(o,a),e.ancestors.pop(),e.schema=r,e.create(t,i,o,n)}function yD(e,t,n){const r={};return Mf(r,Af(e,t)),e.create(t,e.Fragment,r,n)}function xD(e,t){return t.value}function RP(e,t,n,r){typeof n!="string"&&n!==e.Fragment&&e.passNode&&(t.node=r)}function Mf(e,t){if(t.length>0){const n=t.length>1?t:t[0];n&&(e.children=n)}}function bD(e,t,n){return r;function r(s,i,o,a){const c=Array.isArray(o.children)?n:t;return a?c(i,o,a):c(i,o)}}function wD(e,t){return n;function n(r,s,i,o){const a=Array.isArray(i.children),l=wf(r);return t(s,i,o,a,{columnNumber:l?l.column-1:void 0,fileName:e,lineNumber:l?l.line:void 0},void 0)}}function vD(e,t){const n={};let r,s;for(s in t.properties)if(s!=="children"&&vf.call(t.properties,s)){const i=AD(e,s,t.properties[s]);if(i){const[o,a]=i;e.tableCellAlignToStyle&&o==="align"&&typeof a=="string"&&pD.has(t.tagName)?r=a:n[o]=a}}if(r){const i=n.style||(n.style={});i[e.stylePropertyNameCase==="css"?"text-align":"textAlign"]=r}return n}function MD(e,t){const n={};for(const r of t.attributes)if(r.type==="mdxJsxExpressionAttribute")if(r.data&&r.data.estree&&e.evaluater){const i=r.data.estree.body[0];i.type;const o=i.expression;o.type;const a=o.properties[0];a.type,Object.assign(n,e.evaluater.evaluateExpression(a.argument))}else al(e,t.position);else{const s=r.name;let i;if(r.value&&typeof r.value=="object")if(r.value.data&&r.value.data.estree&&e.evaluater){const a=r.value.data.estree.body[0];a.type,i=e.evaluater.evaluateExpression(a.expression)}else al(e,t.position);else i=r.value===null?!0:r.value;n[s]=i}return n}function Af(e,t){const n=[];let r=-1;const s=e.passKeys?new Map:cD;for(;++r<t.children.length;){const i=t.children[r];let o;if(e.passKeys){const l=i.type==="element"?i.tagName:i.type==="mdxJsxFlowElement"||i.type==="mdxJsxTextElement"?i.name:void 0;if(l){const c=s.get(l)||0;o=l+"-"+c,s.set(l,c+1)}}const a=jP(e,i,o);a!==void 0&&n.push(a)}return n}function AD(e,t,n){const r=YL(e.schema,t);if(!(n==null||typeof n=="number"&&Number.isNaN(n))){if(Array.isArray(n)&&(n=r.commaSeparated?NL(n):tD(n)),r.property==="style"){let s=typeof n=="object"?n:TD(e,String(n));return e.stylePropertyNameCase==="css"&&(s=PD(s)),["style",s]}return[e.elementAttributeNameCase==="react"&&r.space?qL[r.property]||r.property:r.attribute,n]}}function TD(e,t){try{return aD(t,{reactCompat:!0})}catch(n){if(e.ignoreInvalidStyle)return{};const r=n,s=new ur("Cannot parse `style` attribute",{ancestors:e.ancestors,cause:r,ruleId:"style",source:"hast-util-to-jsx-runtime"});throw s.file=e.filePath||void 0,s.url=OP+"#cannot-parse-style-attribute",s}}function NP(e,t,n){let r;if(!n)r={type:"Literal",value:t};else if(t.includes(".")){const s=t.split(".");let i=-1,o;for(;++i<s.length;){const a=dw(s[i])?{type:"Identifier",name:s[i]}:{type:"Literal",value:s[i]};o=o?{type:"MemberExpression",object:o,property:a,computed:!!(i&&a.type==="Literal"),optional:!1}:a}r=o}else r=dw(t)&&!/^[a-z]/.test(t)?{type:"Identifier",name:t}:{type:"Literal",value:t};if(r.type==="Literal"){const s=r.value;return vf.call(e.components,s)?e.components[s]:s}if(e.evaluater)return e.evaluater.evaluateExpression(r);al(e)}function al(e,t){const n=new ur("Cannot handle MDX estrees without `createEvaluater`",{ancestors:e.ancestors,place:t,ruleId:"mdx-estree",source:"hast-util-to-jsx-runtime"});throw n.file=e.filePath||void 0,n.url=OP+"#cannot-handle-mdx-estrees-without-createevaluater",n}function PD(e){const t={};let n;for(n in e)vf.call(e,n)&&(t[CD(n)]=e[n]);return t}function CD(e){let t=e.replace(uD,ID);return t.slice(0,3)==="ms-"&&(t="-"+t),t}function ID(e){return"-"+e.toLowerCase()}const Cp={action:["form"],cite:["blockquote","del","ins","q"],data:["object"],formAction:["button","input"],href:["a","area","base","link"],icon:["menuitem"],itemId:null,manifest:["html"],ping:["a","area"],poster:["video"],src:["audio","embed","iframe","img","input","script","source","track","video"]},kD={};function Tf(e,t){const n=kD,r=typeof n.includeImageAlt=="boolean"?n.includeImageAlt:!0,s=typeof n.includeHtml=="boolean"?n.includeHtml:!0;return zP(e,r,s)}function zP(e,t,n){if(ED(e)){if("value"in e)return e.type==="html"&&!n?"":e.value;if(t&&"alt"in e&&e.alt)return e.alt;if("children"in e)return ww(e.children,t,n)}return Array.isArray(e)?ww(e,t,n):""}function ww(e,t,n){const r=[];let s=-1;for(;++s<e.length;)r[s]=zP(e[s],t,n);return r.join("")}function ED(e){return!!(e&&typeof e=="object")}const vw=document.createElement("i");function Pf(e){const t="&"+e+";";vw.innerHTML=t;const n=vw.textContent;return n.charCodeAt(n.length-1)===59&&e!=="semi"||n===t?!1:n}function Er(e,t,n,r){const s=e.length;let i=0,o;if(t<0?t=-t>s?0:s+t:t=t>s?s:t,n=n>0?n:0,r.length<1e4)o=Array.from(r),o.unshift(t,n),e.splice(...o);else for(n&&e.splice(t,n);i<r.length;)o=r.slice(i,i+1e4),o.unshift(t,0),e.splice(...o),i+=1e4,t+=1e4}function Gr(e,t){return e.length>0?(Er(e,e.length,0,t),e):t}const Mw={}.hasOwnProperty;function BP(e){const t={};let n=-1;for(;++n<e.length;)SD(t,e[n]);return t}function SD(e,t){let n;for(n in t){const s=(Mw.call(e,n)?e[n]:void 0)||(e[n]={}),i=t[n];let o;if(i)for(o in i){Mw.call(s,o)||(s[o]=[]);const a=i[o];LD(s[o],Array.isArray(a)?a:a?[a]:[])}}}function LD(e,t){let n=-1;const r=[];for(;++n<t.length;)(t[n].add==="after"?e:r).push(t[n]);Er(e,0,0,r)}function GP(e,t){const n=Number.parseInt(e,t);return n<9||n===11||n>13&&n<32||n>126&&n<160||n>55295&&n<57344||n>64975&&n<65008||(n&65535)===65535||(n&65535)===65534||n>1114111?"�":String.fromCodePoint(n)}function ls(e){return e.replace(/[\t\n\r ]+/g," ").replace(/^ | $/g,"").toLowerCase().toUpperCase()}const mr=di(/[A-Za-z]/),lr=di(/[\dA-Za-z]/),DD=di(/[#-'*+\--9=?A-Z^-~]/);function au(e){return e!==null&&(e<32||e===127)}const _m=di(/\d/),$D=di(/[\dA-Fa-f]/),FD=di(/[!-/:-@[-`{-~]/);function Tt(e){return e!==null&&e<-2}function cn(e){return e!==null&&(e<0||e===32)}function Wt(e){return e===-2||e===-1||e===32}const Nu=di(new RegExp("\\p{P}|\\p{S}","u")),Ui=di(/\s/);function di(e){return t;function t(n){return n!==null&&n>-1&&e.test(String.fromCharCode(n))}}function Qo(e){const t=[];let n=-1,r=0,s=0;for(;++n<e.length;){const i=e.charCodeAt(n);let o="";if(i===37&&lr(e.charCodeAt(n+1))&&lr(e.charCodeAt(n+2)))s=2;else if(i<128)/[!#$&-;=?-Z_a-z~]/.test(String.fromCharCode(i))||(o=String.fromCharCode(i));else if(i>55295&&i<57344){const a=e.charCodeAt(n+1);i<56320&&a>56319&&a<57344?(o=String.fromCharCode(i,a),s=1):o="�"}else o=String.fromCharCode(i);o&&(t.push(e.slice(r,n),encodeURIComponent(o)),r=n+s+1,o=""),s&&(n+=s,s=0)}return t.join("")+e.slice(r)}function qt(e,t,n,r){const s=r?r-1:Number.POSITIVE_INFINITY;let i=0;return o;function o(l){return Wt(l)?(e.enter(n),a(l)):t(l)}function a(l){return Wt(l)&&i++<s?(e.consume(l),a):(e.exit(n),t(l))}}const OD={tokenize:jD};function jD(e){const t=e.attempt(this.parser.constructs.contentInitial,r,s);let n;return t;function r(a){if(a===null){e.consume(a);return}return e.enter("lineEnding"),e.consume(a),e.exit("lineEnding"),qt(e,t,"linePrefix")}function s(a){return e.enter("paragraph"),i(a)}function i(a){const l=e.enter("chunkText",{contentType:"text",previous:n});return n&&(n.next=l),n=l,o(a)}function o(a){if(a===null){e.exit("chunkText"),e.exit("paragraph"),e.consume(a);return}return Tt(a)?(e.consume(a),e.exit("chunkText"),i):(e.consume(a),o)}}const RD={tokenize:ND},Aw={tokenize:zD};function ND(e){const t=this,n=[];let r=0,s,i,o;return a;function a(T){if(r<n.length){const M=n[r];return t.containerState=M[1],e.attempt(M[0].continuation,l,c)(T)}return c(T)}function l(T){if(r++,t.containerState._closeFlow){t.containerState._closeFlow=void 0,s&&_();const M=t.events.length;let v=M,b;for(;v--;)if(t.events[v][0]==="exit"&&t.events[v][1].type==="chunkFlow"){b=t.events[v][1].end;break}f(r);let A=M;for(;A<t.events.length;)t.events[A][1].end={...b},A++;return Er(t.events,v+1,0,t.events.slice(M)),t.events.length=A,c(T)}return a(T)}function c(T){if(r===n.length){if(!s)return p(T);if(s.currentConstruct&&s.currentConstruct.concrete)return m(T);t.interrupt=!!(s.currentConstruct&&!s._gfmTableDynamicInterruptHack)}return t.containerState={},e.check(Aw,d,u)(T)}function d(T){return s&&_(),f(r),p(T)}function u(T){return t.parser.lazy[t.now().line]=r!==n.length,o=t.now().offset,m(T)}function p(T){return t.containerState={},e.attempt(Aw,h,m)(T)}function h(T){return r++,n.push([t.currentConstruct,t.containerState]),p(T)}function m(T){if(T===null){s&&_(),f(0),e.consume(T);return}return s=s||t.parser.flow(t.now()),e.enter("chunkFlow",{_tokenizer:s,contentType:"flow",previous:i}),g(T)}function g(T){if(T===null){I(e.exit("chunkFlow"),!0),f(0),e.consume(T);return}return Tt(T)?(e.consume(T),I(e.exit("chunkFlow")),r=0,t.interrupt=void 0,a):(e.consume(T),g)}function I(T,M){const v=t.sliceStream(T);if(M&&v.push(null),T.previous=i,i&&(i.next=T),i=T,s.defineSkip(T.start),s.write(v),t.parser.lazy[T.start.line]){let b=s.events.length;for(;b--;)if(s.events[b][1].start.offset<o&&(!s.events[b][1].end||s.events[b][1].end.offset>o))return;const A=t.events.length;let k=A,F,L;for(;k--;)if(t.events[k][0]==="exit"&&t.events[k][1].type==="chunkFlow"){if(F){L=t.events[k][1].end;break}F=!0}for(f(r),b=A;b<t.events.length;)t.events[b][1].end={...L},b++;Er(t.events,k+1,0,t.events.slice(A)),t.events.length=b}}function f(T){let M=n.length;for(;M-- >T;){const v=n[M];t.containerState=v[1],v[0].exit.call(t,e)}n.length=T}function _(){s.write([null]),i=void 0,s=void 0,t.containerState._closeFlow=void 0}}function zD(e,t,n){return qt(e,e.attempt(this.parser.constructs.document,t,n),"linePrefix",this.parser.constructs.disable.null.includes("codeIndented")?void 0:4)}function Bo(e){if(e===null||cn(e)||Ui(e))return 1;if(Nu(e))return 2}function zu(e,t,n){const r=[];let s=-1;for(;++s<e.length;){const i=e[s].resolveAll;i&&!r.includes(i)&&(t=i(t,n),r.push(i))}return t}const ym={name:"attention",resolveAll:BD,tokenize:GD};function BD(e,t){let n=-1,r,s,i,o,a,l,c,d;for(;++n<e.length;)if(e[n][0]==="enter"&&e[n][1].type==="attentionSequence"&&e[n][1]._close){for(r=n;r--;)if(e[r][0]==="exit"&&e[r][1].type==="attentionSequence"&&e[r][1]._open&&t.sliceSerialize(e[r][1]).charCodeAt(0)===t.sliceSerialize(e[n][1]).charCodeAt(0)){if((e[r][1]._close||e[n][1]._open)&&(e[n][1].end.offset-e[n][1].start.offset)%3&&!((e[r][1].end.offset-e[r][1].start.offset+e[n][1].end.offset-e[n][1].start.offset)%3))continue;l=e[r][1].end.offset-e[r][1].start.offset>1&&e[n][1].end.offset-e[n][1].start.offset>1?2:1;const u={...e[r][1].end},p={...e[n][1].start};Tw(u,-l),Tw(p,l),o={type:l>1?"strongSequence":"emphasisSequence",start:u,end:{...e[r][1].end}},a={type:l>1?"strongSequence":"emphasisSequence",start:{...e[n][1].start},end:p},i={type:l>1?"strongText":"emphasisText",start:{...e[r][1].end},end:{...e[n][1].start}},s={type:l>1?"strong":"emphasis",start:{...o.start},end:{...a.end}},e[r][1].end={...o.start},e[n][1].start={...a.end},c=[],e[r][1].end.offset-e[r][1].start.offset&&(c=Gr(c,[["enter",e[r][1],t],["exit",e[r][1],t]])),c=Gr(c,[["enter",s,t],["enter",o,t],["exit",o,t],["enter",i,t]]),c=Gr(c,zu(t.parser.constructs.insideSpan.null,e.slice(r+1,n),t)),c=Gr(c,[["exit",i,t],["enter",a,t],["exit",a,t],["exit",s,t]]),e[n][1].end.offset-e[n][1].start.offset?(d=2,c=Gr(c,[["enter",e[n][1],t],["exit",e[n][1],t]])):d=0,Er(e,r-1,n-r+3,c),n=r+c.length-d-2;break}}for(n=-1;++n<e.length;)e[n][1].type==="attentionSequence"&&(e[n][1].type="data");return e}function GD(e,t){const n=this.parser.constructs.attentionMarkers.null,r=this.previous,s=Bo(r);let i;return o;function o(l){return i=l,e.enter("attentionSequence"),a(l)}function a(l){if(l===i)return e.consume(l),a;const c=e.exit("attentionSequence"),d=Bo(l),u=!d||d===2&&s||n.includes(l),p=!s||s===2&&d||n.includes(r);return c._open=!!(i===42?u:u&&(s||!p)),c._close=!!(i===42?p:p&&(d||!u)),t(l)}}function Tw(e,t){e.column+=t,e.offset+=t,e._bufferIndex+=t}const VD={name:"autolink",tokenize:UD};function UD(e,t,n){let r=0;return s;function s(h){return e.enter("autolink"),e.enter("autolinkMarker"),e.consume(h),e.exit("autolinkMarker"),e.enter("autolinkProtocol"),i}function i(h){return mr(h)?(e.consume(h),o):h===64?n(h):c(h)}function o(h){return h===43||h===45||h===46||lr(h)?(r=1,a(h)):c(h)}function a(h){return h===58?(e.consume(h),r=0,l):(h===43||h===45||h===46||lr(h))&&r++<32?(e.consume(h),a):(r=0,c(h))}function l(h){return h===62?(e.exit("autolinkProtocol"),e.enter("autolinkMarker"),e.consume(h),e.exit("autolinkMarker"),e.exit("autolink"),t):h===null||h===32||h===60||au(h)?n(h):(e.consume(h),l)}function c(h){return h===64?(e.consume(h),d):DD(h)?(e.consume(h),c):n(h)}function d(h){return lr(h)?u(h):n(h)}function u(h){return h===46?(e.consume(h),r=0,d):h===62?(e.exit("autolinkProtocol").type="autolinkEmail",e.enter("autolinkMarker"),e.consume(h),e.exit("autolinkMarker"),e.exit("autolink"),t):p(h)}function p(h){if((h===45||lr(h))&&r++<63){const m=h===45?p:u;return e.consume(h),m}return n(h)}}const wl={partial:!0,tokenize:WD};function WD(e,t,n){return r;function r(i){return Wt(i)?qt(e,s,"linePrefix")(i):s(i)}function s(i){return i===null||Tt(i)?t(i):n(i)}}const VP={continuation:{tokenize:KD},exit:qD,name:"blockQuote",tokenize:HD};function HD(e,t,n){const r=this;return s;function s(o){if(o===62){const a=r.containerState;return a.open||(e.enter("blockQuote",{_container:!0}),a.open=!0),e.enter("blockQuotePrefix"),e.enter("blockQuoteMarker"),e.consume(o),e.exit("blockQuoteMarker"),i}return n(o)}function i(o){return Wt(o)?(e.enter("blockQuotePrefixWhitespace"),e.consume(o),e.exit("blockQuotePrefixWhitespace"),e.exit("blockQuotePrefix"),t):(e.exit("blockQuotePrefix"),t(o))}}function KD(e,t,n){const r=this;return s;function s(o){return Wt(o)?qt(e,i,"linePrefix",r.parser.constructs.disable.null.includes("codeIndented")?void 0:4)(o):i(o)}function i(o){return e.attempt(VP,t,n)(o)}}function qD(e){e.exit("blockQuote")}const UP={name:"characterEscape",tokenize:QD};function QD(e,t,n){return r;function r(i){return e.enter("characterEscape"),e.enter("escapeMarker"),e.consume(i),e.exit("escapeMarker"),s}function s(i){return FD(i)?(e.enter("characterEscapeValue"),e.consume(i),e.exit("characterEscapeValue"),e.exit("characterEscape"),t):n(i)}}const WP={name:"characterReference",tokenize:XD};function XD(e,t,n){const r=this;let s=0,i,o;return a;function a(u){return e.enter("characterReference"),e.enter("characterReferenceMarker"),e.consume(u),e.exit("characterReferenceMarker"),l}function l(u){return u===35?(e.enter("characterReferenceMarkerNumeric"),e.consume(u),e.exit("characterReferenceMarkerNumeric"),c):(e.enter("characterReferenceValue"),i=31,o=lr,d(u))}function c(u){return u===88||u===120?(e.enter("characterReferenceMarkerHexadecimal"),e.consume(u),e.exit("characterReferenceMarkerHexadecimal"),e.enter("characterReferenceValue"),i=6,o=$D,d):(e.enter("characterReferenceValue"),i=7,o=_m,d(u))}function d(u){if(u===59&&s){const p=e.exit("characterReferenceValue");return o===lr&&!Pf(r.sliceSerialize(p))?n(u):(e.enter("characterReferenceMarker"),e.consume(u),e.exit("characterReferenceMarker"),e.exit("characterReference"),t)}return o(u)&&s++<i?(e.consume(u),d):n(u)}}const Pw={partial:!0,tokenize:JD},Cw={concrete:!0,name:"codeFenced",tokenize:YD};function YD(e,t,n){const r=this,s={partial:!0,tokenize:v};let i=0,o=0,a;return l;function l(b){return c(b)}function c(b){const A=r.events[r.events.length-1];return i=A&&A[1].type==="linePrefix"?A[2].sliceSerialize(A[1],!0).length:0,a=b,e.enter("codeFenced"),e.enter("codeFencedFence"),e.enter("codeFencedFenceSequence"),d(b)}function d(b){return b===a?(o++,e.consume(b),d):o<3?n(b):(e.exit("codeFencedFenceSequence"),Wt(b)?qt(e,u,"whitespace")(b):u(b))}function u(b){return b===null||Tt(b)?(e.exit("codeFencedFence"),r.interrupt?t(b):e.check(Pw,g,M)(b)):(e.enter("codeFencedFenceInfo"),e.enter("chunkString",{contentType:"string"}),p(b))}function p(b){return b===null||Tt(b)?(e.exit("chunkString"),e.exit("codeFencedFenceInfo"),u(b)):Wt(b)?(e.exit("chunkString"),e.exit("codeFencedFenceInfo"),qt(e,h,"whitespace")(b)):b===96&&b===a?n(b):(e.consume(b),p)}function h(b){return b===null||Tt(b)?u(b):(e.enter("codeFencedFenceMeta"),e.enter("chunkString",{contentType:"string"}),m(b))}function m(b){return b===null||Tt(b)?(e.exit("chunkString"),e.exit("codeFencedFenceMeta"),u(b)):b===96&&b===a?n(b):(e.consume(b),m)}function g(b){return e.attempt(s,M,I)(b)}function I(b){return e.enter("lineEnding"),e.consume(b),e.exit("lineEnding"),f}function f(b){return i>0&&Wt(b)?qt(e,_,"linePrefix",i+1)(b):_(b)}function _(b){return b===null||Tt(b)?e.check(Pw,g,M)(b):(e.enter("codeFlowValue"),T(b))}function T(b){return b===null||Tt(b)?(e.exit("codeFlowValue"),_(b)):(e.consume(b),T)}function M(b){return e.exit("codeFenced"),t(b)}function v(b,A,k){let F=0;return L;function L(U){return b.enter("lineEnding"),b.consume(U),b.exit("lineEnding"),G}function G(U){return b.enter("codeFencedFence"),Wt(U)?qt(b,j,"linePrefix",r.parser.constructs.disable.null.includes("codeIndented")?void 0:4)(U):j(U)}function j(U){return U===a?(b.enter("codeFencedFenceSequence"),R(U)):k(U)}function R(U){return U===a?(F++,b.consume(U),R):F>=o?(b.exit("codeFencedFenceSequence"),Wt(U)?qt(b,K,"whitespace")(U):K(U)):k(U)}function K(U){return U===null||Tt(U)?(b.exit("codeFencedFence"),A(U)):k(U)}}}function JD(e,t,n){const r=this;return s;function s(o){return o===null?n(o):(e.enter("lineEnding"),e.consume(o),e.exit("lineEnding"),i)}function i(o){return r.parser.lazy[r.now().line]?n(o):t(o)}}const Ip={name:"codeIndented",tokenize:e$},ZD={partial:!0,tokenize:t$};function e$(e,t,n){const r=this;return s;function s(c){return e.enter("codeIndented"),qt(e,i,"linePrefix",5)(c)}function i(c){const d=r.events[r.events.length-1];return d&&d[1].type==="linePrefix"&&d[2].sliceSerialize(d[1],!0).length>=4?o(c):n(c)}function o(c){return c===null?l(c):Tt(c)?e.attempt(ZD,o,l)(c):(e.enter("codeFlowValue"),a(c))}function a(c){return c===null||Tt(c)?(e.exit("codeFlowValue"),o(c)):(e.consume(c),a)}function l(c){return e.exit("codeIndented"),t(c)}}function t$(e,t,n){const r=this;return s;function s(o){return r.parser.lazy[r.now().line]?n(o):Tt(o)?(e.enter("lineEnding"),e.consume(o),e.exit("lineEnding"),s):qt(e,i,"linePrefix",5)(o)}function i(o){const a=r.events[r.events.length-1];return a&&a[1].type==="linePrefix"&&a[2].sliceSerialize(a[1],!0).length>=4?t(o):Tt(o)?s(o):n(o)}}const n$={name:"codeText",previous:s$,resolve:r$,tokenize:i$};function r$(e){let t=e.length-4,n=3,r,s;if((e[n][1].type==="lineEnding"||e[n][1].type==="space")&&(e[t][1].type==="lineEnding"||e[t][1].type==="space")){for(r=n;++r<t;)if(e[r][1].type==="codeTextData"){e[n][1].type="codeTextPadding",e[t][1].type="codeTextPadding",n+=2,t-=2;break}}for(r=n-1,t++;++r<=t;)s===void 0?r!==t&&e[r][1].type!=="lineEnding"&&(s=r):(r===t||e[r][1].type==="lineEnding")&&(e[s][1].type="codeTextData",r!==s+2&&(e[s][1].end=e[r-1][1].end,e.splice(s+2,r-s-2),t-=r-s-2,r=s+2),s=void 0);return e}function s$(e){return e!==96||this.events[this.events.length-1][1].type==="characterEscape"}function i$(e,t,n){let r=0,s,i;return o;function o(u){return e.enter("codeText"),e.enter("codeTextSequence"),a(u)}function a(u){return u===96?(e.consume(u),r++,a):(e.exit("codeTextSequence"),l(u))}function l(u){return u===null?n(u):u===32?(e.enter("space"),e.consume(u),e.exit("space"),l):u===96?(i=e.enter("codeTextSequence"),s=0,d(u)):Tt(u)?(e.enter("lineEnding"),e.consume(u),e.exit("lineEnding"),l):(e.enter("codeTextData"),c(u))}function c(u){return u===null||u===32||u===96||Tt(u)?(e.exit("codeTextData"),l(u)):(e.consume(u),c)}function d(u){return u===96?(e.consume(u),s++,d):s===r?(e.exit("codeTextSequence"),e.exit("codeText"),t(u)):(i.type="codeTextData",c(u))}}class o${constructor(t){this.left=t?[...t]:[],this.right=[]}get(t){if(t<0||t>=this.left.length+this.right.length)throw new RangeError("Cannot access index `"+t+"` in a splice buffer of size `"+(this.left.length+this.right.length)+"`");return t<this.left.length?this.left[t]:this.right[this.right.length-t+this.left.length-1]}get length(){return this.left.length+this.right.length}shift(){return this.setCursor(0),this.right.pop()}slice(t,n){const r=n??Number.POSITIVE_INFINITY;return r<this.left.length?this.left.slice(t,r):t>this.left.length?this.right.slice(this.right.length-r+this.left.length,this.right.length-t+this.left.length).reverse():this.left.slice(t).concat(this.right.slice(this.right.length-r+this.left.length).reverse())}splice(t,n,r){const s=n||0;this.setCursor(Math.trunc(t));const i=this.right.splice(this.right.length-s,Number.POSITIVE_INFINITY);return r&&Pa(this.left,r),i.reverse()}pop(){return this.setCursor(Number.POSITIVE_INFINITY),this.left.pop()}push(t){this.setCursor(Number.POSITIVE_INFINITY),this.left.push(t)}pushMany(t){this.setCursor(Number.POSITIVE_INFINITY),Pa(this.left,t)}unshift(t){this.setCursor(0),this.right.push(t)}unshiftMany(t){this.setCursor(0),Pa(this.right,t.reverse())}setCursor(t){if(!(t===this.left.length||t>this.left.length&&this.right.length===0||t<0&&this.left.length===0))if(t<this.left.length){const n=this.left.splice(t,Number.POSITIVE_INFINITY);Pa(this.right,n.reverse())}else{const n=this.right.splice(this.left.length+this.right.length-t,Number.POSITIVE_INFINITY);Pa(this.left,n.reverse())}}}function Pa(e,t){let n=0;if(t.length<1e4)e.push(...t);else for(;n<t.length;)e.push(...t.slice(n,n+1e4)),n+=1e4}function HP(e){const t={};let n=-1,r,s,i,o,a,l,c;const d=new o$(e);for(;++n<d.length;){for(;n in t;)n=t[n];if(r=d.get(n),n&&r[1].type==="chunkFlow"&&d.get(n-1)[1].type==="listItemPrefix"&&(l=r[1]._tokenizer.events,i=0,i<l.length&&l[i][1].type==="lineEndingBlank"&&(i+=2),i<l.length&&l[i][1].type==="content"))for(;++i<l.length&&l[i][1].type!=="content";)l[i][1].type==="chunkText"&&(l[i][1]._isInFirstContentOfListItem=!0,i++);if(r[0]==="enter")r[1].contentType&&(Object.assign(t,a$(d,n)),n=t[n],c=!0);else if(r[1]._container){for(i=n,s=void 0;i--;)if(o=d.get(i),o[1].type==="lineEnding"||o[1].type==="lineEndingBlank")o[0]==="enter"&&(s&&(d.get(s)[1].type="lineEndingBlank"),o[1].type="lineEnding",s=i);else if(!(o[1].type==="linePrefix"||o[1].type==="listItemIndent"))break;s&&(r[1].end={...d.get(s)[1].start},a=d.slice(s,n),a.unshift(r),d.splice(s,n-s+1,a))}}return Er(e,0,Number.POSITIVE_INFINITY,d.slice(0)),!c}function a$(e,t){const n=e.get(t)[1],r=e.get(t)[2];let s=t-1;const i=[];let o=n._tokenizer;o||(o=r.parser[n.contentType](n.start),n._contentTypeTextTrailing&&(o._contentTypeTextTrailing=!0));const a=o.events,l=[],c={};let d,u,p=-1,h=n,m=0,g=0;const I=[g];for(;h;){for(;e.get(++s)[1]!==h;);i.push(s),h._tokenizer||(d=r.sliceStream(h),h.next||d.push(null),u&&o.defineSkip(h.start),h._isInFirstContentOfListItem&&(o._gfmTasklistFirstContentOfListItem=!0),o.write(d),h._isInFirstContentOfListItem&&(o._gfmTasklistFirstContentOfListItem=void 0)),u=h,h=h.next}for(h=n;++p<a.length;)a[p][0]==="exit"&&a[p-1][0]==="enter"&&a[p][1].type===a[p-1][1].type&&a[p][1].start.line!==a[p][1].end.line&&(g=p+1,I.push(g),h._tokenizer=void 0,h.previous=void 0,h=h.next);for(o.events=[],h?(h._tokenizer=void 0,h.previous=void 0):I.pop(),p=I.length;p--;){const f=a.slice(I[p],I[p+1]),_=i.pop();l.push([_,_+f.length-1]),e.splice(_,2,f)}for(l.reverse(),p=-1;++p<l.length;)c[m+l[p][0]]=m+l[p][1],m+=l[p][1]-l[p][0]-1;return c}const l$={resolve:u$,tokenize:d$},c$={partial:!0,tokenize:p$};function u$(e){return HP(e),e}function d$(e,t){let n;return r;function r(a){return e.enter("content"),n=e.enter("chunkContent",{contentType:"content"}),s(a)}function s(a){return a===null?i(a):Tt(a)?e.check(c$,o,i)(a):(e.consume(a),s)}function i(a){return e.exit("chunkContent"),e.exit("content"),t(a)}function o(a){return e.consume(a),e.exit("chunkContent"),n.next=e.enter("chunkContent",{contentType:"content",previous:n}),n=n.next,s}}function p$(e,t,n){const r=this;return s;function s(o){return e.exit("chunkContent"),e.enter("lineEnding"),e.consume(o),e.exit("lineEnding"),qt(e,i,"linePrefix")}function i(o){if(o===null||Tt(o))return n(o);const a=r.events[r.events.length-1];return!r.parser.constructs.disable.null.includes("codeIndented")&&a&&a[1].type==="linePrefix"&&a[2].sliceSerialize(a[1],!0).length>=4?t(o):e.interrupt(r.parser.constructs.flow,n,t)(o)}}function KP(e,t,n,r,s,i,o,a,l){const c=l||Number.POSITIVE_INFINITY;let d=0;return u;function u(f){return f===60?(e.enter(r),e.enter(s),e.enter(i),e.consume(f),e.exit(i),p):f===null||f===32||f===41||au(f)?n(f):(e.enter(r),e.enter(o),e.enter(a),e.enter("chunkString",{contentType:"string"}),g(f))}function p(f){return f===62?(e.enter(i),e.consume(f),e.exit(i),e.exit(s),e.exit(r),t):(e.enter(a),e.enter("chunkString",{contentType:"string"}),h(f))}function h(f){return f===62?(e.exit("chunkString"),e.exit(a),p(f)):f===null||f===60||Tt(f)?n(f):(e.consume(f),f===92?m:h)}function m(f){return f===60||f===62||f===92?(e.consume(f),h):h(f)}function g(f){return!d&&(f===null||f===41||cn(f))?(e.exit("chunkString"),e.exit(a),e.exit(o),e.exit(r),t(f)):d<c&&f===40?(e.consume(f),d++,g):f===41?(e.consume(f),d--,g):f===null||f===32||f===40||au(f)?n(f):(e.consume(f),f===92?I:g)}function I(f){return f===40||f===41||f===92?(e.consume(f),g):g(f)}}function qP(e,t,n,r,s,i){const o=this;let a=0,l;return c;function c(h){return e.enter(r),e.enter(s),e.consume(h),e.exit(s),e.enter(i),d}function d(h){return a>999||h===null||h===91||h===93&&!l||h===94&&!a&&"_hiddenFootnoteSupport"in o.parser.constructs?n(h):h===93?(e.exit(i),e.enter(s),e.consume(h),e.exit(s),e.exit(r),t):Tt(h)?(e.enter("lineEnding"),e.consume(h),e.exit("lineEnding"),d):(e.enter("chunkString",{contentType:"string"}),u(h))}function u(h){return h===null||h===91||h===93||Tt(h)||a++>999?(e.exit("chunkString"),d(h)):(e.consume(h),l||(l=!Wt(h)),h===92?p:u)}function p(h){return h===91||h===92||h===93?(e.consume(h),a++,u):u(h)}}function QP(e,t,n,r,s,i){let o;return a;function a(p){return p===34||p===39||p===40?(e.enter(r),e.enter(s),e.consume(p),e.exit(s),o=p===40?41:p,l):n(p)}function l(p){return p===o?(e.enter(s),e.consume(p),e.exit(s),e.exit(r),t):(e.enter(i),c(p))}function c(p){return p===o?(e.exit(i),l(o)):p===null?n(p):Tt(p)?(e.enter("lineEnding"),e.consume(p),e.exit("lineEnding"),qt(e,c,"linePrefix")):(e.enter("chunkString",{contentType:"string"}),d(p))}function d(p){return p===o||p===null||Tt(p)?(e.exit("chunkString"),c(p)):(e.consume(p),p===92?u:d)}function u(p){return p===o||p===92?(e.consume(p),d):d(p)}}function el(e,t){let n;return r;function r(s){return Tt(s)?(e.enter("lineEnding"),e.consume(s),e.exit("lineEnding"),n=!0,r):Wt(s)?qt(e,r,n?"linePrefix":"lineSuffix")(s):t(s)}}const h$={name:"definition",tokenize:f$},m$={partial:!0,tokenize:g$};function f$(e,t,n){const r=this;let s;return i;function i(h){return e.enter("definition"),o(h)}function o(h){return qP.call(r,e,a,n,"definitionLabel","definitionLabelMarker","definitionLabelString")(h)}function a(h){return s=ls(r.sliceSerialize(r.events[r.events.length-1][1]).slice(1,-1)),h===58?(e.enter("definitionMarker"),e.consume(h),e.exit("definitionMarker"),l):n(h)}function l(h){return cn(h)?el(e,c)(h):c(h)}function c(h){return KP(e,d,n,"definitionDestination","definitionDestinationLiteral","definitionDestinationLiteralMarker","definitionDestinationRaw","definitionDestinationString")(h)}function d(h){return e.attempt(m$,u,u)(h)}function u(h){return Wt(h)?qt(e,p,"whitespace")(h):p(h)}function p(h){return h===null||Tt(h)?(e.exit("definition"),r.parser.defined.push(s),t(h)):n(h)}}function g$(e,t,n){return r;function r(a){return cn(a)?el(e,s)(a):n(a)}function s(a){return QP(e,i,n,"definitionTitle","definitionTitleMarker","definitionTitleString")(a)}function i(a){return Wt(a)?qt(e,o,"whitespace")(a):o(a)}function o(a){return a===null||Tt(a)?t(a):n(a)}}const _$={name:"hardBreakEscape",tokenize:y$};function y$(e,t,n){return r;function r(i){return e.enter("hardBreakEscape"),e.consume(i),s}function s(i){return Tt(i)?(e.exit("hardBreakEscape"),t(i)):n(i)}}const x$={name:"headingAtx",resolve:b$,tokenize:w$};function b$(e,t){let n=e.length-2,r=3,s,i;return e[r][1].type==="whitespace"&&(r+=2),n-2>r&&e[n][1].type==="whitespace"&&(n-=2),e[n][1].type==="atxHeadingSequence"&&(r===n-1||n-4>r&&e[n-2][1].type==="whitespace")&&(n-=r+1===n?2:4),n>r&&(s={type:"atxHeadingText",start:e[r][1].start,end:e[n][1].end},i={type:"chunkText",start:e[r][1].start,end:e[n][1].end,contentType:"text"},Er(e,r,n-r+1,[["enter",s,t],["enter",i,t],["exit",i,t],["exit",s,t]])),e}function w$(e,t,n){let r=0;return s;function s(d){return e.enter("atxHeading"),i(d)}function i(d){return e.enter("atxHeadingSequence"),o(d)}function o(d){return d===35&&r++<6?(e.consume(d),o):d===null||cn(d)?(e.exit("atxHeadingSequence"),a(d)):n(d)}function a(d){return d===35?(e.enter("atxHeadingSequence"),l(d)):d===null||Tt(d)?(e.exit("atxHeading"),t(d)):Wt(d)?qt(e,a,"whitespace")(d):(e.enter("atxHeadingText"),c(d))}function l(d){return d===35?(e.consume(d),l):(e.exit("atxHeadingSequence"),a(d))}function c(d){return d===null||d===35||cn(d)?(e.exit("atxHeadingText"),a(d)):(e.consume(d),c)}}const v$=["address","article","aside","base","basefont","blockquote","body","caption","center","col","colgroup","dd","details","dialog","dir","div","dl","dt","fieldset","figcaption","figure","footer","form","frame","frameset","h1","h2","h3","h4","h5","h6","head","header","hr","html","iframe","legend","li","link","main","menu","menuitem","nav","noframes","ol","optgroup","option","p","param","search","section","summary","table","tbody","td","tfoot","th","thead","title","tr","track","ul"],Iw=["pre","script","style","textarea"],M$={concrete:!0,name:"htmlFlow",resolveTo:P$,tokenize:C$},A$={partial:!0,tokenize:k$},T$={partial:!0,tokenize:I$};function P$(e){let t=e.length;for(;t--&&!(e[t][0]==="enter"&&e[t][1].type==="htmlFlow"););return t>1&&e[t-2][1].type==="linePrefix"&&(e[t][1].start=e[t-2][1].start,e[t+1][1].start=e[t-2][1].start,e.splice(t-2,2)),e}function C$(e,t,n){const r=this;let s,i,o,a,l;return c;function c(D){return d(D)}function d(D){return e.enter("htmlFlow"),e.enter("htmlFlowData"),e.consume(D),u}function u(D){return D===33?(e.consume(D),p):D===47?(e.consume(D),i=!0,g):D===63?(e.consume(D),s=3,r.interrupt?t:N):mr(D)?(e.consume(D),o=String.fromCharCode(D),I):n(D)}function p(D){return D===45?(e.consume(D),s=2,h):D===91?(e.consume(D),s=5,a=0,m):mr(D)?(e.consume(D),s=4,r.interrupt?t:N):n(D)}function h(D){return D===45?(e.consume(D),r.interrupt?t:N):n(D)}function m(D){const z="CDATA[";return D===z.charCodeAt(a++)?(e.consume(D),a===z.length?r.interrupt?t:j:m):n(D)}function g(D){return mr(D)?(e.consume(D),o=String.fromCharCode(D),I):n(D)}function I(D){if(D===null||D===47||D===62||cn(D)){const z=D===47,se=o.toLowerCase();return!z&&!i&&Iw.includes(se)?(s=1,r.interrupt?t(D):j(D)):v$.includes(o.toLowerCase())?(s=6,z?(e.consume(D),f):r.interrupt?t(D):j(D)):(s=7,r.interrupt&&!r.parser.lazy[r.now().line]?n(D):i?_(D):T(D))}return D===45||lr(D)?(e.consume(D),o+=String.fromCharCode(D),I):n(D)}function f(D){return D===62?(e.consume(D),r.interrupt?t:j):n(D)}function _(D){return Wt(D)?(e.consume(D),_):L(D)}function T(D){return D===47?(e.consume(D),L):D===58||D===95||mr(D)?(e.consume(D),M):Wt(D)?(e.consume(D),T):L(D)}function M(D){return D===45||D===46||D===58||D===95||lr(D)?(e.consume(D),M):v(D)}function v(D){return D===61?(e.consume(D),b):Wt(D)?(e.consume(D),v):T(D)}function b(D){return D===null||D===60||D===61||D===62||D===96?n(D):D===34||D===39?(e.consume(D),l=D,A):Wt(D)?(e.consume(D),b):k(D)}function A(D){return D===l?(e.consume(D),l=null,F):D===null||Tt(D)?n(D):(e.consume(D),A)}function k(D){return D===null||D===34||D===39||D===47||D===60||D===61||D===62||D===96||cn(D)?v(D):(e.consume(D),k)}function F(D){return D===47||D===62||Wt(D)?T(D):n(D)}function L(D){return D===62?(e.consume(D),G):n(D)}function G(D){return D===null||Tt(D)?j(D):Wt(D)?(e.consume(D),G):n(D)}function j(D){return D===45&&s===2?(e.consume(D),Y):D===60&&s===1?(e.consume(D),te):D===62&&s===4?(e.consume(D),oe):D===63&&s===3?(e.consume(D),N):D===93&&s===5?(e.consume(D),le):Tt(D)&&(s===6||s===7)?(e.exit("htmlFlowData"),e.check(A$,X,R)(D)):D===null||Tt(D)?(e.exit("htmlFlowData"),R(D)):(e.consume(D),j)}function R(D){return e.check(T$,K,X)(D)}function K(D){return e.enter("lineEnding"),e.consume(D),e.exit("lineEnding"),U}function U(D){return D===null||Tt(D)?R(D):(e.enter("htmlFlowData"),j(D))}function Y(D){return D===45?(e.consume(D),N):j(D)}function te(D){return D===47?(e.consume(D),o="",ne):j(D)}function ne(D){if(D===62){const z=o.toLowerCase();return Iw.includes(z)?(e.consume(D),oe):j(D)}return mr(D)&&o.length<8?(e.consume(D),o+=String.fromCharCode(D),ne):j(D)}function le(D){return D===93?(e.consume(D),N):j(D)}function N(D){return D===62?(e.consume(D),oe):D===45&&s===2?(e.consume(D),N):j(D)}function oe(D){return D===null||Tt(D)?(e.exit("htmlFlowData"),X(D)):(e.consume(D),oe)}function X(D){return e.exit("htmlFlow"),t(D)}}function I$(e,t,n){const r=this;return s;function s(o){return Tt(o)?(e.enter("lineEnding"),e.consume(o),e.exit("lineEnding"),i):n(o)}function i(o){return r.parser.lazy[r.now().line]?n(o):t(o)}}function k$(e,t,n){return r;function r(s){return e.enter("lineEnding"),e.consume(s),e.exit("lineEnding"),e.attempt(wl,t,n)}}const E$={name:"htmlText",tokenize:S$};function S$(e,t,n){const r=this;let s,i,o;return a;function a(N){return e.enter("htmlText"),e.enter("htmlTextData"),e.consume(N),l}function l(N){return N===33?(e.consume(N),c):N===47?(e.consume(N),v):N===63?(e.consume(N),T):mr(N)?(e.consume(N),k):n(N)}function c(N){return N===45?(e.consume(N),d):N===91?(e.consume(N),i=0,m):mr(N)?(e.consume(N),_):n(N)}function d(N){return N===45?(e.consume(N),h):n(N)}function u(N){return N===null?n(N):N===45?(e.consume(N),p):Tt(N)?(o=u,te(N)):(e.consume(N),u)}function p(N){return N===45?(e.consume(N),h):u(N)}function h(N){return N===62?Y(N):N===45?p(N):u(N)}function m(N){const oe="CDATA[";return N===oe.charCodeAt(i++)?(e.consume(N),i===oe.length?g:m):n(N)}function g(N){return N===null?n(N):N===93?(e.consume(N),I):Tt(N)?(o=g,te(N)):(e.consume(N),g)}function I(N){return N===93?(e.consume(N),f):g(N)}function f(N){return N===62?Y(N):N===93?(e.consume(N),f):g(N)}function _(N){return N===null||N===62?Y(N):Tt(N)?(o=_,te(N)):(e.consume(N),_)}function T(N){return N===null?n(N):N===63?(e.consume(N),M):Tt(N)?(o=T,te(N)):(e.consume(N),T)}function M(N){return N===62?Y(N):T(N)}function v(N){return mr(N)?(e.consume(N),b):n(N)}function b(N){return N===45||lr(N)?(e.consume(N),b):A(N)}function A(N){return Tt(N)?(o=A,te(N)):Wt(N)?(e.consume(N),A):Y(N)}function k(N){return N===45||lr(N)?(e.consume(N),k):N===47||N===62||cn(N)?F(N):n(N)}function F(N){return N===47?(e.consume(N),Y):N===58||N===95||mr(N)?(e.consume(N),L):Tt(N)?(o=F,te(N)):Wt(N)?(e.consume(N),F):Y(N)}function L(N){return N===45||N===46||N===58||N===95||lr(N)?(e.consume(N),L):G(N)}function G(N){return N===61?(e.consume(N),j):Tt(N)?(o=G,te(N)):Wt(N)?(e.consume(N),G):F(N)}function j(N){return N===null||N===60||N===61||N===62||N===96?n(N):N===34||N===39?(e.consume(N),s=N,R):Tt(N)?(o=j,te(N)):Wt(N)?(e.consume(N),j):(e.consume(N),K)}function R(N){return N===s?(e.consume(N),s=void 0,U):N===null?n(N):Tt(N)?(o=R,te(N)):(e.consume(N),R)}function K(N){return N===null||N===34||N===39||N===60||N===61||N===96?n(N):N===47||N===62||cn(N)?F(N):(e.consume(N),K)}function U(N){return N===47||N===62||cn(N)?F(N):n(N)}function Y(N){return N===62?(e.consume(N),e.exit("htmlTextData"),e.exit("htmlText"),t):n(N)}function te(N){return e.exit("htmlTextData"),e.enter("lineEnding"),e.consume(N),e.exit("lineEnding"),ne}function ne(N){return Wt(N)?qt(e,le,"linePrefix",r.parser.constructs.disable.null.includes("codeIndented")?void 0:4)(N):le(N)}function le(N){return e.enter("htmlTextData"),o(N)}}const Cf={name:"labelEnd",resolveAll:F$,resolveTo:O$,tokenize:j$},L$={tokenize:R$},D$={tokenize:N$},$$={tokenize:z$};function F$(e){let t=-1;const n=[];for(;++t<e.length;){const r=e[t][1];if(n.push(e[t]),r.type==="labelImage"||r.type==="labelLink"||r.type==="labelEnd"){const s=r.type==="labelImage"?4:2;r.type="data",t+=s}}return e.length!==n.length&&Er(e,0,e.length,n),e}function O$(e,t){let n=e.length,r=0,s,i,o,a;for(;n--;)if(s=e[n][1],i){if(s.type==="link"||s.type==="labelLink"&&s._inactive)break;e[n][0]==="enter"&&s.type==="labelLink"&&(s._inactive=!0)}else if(o){if(e[n][0]==="enter"&&(s.type==="labelImage"||s.type==="labelLink")&&!s._balanced&&(i=n,s.type!=="labelLink")){r=2;break}}else s.type==="labelEnd"&&(o=n);const l={type:e[i][1].type==="labelLink"?"link":"image",start:{...e[i][1].start},end:{...e[e.length-1][1].end}},c={type:"label",start:{...e[i][1].start},end:{...e[o][1].end}},d={type:"labelText",start:{...e[i+r+2][1].end},end:{...e[o-2][1].start}};return a=[["enter",l,t],["enter",c,t]],a=Gr(a,e.slice(i+1,i+r+3)),a=Gr(a,[["enter",d,t]]),a=Gr(a,zu(t.parser.constructs.insideSpan.null,e.slice(i+r+4,o-3),t)),a=Gr(a,[["exit",d,t],e[o-2],e[o-1],["exit",c,t]]),a=Gr(a,e.slice(o+1)),a=Gr(a,[["exit",l,t]]),Er(e,i,e.length,a),e}function j$(e,t,n){const r=this;let s=r.events.length,i,o;for(;s--;)if((r.events[s][1].type==="labelImage"||r.events[s][1].type==="labelLink")&&!r.events[s][1]._balanced){i=r.events[s][1];break}return a;function a(p){return i?i._inactive?u(p):(o=r.parser.defined.includes(ls(r.sliceSerialize({start:i.end,end:r.now()}))),e.enter("labelEnd"),e.enter("labelMarker"),e.consume(p),e.exit("labelMarker"),e.exit("labelEnd"),l):n(p)}function l(p){return p===40?e.attempt(L$,d,o?d:u)(p):p===91?e.attempt(D$,d,o?c:u)(p):o?d(p):u(p)}function c(p){return e.attempt($$,d,u)(p)}function d(p){return t(p)}function u(p){return i._balanced=!0,n(p)}}function R$(e,t,n){return r;function r(u){return e.enter("resource"),e.enter("resourceMarker"),e.consume(u),e.exit("resourceMarker"),s}function s(u){return cn(u)?el(e,i)(u):i(u)}function i(u){return u===41?d(u):KP(e,o,a,"resourceDestination","resourceDestinationLiteral","resourceDestinationLiteralMarker","resourceDestinationRaw","resourceDestinationString",32)(u)}function o(u){return cn(u)?el(e,l)(u):d(u)}function a(u){return n(u)}function l(u){return u===34||u===39||u===40?QP(e,c,n,"resourceTitle","resourceTitleMarker","resourceTitleString")(u):d(u)}function c(u){return cn(u)?el(e,d)(u):d(u)}function d(u){return u===41?(e.enter("resourceMarker"),e.consume(u),e.exit("resourceMarker"),e.exit("resource"),t):n(u)}}function N$(e,t,n){const r=this;return s;function s(a){return qP.call(r,e,i,o,"reference","referenceMarker","referenceString")(a)}function i(a){return r.parser.defined.includes(ls(r.sliceSerialize(r.events[r.events.length-1][1]).slice(1,-1)))?t(a):n(a)}function o(a){return n(a)}}function z$(e,t,n){return r;function r(i){return e.enter("reference"),e.enter("referenceMarker"),e.consume(i),e.exit("referenceMarker"),s}function s(i){return i===93?(e.enter("referenceMarker"),e.consume(i),e.exit("referenceMarker"),e.exit("reference"),t):n(i)}}const B$={name:"labelStartImage",resolveAll:Cf.resolveAll,tokenize:G$};function G$(e,t,n){const r=this;return s;function s(a){return e.enter("labelImage"),e.enter("labelImageMarker"),e.consume(a),e.exit("labelImageMarker"),i}function i(a){return a===91?(e.enter("labelMarker"),e.consume(a),e.exit("labelMarker"),e.exit("labelImage"),o):n(a)}function o(a){return a===94&&"_hiddenFootnoteSupport"in r.parser.constructs?n(a):t(a)}}const V$={name:"labelStartLink",resolveAll:Cf.resolveAll,tokenize:U$};function U$(e,t,n){const r=this;return s;function s(o){return e.enter("labelLink"),e.enter("labelMarker"),e.consume(o),e.exit("labelMarker"),e.exit("labelLink"),i}function i(o){return o===94&&"_hiddenFootnoteSupport"in r.parser.constructs?n(o):t(o)}}const kp={name:"lineEnding",tokenize:W$};function W$(e,t){return n;function n(r){return e.enter("lineEnding"),e.consume(r),e.exit("lineEnding"),qt(e,t,"linePrefix")}}const Qc={name:"thematicBreak",tokenize:H$};function H$(e,t,n){let r=0,s;return i;function i(c){return e.enter("thematicBreak"),o(c)}function o(c){return s=c,a(c)}function a(c){return c===s?(e.enter("thematicBreakSequence"),l(c)):r>=3&&(c===null||Tt(c))?(e.exit("thematicBreak"),t(c)):n(c)}function l(c){return c===s?(e.consume(c),r++,l):(e.exit("thematicBreakSequence"),Wt(c)?qt(e,a,"whitespace")(c):a(c))}}const br={continuation:{tokenize:X$},exit:J$,name:"list",tokenize:Q$},K$={partial:!0,tokenize:Z$},q$={partial:!0,tokenize:Y$};function Q$(e,t,n){const r=this,s=r.events[r.events.length-1];let i=s&&s[1].type==="linePrefix"?s[2].sliceSerialize(s[1],!0).length:0,o=0;return a;function a(h){const m=r.containerState.type||(h===42||h===43||h===45?"listUnordered":"listOrdered");if(m==="listUnordered"?!r.containerState.marker||h===r.containerState.marker:_m(h)){if(r.containerState.type||(r.containerState.type=m,e.enter(m,{_container:!0})),m==="listUnordered")return e.enter("listItemPrefix"),h===42||h===45?e.check(Qc,n,c)(h):c(h);if(!r.interrupt||h===49)return e.enter("listItemPrefix"),e.enter("listItemValue"),l(h)}return n(h)}function l(h){return _m(h)&&++o<10?(e.consume(h),l):(!r.interrupt||o<2)&&(r.containerState.marker?h===r.containerState.marker:h===41||h===46)?(e.exit("listItemValue"),c(h)):n(h)}function c(h){return e.enter("listItemMarker"),e.consume(h),e.exit("listItemMarker"),r.containerState.marker=r.containerState.marker||h,e.check(wl,r.interrupt?n:d,e.attempt(K$,p,u))}function d(h){return r.containerState.initialBlankLine=!0,i++,p(h)}function u(h){return Wt(h)?(e.enter("listItemPrefixWhitespace"),e.consume(h),e.exit("listItemPrefixWhitespace"),p):n(h)}function p(h){return r.containerState.size=i+r.sliceSerialize(e.exit("listItemPrefix"),!0).length,t(h)}}function X$(e,t,n){const r=this;return r.containerState._closeFlow=void 0,e.check(wl,s,i);function s(a){return r.containerState.furtherBlankLines=r.containerState.furtherBlankLines||r.containerState.initialBlankLine,qt(e,t,"listItemIndent",r.containerState.size+1)(a)}function i(a){return r.containerState.furtherBlankLines||!Wt(a)?(r.containerState.furtherBlankLines=void 0,r.containerState.initialBlankLine=void 0,o(a)):(r.containerState.furtherBlankLines=void 0,r.containerState.initialBlankLine=void 0,e.attempt(q$,t,o)(a))}function o(a){return r.containerState._closeFlow=!0,r.interrupt=void 0,qt(e,e.attempt(br,t,n),"linePrefix",r.parser.constructs.disable.null.includes("codeIndented")?void 0:4)(a)}}function Y$(e,t,n){const r=this;return qt(e,s,"listItemIndent",r.containerState.size+1);function s(i){const o=r.events[r.events.length-1];return o&&o[1].type==="listItemIndent"&&o[2].sliceSerialize(o[1],!0).length===r.containerState.size?t(i):n(i)}}function J$(e){e.exit(this.containerState.type)}function Z$(e,t,n){const r=this;return qt(e,s,"listItemPrefixWhitespace",r.parser.constructs.disable.null.includes("codeIndented")?void 0:5);function s(i){const o=r.events[r.events.length-1];return!Wt(i)&&o&&o[1].type==="listItemPrefixWhitespace"?t(i):n(i)}}const kw={name:"setextUnderline",resolveTo:eF,tokenize:tF};function eF(e,t){let n=e.length,r,s,i;for(;n--;)if(e[n][0]==="enter"){if(e[n][1].type==="content"){r=n;break}e[n][1].type==="paragraph"&&(s=n)}else e[n][1].type==="content"&&e.splice(n,1),!i&&e[n][1].type==="definition"&&(i=n);const o={type:"setextHeading",start:{...e[r][1].start},end:{...e[e.length-1][1].end}};return e[s][1].type="setextHeadingText",i?(e.splice(s,0,["enter",o,t]),e.splice(i+1,0,["exit",e[r][1],t]),e[r][1].end={...e[i][1].end}):e[r][1]=o,e.push(["exit",o,t]),e}function tF(e,t,n){const r=this;let s;return i;function i(c){let d=r.events.length,u;for(;d--;)if(r.events[d][1].type!=="lineEnding"&&r.events[d][1].type!=="linePrefix"&&r.events[d][1].type!=="content"){u=r.events[d][1].type==="paragraph";break}return!r.parser.lazy[r.now().line]&&(r.interrupt||u)?(e.enter("setextHeadingLine"),s=c,o(c)):n(c)}function o(c){return e.enter("setextHeadingLineSequence"),a(c)}function a(c){return c===s?(e.consume(c),a):(e.exit("setextHeadingLineSequence"),Wt(c)?qt(e,l,"lineSuffix")(c):l(c))}function l(c){return c===null||Tt(c)?(e.exit("setextHeadingLine"),t(c)):n(c)}}const nF={tokenize:rF};function rF(e){const t=this,n=e.attempt(wl,r,e.attempt(this.parser.constructs.flowInitial,s,qt(e,e.attempt(this.parser.constructs.flow,s,e.attempt(l$,s)),"linePrefix")));return n;function r(i){if(i===null){e.consume(i);return}return e.enter("lineEndingBlank"),e.consume(i),e.exit("lineEndingBlank"),t.currentConstruct=void 0,n}function s(i){if(i===null){e.consume(i);return}return e.enter("lineEnding"),e.consume(i),e.exit("lineEnding"),t.currentConstruct=void 0,n}}const sF={resolveAll:YP()},iF=XP("string"),oF=XP("text");function XP(e){return{resolveAll:YP(e==="text"?aF:void 0),tokenize:t};function t(n){const r=this,s=this.parser.constructs[e],i=n.attempt(s,o,a);return o;function o(d){return c(d)?i(d):a(d)}function a(d){if(d===null){n.consume(d);return}return n.enter("data"),n.consume(d),l}function l(d){return c(d)?(n.exit("data"),i(d)):(n.consume(d),l)}function c(d){if(d===null)return!0;const u=s[d];let p=-1;if(u)for(;++p<u.length;){const h=u[p];if(!h.previous||h.previous.call(r,r.previous))return!0}return!1}}}function YP(e){return t;function t(n,r){let s=-1,i;for(;++s<=n.length;)i===void 0?n[s]&&n[s][1].type==="data"&&(i=s,s++):(!n[s]||n[s][1].type!=="data")&&(s!==i+2&&(n[i][1].end=n[s-1][1].end,n.splice(i+2,s-i-2),s=i+2),i=void 0);return e?e(n,r):n}}function aF(e,t){let n=0;for(;++n<=e.length;)if((n===e.length||e[n][1].type==="lineEnding")&&e[n-1][1].type==="data"){const r=e[n-1][1],s=t.sliceStream(r);let i=s.length,o=-1,a=0,l;for(;i--;){const c=s[i];if(typeof c=="string"){for(o=c.length;c.charCodeAt(o-1)===32;)a++,o--;if(o)break;o=-1}else if(c===-2)l=!0,a++;else if(c!==-1){i++;break}}if(t._contentTypeTextTrailing&&n===e.length&&(a=0),a){const c={type:n===e.length||l||a<2?"lineSuffix":"hardBreakTrailing",start:{_bufferIndex:i?o:r.start._bufferIndex+o,_index:r.start._index+i,line:r.end.line,column:r.end.column-a,offset:r.end.offset-a},end:{...r.end}};r.end={...c.start},r.start.offset===r.end.offset?Object.assign(r,c):(e.splice(n,0,["enter",c,t],["exit",c,t]),n+=2)}n++}return e}const lF={42:br,43:br,45:br,48:br,49:br,50:br,51:br,52:br,53:br,54:br,55:br,56:br,57:br,62:VP},cF={91:h$},uF={[-2]:Ip,[-1]:Ip,32:Ip},dF={35:x$,42:Qc,45:[kw,Qc],60:M$,61:kw,95:Qc,96:Cw,126:Cw},pF={38:WP,92:UP},hF={[-5]:kp,[-4]:kp,[-3]:kp,33:B$,38:WP,42:ym,60:[VD,E$],91:V$,92:[_$,UP],93:Cf,95:ym,96:n$},mF={null:[ym,sF]},fF={null:[42,95]},gF={null:[]},_F=Object.freeze(Object.defineProperty({__proto__:null,attentionMarkers:fF,contentInitial:cF,disable:gF,document:lF,flow:dF,flowInitial:uF,insideSpan:mF,string:pF,text:hF},Symbol.toStringTag,{value:"Module"}));function yF(e,t,n){let r={_bufferIndex:-1,_index:0,line:n&&n.line||1,column:n&&n.column||1,offset:n&&n.offset||0};const s={},i=[];let o=[],a=[];const l={attempt:A(v),check:A(b),consume:_,enter:T,exit:M,interrupt:A(b,{interrupt:!0})},c={code:null,containerState:{},defineSkip:g,events:[],now:m,parser:e,previous:null,sliceSerialize:p,sliceStream:h,write:u};let d=t.tokenize.call(c,l);return t.resolveAll&&i.push(t),c;function u(G){return o=Gr(o,G),I(),o[o.length-1]!==null?[]:(k(t,0),c.events=zu(i,c.events,c),c.events)}function p(G,j){return bF(h(G),j)}function h(G){return xF(o,G)}function m(){const{_bufferIndex:G,_index:j,line:R,column:K,offset:U}=r;return{_bufferIndex:G,_index:j,line:R,column:K,offset:U}}function g(G){s[G.line]=G.column,L()}function I(){let G;for(;r._index<o.length;){const j=o[r._index];if(typeof j=="string")for(G=r._index,r._bufferIndex<0&&(r._bufferIndex=0);r._index===G&&r._bufferIndex<j.length;)f(j.charCodeAt(r._bufferIndex));else f(j)}}function f(G){d=d(G)}function _(G){Tt(G)?(r.line++,r.column=1,r.offset+=G===-3?2:1,L()):G!==-1&&(r.column++,r.offset++),r._bufferIndex<0?r._index++:(r._bufferIndex++,r._bufferIndex===o[r._index].length&&(r._bufferIndex=-1,r._index++)),c.previous=G}function T(G,j){const R=j||{};return R.type=G,R.start=m(),c.events.push(["enter",R,c]),a.push(R),R}function M(G){const j=a.pop();return j.end=m(),c.events.push(["exit",j,c]),j}function v(G,j){k(G,j.from)}function b(G,j){j.restore()}function A(G,j){return R;function R(K,U,Y){let te,ne,le,N;return Array.isArray(K)?X(K):"tokenize"in K?X([K]):oe(K);function oe(me){return $e;function $e(ke){const Be=ke!==null&&me[ke],Ce=ke!==null&&me.null,Z=[...Array.isArray(Be)?Be:Be?[Be]:[],...Array.isArray(Ce)?Ce:Ce?[Ce]:[]];return X(Z)(ke)}}function X(me){return te=me,ne=0,me.length===0?Y:D(me[ne])}function D(me){return $e;function $e(ke){return N=F(),le=me,me.partial||(c.currentConstruct=me),me.name&&c.parser.constructs.disable.null.includes(me.name)?se():me.tokenize.call(j?Object.assign(Object.create(c),j):c,l,z,se)(ke)}}function z(me){return G(le,N),U}function se(me){return N.restore(),++ne<te.length?D(te[ne]):Y}}}function k(G,j){G.resolveAll&&!i.includes(G)&&i.push(G),G.resolve&&Er(c.events,j,c.events.length-j,G.resolve(c.events.slice(j),c)),G.resolveTo&&(c.events=G.resolveTo(c.events,c))}function F(){const G=m(),j=c.previous,R=c.currentConstruct,K=c.events.length,U=Array.from(a);return{from:K,restore:Y};function Y(){r=G,c.previous=j,c.currentConstruct=R,c.events.length=K,a=U,L()}}function L(){r.line in s&&r.column<2&&(r.column=s[r.line],r.offset+=s[r.line]-1)}}function xF(e,t){const n=t.start._index,r=t.start._bufferIndex,s=t.end._index,i=t.end._bufferIndex;let o;if(n===s)o=[e[n].slice(r,i)];else{if(o=e.slice(n,s),r>-1){const a=o[0];typeof a=="string"?o[0]=a.slice(r):o.shift()}i>0&&o.push(e[s].slice(0,i))}return o}function bF(e,t){let n=-1;const r=[];let s;for(;++n<e.length;){const i=e[n];let o;if(typeof i=="string")o=i;else switch(i){case-5:{o="\r";break}case-4:{o=`
`;break}case-3:{o=`\r
`;break}case-2:{o=t?" ":"	";break}case-1:{if(!t&&s)continue;o=" ";break}default:o=String.fromCharCode(i)}s=i===-2,r.push(o)}return r.join("")}function wF(e){const r={constructs:BP([_F,...(e||{}).extensions||[]]),content:s(OD),defined:[],document:s(RD),flow:s(nF),lazy:{},string:s(iF),text:s(oF)};return r;function s(i){return o;function o(a){return yF(r,i,a)}}}function vF(e){for(;!HP(e););return e}const Ew=/[\0\t\n\r]/g;function MF(){let e=1,t="",n=!0,r;return s;function s(i,o,a){const l=[];let c,d,u,p,h;for(i=t+(typeof i=="string"?i.toString():new TextDecoder(o||void 0).decode(i)),u=0,t="",n&&(i.charCodeAt(0)===65279&&u++,n=void 0);u<i.length;){if(Ew.lastIndex=u,c=Ew.exec(i),p=c&&c.index!==void 0?c.index:i.length,h=i.charCodeAt(p),!c){t=i.slice(u);break}if(h===10&&u===p&&r)l.push(-3),r=void 0;else switch(r&&(l.push(-5),r=void 0),u<p&&(l.push(i.slice(u,p)),e+=p-u),h){case 0:{l.push(65533),e++;break}case 9:{for(d=Math.ceil(e/4)*4,l.push(-2);e++<d;)l.push(-1);break}case 10:{l.push(-4),e=1;break}default:r=!0,e=1}u=p+1}return a&&(r&&l.push(-5),t&&l.push(t),l.push(null)),l}}const AF=/\\([!-/:-@[-`{-~])|&(#(?:\d{1,7}|x[\da-f]{1,6})|[\da-z]{1,31});/gi;function TF(e){return e.replace(AF,PF)}function PF(e,t,n){if(t)return t;if(n.charCodeAt(0)===35){const s=n.charCodeAt(1),i=s===120||s===88;return GP(n.slice(i?2:1),i?16:10)}return Pf(n)||e}const JP={}.hasOwnProperty;function CF(e,t,n){return typeof t!="string"&&(n=t,t=void 0),IF(n)(vF(wF(n).document().write(MF()(e,t,!0))))}function IF(e){const t={transforms:[],canContainEols:["emphasis","fragment","heading","paragraph","strong"],enter:{autolink:i(ze),autolinkProtocol:F,autolinkEmail:F,atxHeading:i(Ie),blockQuote:i(Ce),characterEscape:F,characterReference:F,codeFenced:i(Z),codeFencedFenceInfo:o,codeFencedFenceMeta:o,codeIndented:i(Z,o),codeText:i(V,o),codeTextData:F,data:F,codeFlowValue:F,definition:i(fe),definitionDestinationString:o,definitionLabelString:o,definitionTitleString:o,emphasis:i(Te),hardBreakEscape:i(Ee),hardBreakTrailing:i(Ee),htmlFlow:i(De,o),htmlFlowData:F,htmlText:i(De,o),htmlTextData:F,image:i(xe),label:o,link:i(ze),listItem:i(Le),listItemValue:p,listOrdered:i(_e,u),listUnordered:i(_e),paragraph:i(qe),reference:D,referenceString:o,resourceDestinationString:o,resourceTitleString:o,setextHeading:i(Ie),strong:i(Ne),thematicBreak:i(Ve)},exit:{atxHeading:l(),atxHeadingSequence:v,autolink:l(),autolinkEmail:Be,autolinkProtocol:ke,blockQuote:l(),characterEscapeValue:L,characterReferenceMarkerHexadecimal:se,characterReferenceMarkerNumeric:se,characterReferenceValue:me,characterReference:$e,codeFenced:l(I),codeFencedFence:g,codeFencedFenceInfo:h,codeFencedFenceMeta:m,codeFlowValue:L,codeIndented:l(f),codeText:l(U),codeTextData:L,data:L,definition:l(),definitionDestinationString:M,definitionLabelString:_,definitionTitleString:T,emphasis:l(),hardBreakEscape:l(j),hardBreakTrailing:l(j),htmlFlow:l(R),htmlFlowData:L,htmlText:l(K),htmlTextData:L,image:l(te),label:le,labelText:ne,lineEnding:G,link:l(Y),listItem:l(),listOrdered:l(),listUnordered:l(),paragraph:l(),referenceString:z,resourceDestinationString:N,resourceTitleString:oe,resource:X,setextHeading:l(k),setextHeadingLineSequence:A,setextHeadingText:b,strong:l(),thematicBreak:l()}};ZP(t,(e||{}).mdastExtensions||[]);const n={};return r;function r(de){let ye={type:"root",children:[]};const Pe={stack:[ye],tokenStack:[],config:t,enter:a,exit:c,buffer:o,resume:d,data:n},ve=[];let Qe=-1;for(;++Qe<de.length;)if(de[Qe][1].type==="listOrdered"||de[Qe][1].type==="listUnordered")if(de[Qe][0]==="enter")ve.push(Qe);else{const ct=ve.pop();Qe=s(de,ct,Qe)}for(Qe=-1;++Qe<de.length;){const ct=t[de[Qe][0]];JP.call(ct,de[Qe][1].type)&&ct[de[Qe][1].type].call(Object.assign({sliceSerialize:de[Qe][2].sliceSerialize},Pe),de[Qe][1])}if(Pe.tokenStack.length>0){const ct=Pe.tokenStack[Pe.tokenStack.length-1];(ct[1]||Sw).call(Pe,void 0,ct[0])}for(ye.position={start:ti(de.length>0?de[0][1].start:{line:1,column:1,offset:0}),end:ti(de.length>0?de[de.length-2][1].end:{line:1,column:1,offset:0})},Qe=-1;++Qe<t.transforms.length;)ye=t.transforms[Qe](ye)||ye;return ye}function s(de,ye,Pe){let ve=ye-1,Qe=-1,ct=!1,zt,wt,on,Tn;for(;++ve<=Pe;){const kt=de[ve];switch(kt[1].type){case"listUnordered":case"listOrdered":case"blockQuote":{kt[0]==="enter"?Qe++:Qe--,Tn=void 0;break}case"lineEndingBlank":{kt[0]==="enter"&&(zt&&!Tn&&!Qe&&!on&&(on=ve),Tn=void 0);break}case"linePrefix":case"listItemValue":case"listItemMarker":case"listItemPrefix":case"listItemPrefixWhitespace":break;default:Tn=void 0}if(!Qe&&kt[0]==="enter"&&kt[1].type==="listItemPrefix"||Qe===-1&&kt[0]==="exit"&&(kt[1].type==="listUnordered"||kt[1].type==="listOrdered")){if(zt){let Pn=ve;for(wt=void 0;Pn--;){const Zn=de[Pn];if(Zn[1].type==="lineEnding"||Zn[1].type==="lineEndingBlank"){if(Zn[0]==="exit")continue;wt&&(de[wt][1].type="lineEndingBlank",ct=!0),Zn[1].type="lineEnding",wt=Pn}else if(!(Zn[1].type==="linePrefix"||Zn[1].type==="blockQuotePrefix"||Zn[1].type==="blockQuotePrefixWhitespace"||Zn[1].type==="blockQuoteMarker"||Zn[1].type==="listItemIndent"))break}on&&(!wt||on<wt)&&(zt._spread=!0),zt.end=Object.assign({},wt?de[wt][1].start:kt[1].end),de.splice(wt||ve,0,["exit",zt,kt[2]]),ve++,Pe++}if(kt[1].type==="listItemPrefix"){const Pn={type:"listItem",_spread:!1,start:Object.assign({},kt[1].start),end:void 0};zt=Pn,de.splice(ve,0,["enter",Pn,kt[2]]),ve++,Pe++,on=void 0,Tn=!0}}}return de[ye][1]._spread=ct,Pe}function i(de,ye){return Pe;function Pe(ve){a.call(this,de(ve),ve),ye&&ye.call(this,ve)}}function o(){this.stack.push({type:"fragment",children:[]})}function a(de,ye,Pe){this.stack[this.stack.length-1].children.push(de),this.stack.push(de),this.tokenStack.push([ye,Pe||void 0]),de.position={start:ti(ye.start),end:void 0}}function l(de){return ye;function ye(Pe){de&&de.call(this,Pe),c.call(this,Pe)}}function c(de,ye){const Pe=this.stack.pop(),ve=this.tokenStack.pop();if(ve)ve[0].type!==de.type&&(ye?ye.call(this,de,ve[0]):(ve[1]||Sw).call(this,de,ve[0]));else throw new Error("Cannot close `"+de.type+"` ("+Za({start:de.start,end:de.end})+"): it’s not open");Pe.position.end=ti(de.end)}function d(){return Tf(this.stack.pop())}function u(){this.data.expectingFirstListItemValue=!0}function p(de){if(this.data.expectingFirstListItemValue){const ye=this.stack[this.stack.length-2];ye.start=Number.parseInt(this.sliceSerialize(de),10),this.data.expectingFirstListItemValue=void 0}}function h(){const de=this.resume(),ye=this.stack[this.stack.length-1];ye.lang=de}function m(){const de=this.resume(),ye=this.stack[this.stack.length-1];ye.meta=de}function g(){this.data.flowCodeInside||(this.buffer(),this.data.flowCodeInside=!0)}function I(){const de=this.resume(),ye=this.stack[this.stack.length-1];ye.value=de.replace(/^(\r?\n|\r)|(\r?\n|\r)$/g,""),this.data.flowCodeInside=void 0}function f(){const de=this.resume(),ye=this.stack[this.stack.length-1];ye.value=de.replace(/(\r?\n|\r)$/g,"")}function _(de){const ye=this.resume(),Pe=this.stack[this.stack.length-1];Pe.label=ye,Pe.identifier=ls(this.sliceSerialize(de)).toLowerCase()}function T(){const de=this.resume(),ye=this.stack[this.stack.length-1];ye.title=de}function M(){const de=this.resume(),ye=this.stack[this.stack.length-1];ye.url=de}function v(de){const ye=this.stack[this.stack.length-1];if(!ye.depth){const Pe=this.sliceSerialize(de).length;ye.depth=Pe}}function b(){this.data.setextHeadingSlurpLineEnding=!0}function A(de){const ye=this.stack[this.stack.length-1];ye.depth=this.sliceSerialize(de).codePointAt(0)===61?1:2}function k(){this.data.setextHeadingSlurpLineEnding=void 0}function F(de){const Pe=this.stack[this.stack.length-1].children;let ve=Pe[Pe.length-1];(!ve||ve.type!=="text")&&(ve=ot(),ve.position={start:ti(de.start),end:void 0},Pe.push(ve)),this.stack.push(ve)}function L(de){const ye=this.stack.pop();ye.value+=this.sliceSerialize(de),ye.position.end=ti(de.end)}function G(de){const ye=this.stack[this.stack.length-1];if(this.data.atHardBreak){const Pe=ye.children[ye.children.length-1];Pe.position.end=ti(de.end),this.data.atHardBreak=void 0;return}!this.data.setextHeadingSlurpLineEnding&&t.canContainEols.includes(ye.type)&&(F.call(this,de),L.call(this,de))}function j(){this.data.atHardBreak=!0}function R(){const de=this.resume(),ye=this.stack[this.stack.length-1];ye.value=de}function K(){const de=this.resume(),ye=this.stack[this.stack.length-1];ye.value=de}function U(){const de=this.resume(),ye=this.stack[this.stack.length-1];ye.value=de}function Y(){const de=this.stack[this.stack.length-1];if(this.data.inReference){const ye=this.data.referenceType||"shortcut";de.type+="Reference",de.referenceType=ye,delete de.url,delete de.title}else delete de.identifier,delete de.label;this.data.referenceType=void 0}function te(){const de=this.stack[this.stack.length-1];if(this.data.inReference){const ye=this.data.referenceType||"shortcut";de.type+="Reference",de.referenceType=ye,delete de.url,delete de.title}else delete de.identifier,delete de.label;this.data.referenceType=void 0}function ne(de){const ye=this.sliceSerialize(de),Pe=this.stack[this.stack.length-2];Pe.label=TF(ye),Pe.identifier=ls(ye).toLowerCase()}function le(){const de=this.stack[this.stack.length-1],ye=this.resume(),Pe=this.stack[this.stack.length-1];if(this.data.inReference=!0,Pe.type==="link"){const ve=de.children;Pe.children=ve}else Pe.alt=ye}function N(){const de=this.resume(),ye=this.stack[this.stack.length-1];ye.url=de}function oe(){const de=this.resume(),ye=this.stack[this.stack.length-1];ye.title=de}function X(){this.data.inReference=void 0}function D(){this.data.referenceType="collapsed"}function z(de){const ye=this.resume(),Pe=this.stack[this.stack.length-1];Pe.label=ye,Pe.identifier=ls(this.sliceSerialize(de)).toLowerCase(),this.data.referenceType="full"}function se(de){this.data.characterReferenceType=de.type}function me(de){const ye=this.sliceSerialize(de),Pe=this.data.characterReferenceType;let ve;Pe?(ve=GP(ye,Pe==="characterReferenceMarkerNumeric"?10:16),this.data.characterReferenceType=void 0):ve=Pf(ye);const Qe=this.stack[this.stack.length-1];Qe.value+=ve}function $e(de){const ye=this.stack.pop();ye.position.end=ti(de.end)}function ke(de){L.call(this,de);const ye=this.stack[this.stack.length-1];ye.url=this.sliceSerialize(de)}function Be(de){L.call(this,de);const ye=this.stack[this.stack.length-1];ye.url="mailto:"+this.sliceSerialize(de)}function Ce(){return{type:"blockquote",children:[]}}function Z(){return{type:"code",lang:null,meta:null,value:""}}function V(){return{type:"inlineCode",value:""}}function fe(){return{type:"definition",identifier:"",label:null,title:null,url:""}}function Te(){return{type:"emphasis",children:[]}}function Ie(){return{type:"heading",depth:0,children:[]}}function Ee(){return{type:"break"}}function De(){return{type:"html",value:""}}function xe(){return{type:"image",title:null,url:"",alt:null}}function ze(){return{type:"link",title:null,url:"",children:[]}}function _e(de){return{type:"list",ordered:de.type==="listOrdered",start:null,spread:de._spread,children:[]}}function Le(de){return{type:"listItem",spread:de._spread,checked:null,children:[]}}function qe(){return{type:"paragraph",children:[]}}function Ne(){return{type:"strong",children:[]}}function ot(){return{type:"text",value:""}}function Ve(){return{type:"thematicBreak"}}}function ti(e){return{line:e.line,column:e.column,offset:e.offset}}function ZP(e,t){let n=-1;for(;++n<t.length;){const r=t[n];Array.isArray(r)?ZP(e,r):kF(e,r)}}function kF(e,t){let n;for(n in t)if(JP.call(t,n))switch(n){case"canContainEols":{const r=t[n];r&&e[n].push(...r);break}case"transforms":{const r=t[n];r&&e[n].push(...r);break}case"enter":case"exit":{const r=t[n];r&&Object.assign(e[n],r);break}}}function Sw(e,t){throw e?new Error("Cannot close `"+e.type+"` ("+Za({start:e.start,end:e.end})+"): a different token (`"+t.type+"`, "+Za({start:t.start,end:t.end})+") is open"):new Error("Cannot close document, a token (`"+t.type+"`, "+Za({start:t.start,end:t.end})+") is still open")}function EF(e){const t=this;t.parser=n;function n(r){return CF(r,{...t.data("settings"),...e,extensions:t.data("micromarkExtensions")||[],mdastExtensions:t.data("fromMarkdownExtensions")||[]})}}function SF(e,t){const n={type:"element",tagName:"blockquote",properties:{},children:e.wrap(e.all(t),!0)};return e.patch(t,n),e.applyData(t,n)}function LF(e,t){const n={type:"element",tagName:"br",properties:{},children:[]};return e.patch(t,n),[e.applyData(t,n),{type:"text",value:`
`}]}function DF(e,t){const n=t.value?t.value+`
`:"",r={},s=t.lang?t.lang.split(/\s+/):[];s.length>0&&(r.className=["language-"+s[0]]);let i={type:"element",tagName:"code",properties:r,children:[{type:"text",value:n}]};return t.meta&&(i.data={meta:t.meta}),e.patch(t,i),i=e.applyData(t,i),i={type:"element",tagName:"pre",properties:{},children:[i]},e.patch(t,i),i}function $F(e,t){const n={type:"element",tagName:"del",properties:{},children:e.all(t)};return e.patch(t,n),e.applyData(t,n)}function FF(e,t){const n={type:"element",tagName:"em",properties:{},children:e.all(t)};return e.patch(t,n),e.applyData(t,n)}function OF(e,t){const n=typeof e.options.clobberPrefix=="string"?e.options.clobberPrefix:"user-content-",r=String(t.identifier).toUpperCase(),s=Qo(r.toLowerCase()),i=e.footnoteOrder.indexOf(r);let o,a=e.footnoteCounts.get(r);a===void 0?(a=0,e.footnoteOrder.push(r),o=e.footnoteOrder.length):o=i+1,a+=1,e.footnoteCounts.set(r,a);const l={type:"element",tagName:"a",properties:{href:"#"+n+"fn-"+s,id:n+"fnref-"+s+(a>1?"-"+a:""),dataFootnoteRef:!0,ariaDescribedBy:["footnote-label"]},children:[{type:"text",value:String(o)}]};e.patch(t,l);const c={type:"element",tagName:"sup",properties:{},children:[l]};return e.patch(t,c),e.applyData(t,c)}function jF(e,t){const n={type:"element",tagName:"h"+t.depth,properties:{},children:e.all(t)};return e.patch(t,n),e.applyData(t,n)}function RF(e,t){if(e.options.allowDangerousHtml){const n={type:"raw",value:t.value};return e.patch(t,n),e.applyData(t,n)}}function e2(e,t){const n=t.referenceType;let r="]";if(n==="collapsed"?r+="[]":n==="full"&&(r+="["+(t.label||t.identifier)+"]"),t.type==="imageReference")return[{type:"text",value:"!["+t.alt+r}];const s=e.all(t),i=s[0];i&&i.type==="text"?i.value="["+i.value:s.unshift({type:"text",value:"["});const o=s[s.length-1];return o&&o.type==="text"?o.value+=r:s.push({type:"text",value:r}),s}function NF(e,t){const n=String(t.identifier).toUpperCase(),r=e.definitionById.get(n);if(!r)return e2(e,t);const s={src:Qo(r.url||""),alt:t.alt};r.title!==null&&r.title!==void 0&&(s.title=r.title);const i={type:"element",tagName:"img",properties:s,children:[]};return e.patch(t,i),e.applyData(t,i)}function zF(e,t){const n={src:Qo(t.url)};t.alt!==null&&t.alt!==void 0&&(n.alt=t.alt),t.title!==null&&t.title!==void 0&&(n.title=t.title);const r={type:"element",tagName:"img",properties:n,children:[]};return e.patch(t,r),e.applyData(t,r)}function BF(e,t){const n={type:"text",value:t.value.replace(/\r?\n|\r/g," ")};e.patch(t,n);const r={type:"element",tagName:"code",properties:{},children:[n]};return e.patch(t,r),e.applyData(t,r)}function GF(e,t){const n=String(t.identifier).toUpperCase(),r=e.definitionById.get(n);if(!r)return e2(e,t);const s={href:Qo(r.url||"")};r.title!==null&&r.title!==void 0&&(s.title=r.title);const i={type:"element",tagName:"a",properties:s,children:e.all(t)};return e.patch(t,i),e.applyData(t,i)}function VF(e,t){const n={href:Qo(t.url)};t.title!==null&&t.title!==void 0&&(n.title=t.title);const r={type:"element",tagName:"a",properties:n,children:e.all(t)};return e.patch(t,r),e.applyData(t,r)}function UF(e,t,n){const r=e.all(t),s=n?WF(n):t2(t),i={},o=[];if(typeof t.checked=="boolean"){const d=r[0];let u;d&&d.type==="element"&&d.tagName==="p"?u=d:(u={type:"element",tagName:"p",properties:{},children:[]},r.unshift(u)),u.children.length>0&&u.children.unshift({type:"text",value:" "}),u.children.unshift({type:"element",tagName:"input",properties:{type:"checkbox",checked:t.checked,disabled:!0},children:[]}),i.className=["task-list-item"]}let a=-1;for(;++a<r.length;){const d=r[a];(s||a!==0||d.type!=="element"||d.tagName!=="p")&&o.push({type:"text",value:`
`}),d.type==="element"&&d.tagName==="p"&&!s?o.push(...d.children):o.push(d)}const l=r[r.length-1];l&&(s||l.type!=="element"||l.tagName!=="p")&&o.push({type:"text",value:`
`});const c={type:"element",tagName:"li",properties:i,children:o};return e.patch(t,c),e.applyData(t,c)}function WF(e){let t=!1;if(e.type==="list"){t=e.spread||!1;const n=e.children;let r=-1;for(;!t&&++r<n.length;)t=t2(n[r])}return t}function t2(e){const t=e.spread;return t??e.children.length>1}function HF(e,t){const n={},r=e.all(t);let s=-1;for(typeof t.start=="number"&&t.start!==1&&(n.start=t.start);++s<r.length;){const o=r[s];if(o.type==="element"&&o.tagName==="li"&&o.properties&&Array.isArray(o.properties.className)&&o.properties.className.includes("task-list-item")){n.className=["contains-task-list"];break}}const i={type:"element",tagName:t.ordered?"ol":"ul",properties:n,children:e.wrap(r,!0)};return e.patch(t,i),e.applyData(t,i)}function KF(e,t){const n={type:"element",tagName:"p",properties:{},children:e.all(t)};return e.patch(t,n),e.applyData(t,n)}function qF(e,t){const n={type:"root",children:e.wrap(e.all(t))};return e.patch(t,n),e.applyData(t,n)}function QF(e,t){const n={type:"element",tagName:"strong",properties:{},children:e.all(t)};return e.patch(t,n),e.applyData(t,n)}function XF(e,t){const n=e.all(t),r=n.shift(),s=[];if(r){const o={type:"element",tagName:"thead",properties:{},children:e.wrap([r],!0)};e.patch(t.children[0],o),s.push(o)}if(n.length>0){const o={type:"element",tagName:"tbody",properties:{},children:e.wrap(n,!0)},a=wf(t.children[1]),l=$P(t.children[t.children.length-1]);a&&l&&(o.position={start:a,end:l}),s.push(o)}const i={type:"element",tagName:"table",properties:{},children:e.wrap(s,!0)};return e.patch(t,i),e.applyData(t,i)}function YF(e,t,n){const r=n?n.children:void 0,i=(r?r.indexOf(t):1)===0?"th":"td",o=n&&n.type==="table"?n.align:void 0,a=o?o.length:t.children.length;let l=-1;const c=[];for(;++l<a;){const u=t.children[l],p={},h=o?o[l]:void 0;h&&(p.align=h);let m={type:"element",tagName:i,properties:p,children:[]};u&&(m.children=e.all(u),e.patch(u,m),m=e.applyData(u,m)),c.push(m)}const d={type:"element",tagName:"tr",properties:{},children:e.wrap(c,!0)};return e.patch(t,d),e.applyData(t,d)}function JF(e,t){const n={type:"element",tagName:"td",properties:{},children:e.all(t)};return e.patch(t,n),e.applyData(t,n)}const Lw=9,Dw=32;function ZF(e){const t=String(e),n=/\r?\n|\r/g;let r=n.exec(t),s=0;const i=[];for(;r;)i.push($w(t.slice(s,r.index),s>0,!0),r[0]),s=r.index+r[0].length,r=n.exec(t);return i.push($w(t.slice(s),s>0,!1)),i.join("")}function $w(e,t,n){let r=0,s=e.length;if(t){let i=e.codePointAt(r);for(;i===Lw||i===Dw;)r++,i=e.codePointAt(r)}if(n){let i=e.codePointAt(s-1);for(;i===Lw||i===Dw;)s--,i=e.codePointAt(s-1)}return s>r?e.slice(r,s):""}function eO(e,t){const n={type:"text",value:ZF(String(t.value))};return e.patch(t,n),e.applyData(t,n)}function tO(e,t){const n={type:"element",tagName:"hr",properties:{},children:[]};return e.patch(t,n),e.applyData(t,n)}const nO={blockquote:SF,break:LF,code:DF,delete:$F,emphasis:FF,footnoteReference:OF,heading:jF,html:RF,imageReference:NF,image:zF,inlineCode:BF,linkReference:GF,link:VF,listItem:UF,list:HF,paragraph:KF,root:qF,strong:QF,table:XF,tableCell:JF,tableRow:YF,text:eO,thematicBreak:tO,toml:bc,yaml:bc,definition:bc,footnoteDefinition:bc};function bc(){}const n2=-1,Bu=0,tl=1,lu=2,If=3,kf=4,Ef=5,Sf=6,r2=7,s2=8,Fw=typeof self=="object"?self:globalThis,rO=(e,t)=>{const n=(s,i)=>(e.set(i,s),s),r=s=>{if(e.has(s))return e.get(s);const[i,o]=t[s];switch(i){case Bu:case n2:return n(o,s);case tl:{const a=n([],s);for(const l of o)a.push(r(l));return a}case lu:{const a=n({},s);for(const[l,c]of o)a[r(l)]=r(c);return a}case If:return n(new Date(o),s);case kf:{const{source:a,flags:l}=o;return n(new RegExp(a,l),s)}case Ef:{const a=n(new Map,s);for(const[l,c]of o)a.set(r(l),r(c));return a}case Sf:{const a=n(new Set,s);for(const l of o)a.add(r(l));return a}case r2:{const{name:a,message:l}=o;return n(new Fw[a](l),s)}case s2:return n(BigInt(o),s);case"BigInt":return n(Object(BigInt(o)),s);case"ArrayBuffer":return n(new Uint8Array(o).buffer,o);case"DataView":{const{buffer:a}=new Uint8Array(o);return n(new DataView(a),o)}}return n(new Fw[i](o),s)};return r},Ow=e=>rO(new Map,e)(0),vo="",{toString:sO}={},{keys:iO}=Object,Ca=e=>{const t=typeof e;if(t!=="object"||!e)return[Bu,t];const n=sO.call(e).slice(8,-1);switch(n){case"Array":return[tl,vo];case"Object":return[lu,vo];case"Date":return[If,vo];case"RegExp":return[kf,vo];case"Map":return[Ef,vo];case"Set":return[Sf,vo];case"DataView":return[tl,n]}return n.includes("Array")?[tl,n]:n.includes("Error")?[r2,n]:[lu,n]},wc=([e,t])=>e===Bu&&(t==="function"||t==="symbol"),oO=(e,t,n,r)=>{const s=(o,a)=>{const l=r.push(o)-1;return n.set(a,l),l},i=o=>{if(n.has(o))return n.get(o);let[a,l]=Ca(o);switch(a){case Bu:{let d=o;switch(l){case"bigint":a=s2,d=o.toString();break;case"function":case"symbol":if(e)throw new TypeError("unable to serialize "+l);d=null;break;case"undefined":return s([n2],o)}return s([a,d],o)}case tl:{if(l){let p=o;return l==="DataView"?p=new Uint8Array(o.buffer):l==="ArrayBuffer"&&(p=new Uint8Array(o)),s([l,[...p]],o)}const d=[],u=s([a,d],o);for(const p of o)d.push(i(p));return u}case lu:{if(l)switch(l){case"BigInt":return s([l,o.toString()],o);case"Boolean":case"Number":case"String":return s([l,o.valueOf()],o)}if(t&&"toJSON"in o)return i(o.toJSON());const d=[],u=s([a,d],o);for(const p of iO(o))(e||!wc(Ca(o[p])))&&d.push([i(p),i(o[p])]);return u}case If:return s([a,o.toISOString()],o);case kf:{const{source:d,flags:u}=o;return s([a,{source:d,flags:u}],o)}case Ef:{const d=[],u=s([a,d],o);for(const[p,h]of o)(e||!(wc(Ca(p))||wc(Ca(h))))&&d.push([i(p),i(h)]);return u}case Sf:{const d=[],u=s([a,d],o);for(const p of o)(e||!wc(Ca(p)))&&d.push(i(p));return u}}const{message:c}=o;return s([a,{name:l,message:c}],o)};return i},jw=(e,{json:t,lossy:n}={})=>{const r=[];return oO(!(t||n),!!t,new Map,r)(e),r},cu=typeof structuredClone=="function"?(e,t)=>t&&("json"in t||"lossy"in t)?Ow(jw(e,t)):structuredClone(e):(e,t)=>Ow(jw(e,t));function aO(e,t){const n=[{type:"text",value:"↩"}];return t>1&&n.push({type:"element",tagName:"sup",properties:{},children:[{type:"text",value:String(t)}]}),n}function lO(e,t){return"Back to reference "+(e+1)+(t>1?"-"+t:"")}function cO(e){const t=typeof e.options.clobberPrefix=="string"?e.options.clobberPrefix:"user-content-",n=e.options.footnoteBackContent||aO,r=e.options.footnoteBackLabel||lO,s=e.options.footnoteLabel||"Footnotes",i=e.options.footnoteLabelTagName||"h2",o=e.options.footnoteLabelProperties||{className:["sr-only"]},a=[];let l=-1;for(;++l<e.footnoteOrder.length;){const c=e.footnoteById.get(e.footnoteOrder[l]);if(!c)continue;const d=e.all(c),u=String(c.identifier).toUpperCase(),p=Qo(u.toLowerCase());let h=0;const m=[],g=e.footnoteCounts.get(u);for(;g!==void 0&&++h<=g;){m.length>0&&m.push({type:"text",value:" "});let _=typeof n=="string"?n:n(l,h);typeof _=="string"&&(_={type:"text",value:_}),m.push({type:"element",tagName:"a",properties:{href:"#"+t+"fnref-"+p+(h>1?"-"+h:""),dataFootnoteBackref:"",ariaLabel:typeof r=="string"?r:r(l,h),className:["data-footnote-backref"]},children:Array.isArray(_)?_:[_]})}const I=d[d.length-1];if(I&&I.type==="element"&&I.tagName==="p"){const _=I.children[I.children.length-1];_&&_.type==="text"?_.value+=" ":I.children.push({type:"text",value:" "}),I.children.push(...m)}else d.push(...m);const f={type:"element",tagName:"li",properties:{id:t+"fn-"+p},children:e.wrap(d,!0)};e.patch(c,f),a.push(f)}if(a.length!==0)return{type:"element",tagName:"section",properties:{dataFootnotes:!0,className:["footnotes"]},children:[{type:"element",tagName:i,properties:{...cu(o),id:"footnote-label"},children:[{type:"text",value:s}]},{type:"text",value:`
`},{type:"element",tagName:"ol",properties:{},children:e.wrap(a,!0)},{type:"text",value:`
`}]}}const Gu=(function(e){if(e==null)return hO;if(typeof e=="function")return Vu(e);if(typeof e=="object")return Array.isArray(e)?uO(e):dO(e);if(typeof e=="string")return pO(e);throw new Error("Expected function, string, or object as test")});function uO(e){const t=[];let n=-1;for(;++n<e.length;)t[n]=Gu(e[n]);return Vu(r);function r(...s){let i=-1;for(;++i<t.length;)if(t[i].apply(this,s))return!0;return!1}}function dO(e){const t=e;return Vu(n);function n(r){const s=r;let i;for(i in e)if(s[i]!==t[i])return!1;return!0}}function pO(e){return Vu(t);function t(n){return n&&n.type===e}}function Vu(e){return t;function t(n,r,s){return!!(mO(n)&&e.call(this,n,typeof r=="number"?r:void 0,s||void 0))}}function hO(){return!0}function mO(e){return e!==null&&typeof e=="object"&&"type"in e}const i2=[],fO=!0,xm=!1,gO="skip";function o2(e,t,n,r){let s;typeof t=="function"&&typeof n!="function"?(r=n,n=t):s=t;const i=Gu(s),o=r?-1:1;a(e,void 0,[])();function a(l,c,d){const u=l&&typeof l=="object"?l:{};if(typeof u.type=="string"){const h=typeof u.tagName=="string"?u.tagName:typeof u.name=="string"?u.name:void 0;Object.defineProperty(p,"name",{value:"node ("+(l.type+(h?"<"+h+">":""))+")"})}return p;function p(){let h=i2,m,g,I;if((!t||i(l,c,d[d.length-1]||void 0))&&(h=_O(n(l,d)),h[0]===xm))return h;if("children"in l&&l.children){const f=l;if(f.children&&h[0]!==gO)for(g=(r?f.children.length:-1)+o,I=d.concat(f);g>-1&&g<f.children.length;){const _=f.children[g];if(m=a(_,g,I)(),m[0]===xm)return m;g=typeof m[1]=="number"?m[1]:g+o}}return h}}}function _O(e){return Array.isArray(e)?e:typeof e=="number"?[fO,e]:e==null?i2:[e]}function Lf(e,t,n,r){let s,i,o;typeof t=="function"&&typeof n!="function"?(i=void 0,o=t,s=n):(i=t,o=n,s=r),o2(e,i,a,s);function a(l,c){const d=c[c.length-1],u=d?d.children.indexOf(l):void 0;return o(l,u,d)}}const bm={}.hasOwnProperty,yO={};function xO(e,t){const n=t||yO,r=new Map,s=new Map,i=new Map,o={...nO,...n.handlers},a={all:c,applyData:wO,definitionById:r,footnoteById:s,footnoteCounts:i,footnoteOrder:[],handlers:o,one:l,options:n,patch:bO,wrap:MO};return Lf(e,function(d){if(d.type==="definition"||d.type==="footnoteDefinition"){const u=d.type==="definition"?r:s,p=String(d.identifier).toUpperCase();u.has(p)||u.set(p,d)}}),a;function l(d,u){const p=d.type,h=a.handlers[p];if(bm.call(a.handlers,p)&&h)return h(a,d,u);if(a.options.passThrough&&a.options.passThrough.includes(p)){if("children"in d){const{children:g,...I}=d,f=cu(I);return f.children=a.all(d),f}return cu(d)}return(a.options.unknownHandler||vO)(a,d,u)}function c(d){const u=[];if("children"in d){const p=d.children;let h=-1;for(;++h<p.length;){const m=a.one(p[h],d);if(m){if(h&&p[h-1].type==="break"&&(!Array.isArray(m)&&m.type==="text"&&(m.value=Rw(m.value)),!Array.isArray(m)&&m.type==="element")){const g=m.children[0];g&&g.type==="text"&&(g.value=Rw(g.value))}Array.isArray(m)?u.push(...m):u.push(m)}}}return u}}function bO(e,t){e.position&&(t.position=lD(e))}function wO(e,t){let n=t;if(e&&e.data){const r=e.data.hName,s=e.data.hChildren,i=e.data.hProperties;if(typeof r=="string")if(n.type==="element")n.tagName=r;else{const o="children"in n?n.children:[n];n={type:"element",tagName:r,properties:{},children:o}}n.type==="element"&&i&&Object.assign(n.properties,cu(i)),"children"in n&&n.children&&s!==null&&s!==void 0&&(n.children=s)}return n}function vO(e,t){const n=t.data||{},r="value"in t&&!(bm.call(n,"hProperties")||bm.call(n,"hChildren"))?{type:"text",value:t.value}:{type:"element",tagName:"div",properties:{},children:e.all(t)};return e.patch(t,r),e.applyData(t,r)}function MO(e,t){const n=[];let r=-1;for(t&&n.push({type:"text",value:`
`});++r<e.length;)r&&n.push({type:"text",value:`
`}),n.push(e[r]);return t&&e.length>0&&n.push({type:"text",value:`
`}),n}function Rw(e){let t=0,n=e.charCodeAt(t);for(;n===9||n===32;)t++,n=e.charCodeAt(t);return e.slice(t)}function Nw(e,t){const n=xO(e,t),r=n.one(e,void 0),s=cO(n),i=Array.isArray(r)?{type:"root",children:r}:r||{type:"root",children:[]};return s&&i.children.push({type:"text",value:`
`},s),i}function AO(e,t){return e&&"run"in e?async function(n,r){const s=Nw(n,{file:r,...t});await e.run(s,r)}:function(n,r){return Nw(n,{file:r,...e||t})}}function zw(e){if(e)throw e}var Ep,Bw;function TO(){if(Bw)return Ep;Bw=1;var e=Object.prototype.hasOwnProperty,t=Object.prototype.toString,n=Object.defineProperty,r=Object.getOwnPropertyDescriptor,s=function(c){return typeof Array.isArray=="function"?Array.isArray(c):t.call(c)==="[object Array]"},i=function(c){if(!c||t.call(c)!=="[object Object]")return!1;var d=e.call(c,"constructor"),u=c.constructor&&c.constructor.prototype&&e.call(c.constructor.prototype,"isPrototypeOf");if(c.constructor&&!d&&!u)return!1;var p;for(p in c);return typeof p>"u"||e.call(c,p)},o=function(c,d){n&&d.name==="__proto__"?n(c,d.name,{enumerable:!0,configurable:!0,value:d.newValue,writable:!0}):c[d.name]=d.newValue},a=function(c,d){if(d==="__proto__")if(e.call(c,d)){if(r)return r(c,d).value}else return;return c[d]};return Ep=function l(){var c,d,u,p,h,m,g=arguments[0],I=1,f=arguments.length,_=!1;for(typeof g=="boolean"&&(_=g,g=arguments[1]||{},I=2),(g==null||typeof g!="object"&&typeof g!="function")&&(g={});I<f;++I)if(c=arguments[I],c!=null)for(d in c)u=a(g,d),p=a(c,d),g!==p&&(_&&p&&(i(p)||(h=s(p)))?(h?(h=!1,m=u&&s(u)?u:[]):m=u&&i(u)?u:{},o(g,{name:d,newValue:l(_,m,p)})):typeof p<"u"&&o(g,{name:d,newValue:p}));return g},Ep}var PO=TO();const Sp=AP(PO);function wm(e){if(typeof e!="object"||e===null)return!1;const t=Object.getPrototypeOf(e);return(t===null||t===Object.prototype||Object.getPrototypeOf(t)===null)&&!(Symbol.toStringTag in e)&&!(Symbol.iterator in e)}function CO(){const e=[],t={run:n,use:r};return t;function n(...s){let i=-1;const o=s.pop();if(typeof o!="function")throw new TypeError("Expected function as last argument, not "+o);a(null,...s);function a(l,...c){const d=e[++i];let u=-1;if(l){o(l);return}for(;++u<s.length;)(c[u]===null||c[u]===void 0)&&(c[u]=s[u]);s=c,d?IO(d,a)(...c):o(null,...c)}}function r(s){if(typeof s!="function")throw new TypeError("Expected `middelware` to be a function, not "+s);return e.push(s),t}}function IO(e,t){let n;return r;function r(...o){const a=e.length>o.length;let l;a&&o.push(s);try{l=e.apply(this,o)}catch(c){const d=c;if(a&&n)throw d;return s(d)}a||(l&&l.then&&typeof l.then=="function"?l.then(i,s):l instanceof Error?s(l):i(l))}function s(o,...a){n||(n=!0,t(o,...a))}function i(o){s(null,o)}}const Ts={basename:kO,dirname:EO,extname:SO,join:LO,sep:"/"};function kO(e,t){if(t!==void 0&&typeof t!="string")throw new TypeError('"ext" argument must be a string');vl(e);let n=0,r=-1,s=e.length,i;if(t===void 0||t.length===0||t.length>e.length){for(;s--;)if(e.codePointAt(s)===47){if(i){n=s+1;break}}else r<0&&(i=!0,r=s+1);return r<0?"":e.slice(n,r)}if(t===e)return"";let o=-1,a=t.length-1;for(;s--;)if(e.codePointAt(s)===47){if(i){n=s+1;break}}else o<0&&(i=!0,o=s+1),a>-1&&(e.codePointAt(s)===t.codePointAt(a--)?a<0&&(r=s):(a=-1,r=o));return n===r?r=o:r<0&&(r=e.length),e.slice(n,r)}function EO(e){if(vl(e),e.length===0)return".";let t=-1,n=e.length,r;for(;--n;)if(e.codePointAt(n)===47){if(r){t=n;break}}else r||(r=!0);return t<0?e.codePointAt(0)===47?"/":".":t===1&&e.codePointAt(0)===47?"//":e.slice(0,t)}function SO(e){vl(e);let t=e.length,n=-1,r=0,s=-1,i=0,o;for(;t--;){const a=e.codePointAt(t);if(a===47){if(o){r=t+1;break}continue}n<0&&(o=!0,n=t+1),a===46?s<0?s=t:i!==1&&(i=1):s>-1&&(i=-1)}return s<0||n<0||i===0||i===1&&s===n-1&&s===r+1?"":e.slice(s,n)}function LO(...e){let t=-1,n;for(;++t<e.length;)vl(e[t]),e[t]&&(n=n===void 0?e[t]:n+"/"+e[t]);return n===void 0?".":DO(n)}function DO(e){vl(e);const t=e.codePointAt(0)===47;let n=$O(e,!t);return n.length===0&&!t&&(n="."),n.length>0&&e.codePointAt(e.length-1)===47&&(n+="/"),t?"/"+n:n}function $O(e,t){let n="",r=0,s=-1,i=0,o=-1,a,l;for(;++o<=e.length;){if(o<e.length)a=e.codePointAt(o);else{if(a===47)break;a=47}if(a===47){if(!(s===o-1||i===1))if(s!==o-1&&i===2){if(n.length<2||r!==2||n.codePointAt(n.length-1)!==46||n.codePointAt(n.length-2)!==46){if(n.length>2){if(l=n.lastIndexOf("/"),l!==n.length-1){l<0?(n="",r=0):(n=n.slice(0,l),r=n.length-1-n.lastIndexOf("/")),s=o,i=0;continue}}else if(n.length>0){n="",r=0,s=o,i=0;continue}}t&&(n=n.length>0?n+"/..":"..",r=2)}else n.length>0?n+="/"+e.slice(s+1,o):n=e.slice(s+1,o),r=o-s-1;s=o,i=0}else a===46&&i>-1?i++:i=-1}return n}function vl(e){if(typeof e!="string")throw new TypeError("Path must be a string. Received "+JSON.stringify(e))}const FO={cwd:OO};function OO(){return"/"}function vm(e){return!!(e!==null&&typeof e=="object"&&"href"in e&&e.href&&"protocol"in e&&e.protocol&&e.auth===void 0)}function jO(e){if(typeof e=="string")e=new URL(e);else if(!vm(e)){const t=new TypeError('The "path" argument must be of type string or an instance of URL. Received `'+e+"`");throw t.code="ERR_INVALID_ARG_TYPE",t}if(e.protocol!=="file:"){const t=new TypeError("The URL must be of scheme file");throw t.code="ERR_INVALID_URL_SCHEME",t}return RO(e)}function RO(e){if(e.hostname!==""){const r=new TypeError('File URL host must be "localhost" or empty on darwin');throw r.code="ERR_INVALID_FILE_URL_HOST",r}const t=e.pathname;let n=-1;for(;++n<t.length;)if(t.codePointAt(n)===37&&t.codePointAt(n+1)===50){const r=t.codePointAt(n+2);if(r===70||r===102){const s=new TypeError("File URL path must not include encoded / characters");throw s.code="ERR_INVALID_FILE_URL_PATH",s}}return decodeURIComponent(t)}const Lp=["history","path","basename","stem","extname","dirname"];class a2{constructor(t){let n;t?vm(t)?n={path:t}:typeof t=="string"||NO(t)?n={value:t}:n=t:n={},this.cwd="cwd"in n?"":FO.cwd(),this.data={},this.history=[],this.messages=[],this.value,this.map,this.result,this.stored;let r=-1;for(;++r<Lp.length;){const i=Lp[r];i in n&&n[i]!==void 0&&n[i]!==null&&(this[i]=i==="history"?[...n[i]]:n[i])}let s;for(s in n)Lp.includes(s)||(this[s]=n[s])}get basename(){return typeof this.path=="string"?Ts.basename(this.path):void 0}set basename(t){$p(t,"basename"),Dp(t,"basename"),this.path=Ts.join(this.dirname||"",t)}get dirname(){return typeof this.path=="string"?Ts.dirname(this.path):void 0}set dirname(t){Gw(this.basename,"dirname"),this.path=Ts.join(t||"",this.basename)}get extname(){return typeof this.path=="string"?Ts.extname(this.path):void 0}set extname(t){if(Dp(t,"extname"),Gw(this.dirname,"extname"),t){if(t.codePointAt(0)!==46)throw new Error("`extname` must start with `.`");if(t.includes(".",1))throw new Error("`extname` cannot contain multiple dots")}this.path=Ts.join(this.dirname,this.stem+(t||""))}get path(){return this.history[this.history.length-1]}set path(t){vm(t)&&(t=jO(t)),$p(t,"path"),this.path!==t&&this.history.push(t)}get stem(){return typeof this.path=="string"?Ts.basename(this.path,this.extname):void 0}set stem(t){$p(t,"stem"),Dp(t,"stem"),this.path=Ts.join(this.dirname||"",t+(this.extname||""))}fail(t,n,r){const s=this.message(t,n,r);throw s.fatal=!0,s}info(t,n,r){const s=this.message(t,n,r);return s.fatal=void 0,s}message(t,n,r){const s=new ur(t,n,r);return this.path&&(s.name=this.path+":"+s.name,s.file=this.path),s.fatal=!1,this.messages.push(s),s}toString(t){return this.value===void 0?"":typeof this.value=="string"?this.value:new TextDecoder(t||void 0).decode(this.value)}}function Dp(e,t){if(e&&e.includes(Ts.sep))throw new Error("`"+t+"` cannot be a path: did not expect `"+Ts.sep+"`")}function $p(e,t){if(!e)throw new Error("`"+t+"` cannot be empty")}function Gw(e,t){if(!e)throw new Error("Setting `"+t+"` requires `path` to be set too")}function NO(e){return!!(e&&typeof e=="object"&&"byteLength"in e&&"byteOffset"in e)}const zO=(function(e){const r=this.constructor.prototype,s=r[e],i=function(){return s.apply(i,arguments)};return Object.setPrototypeOf(i,r),i}),BO={}.hasOwnProperty;class Df extends zO{constructor(){super("copy"),this.Compiler=void 0,this.Parser=void 0,this.attachers=[],this.compiler=void 0,this.freezeIndex=-1,this.frozen=void 0,this.namespace={},this.parser=void 0,this.transformers=CO()}copy(){const t=new Df;let n=-1;for(;++n<this.attachers.length;){const r=this.attachers[n];t.use(...r)}return t.data(Sp(!0,{},this.namespace)),t}data(t,n){return typeof t=="string"?arguments.length===2?(jp("data",this.frozen),this.namespace[t]=n,this):BO.call(this.namespace,t)&&this.namespace[t]||void 0:t?(jp("data",this.frozen),this.namespace=t,this):this.namespace}freeze(){if(this.frozen)return this;const t=this;for(;++this.freezeIndex<this.attachers.length;){const[n,...r]=this.attachers[this.freezeIndex];if(r[0]===!1)continue;r[0]===!0&&(r[0]=void 0);const s=n.call(t,...r);typeof s=="function"&&this.transformers.use(s)}return this.frozen=!0,this.freezeIndex=Number.POSITIVE_INFINITY,this}parse(t){this.freeze();const n=vc(t),r=this.parser||this.Parser;return Fp("parse",r),r(String(n),n)}process(t,n){const r=this;return this.freeze(),Fp("process",this.parser||this.Parser),Op("process",this.compiler||this.Compiler),n?s(void 0,n):new Promise(s);function s(i,o){const a=vc(t),l=r.parse(a);r.run(l,a,function(d,u,p){if(d||!u||!p)return c(d);const h=u,m=r.stringify(h,p);UO(m)?p.value=m:p.result=m,c(d,p)});function c(d,u){d||!u?o(d):i?i(u):n(void 0,u)}}}processSync(t){let n=!1,r;return this.freeze(),Fp("processSync",this.parser||this.Parser),Op("processSync",this.compiler||this.Compiler),this.process(t,s),Uw("processSync","process",n),r;function s(i,o){n=!0,zw(i),r=o}}run(t,n,r){Vw(t),this.freeze();const s=this.transformers;return!r&&typeof n=="function"&&(r=n,n=void 0),r?i(void 0,r):new Promise(i);function i(o,a){const l=vc(n);s.run(t,l,c);function c(d,u,p){const h=u||t;d?a(d):o?o(h):r(void 0,h,p)}}}runSync(t,n){let r=!1,s;return this.run(t,n,i),Uw("runSync","run",r),s;function i(o,a){zw(o),s=a,r=!0}}stringify(t,n){this.freeze();const r=vc(n),s=this.compiler||this.Compiler;return Op("stringify",s),Vw(t),s(t,r)}use(t,...n){const r=this.attachers,s=this.namespace;if(jp("use",this.frozen),t!=null)if(typeof t=="function")l(t,n);else if(typeof t=="object")Array.isArray(t)?a(t):o(t);else throw new TypeError("Expected usable value, not `"+t+"`");return this;function i(c){if(typeof c=="function")l(c,[]);else if(typeof c=="object")if(Array.isArray(c)){const[d,...u]=c;l(d,u)}else o(c);else throw new TypeError("Expected usable value, not `"+c+"`")}function o(c){if(!("plugins"in c)&&!("settings"in c))throw new Error("Expected usable value but received an empty preset, which is probably a mistake: presets typically come with `plugins` and sometimes with `settings`, but this has neither");a(c.plugins),c.settings&&(s.settings=Sp(!0,s.settings,c.settings))}function a(c){let d=-1;if(c!=null)if(Array.isArray(c))for(;++d<c.length;){const u=c[d];i(u)}else throw new TypeError("Expected a list of plugins, not `"+c+"`")}function l(c,d){let u=-1,p=-1;for(;++u<r.length;)if(r[u][0]===c){p=u;break}if(p===-1)r.push([c,...d]);else if(d.length>0){let[h,...m]=d;const g=r[p][1];wm(g)&&wm(h)&&(h=Sp(!0,g,h)),r[p]=[c,h,...m]}}}}const GO=new Df().freeze();function Fp(e,t){if(typeof t!="function")throw new TypeError("Cannot `"+e+"` without `parser`")}function Op(e,t){if(typeof t!="function")throw new TypeError("Cannot `"+e+"` without `compiler`")}function jp(e,t){if(t)throw new Error("Cannot call `"+e+"` on a frozen processor.\nCreate a new processor first, by calling it: use `processor()` instead of `processor`.")}function Vw(e){if(!wm(e)||typeof e.type!="string")throw new TypeError("Expected node, got `"+e+"`")}function Uw(e,t,n){if(!n)throw new Error("`"+e+"` finished async. Use `"+t+"` instead")}function vc(e){return VO(e)?e:new a2(e)}function VO(e){return!!(e&&typeof e=="object"&&"message"in e&&"messages"in e)}function UO(e){return typeof e=="string"||WO(e)}function WO(e){return!!(e&&typeof e=="object"&&"byteLength"in e&&"byteOffset"in e)}const HO="https://github.com/remarkjs/react-markdown/blob/main/changelog.md",Ww=[],Hw={allowDangerousHtml:!0},KO=/^(https?|ircs?|mailto|xmpp)$/i,qO=[{from:"astPlugins",id:"remove-buggy-html-in-markdown-parser"},{from:"allowDangerousHtml",id:"remove-buggy-html-in-markdown-parser"},{from:"allowNode",id:"replace-allownode-allowedtypes-and-disallowedtypes",to:"allowElement"},{from:"allowedTypes",id:"replace-allownode-allowedtypes-and-disallowedtypes",to:"allowedElements"},{from:"className",id:"remove-classname"},{from:"disallowedTypes",id:"replace-allownode-allowedtypes-and-disallowedtypes",to:"disallowedElements"},{from:"escapeHtml",id:"remove-buggy-html-in-markdown-parser"},{from:"includeElementIndex",id:"#remove-includeelementindex"},{from:"includeNodeIndex",id:"change-includenodeindex-to-includeelementindex"},{from:"linkTarget",id:"remove-linktarget"},{from:"plugins",id:"change-plugins-to-remarkplugins",to:"remarkPlugins"},{from:"rawSourcePos",id:"#remove-rawsourcepos"},{from:"renderers",id:"change-renderers-to-components",to:"components"},{from:"source",id:"change-source-to-children",to:"children"},{from:"sourcePos",id:"#remove-sourcepos"},{from:"transformImageUri",id:"#add-urltransform",to:"urlTransform"},{from:"transformLinkUri",id:"#add-urltransform",to:"urlTransform"}];function Kw(e){const t=QO(e),n=XO(e);return YO(t.runSync(t.parse(n),n),e)}function QO(e){const t=e.rehypePlugins||Ww,n=e.remarkPlugins||Ww,r=e.remarkRehypeOptions?{...e.remarkRehypeOptions,...Hw}:Hw;return GO().use(EF).use(n).use(AO,r).use(t)}function XO(e){const t=e.children||"",n=new a2;return typeof t=="string"&&(n.value=t),n}function YO(e,t){const n=t.allowedElements,r=t.allowElement,s=t.components,i=t.disallowedElements,o=t.skipHtml,a=t.unwrapDisallowed,l=t.urlTransform||JO;for(const d of qO)Object.hasOwn(t,d.from)&&(""+d.from+(d.to?"use `"+d.to+"` instead":"remove it")+HO+d.id,void 0);return Lf(e,c),hD(e,{Fragment:w.Fragment,components:s,ignoreInvalidStyle:!0,jsx:w.jsx,jsxs:w.jsxs,passKeys:!0,passNode:!0});function c(d,u,p){if(d.type==="raw"&&p&&typeof u=="number")return o?p.children.splice(u,1):p.children[u]={type:"text",value:d.value},u;if(d.type==="element"){let h;for(h in Cp)if(Object.hasOwn(Cp,h)&&Object.hasOwn(d.properties,h)){const m=d.properties[h],g=Cp[h];(g===null||g.includes(d.tagName))&&(d.properties[h]=l(String(m||""),h,d))}}if(d.type==="element"){let h=n?!n.includes(d.tagName):i?i.includes(d.tagName):!1;if(!h&&r&&typeof u=="number"&&(h=!r(d,u,p)),h&&p&&typeof u=="number")return a&&d.children?p.children.splice(u,1,...d.children):p.children.splice(u,1),u}}}function JO(e){const t=e.indexOf(":"),n=e.indexOf("?"),r=e.indexOf("#"),s=e.indexOf("/");return t===-1||s!==-1&&t>s||n!==-1&&t>n||r!==-1&&t>r||KO.test(e.slice(0,t))?e:""}function qw(e,t){const n=String(e);if(typeof t!="string")throw new TypeError("Expected character");let r=0,s=n.indexOf(t);for(;s!==-1;)r++,s=n.indexOf(t,s+t.length);return r}function ZO(e){if(typeof e!="string")throw new TypeError("Expected a string");return e.replace(/[|\\{}()[\]^$+*?.]/g,"\\$&").replace(/-/g,"\\x2d")}function ej(e,t,n){const s=Gu((n||{}).ignore||[]),i=tj(t);let o=-1;for(;++o<i.length;)o2(e,"text",a);function a(c,d){let u=-1,p;for(;++u<d.length;){const h=d[u],m=p?p.children:void 0;if(s(h,m?m.indexOf(h):void 0,p))return;p=h}if(p)return l(c,d)}function l(c,d){const u=d[d.length-1],p=i[o][0],h=i[o][1];let m=0;const I=u.children.indexOf(c);let f=!1,_=[];p.lastIndex=0;let T=p.exec(c.value);for(;T;){const M=T.index,v={index:T.index,input:T.input,stack:[...d,c]};let b=h(...T,v);if(typeof b=="string"&&(b=b.length>0?{type:"text",value:b}:void 0),b===!1?p.lastIndex=M+1:(m!==M&&_.push({type:"text",value:c.value.slice(m,M)}),Array.isArray(b)?_.push(...b):b&&_.push(b),m=M+T[0].length,f=!0),!p.global)break;T=p.exec(c.value)}return f?(m<c.value.length&&_.push({type:"text",value:c.value.slice(m)}),u.children.splice(I,1,..._)):_=[c],I+_.length}}function tj(e){const t=[];if(!Array.isArray(e))throw new TypeError("Expected find and replace tuple or list of tuples");const n=!e[0]||Array.isArray(e[0])?e:[e];let r=-1;for(;++r<n.length;){const s=n[r];t.push([nj(s[0]),rj(s[1])])}return t}function nj(e){return typeof e=="string"?new RegExp(ZO(e),"g"):e}function rj(e){return typeof e=="function"?e:function(){return e}}const Rp="phrasing",Np=["autolink","link","image","label"];function sj(){return{transforms:[dj],enter:{literalAutolink:oj,literalAutolinkEmail:zp,literalAutolinkHttp:zp,literalAutolinkWww:zp},exit:{literalAutolink:uj,literalAutolinkEmail:cj,literalAutolinkHttp:aj,literalAutolinkWww:lj}}}function ij(){return{unsafe:[{character:"@",before:"[+\\-.\\w]",after:"[\\-.\\w]",inConstruct:Rp,notInConstruct:Np},{character:".",before:"[Ww]",after:"[\\-.\\w]",inConstruct:Rp,notInConstruct:Np},{character:":",before:"[ps]",after:"\\/",inConstruct:Rp,notInConstruct:Np}]}}function oj(e){this.enter({type:"link",title:null,url:"",children:[]},e)}function zp(e){this.config.enter.autolinkProtocol.call(this,e)}function aj(e){this.config.exit.autolinkProtocol.call(this,e)}function lj(e){this.config.exit.data.call(this,e);const t=this.stack[this.stack.length-1];t.type,t.url="http://"+this.sliceSerialize(e)}function cj(e){this.config.exit.autolinkEmail.call(this,e)}function uj(e){this.exit(e)}function dj(e){ej(e,[[/(https?:\/\/|www(?=\.))([-.\w]+)([^ \t\r\n]*)/gi,pj],[new RegExp("(?<=^|\\s|\\p{P}|\\p{S})([-.\\w+]+)@([-\\w]+(?:\\.[-\\w]+)+)","gu"),hj]],{ignore:["link","linkReference"]})}function pj(e,t,n,r,s){let i="";if(!l2(s)||(/^w/i.test(t)&&(n=t+n,t="",i="http://"),!mj(n)))return!1;const o=fj(n+r);if(!o[0])return!1;const a={type:"link",title:null,url:i+t+o[0],children:[{type:"text",value:t+o[0]}]};return o[1]?[a,{type:"text",value:o[1]}]:a}function hj(e,t,n,r){return!l2(r,!0)||/[-\d_]$/.test(n)?!1:{type:"link",title:null,url:"mailto:"+t+"@"+n,children:[{type:"text",value:t+"@"+n}]}}function mj(e){const t=e.split(".");return!(t.length<2||t[t.length-1]&&(/_/.test(t[t.length-1])||!/[a-zA-Z\d]/.test(t[t.length-1]))||t[t.length-2]&&(/_/.test(t[t.length-2])||!/[a-zA-Z\d]/.test(t[t.length-2])))}function fj(e){const t=/[!"&'),.:;<>?\]}]+$/.exec(e);if(!t)return[e,void 0];e=e.slice(0,t.index);let n=t[0],r=n.indexOf(")");const s=qw(e,"(");let i=qw(e,")");for(;r!==-1&&s>i;)e+=n.slice(0,r+1),n=n.slice(r+1),r=n.indexOf(")"),i++;return[e,n]}function l2(e,t){const n=e.input.charCodeAt(e.index-1);return(e.index===0||Ui(n)||Nu(n))&&(!t||n!==47)}c2.peek=Aj;function gj(){this.buffer()}function _j(e){this.enter({type:"footnoteReference",identifier:"",label:""},e)}function yj(){this.buffer()}function xj(e){this.enter({type:"footnoteDefinition",identifier:"",label:"",children:[]},e)}function bj(e){const t=this.resume(),n=this.stack[this.stack.length-1];n.type,n.identifier=ls(this.sliceSerialize(e)).toLowerCase(),n.label=t}function wj(e){this.exit(e)}function vj(e){const t=this.resume(),n=this.stack[this.stack.length-1];n.type,n.identifier=ls(this.sliceSerialize(e)).toLowerCase(),n.label=t}function Mj(e){this.exit(e)}function Aj(){return"["}function c2(e,t,n,r){const s=n.createTracker(r);let i=s.move("[^");const o=n.enter("footnoteReference"),a=n.enter("reference");return i+=s.move(n.safe(n.associationId(e),{after:"]",before:i})),a(),o(),i+=s.move("]"),i}function Tj(){return{enter:{gfmFootnoteCallString:gj,gfmFootnoteCall:_j,gfmFootnoteDefinitionLabelString:yj,gfmFootnoteDefinition:xj},exit:{gfmFootnoteCallString:bj,gfmFootnoteCall:wj,gfmFootnoteDefinitionLabelString:vj,gfmFootnoteDefinition:Mj}}}function Pj(e){let t=!1;return e&&e.firstLineBlank&&(t=!0),{handlers:{footnoteDefinition:n,footnoteReference:c2},unsafe:[{character:"[",inConstruct:["label","phrasing","reference"]}]};function n(r,s,i,o){const a=i.createTracker(o);let l=a.move("[^");const c=i.enter("footnoteDefinition"),d=i.enter("label");return l+=a.move(i.safe(i.associationId(r),{before:l,after:"]"})),d(),l+=a.move("]:"),r.children&&r.children.length>0&&(a.shift(4),l+=a.move((t?`
`:" ")+i.indentLines(i.containerFlow(r,a.current()),t?u2:Cj))),c(),l}}function Cj(e,t,n){return t===0?e:u2(e,t,n)}function u2(e,t,n){return(n?"":"    ")+e}const Ij=["autolink","destinationLiteral","destinationRaw","reference","titleQuote","titleApostrophe"];d2.peek=Dj;function kj(){return{canContainEols:["delete"],enter:{strikethrough:Sj},exit:{strikethrough:Lj}}}function Ej(){return{unsafe:[{character:"~",inConstruct:"phrasing",notInConstruct:Ij}],handlers:{delete:d2}}}function Sj(e){this.enter({type:"delete",children:[]},e)}function Lj(e){this.exit(e)}function d2(e,t,n,r){const s=n.createTracker(r),i=n.enter("strikethrough");let o=s.move("~~");return o+=n.containerPhrasing(e,{...s.current(),before:o,after:"~"}),o+=s.move("~~"),i(),o}function Dj(){return"~"}function $j(e){return e.length}function Fj(e,t){const n=t||{},r=(n.align||[]).concat(),s=n.stringLength||$j,i=[],o=[],a=[],l=[];let c=0,d=-1;for(;++d<e.length;){const g=[],I=[];let f=-1;for(e[d].length>c&&(c=e[d].length);++f<e[d].length;){const _=Oj(e[d][f]);if(n.alignDelimiters!==!1){const T=s(_);I[f]=T,(l[f]===void 0||T>l[f])&&(l[f]=T)}g.push(_)}o[d]=g,a[d]=I}let u=-1;if(typeof r=="object"&&"length"in r)for(;++u<c;)i[u]=Qw(r[u]);else{const g=Qw(r);for(;++u<c;)i[u]=g}u=-1;const p=[],h=[];for(;++u<c;){const g=i[u];let I="",f="";g===99?(I=":",f=":"):g===108?I=":":g===114&&(f=":");let _=n.alignDelimiters===!1?1:Math.max(1,l[u]-I.length-f.length);const T=I+"-".repeat(_)+f;n.alignDelimiters!==!1&&(_=I.length+_+f.length,_>l[u]&&(l[u]=_),h[u]=_),p[u]=T}o.splice(1,0,p),a.splice(1,0,h),d=-1;const m=[];for(;++d<o.length;){const g=o[d],I=a[d];u=-1;const f=[];for(;++u<c;){const _=g[u]||"";let T="",M="";if(n.alignDelimiters!==!1){const v=l[u]-(I[u]||0),b=i[u];b===114?T=" ".repeat(v):b===99?v%2?(T=" ".repeat(v/2+.5),M=" ".repeat(v/2-.5)):(T=" ".repeat(v/2),M=T):M=" ".repeat(v)}n.delimiterStart!==!1&&!u&&f.push("|"),n.padding!==!1&&!(n.alignDelimiters===!1&&_==="")&&(n.delimiterStart!==!1||u)&&f.push(" "),n.alignDelimiters!==!1&&f.push(T),f.push(_),n.alignDelimiters!==!1&&f.push(M),n.padding!==!1&&f.push(" "),(n.delimiterEnd!==!1||u!==c-1)&&f.push("|")}m.push(n.delimiterEnd===!1?f.join("").replace(/ +$/,""):f.join(""))}return m.join(`
`)}function Oj(e){return e==null?"":String(e)}function Qw(e){const t=typeof e=="string"?e.codePointAt(0):0;return t===67||t===99?99:t===76||t===108?108:t===82||t===114?114:0}function jj(e,t,n,r){const s=n.enter("blockquote"),i=n.createTracker(r);i.move("> "),i.shift(2);const o=n.indentLines(n.containerFlow(e,i.current()),Rj);return s(),o}function Rj(e,t,n){return">"+(n?"":" ")+e}function Nj(e,t){return Xw(e,t.inConstruct,!0)&&!Xw(e,t.notInConstruct,!1)}function Xw(e,t,n){if(typeof t=="string"&&(t=[t]),!t||t.length===0)return n;let r=-1;for(;++r<t.length;)if(e.includes(t[r]))return!0;return!1}function Yw(e,t,n,r){let s=-1;for(;++s<n.unsafe.length;)if(n.unsafe[s].character===`
`&&Nj(n.stack,n.unsafe[s]))return/[ \t]/.test(r.before)?"":" ";return`\\
`}function zj(e,t){const n=String(e);let r=n.indexOf(t),s=r,i=0,o=0;if(typeof t!="string")throw new TypeError("Expected substring");for(;r!==-1;)r===s?++i>o&&(o=i):i=1,s=r+t.length,r=n.indexOf(t,s);return o}function Bj(e,t){return!!(t.options.fences===!1&&e.value&&!e.lang&&/[^ \r\n]/.test(e.value)&&!/^[\t ]*(?:[\r\n]|$)|(?:^|[\r\n])[\t ]*$/.test(e.value))}function Gj(e){const t=e.options.fence||"`";if(t!=="`"&&t!=="~")throw new Error("Cannot serialize code with `"+t+"` for `options.fence`, expected `` ` `` or `~`");return t}function Vj(e,t,n,r){const s=Gj(n),i=e.value||"",o=s==="`"?"GraveAccent":"Tilde";if(Bj(e,n)){const u=n.enter("codeIndented"),p=n.indentLines(i,Uj);return u(),p}const a=n.createTracker(r),l=s.repeat(Math.max(zj(i,s)+1,3)),c=n.enter("codeFenced");let d=a.move(l);if(e.lang){const u=n.enter(`codeFencedLang${o}`);d+=a.move(n.safe(e.lang,{before:d,after:" ",encode:["`"],...a.current()})),u()}if(e.lang&&e.meta){const u=n.enter(`codeFencedMeta${o}`);d+=a.move(" "),d+=a.move(n.safe(e.meta,{before:d,after:`
`,encode:["`"],...a.current()})),u()}return d+=a.move(`
`),i&&(d+=a.move(i+`
`)),d+=a.move(l),c(),d}function Uj(e,t,n){return(n?"":"    ")+e}function $f(e){const t=e.options.quote||'"';if(t!=='"'&&t!=="'")throw new Error("Cannot serialize title with `"+t+"` for `options.quote`, expected `\"`, or `'`");return t}function Wj(e,t,n,r){const s=$f(n),i=s==='"'?"Quote":"Apostrophe",o=n.enter("definition");let a=n.enter("label");const l=n.createTracker(r);let c=l.move("[");return c+=l.move(n.safe(n.associationId(e),{before:c,after:"]",...l.current()})),c+=l.move("]: "),a(),!e.url||/[\0- \u007F]/.test(e.url)?(a=n.enter("destinationLiteral"),c+=l.move("<"),c+=l.move(n.safe(e.url,{before:c,after:">",...l.current()})),c+=l.move(">")):(a=n.enter("destinationRaw"),c+=l.move(n.safe(e.url,{before:c,after:e.title?" ":`
`,...l.current()}))),a(),e.title&&(a=n.enter(`title${i}`),c+=l.move(" "+s),c+=l.move(n.safe(e.title,{before:c,after:s,...l.current()})),c+=l.move(s),a()),o(),c}function Hj(e){const t=e.options.emphasis||"*";if(t!=="*"&&t!=="_")throw new Error("Cannot serialize emphasis with `"+t+"` for `options.emphasis`, expected `*`, or `_`");return t}function ll(e){return"&#x"+e.toString(16).toUpperCase()+";"}function uu(e,t,n){const r=Bo(e),s=Bo(t);return r===void 0?s===void 0?n==="_"?{inside:!0,outside:!0}:{inside:!1,outside:!1}:s===1?{inside:!0,outside:!0}:{inside:!1,outside:!0}:r===1?s===void 0?{inside:!1,outside:!1}:s===1?{inside:!0,outside:!0}:{inside:!1,outside:!1}:s===void 0?{inside:!1,outside:!1}:s===1?{inside:!0,outside:!1}:{inside:!1,outside:!1}}p2.peek=Kj;function p2(e,t,n,r){const s=Hj(n),i=n.enter("emphasis"),o=n.createTracker(r),a=o.move(s);let l=o.move(n.containerPhrasing(e,{after:s,before:a,...o.current()}));const c=l.charCodeAt(0),d=uu(r.before.charCodeAt(r.before.length-1),c,s);d.inside&&(l=ll(c)+l.slice(1));const u=l.charCodeAt(l.length-1),p=uu(r.after.charCodeAt(0),u,s);p.inside&&(l=l.slice(0,-1)+ll(u));const h=o.move(s);return i(),n.attentionEncodeSurroundingInfo={after:p.outside,before:d.outside},a+l+h}function Kj(e,t,n){return n.options.emphasis||"*"}function qj(e,t){let n=!1;return Lf(e,function(r){if("value"in r&&/\r?\n|\r/.test(r.value)||r.type==="break")return n=!0,xm}),!!((!e.depth||e.depth<3)&&Tf(e)&&(t.options.setext||n))}function Qj(e,t,n,r){const s=Math.max(Math.min(6,e.depth||1),1),i=n.createTracker(r);if(qj(e,n)){const d=n.enter("headingSetext"),u=n.enter("phrasing"),p=n.containerPhrasing(e,{...i.current(),before:`
`,after:`
`});return u(),d(),p+`
`+(s===1?"=":"-").repeat(p.length-(Math.max(p.lastIndexOf("\r"),p.lastIndexOf(`
`))+1))}const o="#".repeat(s),a=n.enter("headingAtx"),l=n.enter("phrasing");i.move(o+" ");let c=n.containerPhrasing(e,{before:"# ",after:`
`,...i.current()});return/^[\t ]/.test(c)&&(c=ll(c.charCodeAt(0))+c.slice(1)),c=c?o+" "+c:o,n.options.closeAtx&&(c+=" "+o),l(),a(),c}h2.peek=Xj;function h2(e){return e.value||""}function Xj(){return"<"}m2.peek=Yj;function m2(e,t,n,r){const s=$f(n),i=s==='"'?"Quote":"Apostrophe",o=n.enter("image");let a=n.enter("label");const l=n.createTracker(r);let c=l.move("![");return c+=l.move(n.safe(e.alt,{before:c,after:"]",...l.current()})),c+=l.move("]("),a(),!e.url&&e.title||/[\0- \u007F]/.test(e.url)?(a=n.enter("destinationLiteral"),c+=l.move("<"),c+=l.move(n.safe(e.url,{before:c,after:">",...l.current()})),c+=l.move(">")):(a=n.enter("destinationRaw"),c+=l.move(n.safe(e.url,{before:c,after:e.title?" ":")",...l.current()}))),a(),e.title&&(a=n.enter(`title${i}`),c+=l.move(" "+s),c+=l.move(n.safe(e.title,{before:c,after:s,...l.current()})),c+=l.move(s),a()),c+=l.move(")"),o(),c}function Yj(){return"!"}f2.peek=Jj;function f2(e,t,n,r){const s=e.referenceType,i=n.enter("imageReference");let o=n.enter("label");const a=n.createTracker(r);let l=a.move("![");const c=n.safe(e.alt,{before:l,after:"]",...a.current()});l+=a.move(c+"]["),o();const d=n.stack;n.stack=[],o=n.enter("reference");const u=n.safe(n.associationId(e),{before:l,after:"]",...a.current()});return o(),n.stack=d,i(),s==="full"||!c||c!==u?l+=a.move(u+"]"):s==="shortcut"?l=l.slice(0,-1):l+=a.move("]"),l}function Jj(){return"!"}g2.peek=Zj;function g2(e,t,n){let r=e.value||"",s="`",i=-1;for(;new RegExp("(^|[^`])"+s+"([^`]|$)").test(r);)s+="`";for(/[^ \r\n]/.test(r)&&(/^[ \r\n]/.test(r)&&/[ \r\n]$/.test(r)||/^`|`$/.test(r))&&(r=" "+r+" ");++i<n.unsafe.length;){const o=n.unsafe[i],a=n.compilePattern(o);let l;if(o.atBreak)for(;l=a.exec(r);){let c=l.index;r.charCodeAt(c)===10&&r.charCodeAt(c-1)===13&&c--,r=r.slice(0,c)+" "+r.slice(l.index+1)}}return s+r+s}function Zj(){return"`"}function _2(e,t){const n=Tf(e);return!!(!t.options.resourceLink&&e.url&&!e.title&&e.children&&e.children.length===1&&e.children[0].type==="text"&&(n===e.url||"mailto:"+n===e.url)&&/^[a-z][a-z+.-]+:/i.test(e.url)&&!/[\0- <>\u007F]/.test(e.url))}y2.peek=eR;function y2(e,t,n,r){const s=$f(n),i=s==='"'?"Quote":"Apostrophe",o=n.createTracker(r);let a,l;if(_2(e,n)){const d=n.stack;n.stack=[],a=n.enter("autolink");let u=o.move("<");return u+=o.move(n.containerPhrasing(e,{before:u,after:">",...o.current()})),u+=o.move(">"),a(),n.stack=d,u}a=n.enter("link"),l=n.enter("label");let c=o.move("[");return c+=o.move(n.containerPhrasing(e,{before:c,after:"](",...o.current()})),c+=o.move("]("),l(),!e.url&&e.title||/[\0- \u007F]/.test(e.url)?(l=n.enter("destinationLiteral"),c+=o.move("<"),c+=o.move(n.safe(e.url,{before:c,after:">",...o.current()})),c+=o.move(">")):(l=n.enter("destinationRaw"),c+=o.move(n.safe(e.url,{before:c,after:e.title?" ":")",...o.current()}))),l(),e.title&&(l=n.enter(`title${i}`),c+=o.move(" "+s),c+=o.move(n.safe(e.title,{before:c,after:s,...o.current()})),c+=o.move(s),l()),c+=o.move(")"),a(),c}function eR(e,t,n){return _2(e,n)?"<":"["}x2.peek=tR;function x2(e,t,n,r){const s=e.referenceType,i=n.enter("linkReference");let o=n.enter("label");const a=n.createTracker(r);let l=a.move("[");const c=n.containerPhrasing(e,{before:l,after:"]",...a.current()});l+=a.move(c+"]["),o();const d=n.stack;n.stack=[],o=n.enter("reference");const u=n.safe(n.associationId(e),{before:l,after:"]",...a.current()});return o(),n.stack=d,i(),s==="full"||!c||c!==u?l+=a.move(u+"]"):s==="shortcut"?l=l.slice(0,-1):l+=a.move("]"),l}function tR(){return"["}function Ff(e){const t=e.options.bullet||"*";if(t!=="*"&&t!=="+"&&t!=="-")throw new Error("Cannot serialize items with `"+t+"` for `options.bullet`, expected `*`, `+`, or `-`");return t}function nR(e){const t=Ff(e),n=e.options.bulletOther;if(!n)return t==="*"?"-":"*";if(n!=="*"&&n!=="+"&&n!=="-")throw new Error("Cannot serialize items with `"+n+"` for `options.bulletOther`, expected `*`, `+`, or `-`");if(n===t)throw new Error("Expected `bullet` (`"+t+"`) and `bulletOther` (`"+n+"`) to be different");return n}function rR(e){const t=e.options.bulletOrdered||".";if(t!=="."&&t!==")")throw new Error("Cannot serialize items with `"+t+"` for `options.bulletOrdered`, expected `.` or `)`");return t}function b2(e){const t=e.options.rule||"*";if(t!=="*"&&t!=="-"&&t!=="_")throw new Error("Cannot serialize rules with `"+t+"` for `options.rule`, expected `*`, `-`, or `_`");return t}function sR(e,t,n,r){const s=n.enter("list"),i=n.bulletCurrent;let o=e.ordered?rR(n):Ff(n);const a=e.ordered?o==="."?")":".":nR(n);let l=t&&n.bulletLastUsed?o===n.bulletLastUsed:!1;if(!e.ordered){const d=e.children?e.children[0]:void 0;if((o==="*"||o==="-")&&d&&(!d.children||!d.children[0])&&n.stack[n.stack.length-1]==="list"&&n.stack[n.stack.length-2]==="listItem"&&n.stack[n.stack.length-3]==="list"&&n.stack[n.stack.length-4]==="listItem"&&n.indexStack[n.indexStack.length-1]===0&&n.indexStack[n.indexStack.length-2]===0&&n.indexStack[n.indexStack.length-3]===0&&(l=!0),b2(n)===o&&d){let u=-1;for(;++u<e.children.length;){const p=e.children[u];if(p&&p.type==="listItem"&&p.children&&p.children[0]&&p.children[0].type==="thematicBreak"){l=!0;break}}}}l&&(o=a),n.bulletCurrent=o;const c=n.containerFlow(e,r);return n.bulletLastUsed=o,n.bulletCurrent=i,s(),c}function iR(e){const t=e.options.listItemIndent||"one";if(t!=="tab"&&t!=="one"&&t!=="mixed")throw new Error("Cannot serialize items with `"+t+"` for `options.listItemIndent`, expected `tab`, `one`, or `mixed`");return t}function oR(e,t,n,r){const s=iR(n);let i=n.bulletCurrent||Ff(n);t&&t.type==="list"&&t.ordered&&(i=(typeof t.start=="number"&&t.start>-1?t.start:1)+(n.options.incrementListMarker===!1?0:t.children.indexOf(e))+i);let o=i.length+1;(s==="tab"||s==="mixed"&&(t&&t.type==="list"&&t.spread||e.spread))&&(o=Math.ceil(o/4)*4);const a=n.createTracker(r);a.move(i+" ".repeat(o-i.length)),a.shift(o);const l=n.enter("listItem"),c=n.indentLines(n.containerFlow(e,a.current()),d);return l(),c;function d(u,p,h){return p?(h?"":" ".repeat(o))+u:(h?i:i+" ".repeat(o-i.length))+u}}function aR(e,t,n,r){const s=n.enter("paragraph"),i=n.enter("phrasing"),o=n.containerPhrasing(e,r);return i(),s(),o}const lR=Gu(["break","delete","emphasis","footnote","footnoteReference","image","imageReference","inlineCode","inlineMath","link","linkReference","mdxJsxTextElement","mdxTextExpression","strong","text","textDirective"]);function cR(e,t,n,r){return(e.children.some(function(o){return lR(o)})?n.containerPhrasing:n.containerFlow).call(n,e,r)}function uR(e){const t=e.options.strong||"*";if(t!=="*"&&t!=="_")throw new Error("Cannot serialize strong with `"+t+"` for `options.strong`, expected `*`, or `_`");return t}w2.peek=dR;function w2(e,t,n,r){const s=uR(n),i=n.enter("strong"),o=n.createTracker(r),a=o.move(s+s);let l=o.move(n.containerPhrasing(e,{after:s,before:a,...o.current()}));const c=l.charCodeAt(0),d=uu(r.before.charCodeAt(r.before.length-1),c,s);d.inside&&(l=ll(c)+l.slice(1));const u=l.charCodeAt(l.length-1),p=uu(r.after.charCodeAt(0),u,s);p.inside&&(l=l.slice(0,-1)+ll(u));const h=o.move(s+s);return i(),n.attentionEncodeSurroundingInfo={after:p.outside,before:d.outside},a+l+h}function dR(e,t,n){return n.options.strong||"*"}function pR(e,t,n,r){return n.safe(e.value,r)}function hR(e){const t=e.options.ruleRepetition||3;if(t<3)throw new Error("Cannot serialize rules with repetition `"+t+"` for `options.ruleRepetition`, expected `3` or more");return t}function mR(e,t,n){const r=(b2(n)+(n.options.ruleSpaces?" ":"")).repeat(hR(n));return n.options.ruleSpaces?r.slice(0,-1):r}const v2={blockquote:jj,break:Yw,code:Vj,definition:Wj,emphasis:p2,hardBreak:Yw,heading:Qj,html:h2,image:m2,imageReference:f2,inlineCode:g2,link:y2,linkReference:x2,list:sR,listItem:oR,paragraph:aR,root:cR,strong:w2,text:pR,thematicBreak:mR};function fR(){return{enter:{table:gR,tableData:Jw,tableHeader:Jw,tableRow:yR},exit:{codeText:xR,table:_R,tableData:Bp,tableHeader:Bp,tableRow:Bp}}}function gR(e){const t=e._align;this.enter({type:"table",align:t.map(function(n){return n==="none"?null:n}),children:[]},e),this.data.inTable=!0}function _R(e){this.exit(e),this.data.inTable=void 0}function yR(e){this.enter({type:"tableRow",children:[]},e)}function Bp(e){this.exit(e)}function Jw(e){this.enter({type:"tableCell",children:[]},e)}function xR(e){let t=this.resume();this.data.inTable&&(t=t.replace(/\\([\\|])/g,bR));const n=this.stack[this.stack.length-1];n.type,n.value=t,this.exit(e)}function bR(e,t){return t==="|"?t:e}function wR(e){const t=e||{},n=t.tableCellPadding,r=t.tablePipeAlign,s=t.stringLength,i=n?" ":"|";return{unsafe:[{character:"\r",inConstruct:"tableCell"},{character:`
`,inConstruct:"tableCell"},{atBreak:!0,character:"|",after:"[	 :-]"},{character:"|",inConstruct:"tableCell"},{atBreak:!0,character:":",after:"-"},{atBreak:!0,character:"-",after:"[:|-]"}],handlers:{inlineCode:p,table:o,tableCell:l,tableRow:a}};function o(h,m,g,I){return c(d(h,g,I),h.align)}function a(h,m,g,I){const f=u(h,g,I),_=c([f]);return _.slice(0,_.indexOf(`
`))}function l(h,m,g,I){const f=g.enter("tableCell"),_=g.enter("phrasing"),T=g.containerPhrasing(h,{...I,before:i,after:i});return _(),f(),T}function c(h,m){return Fj(h,{align:m,alignDelimiters:r,padding:n,stringLength:s})}function d(h,m,g){const I=h.children;let f=-1;const _=[],T=m.enter("table");for(;++f<I.length;)_[f]=u(I[f],m,g);return T(),_}function u(h,m,g){const I=h.children;let f=-1;const _=[],T=m.enter("tableRow");for(;++f<I.length;)_[f]=l(I[f],h,m,g);return T(),_}function p(h,m,g){let I=v2.inlineCode(h,m,g);return g.stack.includes("tableCell")&&(I=I.replace(/\|/g,"\\$&")),I}}function vR(){return{exit:{taskListCheckValueChecked:Zw,taskListCheckValueUnchecked:Zw,paragraph:AR}}}function MR(){return{unsafe:[{atBreak:!0,character:"-",after:"[:|-]"}],handlers:{listItem:TR}}}function Zw(e){const t=this.stack[this.stack.length-2];t.type,t.checked=e.type==="taskListCheckValueChecked"}function AR(e){const t=this.stack[this.stack.length-2];if(t&&t.type==="listItem"&&typeof t.checked=="boolean"){const n=this.stack[this.stack.length-1];n.type;const r=n.children[0];if(r&&r.type==="text"){const s=t.children;let i=-1,o;for(;++i<s.length;){const a=s[i];if(a.type==="paragraph"){o=a;break}}o===n&&(r.value=r.value.slice(1),r.value.length===0?n.children.shift():n.position&&r.position&&typeof r.position.start.offset=="number"&&(r.position.start.column++,r.position.start.offset++,n.position.start=Object.assign({},r.position.start)))}}this.exit(e)}function TR(e,t,n,r){const s=e.children[0],i=typeof e.checked=="boolean"&&s&&s.type==="paragraph",o="["+(e.checked?"x":" ")+"] ",a=n.createTracker(r);i&&a.move(o);let l=v2.listItem(e,t,n,{...r,...a.current()});return i&&(l=l.replace(/^(?:[*+-]|\d+\.)([\r\n]| {1,3})/,c)),l;function c(d){return d+o}}function PR(){return[sj(),Tj(),kj(),fR(),vR()]}function CR(e){return{extensions:[ij(),Pj(e),Ej(),wR(e),MR()]}}const IR={tokenize:$R,partial:!0},M2={tokenize:FR,partial:!0},A2={tokenize:OR,partial:!0},T2={tokenize:jR,partial:!0},kR={tokenize:RR,partial:!0},P2={name:"wwwAutolink",tokenize:LR,previous:I2},C2={name:"protocolAutolink",tokenize:DR,previous:k2},Us={name:"emailAutolink",tokenize:SR,previous:E2},Es={};function ER(){return{text:Es}}let Pi=48;for(;Pi<123;)Es[Pi]=Us,Pi++,Pi===58?Pi=65:Pi===91&&(Pi=97);Es[43]=Us;Es[45]=Us;Es[46]=Us;Es[95]=Us;Es[72]=[Us,C2];Es[104]=[Us,C2];Es[87]=[Us,P2];Es[119]=[Us,P2];function SR(e,t,n){const r=this;let s,i;return o;function o(u){return!Mm(u)||!E2.call(r,r.previous)||Of(r.events)?n(u):(e.enter("literalAutolink"),e.enter("literalAutolinkEmail"),a(u))}function a(u){return Mm(u)?(e.consume(u),a):u===64?(e.consume(u),l):n(u)}function l(u){return u===46?e.check(kR,d,c)(u):u===45||u===95||lr(u)?(i=!0,e.consume(u),l):d(u)}function c(u){return e.consume(u),s=!0,l}function d(u){return i&&s&&mr(r.previous)?(e.exit("literalAutolinkEmail"),e.exit("literalAutolink"),t(u)):n(u)}}function LR(e,t,n){const r=this;return s;function s(o){return o!==87&&o!==119||!I2.call(r,r.previous)||Of(r.events)?n(o):(e.enter("literalAutolink"),e.enter("literalAutolinkWww"),e.check(IR,e.attempt(M2,e.attempt(A2,i),n),n)(o))}function i(o){return e.exit("literalAutolinkWww"),e.exit("literalAutolink"),t(o)}}function DR(e,t,n){const r=this;let s="",i=!1;return o;function o(u){return(u===72||u===104)&&k2.call(r,r.previous)&&!Of(r.events)?(e.enter("literalAutolink"),e.enter("literalAutolinkHttp"),s+=String.fromCodePoint(u),e.consume(u),a):n(u)}function a(u){if(mr(u)&&s.length<5)return s+=String.fromCodePoint(u),e.consume(u),a;if(u===58){const p=s.toLowerCase();if(p==="http"||p==="https")return e.consume(u),l}return n(u)}function l(u){return u===47?(e.consume(u),i?c:(i=!0,l)):n(u)}function c(u){return u===null||au(u)||cn(u)||Ui(u)||Nu(u)?n(u):e.attempt(M2,e.attempt(A2,d),n)(u)}function d(u){return e.exit("literalAutolinkHttp"),e.exit("literalAutolink"),t(u)}}function $R(e,t,n){let r=0;return s;function s(o){return(o===87||o===119)&&r<3?(r++,e.consume(o),s):o===46&&r===3?(e.consume(o),i):n(o)}function i(o){return o===null?n(o):t(o)}}function FR(e,t,n){let r,s,i;return o;function o(c){return c===46||c===95?e.check(T2,l,a)(c):c===null||cn(c)||Ui(c)||c!==45&&Nu(c)?l(c):(i=!0,e.consume(c),o)}function a(c){return c===95?r=!0:(s=r,r=void 0),e.consume(c),o}function l(c){return s||r||!i?n(c):t(c)}}function OR(e,t){let n=0,r=0;return s;function s(o){return o===40?(n++,e.consume(o),s):o===41&&r<n?i(o):o===33||o===34||o===38||o===39||o===41||o===42||o===44||o===46||o===58||o===59||o===60||o===63||o===93||o===95||o===126?e.check(T2,t,i)(o):o===null||cn(o)||Ui(o)?t(o):(e.consume(o),s)}function i(o){return o===41&&r++,e.consume(o),s}}function jR(e,t,n){return r;function r(a){return a===33||a===34||a===39||a===41||a===42||a===44||a===46||a===58||a===59||a===63||a===95||a===126?(e.consume(a),r):a===38?(e.consume(a),i):a===93?(e.consume(a),s):a===60||a===null||cn(a)||Ui(a)?t(a):n(a)}function s(a){return a===null||a===40||a===91||cn(a)||Ui(a)?t(a):r(a)}function i(a){return mr(a)?o(a):n(a)}function o(a){return a===59?(e.consume(a),r):mr(a)?(e.consume(a),o):n(a)}}function RR(e,t,n){return r;function r(i){return e.consume(i),s}function s(i){return lr(i)?n(i):t(i)}}function I2(e){return e===null||e===40||e===42||e===95||e===91||e===93||e===126||cn(e)}function k2(e){return!mr(e)}function E2(e){return!(e===47||Mm(e))}function Mm(e){return e===43||e===45||e===46||e===95||lr(e)}function Of(e){let t=e.length,n=!1;for(;t--;){const r=e[t][1];if((r.type==="labelLink"||r.type==="labelImage")&&!r._balanced){n=!0;break}if(r._gfmAutolinkLiteralWalkedInto){n=!1;break}}return e.length>0&&!n&&(e[e.length-1][1]._gfmAutolinkLiteralWalkedInto=!0),n}const NR={tokenize:KR,partial:!0};function zR(){return{document:{91:{name:"gfmFootnoteDefinition",tokenize:UR,continuation:{tokenize:WR},exit:HR}},text:{91:{name:"gfmFootnoteCall",tokenize:VR},93:{name:"gfmPotentialFootnoteCall",add:"after",tokenize:BR,resolveTo:GR}}}}function BR(e,t,n){const r=this;let s=r.events.length;const i=r.parser.gfmFootnotes||(r.parser.gfmFootnotes=[]);let o;for(;s--;){const l=r.events[s][1];if(l.type==="labelImage"){o=l;break}if(l.type==="gfmFootnoteCall"||l.type==="labelLink"||l.type==="label"||l.type==="image"||l.type==="link")break}return a;function a(l){if(!o||!o._balanced)return n(l);const c=ls(r.sliceSerialize({start:o.end,end:r.now()}));return c.codePointAt(0)!==94||!i.includes(c.slice(1))?n(l):(e.enter("gfmFootnoteCallLabelMarker"),e.consume(l),e.exit("gfmFootnoteCallLabelMarker"),t(l))}}function GR(e,t){let n=e.length;for(;n--;)if(e[n][1].type==="labelImage"&&e[n][0]==="enter"){e[n][1];break}e[n+1][1].type="data",e[n+3][1].type="gfmFootnoteCallLabelMarker";const r={type:"gfmFootnoteCall",start:Object.assign({},e[n+3][1].start),end:Object.assign({},e[e.length-1][1].end)},s={type:"gfmFootnoteCallMarker",start:Object.assign({},e[n+3][1].end),end:Object.assign({},e[n+3][1].end)};s.end.column++,s.end.offset++,s.end._bufferIndex++;const i={type:"gfmFootnoteCallString",start:Object.assign({},s.end),end:Object.assign({},e[e.length-1][1].start)},o={type:"chunkString",contentType:"string",start:Object.assign({},i.start),end:Object.assign({},i.end)},a=[e[n+1],e[n+2],["enter",r,t],e[n+3],e[n+4],["enter",s,t],["exit",s,t],["enter",i,t],["enter",o,t],["exit",o,t],["exit",i,t],e[e.length-2],e[e.length-1],["exit",r,t]];return e.splice(n,e.length-n+1,...a),e}function VR(e,t,n){const r=this,s=r.parser.gfmFootnotes||(r.parser.gfmFootnotes=[]);let i=0,o;return a;function a(u){return e.enter("gfmFootnoteCall"),e.enter("gfmFootnoteCallLabelMarker"),e.consume(u),e.exit("gfmFootnoteCallLabelMarker"),l}function l(u){return u!==94?n(u):(e.enter("gfmFootnoteCallMarker"),e.consume(u),e.exit("gfmFootnoteCallMarker"),e.enter("gfmFootnoteCallString"),e.enter("chunkString").contentType="string",c)}function c(u){if(i>999||u===93&&!o||u===null||u===91||cn(u))return n(u);if(u===93){e.exit("chunkString");const p=e.exit("gfmFootnoteCallString");return s.includes(ls(r.sliceSerialize(p)))?(e.enter("gfmFootnoteCallLabelMarker"),e.consume(u),e.exit("gfmFootnoteCallLabelMarker"),e.exit("gfmFootnoteCall"),t):n(u)}return cn(u)||(o=!0),i++,e.consume(u),u===92?d:c}function d(u){return u===91||u===92||u===93?(e.consume(u),i++,c):c(u)}}function UR(e,t,n){const r=this,s=r.parser.gfmFootnotes||(r.parser.gfmFootnotes=[]);let i,o=0,a;return l;function l(m){return e.enter("gfmFootnoteDefinition")._container=!0,e.enter("gfmFootnoteDefinitionLabel"),e.enter("gfmFootnoteDefinitionLabelMarker"),e.consume(m),e.exit("gfmFootnoteDefinitionLabelMarker"),c}function c(m){return m===94?(e.enter("gfmFootnoteDefinitionMarker"),e.consume(m),e.exit("gfmFootnoteDefinitionMarker"),e.enter("gfmFootnoteDefinitionLabelString"),e.enter("chunkString").contentType="string",d):n(m)}function d(m){if(o>999||m===93&&!a||m===null||m===91||cn(m))return n(m);if(m===93){e.exit("chunkString");const g=e.exit("gfmFootnoteDefinitionLabelString");return i=ls(r.sliceSerialize(g)),e.enter("gfmFootnoteDefinitionLabelMarker"),e.consume(m),e.exit("gfmFootnoteDefinitionLabelMarker"),e.exit("gfmFootnoteDefinitionLabel"),p}return cn(m)||(a=!0),o++,e.consume(m),m===92?u:d}function u(m){return m===91||m===92||m===93?(e.consume(m),o++,d):d(m)}function p(m){return m===58?(e.enter("definitionMarker"),e.consume(m),e.exit("definitionMarker"),s.includes(i)||s.push(i),qt(e,h,"gfmFootnoteDefinitionWhitespace")):n(m)}function h(m){return t(m)}}function WR(e,t,n){return e.check(wl,t,e.attempt(NR,t,n))}function HR(e){e.exit("gfmFootnoteDefinition")}function KR(e,t,n){const r=this;return qt(e,s,"gfmFootnoteDefinitionIndent",5);function s(i){const o=r.events[r.events.length-1];return o&&o[1].type==="gfmFootnoteDefinitionIndent"&&o[2].sliceSerialize(o[1],!0).length===4?t(i):n(i)}}function qR(e){let n=(e||{}).singleTilde;const r={name:"strikethrough",tokenize:i,resolveAll:s};return n==null&&(n=!0),{text:{126:r},insideSpan:{null:[r]},attentionMarkers:{null:[126]}};function s(o,a){let l=-1;for(;++l<o.length;)if(o[l][0]==="enter"&&o[l][1].type==="strikethroughSequenceTemporary"&&o[l][1]._close){let c=l;for(;c--;)if(o[c][0]==="exit"&&o[c][1].type==="strikethroughSequenceTemporary"&&o[c][1]._open&&o[l][1].end.offset-o[l][1].start.offset===o[c][1].end.offset-o[c][1].start.offset){o[l][1].type="strikethroughSequence",o[c][1].type="strikethroughSequence";const d={type:"strikethrough",start:Object.assign({},o[c][1].start),end:Object.assign({},o[l][1].end)},u={type:"strikethroughText",start:Object.assign({},o[c][1].end),end:Object.assign({},o[l][1].start)},p=[["enter",d,a],["enter",o[c][1],a],["exit",o[c][1],a],["enter",u,a]],h=a.parser.constructs.insideSpan.null;h&&Er(p,p.length,0,zu(h,o.slice(c+1,l),a)),Er(p,p.length,0,[["exit",u,a],["enter",o[l][1],a],["exit",o[l][1],a],["exit",d,a]]),Er(o,c-1,l-c+3,p),l=c+p.length-2;break}}for(l=-1;++l<o.length;)o[l][1].type==="strikethroughSequenceTemporary"&&(o[l][1].type="data");return o}function i(o,a,l){const c=this.previous,d=this.events;let u=0;return p;function p(m){return c===126&&d[d.length-1][1].type!=="characterEscape"?l(m):(o.enter("strikethroughSequenceTemporary"),h(m))}function h(m){const g=Bo(c);if(m===126)return u>1?l(m):(o.consume(m),u++,h);if(u<2&&!n)return l(m);const I=o.exit("strikethroughSequenceTemporary"),f=Bo(m);return I._open=!f||f===2&&!!g,I._close=!g||g===2&&!!f,a(m)}}}class QR{constructor(){this.map=[]}add(t,n,r){XR(this,t,n,r)}consume(t){if(this.map.sort(function(i,o){return i[0]-o[0]}),this.map.length===0)return;let n=this.map.length;const r=[];for(;n>0;)n-=1,r.push(t.slice(this.map[n][0]+this.map[n][1]),this.map[n][2]),t.length=this.map[n][0];r.push(t.slice()),t.length=0;let s=r.pop();for(;s;){for(const i of s)t.push(i);s=r.pop()}this.map.length=0}}function XR(e,t,n,r){let s=0;if(!(n===0&&r.length===0)){for(;s<e.map.length;){if(e.map[s][0]===t){e.map[s][1]+=n,e.map[s][2].push(...r);return}s+=1}e.map.push([t,n,r])}}function YR(e,t){let n=!1;const r=[];for(;t<e.length;){const s=e[t];if(n){if(s[0]==="enter")s[1].type==="tableContent"&&r.push(e[t+1][1].type==="tableDelimiterMarker"?"left":"none");else if(s[1].type==="tableContent"){if(e[t-1][1].type==="tableDelimiterMarker"){const i=r.length-1;r[i]=r[i]==="left"?"center":"right"}}else if(s[1].type==="tableDelimiterRow")break}else s[0]==="enter"&&s[1].type==="tableDelimiterRow"&&(n=!0);t+=1}return r}function JR(){return{flow:{null:{name:"table",tokenize:ZR,resolveAll:eN}}}}function ZR(e,t,n){const r=this;let s=0,i=0,o;return a;function a(L){let G=r.events.length-1;for(;G>-1;){const K=r.events[G][1].type;if(K==="lineEnding"||K==="linePrefix")G--;else break}const j=G>-1?r.events[G][1].type:null,R=j==="tableHead"||j==="tableRow"?b:l;return R===b&&r.parser.lazy[r.now().line]?n(L):R(L)}function l(L){return e.enter("tableHead"),e.enter("tableRow"),c(L)}function c(L){return L===124||(o=!0,i+=1),d(L)}function d(L){return L===null?n(L):Tt(L)?i>1?(i=0,r.interrupt=!0,e.exit("tableRow"),e.enter("lineEnding"),e.consume(L),e.exit("lineEnding"),h):n(L):Wt(L)?qt(e,d,"whitespace")(L):(i+=1,o&&(o=!1,s+=1),L===124?(e.enter("tableCellDivider"),e.consume(L),e.exit("tableCellDivider"),o=!0,d):(e.enter("data"),u(L)))}function u(L){return L===null||L===124||cn(L)?(e.exit("data"),d(L)):(e.consume(L),L===92?p:u)}function p(L){return L===92||L===124?(e.consume(L),u):u(L)}function h(L){return r.interrupt=!1,r.parser.lazy[r.now().line]?n(L):(e.enter("tableDelimiterRow"),o=!1,Wt(L)?qt(e,m,"linePrefix",r.parser.constructs.disable.null.includes("codeIndented")?void 0:4)(L):m(L))}function m(L){return L===45||L===58?I(L):L===124?(o=!0,e.enter("tableCellDivider"),e.consume(L),e.exit("tableCellDivider"),g):v(L)}function g(L){return Wt(L)?qt(e,I,"whitespace")(L):I(L)}function I(L){return L===58?(i+=1,o=!0,e.enter("tableDelimiterMarker"),e.consume(L),e.exit("tableDelimiterMarker"),f):L===45?(i+=1,f(L)):L===null||Tt(L)?M(L):v(L)}function f(L){return L===45?(e.enter("tableDelimiterFiller"),_(L)):v(L)}function _(L){return L===45?(e.consume(L),_):L===58?(o=!0,e.exit("tableDelimiterFiller"),e.enter("tableDelimiterMarker"),e.consume(L),e.exit("tableDelimiterMarker"),T):(e.exit("tableDelimiterFiller"),T(L))}function T(L){return Wt(L)?qt(e,M,"whitespace")(L):M(L)}function M(L){return L===124?m(L):L===null||Tt(L)?!o||s!==i?v(L):(e.exit("tableDelimiterRow"),e.exit("tableHead"),t(L)):v(L)}function v(L){return n(L)}function b(L){return e.enter("tableRow"),A(L)}function A(L){return L===124?(e.enter("tableCellDivider"),e.consume(L),e.exit("tableCellDivider"),A):L===null||Tt(L)?(e.exit("tableRow"),t(L)):Wt(L)?qt(e,A,"whitespace")(L):(e.enter("data"),k(L))}function k(L){return L===null||L===124||cn(L)?(e.exit("data"),A(L)):(e.consume(L),L===92?F:k)}function F(L){return L===92||L===124?(e.consume(L),k):k(L)}}function eN(e,t){let n=-1,r=!0,s=0,i=[0,0,0,0],o=[0,0,0,0],a=!1,l=0,c,d,u;const p=new QR;for(;++n<e.length;){const h=e[n],m=h[1];h[0]==="enter"?m.type==="tableHead"?(a=!1,l!==0&&(e1(p,t,l,c,d),d=void 0,l=0),c={type:"table",start:Object.assign({},m.start),end:Object.assign({},m.end)},p.add(n,0,[["enter",c,t]])):m.type==="tableRow"||m.type==="tableDelimiterRow"?(r=!0,u=void 0,i=[0,0,0,0],o=[0,n+1,0,0],a&&(a=!1,d={type:"tableBody",start:Object.assign({},m.start),end:Object.assign({},m.end)},p.add(n,0,[["enter",d,t]])),s=m.type==="tableDelimiterRow"?2:d?3:1):s&&(m.type==="data"||m.type==="tableDelimiterMarker"||m.type==="tableDelimiterFiller")?(r=!1,o[2]===0&&(i[1]!==0&&(o[0]=o[1],u=Mc(p,t,i,s,void 0,u),i=[0,0,0,0]),o[2]=n)):m.type==="tableCellDivider"&&(r?r=!1:(i[1]!==0&&(o[0]=o[1],u=Mc(p,t,i,s,void 0,u)),i=o,o=[i[1],n,0,0])):m.type==="tableHead"?(a=!0,l=n):m.type==="tableRow"||m.type==="tableDelimiterRow"?(l=n,i[1]!==0?(o[0]=o[1],u=Mc(p,t,i,s,n,u)):o[1]!==0&&(u=Mc(p,t,o,s,n,u)),s=0):s&&(m.type==="data"||m.type==="tableDelimiterMarker"||m.type==="tableDelimiterFiller")&&(o[3]=n)}for(l!==0&&e1(p,t,l,c,d),p.consume(t.events),n=-1;++n<t.events.length;){const h=t.events[n];h[0]==="enter"&&h[1].type==="table"&&(h[1]._align=YR(t.events,n))}return e}function Mc(e,t,n,r,s,i){const o=r===1?"tableHeader":r===2?"tableDelimiter":"tableData",a="tableContent";n[0]!==0&&(i.end=Object.assign({},Po(t.events,n[0])),e.add(n[0],0,[["exit",i,t]]));const l=Po(t.events,n[1]);if(i={type:o,start:Object.assign({},l),end:Object.assign({},l)},e.add(n[1],0,[["enter",i,t]]),n[2]!==0){const c=Po(t.events,n[2]),d=Po(t.events,n[3]),u={type:a,start:Object.assign({},c),end:Object.assign({},d)};if(e.add(n[2],0,[["enter",u,t]]),r!==2){const p=t.events[n[2]],h=t.events[n[3]];if(p[1].end=Object.assign({},h[1].end),p[1].type="chunkText",p[1].contentType="text",n[3]>n[2]+1){const m=n[2]+1,g=n[3]-n[2]-1;e.add(m,g,[])}}e.add(n[3]+1,0,[["exit",u,t]])}return s!==void 0&&(i.end=Object.assign({},Po(t.events,s)),e.add(s,0,[["exit",i,t]]),i=void 0),i}function e1(e,t,n,r,s){const i=[],o=Po(t.events,n);s&&(s.end=Object.assign({},o),i.push(["exit",s,t])),r.end=Object.assign({},o),i.push(["exit",r,t]),e.add(n+1,0,i)}function Po(e,t){const n=e[t],r=n[0]==="enter"?"start":"end";return n[1][r]}const tN={name:"tasklistCheck",tokenize:rN};function nN(){return{text:{91:tN}}}function rN(e,t,n){const r=this;return s;function s(l){return r.previous!==null||!r._gfmTasklistFirstContentOfListItem?n(l):(e.enter("taskListCheck"),e.enter("taskListCheckMarker"),e.consume(l),e.exit("taskListCheckMarker"),i)}function i(l){return cn(l)?(e.enter("taskListCheckValueUnchecked"),e.consume(l),e.exit("taskListCheckValueUnchecked"),o):l===88||l===120?(e.enter("taskListCheckValueChecked"),e.consume(l),e.exit("taskListCheckValueChecked"),o):n(l)}function o(l){return l===93?(e.enter("taskListCheckMarker"),e.consume(l),e.exit("taskListCheckMarker"),e.exit("taskListCheck"),a):n(l)}function a(l){return Tt(l)?t(l):Wt(l)?e.check({tokenize:sN},t,n)(l):n(l)}}function sN(e,t,n){return qt(e,r,"whitespace");function r(s){return s===null?n(s):t(s)}}function iN(e){return BP([ER(),zR(),qR(e),JR(),nN()])}const oN={};function t1(e){const t=this,n=e||oN,r=t.data(),s=r.micromarkExtensions||(r.micromarkExtensions=[]),i=r.fromMarkdownExtensions||(r.fromMarkdownExtensions=[]),o=r.toMarkdownExtensions||(r.toMarkdownExtensions=[]);s.push(iN(n)),i.push(PR()),o.push(CR(n))}const jf=Me.createContext({});function Rf(e){const t=Me.useRef(null);return t.current===null&&(t.current=e()),t.current}const S2=typeof window<"u",L2=S2?Me.useLayoutEffect:Me.useEffect,Uu=Me.createContext(null);function Nf(e,t){e.indexOf(t)===-1&&e.push(t)}function zf(e,t){const n=e.indexOf(t);n>-1&&e.splice(n,1)}const ks=(e,t,n)=>n>t?t:n<e?e:n;let Bf=()=>{};const Gs={},D2=e=>/^-?(?:\d+(?:\.\d+)?|\.\d+)$/u.test(e);function $2(e){return typeof e=="object"&&e!==null}const F2=e=>/^0[^.\s]+$/u.test(e);function Gf(e){let t;return()=>(t===void 0&&(t=e()),t)}const Hr=e=>e,aN=(e,t)=>n=>t(e(n)),Ml=(...e)=>e.reduce(aN),cl=(e,t,n)=>{const r=t-e;return r===0?1:(n-e)/r};class Vf{constructor(){this.subscriptions=[]}add(t){return Nf(this.subscriptions,t),()=>zf(this.subscriptions,t)}notify(t,n,r){const s=this.subscriptions.length;if(s)if(s===1)this.subscriptions[0](t,n,r);else for(let i=0;i<s;i++){const o=this.subscriptions[i];o&&o(t,n,r)}}getSize(){return this.subscriptions.length}clear(){this.subscriptions.length=0}}const Bs=e=>e*1e3,Wr=e=>e/1e3;function O2(e,t){return t?e*(1e3/t):0}const j2=(e,t,n)=>(((1-3*n+3*t)*e+(3*n-6*t))*e+3*t)*e,lN=1e-7,cN=12;function uN(e,t,n,r,s){let i,o,a=0;do o=t+(n-t)/2,i=j2(o,r,s)-e,i>0?n=o:t=o;while(Math.abs(i)>lN&&++a<cN);return o}function Al(e,t,n,r){if(e===t&&n===r)return Hr;const s=i=>uN(i,0,1,e,n);return i=>i===0||i===1?i:j2(s(i),t,r)}const R2=e=>t=>t<=.5?e(2*t)/2:(2-e(2*(1-t)))/2,N2=e=>t=>1-e(1-t),z2=Al(.33,1.53,.69,.99),Uf=N2(z2),B2=R2(Uf),G2=e=>(e*=2)<1?.5*Uf(e):.5*(2-Math.pow(2,-10*(e-1))),Wf=e=>1-Math.sin(Math.acos(e)),V2=N2(Wf),U2=R2(Wf),dN=Al(.42,0,1,1),pN=Al(0,0,.58,1),W2=Al(.42,0,.58,1),hN=e=>Array.isArray(e)&&typeof e[0]!="number",H2=e=>Array.isArray(e)&&typeof e[0]=="number",mN={linear:Hr,easeIn:dN,easeInOut:W2,easeOut:pN,circIn:Wf,circInOut:U2,circOut:V2,backIn:Uf,backInOut:B2,backOut:z2,anticipate:G2},fN=e=>typeof e=="string",n1=e=>{if(H2(e)){Bf(e.length===4);const[t,n,r,s]=e;return Al(t,n,r,s)}else if(fN(e))return mN[e];return e},Ac=["setup","read","resolveKeyframes","preUpdate","update","preRender","render","postRender"];function gN(e,t){let n=new Set,r=new Set,s=!1,i=!1;const o=new WeakSet;let a={delta:0,timestamp:0,isProcessing:!1};function l(d){o.has(d)&&(c.schedule(d),e()),d(a)}const c={schedule:(d,u=!1,p=!1)=>{const m=p&&s?n:r;return u&&o.add(d),m.has(d)||m.add(d),d},cancel:d=>{r.delete(d),o.delete(d)},process:d=>{if(a=d,s){i=!0;return}s=!0,[n,r]=[r,n],n.forEach(l),n.clear(),s=!1,i&&(i=!1,c.process(d))}};return c}const _N=40;function K2(e,t){let n=!1,r=!0;const s={delta:0,timestamp:0,isProcessing:!1},i=()=>n=!0,o=Ac.reduce((T,M)=>(T[M]=gN(i),T),{}),{setup:a,read:l,resolveKeyframes:c,preUpdate:d,update:u,preRender:p,render:h,postRender:m}=o,g=()=>{const T=Gs.useManualTiming?s.timestamp:performance.now();n=!1,Gs.useManualTiming||(s.delta=r?1e3/60:Math.max(Math.min(T-s.timestamp,_N),1)),s.timestamp=T,s.isProcessing=!0,a.process(s),l.process(s),c.process(s),d.process(s),u.process(s),p.process(s),h.process(s),m.process(s),s.isProcessing=!1,n&&t&&(r=!1,e(g))},I=()=>{n=!0,r=!0,s.isProcessing||e(g)};return{schedule:Ac.reduce((T,M)=>{const v=o[M];return T[M]=(b,A=!1,k=!1)=>(n||I(),v.schedule(b,A,k)),T},{}),cancel:T=>{for(let M=0;M<Ac.length;M++)o[Ac[M]].cancel(T)},state:s,steps:o}}const{schedule:yn,cancel:li,state:ir,steps:Gp}=K2(typeof requestAnimationFrame<"u"?requestAnimationFrame:Hr,!0);let Xc;function yN(){Xc=void 0}const fr={now:()=>(Xc===void 0&&fr.set(ir.isProcessing||Gs.useManualTiming?ir.timestamp:performance.now()),Xc),set:e=>{Xc=e,queueMicrotask(yN)}},q2=e=>t=>typeof t=="string"&&t.startsWith(e),Q2=q2("--"),xN=q2("var(--"),Hf=e=>xN(e)?bN.test(e.split("/*")[0].trim()):!1,bN=/var\(--(?:[\w-]+\s*|[\w-]+\s*,(?:\s*[^)(\s]|\s*\((?:[^)(]|\([^)(]*\))*\))+\s*)\)$/iu;function r1(e){return typeof e!="string"?!1:e.split("/*")[0].includes("var(--")}const Xo={test:e=>typeof e=="number",parse:parseFloat,transform:e=>e},ul={...Xo,transform:e=>ks(0,1,e)},Tc={...Xo,default:1},nl=e=>Math.round(e*1e5)/1e5,Kf=/-?(?:\d+(?:\.\d+)?|\.\d+)/gu;function wN(e){return e==null}const vN=/^(?:#[\da-f]{3,8}|(?:rgb|hsl)a?\((?:-?[\d.]+%?[,\s]+){2}-?[\d.]+%?\s*(?:[,/]\s*)?(?:\b\d+(?:\.\d+)?|\.\d+)?%?\))$/iu,qf=(e,t)=>n=>!!(typeof n=="string"&&vN.test(n)&&n.startsWith(e)||t&&!wN(n)&&Object.prototype.hasOwnProperty.call(n,t)),X2=(e,t,n)=>r=>{if(typeof r!="string")return r;const[s,i,o,a]=r.match(Kf);return{[e]:parseFloat(s),[t]:parseFloat(i),[n]:parseFloat(o),alpha:a!==void 0?parseFloat(a):1}},MN=e=>ks(0,255,e),Vp={...Xo,transform:e=>Math.round(MN(e))},Oi={test:qf("rgb","red"),parse:X2("red","green","blue"),transform:({red:e,green:t,blue:n,alpha:r=1})=>"rgba("+Vp.transform(e)+", "+Vp.transform(t)+", "+Vp.transform(n)+", "+nl(ul.transform(r))+")"};function AN(e){let t="",n="",r="",s="";return e.length>5?(t=e.substring(1,3),n=e.substring(3,5),r=e.substring(5,7),s=e.substring(7,9)):(t=e.substring(1,2),n=e.substring(2,3),r=e.substring(3,4),s=e.substring(4,5),t+=t,n+=n,r+=r,s+=s),{red:parseInt(t,16),green:parseInt(n,16),blue:parseInt(r,16),alpha:s?parseInt(s,16)/255:1}}const Am={test:qf("#"),parse:AN,transform:Oi.transform},Tl=e=>({test:t=>typeof t=="string"&&t.endsWith(e)&&t.split(" ").length===1,parse:parseFloat,transform:t=>`${t}${e}`}),ii=Tl("deg"),Cs=Tl("%"),pt=Tl("px"),TN=Tl("vh"),PN=Tl("vw"),s1={...Cs,parse:e=>Cs.parse(e)/100,transform:e=>Cs.transform(e*100)},ko={test:qf("hsl","hue"),parse:X2("hue","saturation","lightness"),transform:({hue:e,saturation:t,lightness:n,alpha:r=1})=>"hsla("+Math.round(e)+", "+Cs.transform(nl(t))+", "+Cs.transform(nl(n))+", "+nl(ul.transform(r))+")"},Hn={test:e=>Oi.test(e)||Am.test(e)||ko.test(e),parse:e=>Oi.test(e)?Oi.parse(e):ko.test(e)?ko.parse(e):Am.parse(e),transform:e=>typeof e=="string"?e:e.hasOwnProperty("red")?Oi.transform(e):ko.transform(e),getAnimatableNone:e=>{const t=Hn.parse(e);return t.alpha=0,Hn.transform(t)}},CN=/(?:#[\da-f]{3,8}|(?:rgb|hsl)a?\((?:-?[\d.]+%?[,\s]+){2}-?[\d.]+%?\s*(?:[,/]\s*)?(?:\b\d+(?:\.\d+)?|\.\d+)?%?\))/giu;function IN(e){return isNaN(e)&&typeof e=="string"&&(e.match(Kf)?.length||0)+(e.match(CN)?.length||0)>0}const Y2="number",J2="color",kN="var",EN="var(",i1="${}",SN=/var\s*\(\s*--(?:[\w-]+\s*|[\w-]+\s*,(?:\s*[^)(\s]|\s*\((?:[^)(]|\([^)(]*\))*\))+\s*)\)|#[\da-f]{3,8}|(?:rgb|hsl)a?\((?:-?[\d.]+%?[,\s]+){2}-?[\d.]+%?\s*(?:[,/]\s*)?(?:\b\d+(?:\.\d+)?|\.\d+)?%?\)|-?(?:\d+(?:\.\d+)?|\.\d+)/giu;function dl(e){const t=e.toString(),n=[],r={color:[],number:[],var:[]},s=[];let i=0;const a=t.replace(SN,l=>(Hn.test(l)?(r.color.push(i),s.push(J2),n.push(Hn.parse(l))):l.startsWith(EN)?(r.var.push(i),s.push(kN),n.push(l)):(r.number.push(i),s.push(Y2),n.push(parseFloat(l))),++i,i1)).split(i1);return{values:n,split:a,indexes:r,types:s}}function Z2(e){return dl(e).values}function eC(e){const{split:t,types:n}=dl(e),r=t.length;return s=>{let i="";for(let o=0;o<r;o++)if(i+=t[o],s[o]!==void 0){const a=n[o];a===Y2?i+=nl(s[o]):a===J2?i+=Hn.transform(s[o]):i+=s[o]}return i}}const LN=e=>typeof e=="number"?0:Hn.test(e)?Hn.getAnimatableNone(e):e;function DN(e){const t=Z2(e);return eC(e)(t.map(LN))}const ci={test:IN,parse:Z2,createTransformer:eC,getAnimatableNone:DN};function Up(e,t,n){return n<0&&(n+=1),n>1&&(n-=1),n<1/6?e+(t-e)*6*n:n<1/2?t:n<2/3?e+(t-e)*(2/3-n)*6:e}function $N({hue:e,saturation:t,lightness:n,alpha:r}){e/=360,t/=100,n/=100;let s=0,i=0,o=0;if(!t)s=i=o=n;else{const a=n<.5?n*(1+t):n+t-n*t,l=2*n-a;s=Up(l,a,e+1/3),i=Up(l,a,e),o=Up(l,a,e-1/3)}return{red:Math.round(s*255),green:Math.round(i*255),blue:Math.round(o*255),alpha:r}}function du(e,t){return n=>n>0?t:e}const kn=(e,t,n)=>e+(t-e)*n,Wp=(e,t,n)=>{const r=e*e,s=n*(t*t-r)+r;return s<0?0:Math.sqrt(s)},FN=[Am,Oi,ko],ON=e=>FN.find(t=>t.test(e));function o1(e){const t=ON(e);if(!t)return!1;let n=t.parse(e);return t===ko&&(n=$N(n)),n}const a1=(e,t)=>{const n=o1(e),r=o1(t);if(!n||!r)return du(e,t);const s={...n};return i=>(s.red=Wp(n.red,r.red,i),s.green=Wp(n.green,r.green,i),s.blue=Wp(n.blue,r.blue,i),s.alpha=kn(n.alpha,r.alpha,i),Oi.transform(s))},Tm=new Set(["none","hidden"]);function jN(e,t){return Tm.has(e)?n=>n<=0?e:t:n=>n>=1?t:e}function RN(e,t){return n=>kn(e,t,n)}function Qf(e){return typeof e=="number"?RN:typeof e=="string"?Hf(e)?du:Hn.test(e)?a1:BN:Array.isArray(e)?tC:typeof e=="object"?Hn.test(e)?a1:NN:du}function tC(e,t){const n=[...e],r=n.length,s=e.map((i,o)=>Qf(i)(i,t[o]));return i=>{for(let o=0;o<r;o++)n[o]=s[o](i);return n}}function NN(e,t){const n={...e,...t},r={};for(const s in n)e[s]!==void 0&&t[s]!==void 0&&(r[s]=Qf(e[s])(e[s],t[s]));return s=>{for(const i in r)n[i]=r[i](s);return n}}function zN(e,t){const n=[],r={color:0,var:0,number:0};for(let s=0;s<t.values.length;s++){const i=t.types[s],o=e.indexes[i][r[i]],a=e.values[o]??0;n[s]=a,r[i]++}return n}const BN=(e,t)=>{const n=ci.createTransformer(t),r=dl(e),s=dl(t);return r.indexes.var.length===s.indexes.var.length&&r.indexes.color.length===s.indexes.color.length&&r.indexes.number.length>=s.indexes.number.length?Tm.has(e)&&!s.values.length||Tm.has(t)&&!r.values.length?jN(e,t):Ml(tC(zN(r,s),s.values),n):du(e,t)};function nC(e,t,n){return typeof e=="number"&&typeof t=="number"&&typeof n=="number"?kn(e,t,n):Qf(e)(e,t)}const GN=e=>{const t=({timestamp:n})=>e(n);return{start:(n=!0)=>yn.update(t,n),stop:()=>li(t),now:()=>ir.isProcessing?ir.timestamp:fr.now()}},rC=(e,t,n=10)=>{let r="";const s=Math.max(Math.round(t/n),2);for(let i=0;i<s;i++)r+=Math.round(e(i/(s-1))*1e4)/1e4+", ";return`linear(${r.substring(0,r.length-2)})`},pu=2e4;function Xf(e){let t=0;const n=50;let r=e.next(t);for(;!r.done&&t<pu;)t+=n,r=e.next(t);return t>=pu?1/0:t}function VN(e,t=100,n){const r=n({...e,keyframes:[0,t]}),s=Math.min(Xf(r),pu);return{type:"keyframes",ease:i=>r.next(s*i).value/t,duration:Wr(s)}}const UN=5;function sC(e,t,n){const r=Math.max(t-UN,0);return O2(n-e(r),t-r)}const $n={stiffness:100,damping:10,mass:1,velocity:0,duration:800,bounce:.3,visualDuration:.3,restSpeed:{granular:.01,default:2},restDelta:{granular:.005,default:.5},minDuration:.01,maxDuration:10,minDamping:.05,maxDamping:1},Hp=.001;function WN({duration:e=$n.duration,bounce:t=$n.bounce,velocity:n=$n.velocity,mass:r=$n.mass}){let s,i,o=1-t;o=ks($n.minDamping,$n.maxDamping,o),e=ks($n.minDuration,$n.maxDuration,Wr(e)),o<1?(s=c=>{const d=c*o,u=d*e,p=d-n,h=Pm(c,o),m=Math.exp(-u);return Hp-p/h*m},i=c=>{const u=c*o*e,p=u*n+n,h=Math.pow(o,2)*Math.pow(c,2)*e,m=Math.exp(-u),g=Pm(Math.pow(c,2),o);return(-s(c)+Hp>0?-1:1)*((p-h)*m)/g}):(s=c=>{const d=Math.exp(-c*e),u=(c-n)*e+1;return-Hp+d*u},i=c=>{const d=Math.exp(-c*e),u=(n-c)*(e*e);return d*u});const a=5/e,l=KN(s,i,a);if(e=Bs(e),isNaN(l))return{stiffness:$n.stiffness,damping:$n.damping,duration:e};{const c=Math.pow(l,2)*r;return{stiffness:c,damping:o*2*Math.sqrt(r*c),duration:e}}}const HN=12;function KN(e,t,n){let r=n;for(let s=1;s<HN;s++)r=r-e(r)/t(r);return r}function Pm(e,t){return e*Math.sqrt(1-t*t)}const qN=["duration","bounce"],QN=["stiffness","damping","mass"];function l1(e,t){return t.some(n=>e[n]!==void 0)}function XN(e){let t={velocity:$n.velocity,stiffness:$n.stiffness,damping:$n.damping,mass:$n.mass,isResolvedFromDuration:!1,...e};if(!l1(e,QN)&&l1(e,qN))if(e.visualDuration){const n=e.visualDuration,r=2*Math.PI/(n*1.2),s=r*r,i=2*ks(.05,1,1-(e.bounce||0))*Math.sqrt(s);t={...t,mass:$n.mass,stiffness:s,damping:i}}else{const n=WN(e);t={...t,...n,mass:$n.mass},t.isResolvedFromDuration=!0}return t}function hu(e=$n.visualDuration,t=$n.bounce){const n=typeof e!="object"?{visualDuration:e,keyframes:[0,1],bounce:t}:e;let{restSpeed:r,restDelta:s}=n;const i=n.keyframes[0],o=n.keyframes[n.keyframes.length-1],a={done:!1,value:i},{stiffness:l,damping:c,mass:d,duration:u,velocity:p,isResolvedFromDuration:h}=XN({...n,velocity:-Wr(n.velocity||0)}),m=p||0,g=c/(2*Math.sqrt(l*d)),I=o-i,f=Wr(Math.sqrt(l/d)),_=Math.abs(I)<5;r||(r=_?$n.restSpeed.granular:$n.restSpeed.default),s||(s=_?$n.restDelta.granular:$n.restDelta.default);let T;if(g<1){const v=Pm(f,g);T=b=>{const A=Math.exp(-g*f*b);return o-A*((m+g*f*I)/v*Math.sin(v*b)+I*Math.cos(v*b))}}else if(g===1)T=v=>o-Math.exp(-f*v)*(I+(m+f*I)*v);else{const v=f*Math.sqrt(g*g-1);T=b=>{const A=Math.exp(-g*f*b),k=Math.min(v*b,300);return o-A*((m+g*f*I)*Math.sinh(k)+v*I*Math.cosh(k))/v}}const M={calculatedDuration:h&&u||null,next:v=>{const b=T(v);if(h)a.done=v>=u;else{let A=v===0?m:0;g<1&&(A=v===0?Bs(m):sC(T,v,b));const k=Math.abs(A)<=r,F=Math.abs(o-b)<=s;a.done=k&&F}return a.value=a.done?o:b,a},toString:()=>{const v=Math.min(Xf(M),pu),b=rC(A=>M.next(v*A).value,v,30);return v+"ms "+b},toTransition:()=>{}};return M}hu.applyToOptions=e=>{const t=VN(e,100,hu);return e.ease=t.ease,e.duration=Bs(t.duration),e.type="keyframes",e};function Cm({keyframes:e,velocity:t=0,power:n=.8,timeConstant:r=325,bounceDamping:s=10,bounceStiffness:i=500,modifyTarget:o,min:a,max:l,restDelta:c=.5,restSpeed:d}){const u=e[0],p={done:!1,value:u},h=k=>a!==void 0&&k<a||l!==void 0&&k>l,m=k=>a===void 0?l:l===void 0||Math.abs(a-k)<Math.abs(l-k)?a:l;let g=n*t;const I=u+g,f=o===void 0?I:o(I);f!==I&&(g=f-u);const _=k=>-g*Math.exp(-k/r),T=k=>f+_(k),M=k=>{const F=_(k),L=T(k);p.done=Math.abs(F)<=c,p.value=p.done?f:L};let v,b;const A=k=>{h(p.value)&&(v=k,b=hu({keyframes:[p.value,m(p.value)],velocity:sC(T,k,p.value),damping:s,stiffness:i,restDelta:c,restSpeed:d}))};return A(0),{calculatedDuration:null,next:k=>{let F=!1;return!b&&v===void 0&&(F=!0,M(k),A(k)),v!==void 0&&k>=v?b.next(k-v):(!F&&M(k),p)}}}function YN(e,t,n){const r=[],s=n||Gs.mix||nC,i=e.length-1;for(let o=0;o<i;o++){let a=s(e[o],e[o+1]);if(t){const l=Array.isArray(t)?t[o]||Hr:t;a=Ml(l,a)}r.push(a)}return r}function JN(e,t,{clamp:n=!0,ease:r,mixer:s}={}){const i=e.length;if(Bf(i===t.length),i===1)return()=>t[0];if(i===2&&t[0]===t[1])return()=>t[1];const o=e[0]===e[1];e[0]>e[i-1]&&(e=[...e].reverse(),t=[...t].reverse());const a=YN(t,r,s),l=a.length,c=d=>{if(o&&d<e[0])return t[0];let u=0;if(l>1)for(;u<e.length-2&&!(d<e[u+1]);u++);const p=cl(e[u],e[u+1],d);return a[u](p)};return n?d=>c(ks(e[0],e[i-1],d)):c}function ZN(e,t){const n=e[e.length-1];for(let r=1;r<=t;r++){const s=cl(0,t,r);e.push(kn(n,1,s))}}function ez(e){const t=[0];return ZN(t,e.length-1),t}function tz(e,t){return e.map(n=>n*t)}function nz(e,t){return e.map(()=>t||W2).splice(0,e.length-1)}function rl({duration:e=300,keyframes:t,times:n,ease:r="easeInOut"}){const s=hN(r)?r.map(n1):n1(r),i={done:!1,value:t[0]},o=tz(n&&n.length===t.length?n:ez(t),e),a=JN(o,t,{ease:Array.isArray(s)?s:nz(t,s)});return{calculatedDuration:e,next:l=>(i.value=a(l),i.done=l>=e,i)}}const rz=e=>e!==null;function Yf(e,{repeat:t,repeatType:n="loop"},r,s=1){const i=e.filter(rz),a=s<0||t&&n!=="loop"&&t%2===1?0:i.length-1;return!a||r===void 0?i[a]:r}const sz={decay:Cm,inertia:Cm,tween:rl,keyframes:rl,spring:hu};function iC(e){typeof e.type=="string"&&(e.type=sz[e.type])}class Jf{constructor(){this.updateFinished()}get finished(){return this._finished}updateFinished(){this._finished=new Promise(t=>{this.resolve=t})}notifyFinished(){this.resolve()}then(t,n){return this.finished.then(t,n)}}const iz=e=>e/100;class Zf extends Jf{constructor(t){super(),this.state="idle",this.startTime=null,this.isStopped=!1,this.currentTime=0,this.holdTime=null,this.playbackSpeed=1,this.stop=()=>{const{motionValue:n}=this.options;n&&n.updatedAt!==fr.now()&&this.tick(fr.now()),this.isStopped=!0,this.state!=="idle"&&(this.teardown(),this.options.onStop?.())},this.options=t,this.initAnimation(),this.play(),t.autoplay===!1&&this.pause()}initAnimation(){const{options:t}=this;iC(t);const{type:n=rl,repeat:r=0,repeatDelay:s=0,repeatType:i,velocity:o=0}=t;let{keyframes:a}=t;const l=n||rl;l!==rl&&typeof a[0]!="number"&&(this.mixKeyframes=Ml(iz,nC(a[0],a[1])),a=[0,100]);const c=l({...t,keyframes:a});i==="mirror"&&(this.mirroredGenerator=l({...t,keyframes:[...a].reverse(),velocity:-o})),c.calculatedDuration===null&&(c.calculatedDuration=Xf(c));const{calculatedDuration:d}=c;this.calculatedDuration=d,this.resolvedDuration=d+s,this.totalDuration=this.resolvedDuration*(r+1)-s,this.generator=c}updateTime(t){const n=Math.round(t-this.startTime)*this.playbackSpeed;this.holdTime!==null?this.currentTime=this.holdTime:this.currentTime=n}tick(t,n=!1){const{generator:r,totalDuration:s,mixKeyframes:i,mirroredGenerator:o,resolvedDuration:a,calculatedDuration:l}=this;if(this.startTime===null)return r.next(0);const{delay:c=0,keyframes:d,repeat:u,repeatType:p,repeatDelay:h,type:m,onUpdate:g,finalKeyframe:I}=this.options;this.speed>0?this.startTime=Math.min(this.startTime,t):this.speed<0&&(this.startTime=Math.min(t-s/this.speed,this.startTime)),n?this.currentTime=t:this.updateTime(t);const f=this.currentTime-c*(this.playbackSpeed>=0?1:-1),_=this.playbackSpeed>=0?f<0:f>s;this.currentTime=Math.max(f,0),this.state==="finished"&&this.holdTime===null&&(this.currentTime=s);let T=this.currentTime,M=r;if(u){const k=Math.min(this.currentTime,s)/a;let F=Math.floor(k),L=k%1;!L&&k>=1&&(L=1),L===1&&F--,F=Math.min(F,u+1),F%2&&(p==="reverse"?(L=1-L,h&&(L-=h/a)):p==="mirror"&&(M=o)),T=ks(0,1,L)*a}const v=_?{done:!1,value:d[0]}:M.next(T);i&&(v.value=i(v.value));let{done:b}=v;!_&&l!==null&&(b=this.playbackSpeed>=0?this.currentTime>=s:this.currentTime<=0);const A=this.holdTime===null&&(this.state==="finished"||this.state==="running"&&b);return A&&m!==Cm&&(v.value=Yf(d,this.options,I,this.speed)),g&&g(v.value),A&&this.finish(),v}then(t,n){return this.finished.then(t,n)}get duration(){return Wr(this.calculatedDuration)}get iterationDuration(){const{delay:t=0}=this.options||{};return this.duration+Wr(t)}get time(){return Wr(this.currentTime)}set time(t){t=Bs(t),this.currentTime=t,this.startTime===null||this.holdTime!==null||this.playbackSpeed===0?this.holdTime=t:this.driver&&(this.startTime=this.driver.now()-t/this.playbackSpeed),this.driver?.start(!1)}get speed(){return this.playbackSpeed}set speed(t){this.updateTime(fr.now());const n=this.playbackSpeed!==t;this.playbackSpeed=t,n&&(this.time=Wr(this.currentTime))}play(){if(this.isStopped)return;const{driver:t=GN,startTime:n}=this.options;this.driver||(this.driver=t(s=>this.tick(s))),this.options.onPlay?.();const r=this.driver.now();this.state==="finished"?(this.updateFinished(),this.startTime=r):this.holdTime!==null?this.startTime=r-this.holdTime:this.startTime||(this.startTime=n??r),this.state==="finished"&&this.speed<0&&(this.startTime+=this.calculatedDuration),this.holdTime=null,this.state="running",this.driver.start()}pause(){this.state="paused",this.updateTime(fr.now()),this.holdTime=this.currentTime}complete(){this.state!=="running"&&this.play(),this.state="finished",this.holdTime=null}finish(){this.notifyFinished(),this.teardown(),this.state="finished",this.options.onComplete?.()}cancel(){this.holdTime=null,this.startTime=0,this.tick(0),this.teardown(),this.options.onCancel?.()}teardown(){this.state="idle",this.stopDriver(),this.startTime=this.holdTime=null}stopDriver(){this.driver&&(this.driver.stop(),this.driver=void 0)}sample(t){return this.startTime=0,this.tick(t,!0)}attachTimeline(t){return this.options.allowFlatten&&(this.options.type="keyframes",this.options.ease="linear",this.initAnimation()),this.driver?.stop(),t.observe(this)}}function oz(e){for(let t=1;t<e.length;t++)e[t]??(e[t]=e[t-1])}const ji=e=>e*180/Math.PI,Im=e=>{const t=ji(Math.atan2(e[1],e[0]));return km(t)},az={x:4,y:5,translateX:4,translateY:5,scaleX:0,scaleY:3,scale:e=>(Math.abs(e[0])+Math.abs(e[3]))/2,rotate:Im,rotateZ:Im,skewX:e=>ji(Math.atan(e[1])),skewY:e=>ji(Math.atan(e[2])),skew:e=>(Math.abs(e[1])+Math.abs(e[2]))/2},km=e=>(e=e%360,e<0&&(e+=360),e),c1=Im,u1=e=>Math.sqrt(e[0]*e[0]+e[1]*e[1]),d1=e=>Math.sqrt(e[4]*e[4]+e[5]*e[5]),lz={x:12,y:13,z:14,translateX:12,translateY:13,translateZ:14,scaleX:u1,scaleY:d1,scale:e=>(u1(e)+d1(e))/2,rotateX:e=>km(ji(Math.atan2(e[6],e[5]))),rotateY:e=>km(ji(Math.atan2(-e[2],e[0]))),rotateZ:c1,rotate:c1,skewX:e=>ji(Math.atan(e[4])),skewY:e=>ji(Math.atan(e[1])),skew:e=>(Math.abs(e[1])+Math.abs(e[4]))/2};function Em(e){return e.includes("scale")?1:0}function Sm(e,t){if(!e||e==="none")return Em(t);const n=e.match(/^matrix3d\(([-\d.e\s,]+)\)$/u);let r,s;if(n)r=lz,s=n;else{const a=e.match(/^matrix\(([-\d.e\s,]+)\)$/u);r=az,s=a}if(!s)return Em(t);const i=r[t],o=s[1].split(",").map(uz);return typeof i=="function"?i(o):o[i]}const cz=(e,t)=>{const{transform:n="none"}=getComputedStyle(e);return Sm(n,t)};function uz(e){return parseFloat(e.trim())}const Yo=["transformPerspective","x","y","z","translateX","translateY","translateZ","scale","scaleX","scaleY","rotate","rotateX","rotateY","rotateZ","skew","skewX","skewY"],Jo=new Set(Yo),p1=e=>e===Xo||e===pt,dz=new Set(["x","y","z"]),pz=Yo.filter(e=>!dz.has(e));function hz(e){const t=[];return pz.forEach(n=>{const r=e.getValue(n);r!==void 0&&(t.push([n,r.get()]),r.set(n.startsWith("scale")?1:0))}),t}const oi={width:({x:e},{paddingLeft:t="0",paddingRight:n="0"})=>e.max-e.min-parseFloat(t)-parseFloat(n),height:({y:e},{paddingTop:t="0",paddingBottom:n="0"})=>e.max-e.min-parseFloat(t)-parseFloat(n),top:(e,{top:t})=>parseFloat(t),left:(e,{left:t})=>parseFloat(t),bottom:({y:e},{top:t})=>parseFloat(t)+(e.max-e.min),right:({x:e},{left:t})=>parseFloat(t)+(e.max-e.min),x:(e,{transform:t})=>Sm(t,"x"),y:(e,{transform:t})=>Sm(t,"y")};oi.translateX=oi.x;oi.translateY=oi.y;const Bi=new Set;let Lm=!1,Dm=!1,$m=!1;function oC(){if(Dm){const e=Array.from(Bi).filter(r=>r.needsMeasurement),t=new Set(e.map(r=>r.element)),n=new Map;t.forEach(r=>{const s=hz(r);s.length&&(n.set(r,s),r.render())}),e.forEach(r=>r.measureInitialState()),t.forEach(r=>{r.render();const s=n.get(r);s&&s.forEach(([i,o])=>{r.getValue(i)?.set(o)})}),e.forEach(r=>r.measureEndState()),e.forEach(r=>{r.suspendedScrollY!==void 0&&window.scrollTo(0,r.suspendedScrollY)})}Dm=!1,Lm=!1,Bi.forEach(e=>e.complete($m)),Bi.clear()}function aC(){Bi.forEach(e=>{e.readKeyframes(),e.needsMeasurement&&(Dm=!0)})}function mz(){$m=!0,aC(),oC(),$m=!1}class eg{constructor(t,n,r,s,i,o=!1){this.state="pending",this.isAsync=!1,this.needsMeasurement=!1,this.unresolvedKeyframes=[...t],this.onComplete=n,this.name=r,this.motionValue=s,this.element=i,this.isAsync=o}scheduleResolve(){this.state="scheduled",this.isAsync?(Bi.add(this),Lm||(Lm=!0,yn.read(aC),yn.resolveKeyframes(oC))):(this.readKeyframes(),this.complete())}readKeyframes(){const{unresolvedKeyframes:t,name:n,element:r,motionValue:s}=this;if(t[0]===null){const i=s?.get(),o=t[t.length-1];if(i!==void 0)t[0]=i;else if(r&&n){const a=r.readValue(n,o);a!=null&&(t[0]=a)}t[0]===void 0&&(t[0]=o),s&&i===void 0&&s.set(t[0])}oz(t)}setFinalKeyframe(){}measureInitialState(){}renderEndStyles(){}measureEndState(){}complete(t=!1){this.state="complete",this.onComplete(this.unresolvedKeyframes,this.finalKeyframe,t),Bi.delete(this)}cancel(){this.state==="scheduled"&&(Bi.delete(this),this.state="pending")}resume(){this.state==="pending"&&this.scheduleResolve()}}const fz=e=>e.startsWith("--");function gz(e,t,n){fz(t)?e.style.setProperty(t,n):e.style[t]=n}const _z=Gf(()=>window.ScrollTimeline!==void 0),yz={};function xz(e,t){const n=Gf(e);return()=>yz[t]??n()}const lC=xz(()=>{try{document.createElement("div").animate({opacity:0},{easing:"linear(0, 1)"})}catch{return!1}return!0},"linearEasing"),Ua=([e,t,n,r])=>`cubic-bezier(${e}, ${t}, ${n}, ${r})`,h1={linear:"linear",ease:"ease",easeIn:"ease-in",easeOut:"ease-out",easeInOut:"ease-in-out",circIn:Ua([0,.65,.55,1]),circOut:Ua([.55,0,1,.45]),backIn:Ua([.31,.01,.66,-.59]),backOut:Ua([.33,1.53,.69,.99])};function cC(e,t){if(e)return typeof e=="function"?lC()?rC(e,t):"ease-out":H2(e)?Ua(e):Array.isArray(e)?e.map(n=>cC(n,t)||h1.easeOut):h1[e]}function bz(e,t,n,{delay:r=0,duration:s=300,repeat:i=0,repeatType:o="loop",ease:a="easeOut",times:l}={},c=void 0){const d={[t]:n};l&&(d.offset=l);const u=cC(a,s);Array.isArray(u)&&(d.easing=u);const p={delay:r,duration:s,easing:Array.isArray(u)?"linear":u,fill:"both",iterations:i+1,direction:o==="reverse"?"alternate":"normal"};return c&&(p.pseudoElement=c),e.animate(d,p)}function uC(e){return typeof e=="function"&&"applyToOptions"in e}function wz({type:e,...t}){return uC(e)&&lC()?e.applyToOptions(t):(t.duration??(t.duration=300),t.ease??(t.ease="easeOut"),t)}class vz extends Jf{constructor(t){if(super(),this.finishedTime=null,this.isStopped=!1,this.manualStartTime=null,!t)return;const{element:n,name:r,keyframes:s,pseudoElement:i,allowFlatten:o=!1,finalKeyframe:a,onComplete:l}=t;this.isPseudoElement=!!i,this.allowFlatten=o,this.options=t,Bf(typeof t.type!="string");const c=wz(t);this.animation=bz(n,r,s,c,i),c.autoplay===!1&&this.animation.pause(),this.animation.onfinish=()=>{if(this.finishedTime=this.time,!i){const d=Yf(s,this.options,a,this.speed);this.updateMotionValue?this.updateMotionValue(d):gz(n,r,d),this.animation.cancel()}l?.(),this.notifyFinished()}}play(){this.isStopped||(this.manualStartTime=null,this.animation.play(),this.state==="finished"&&this.updateFinished())}pause(){this.animation.pause()}complete(){this.animation.finish?.()}cancel(){try{this.animation.cancel()}catch{}}stop(){if(this.isStopped)return;this.isStopped=!0;const{state:t}=this;t==="idle"||t==="finished"||(this.updateMotionValue?this.updateMotionValue():this.commitStyles(),this.isPseudoElement||this.cancel())}commitStyles(){this.isPseudoElement||this.animation.commitStyles?.()}get duration(){const t=this.animation.effect?.getComputedTiming?.().duration||0;return Wr(Number(t))}get iterationDuration(){const{delay:t=0}=this.options||{};return this.duration+Wr(t)}get time(){return Wr(Number(this.animation.currentTime)||0)}set time(t){this.manualStartTime=null,this.finishedTime=null,this.animation.currentTime=Bs(t)}get speed(){return this.animation.playbackRate}set speed(t){t<0&&(this.finishedTime=null),this.animation.playbackRate=t}get state(){return this.finishedTime!==null?"finished":this.animation.playState}get startTime(){return this.manualStartTime??Number(this.animation.startTime)}set startTime(t){this.manualStartTime=this.animation.startTime=t}attachTimeline({timeline:t,observe:n}){return this.allowFlatten&&this.animation.effect?.updateTiming({easing:"linear"}),this.animation.onfinish=null,t&&_z()?(this.animation.timeline=t,Hr):n(this)}}const dC={anticipate:G2,backInOut:B2,circInOut:U2};function Mz(e){return e in dC}function Az(e){typeof e.ease=="string"&&Mz(e.ease)&&(e.ease=dC[e.ease])}const Kp=10;class Tz extends vz{constructor(t){Az(t),iC(t),super(t),t.startTime!==void 0&&(this.startTime=t.startTime),this.options=t}updateMotionValue(t){const{motionValue:n,onUpdate:r,onComplete:s,element:i,...o}=this.options;if(!n)return;if(t!==void 0){n.set(t);return}const a=new Zf({...o,autoplay:!1}),l=Math.max(Kp,fr.now()-this.startTime),c=ks(0,Kp,l-Kp);n.setWithVelocity(a.sample(Math.max(0,l-c)).value,a.sample(l).value,c),a.stop()}}const m1=(e,t)=>t==="zIndex"?!1:!!(typeof e=="number"||Array.isArray(e)||typeof e=="string"&&(ci.test(e)||e==="0")&&!e.startsWith("url("));function Pz(e){const t=e[0];if(e.length===1)return!0;for(let n=0;n<e.length;n++)if(e[n]!==t)return!0}function Cz(e,t,n,r){const s=e[0];if(s===null)return!1;if(t==="display"||t==="visibility")return!0;const i=e[e.length-1],o=m1(s,t),a=m1(i,t);return!o||!a?!1:Pz(e)||(n==="spring"||uC(n))&&r}function Fm(e){e.duration=0,e.type="keyframes"}const Iz=new Set(["opacity","clipPath","filter","transform"]),kz=Gf(()=>Object.hasOwnProperty.call(Element.prototype,"animate"));function Ez(e){const{motionValue:t,name:n,repeatDelay:r,repeatType:s,damping:i,type:o}=e;if(!(t?.owner?.current instanceof HTMLElement))return!1;const{onUpdate:l,transformTemplate:c}=t.owner.getProps();return kz()&&n&&Iz.has(n)&&(n!=="transform"||!c)&&!l&&!r&&s!=="mirror"&&i!==0&&o!=="inertia"}const Sz=40;class Lz extends Jf{constructor({autoplay:t=!0,delay:n=0,type:r="keyframes",repeat:s=0,repeatDelay:i=0,repeatType:o="loop",keyframes:a,name:l,motionValue:c,element:d,...u}){super(),this.stop=()=>{this._animation&&(this._animation.stop(),this.stopTimeline?.()),this.keyframeResolver?.cancel()},this.createdAt=fr.now();const p={autoplay:t,delay:n,type:r,repeat:s,repeatDelay:i,repeatType:o,name:l,motionValue:c,element:d,...u},h=d?.KeyframeResolver||eg;this.keyframeResolver=new h(a,(m,g,I)=>this.onKeyframesResolved(m,g,p,!I),l,c,d),this.keyframeResolver?.scheduleResolve()}onKeyframesResolved(t,n,r,s){this.keyframeResolver=void 0;const{name:i,type:o,velocity:a,delay:l,isHandoff:c,onUpdate:d}=r;this.resolvedAt=fr.now(),Cz(t,i,o,a)||((Gs.instantAnimations||!l)&&d?.(Yf(t,r,n)),t[0]=t[t.length-1],Fm(r),r.repeat=0);const p={startTime:s?this.resolvedAt?this.resolvedAt-this.createdAt>Sz?this.resolvedAt:this.createdAt:this.createdAt:void 0,finalKeyframe:n,...r,keyframes:t},h=!c&&Ez(p)?new Tz({...p,element:p.motionValue.owner.current}):new Zf(p);h.finished.then(()=>this.notifyFinished()).catch(Hr),this.pendingTimeline&&(this.stopTimeline=h.attachTimeline(this.pendingTimeline),this.pendingTimeline=void 0),this._animation=h}get finished(){return this._animation?this.animation.finished:this._finished}then(t,n){return this.finished.finally(t).then(()=>{})}get animation(){return this._animation||(this.keyframeResolver?.resume(),mz()),this._animation}get duration(){return this.animation.duration}get iterationDuration(){return this.animation.iterationDuration}get time(){return this.animation.time}set time(t){this.animation.time=t}get speed(){return this.animation.speed}get state(){return this.animation.state}set speed(t){this.animation.speed=t}get startTime(){return this.animation.startTime}attachTimeline(t){return this._animation?this.stopTimeline=this.animation.attachTimeline(t):this.pendingTimeline=t,()=>this.stop()}play(){this.animation.play()}pause(){this.animation.pause()}complete(){this.animation.complete()}cancel(){this._animation&&this.animation.cancel(),this.keyframeResolver?.cancel()}}const Dz=/^var\(--(?:([\w-]+)|([\w-]+), ?([a-zA-Z\d ()%#.,-]+))\)/u;function $z(e){const t=Dz.exec(e);if(!t)return[,];const[,n,r,s]=t;return[`--${n??r}`,s]}function pC(e,t,n=1){const[r,s]=$z(e);if(!r)return;const i=window.getComputedStyle(t).getPropertyValue(r);if(i){const o=i.trim();return D2(o)?parseFloat(o):o}return Hf(s)?pC(s,t,n+1):s}function tg(e,t){return e?.[t]??e?.default??e}const Fz={type:"spring",stiffness:500,damping:25,restSpeed:10},Oz=e=>({type:"spring",stiffness:550,damping:e===0?2*Math.sqrt(550):30,restSpeed:10}),jz={type:"keyframes",duration:.8},Rz={type:"keyframes",ease:[.25,.1,.35,1],duration:.3},Nz=(e,{keyframes:t})=>t.length>2?jz:Jo.has(e)?e.startsWith("scale")?Oz(t[1]):Fz:Rz;function zz({when:e,delay:t,delayChildren:n,staggerChildren:r,staggerDirection:s,repeat:i,repeatType:o,repeatDelay:a,from:l,elapsed:c,...d}){return!!Object.keys(d).length}const Bz=e=>e!==null;function Gz(e,{repeat:t,repeatType:n="loop"},r){const s=e.filter(Bz),i=t&&n!=="loop"&&t%2===1?0:s.length-1;return s[i]}function hC(e,t,n,r=0,s=1){const i=Array.from(e).sort((c,d)=>c.sortNodePosition(d)).indexOf(t),o=e.size,a=(o-1)*r;return typeof n=="function"?n(i,o):s===1?i*r:a-i*r}const ng=(e,t,n,r={},s,i)=>o=>{const a=tg(r,e)||{},l=a.delay||r.delay||0;let{elapsed:c=0}=r;c=c-Bs(l);const d={keyframes:Array.isArray(n)?n:[null,n],ease:"easeOut",velocity:t.getVelocity(),...a,delay:-c,onUpdate:p=>{t.set(p),a.onUpdate&&a.onUpdate(p)},onComplete:()=>{o(),a.onComplete&&a.onComplete()},name:e,motionValue:t,element:i?void 0:s};zz(a)||Object.assign(d,Nz(e,d)),d.duration&&(d.duration=Bs(d.duration)),d.repeatDelay&&(d.repeatDelay=Bs(d.repeatDelay)),d.from!==void 0&&(d.keyframes[0]=d.from);let u=!1;if((d.type===!1||d.duration===0&&!d.repeatDelay)&&(Fm(d),d.delay===0&&(u=!0)),(Gs.instantAnimations||Gs.skipAnimations)&&(u=!0,Fm(d),d.delay=0),d.allowFlatten=!a.type&&!a.ease,u&&!i&&t.get()!==void 0){const p=Gz(d.keyframes,a);if(p!==void 0){yn.update(()=>{d.onUpdate(p),d.onComplete()});return}}return a.isSync?new Zf(d):new Lz(d)},mC=new Set(["width","height","top","left","right","bottom",...Yo]),f1=30,Vz=e=>!isNaN(parseFloat(e));class Uz{constructor(t,n={}){this.canTrackVelocity=null,this.events={},this.updateAndNotify=r=>{const s=fr.now();if(this.updatedAt!==s&&this.setPrevFrameValue(),this.prev=this.current,this.setCurrent(r),this.current!==this.prev&&(this.events.change?.notify(this.current),this.dependents))for(const i of this.dependents)i.dirty()},this.hasAnimated=!1,this.setCurrent(t),this.owner=n.owner}setCurrent(t){this.current=t,this.updatedAt=fr.now(),this.canTrackVelocity===null&&t!==void 0&&(this.canTrackVelocity=Vz(this.current))}setPrevFrameValue(t=this.current){this.prevFrameValue=t,this.prevUpdatedAt=this.updatedAt}onChange(t){return this.on("change",t)}on(t,n){this.events[t]||(this.events[t]=new Vf);const r=this.events[t].add(n);return t==="change"?()=>{r(),yn.read(()=>{this.events.change.getSize()||this.stop()})}:r}clearListeners(){for(const t in this.events)this.events[t].clear()}attach(t,n){this.passiveEffect=t,this.stopPassiveEffect=n}set(t){this.passiveEffect?this.passiveEffect(t,this.updateAndNotify):this.updateAndNotify(t)}setWithVelocity(t,n,r){this.set(n),this.prev=void 0,this.prevFrameValue=t,this.prevUpdatedAt=this.updatedAt-r}jump(t,n=!0){this.updateAndNotify(t),this.prev=t,this.prevUpdatedAt=this.prevFrameValue=void 0,n&&this.stop(),this.stopPassiveEffect&&this.stopPassiveEffect()}dirty(){this.events.change?.notify(this.current)}addDependent(t){this.dependents||(this.dependents=new Set),this.dependents.add(t)}removeDependent(t){this.dependents&&this.dependents.delete(t)}get(){return this.current}getPrevious(){return this.prev}getVelocity(){const t=fr.now();if(!this.canTrackVelocity||this.prevFrameValue===void 0||t-this.updatedAt>f1)return 0;const n=Math.min(this.updatedAt-this.prevUpdatedAt,f1);return O2(parseFloat(this.current)-parseFloat(this.prevFrameValue),n)}start(t){return this.stop(),new Promise(n=>{this.hasAnimated=!0,this.animation=t(n),this.events.animationStart&&this.events.animationStart.notify()}).then(()=>{this.events.animationComplete&&this.events.animationComplete.notify(),this.clearAnimation()})}stop(){this.animation&&(this.animation.stop(),this.events.animationCancel&&this.events.animationCancel.notify()),this.clearAnimation()}isAnimating(){return!!this.animation}clearAnimation(){delete this.animation}destroy(){this.dependents?.clear(),this.events.destroy?.notify(),this.clearListeners(),this.stop(),this.stopPassiveEffect&&this.stopPassiveEffect()}}function Go(e,t){return new Uz(e,t)}function g1(e){const t=[{},{}];return e?.values.forEach((n,r)=>{t[0][r]=n.get(),t[1][r]=n.getVelocity()}),t}function rg(e,t,n,r){if(typeof t=="function"){const[s,i]=g1(r);t=t(n!==void 0?n:e.custom,s,i)}if(typeof t=="string"&&(t=e.variants&&e.variants[t]),typeof t=="function"){const[s,i]=g1(r);t=t(n!==void 0?n:e.custom,s,i)}return t}function No(e,t,n){const r=e.getProps();return rg(r,t,n!==void 0?n:r.custom,e)}const Om=e=>Array.isArray(e);function Wz(e,t,n){e.hasValue(t)?e.getValue(t).set(n):e.addValue(t,Go(n))}function Hz(e){return Om(e)?e[e.length-1]||0:e}function Kz(e,t){const n=No(e,t);let{transitionEnd:r={},transition:s={},...i}=n||{};i={...i,...r};for(const o in i){const a=Hz(i[o]);Wz(e,o,a)}}const cr=e=>!!(e&&e.getVelocity);function qz(e){return!!(cr(e)&&e.add)}function jm(e,t){const n=e.getValue("willChange");if(qz(n))return n.add(t);if(!n&&Gs.WillChange){const r=new Gs.WillChange("auto");e.addValue("willChange",r),r.add(t)}}function sg(e){return e.replace(/([A-Z])/g,t=>`-${t.toLowerCase()}`)}const Qz="framerAppearId",fC="data-"+sg(Qz);function gC(e){return e.props[fC]}function Xz({protectedKeys:e,needsAnimating:t},n){const r=e.hasOwnProperty(n)&&t[n]!==!0;return t[n]=!1,r}function _C(e,t,{delay:n=0,transitionOverride:r,type:s}={}){let{transition:i=e.getDefaultTransition(),transitionEnd:o,...a}=t;r&&(i=r);const l=[],c=s&&e.animationState&&e.animationState.getState()[s];for(const d in a){const u=e.getValue(d,e.latestValues[d]??null),p=a[d];if(p===void 0||c&&Xz(c,d))continue;const h={delay:n,...tg(i||{},d)},m=u.get();if(m!==void 0&&!u.isAnimating&&!Array.isArray(p)&&p===m&&!h.velocity)continue;let g=!1;if(window.MotionHandoffAnimation){const f=gC(e);if(f){const _=window.MotionHandoffAnimation(f,d,yn);_!==null&&(h.startTime=_,g=!0)}}jm(e,d),u.start(ng(d,u,p,e.shouldReduceMotion&&mC.has(d)?{type:!1}:h,e,g));const I=u.animation;I&&l.push(I)}return o&&Promise.all(l).then(()=>{yn.update(()=>{o&&Kz(e,o)})}),l}function Rm(e,t,n={}){const r=No(e,t,n.type==="exit"?e.presenceContext?.custom:void 0);let{transition:s=e.getDefaultTransition()||{}}=r||{};n.transitionOverride&&(s=n.transitionOverride);const i=r?()=>Promise.all(_C(e,r,n)):()=>Promise.resolve(),o=e.variantChildren&&e.variantChildren.size?(l=0)=>{const{delayChildren:c=0,staggerChildren:d,staggerDirection:u}=s;return Yz(e,t,l,c,d,u,n)}:()=>Promise.resolve(),{when:a}=s;if(a){const[l,c]=a==="beforeChildren"?[i,o]:[o,i];return l().then(()=>c())}else return Promise.all([i(),o(n.delay)])}function Yz(e,t,n=0,r=0,s=0,i=1,o){const a=[];for(const l of e.variantChildren)l.notify("AnimationStart",t),a.push(Rm(l,t,{...o,delay:n+(typeof r=="function"?0:r)+hC(e.variantChildren,l,r,s,i)}).then(()=>l.notify("AnimationComplete",t)));return Promise.all(a)}function Jz(e,t,n={}){e.notify("AnimationStart",t);let r;if(Array.isArray(t)){const s=t.map(i=>Rm(e,i,n));r=Promise.all(s)}else if(typeof t=="string")r=Rm(e,t,n);else{const s=typeof t=="function"?No(e,t,n.custom):t;r=Promise.all(_C(e,s,n))}return r.then(()=>{e.notify("AnimationComplete",t)})}const Zz={test:e=>e==="auto",parse:e=>e},yC=e=>t=>t.test(e),xC=[Xo,pt,Cs,ii,PN,TN,Zz],_1=e=>xC.find(yC(e));function e4(e){return typeof e=="number"?e===0:e!==null?e==="none"||e==="0"||F2(e):!0}const t4=new Set(["brightness","contrast","saturate","opacity"]);function n4(e){const[t,n]=e.slice(0,-1).split("(");if(t==="drop-shadow")return e;const[r]=n.match(Kf)||[];if(!r)return e;const s=n.replace(r,"");let i=t4.has(t)?1:0;return r!==n&&(i*=100),t+"("+i+s+")"}const r4=/\b([a-z-]*)\(.*?\)/gu,Nm={...ci,getAnimatableNone:e=>{const t=e.match(r4);return t?t.map(n4).join(" "):e}},y1={...Xo,transform:Math.round},s4={rotate:ii,rotateX:ii,rotateY:ii,rotateZ:ii,scale:Tc,scaleX:Tc,scaleY:Tc,scaleZ:Tc,skew:ii,skewX:ii,skewY:ii,distance:pt,translateX:pt,translateY:pt,translateZ:pt,x:pt,y:pt,z:pt,perspective:pt,transformPerspective:pt,opacity:ul,originX:s1,originY:s1,originZ:pt},ig={borderWidth:pt,borderTopWidth:pt,borderRightWidth:pt,borderBottomWidth:pt,borderLeftWidth:pt,borderRadius:pt,radius:pt,borderTopLeftRadius:pt,borderTopRightRadius:pt,borderBottomRightRadius:pt,borderBottomLeftRadius:pt,width:pt,maxWidth:pt,height:pt,maxHeight:pt,top:pt,right:pt,bottom:pt,left:pt,inset:pt,insetBlock:pt,insetBlockStart:pt,insetBlockEnd:pt,insetInline:pt,insetInlineStart:pt,insetInlineEnd:pt,padding:pt,paddingTop:pt,paddingRight:pt,paddingBottom:pt,paddingLeft:pt,paddingBlock:pt,paddingBlockStart:pt,paddingBlockEnd:pt,paddingInline:pt,paddingInlineStart:pt,paddingInlineEnd:pt,margin:pt,marginTop:pt,marginRight:pt,marginBottom:pt,marginLeft:pt,marginBlock:pt,marginBlockStart:pt,marginBlockEnd:pt,marginInline:pt,marginInlineStart:pt,marginInlineEnd:pt,backgroundPositionX:pt,backgroundPositionY:pt,...s4,zIndex:y1,fillOpacity:ul,strokeOpacity:ul,numOctaves:y1},i4={...ig,color:Hn,backgroundColor:Hn,outlineColor:Hn,fill:Hn,stroke:Hn,borderColor:Hn,borderTopColor:Hn,borderRightColor:Hn,borderBottomColor:Hn,borderLeftColor:Hn,filter:Nm,WebkitFilter:Nm},bC=e=>i4[e];function wC(e,t){let n=bC(e);return n!==Nm&&(n=ci),n.getAnimatableNone?n.getAnimatableNone(t):void 0}const o4=new Set(["auto","none","0"]);function a4(e,t,n){let r=0,s;for(;r<e.length&&!s;){const i=e[r];typeof i=="string"&&!o4.has(i)&&dl(i).values.length&&(s=e[r]),r++}if(s&&n)for(const i of t)e[i]=wC(n,s)}class l4 extends eg{constructor(t,n,r,s,i){super(t,n,r,s,i,!0)}readKeyframes(){const{unresolvedKeyframes:t,element:n,name:r}=this;if(!n||!n.current)return;super.readKeyframes();for(let d=0;d<t.length;d++){let u=t[d];if(typeof u=="string"&&(u=u.trim(),Hf(u))){const p=pC(u,n.current);p!==void 0&&(t[d]=p),d===t.length-1&&(this.finalKeyframe=u)}}if(this.resolveNoneKeyframes(),!mC.has(r)||t.length!==2)return;const[s,i]=t,o=_1(s),a=_1(i),l=r1(s),c=r1(i);if(l!==c&&oi[r]){this.needsMeasurement=!0;return}if(o!==a)if(p1(o)&&p1(a))for(let d=0;d<t.length;d++){const u=t[d];typeof u=="string"&&(t[d]=parseFloat(u))}else oi[r]&&(this.needsMeasurement=!0)}resolveNoneKeyframes(){const{unresolvedKeyframes:t,name:n}=this,r=[];for(let s=0;s<t.length;s++)(t[s]===null||e4(t[s]))&&r.push(s);r.length&&a4(t,r,n)}measureInitialState(){const{element:t,unresolvedKeyframes:n,name:r}=this;if(!t||!t.current)return;r==="height"&&(this.suspendedScrollY=window.pageYOffset),this.measuredOrigin=oi[r](t.measureViewportBox(),window.getComputedStyle(t.current)),n[0]=this.measuredOrigin;const s=n[n.length-1];s!==void 0&&t.getValue(r,s).jump(s,!1)}measureEndState(){const{element:t,name:n,unresolvedKeyframes:r}=this;if(!t||!t.current)return;const s=t.getValue(n);s&&s.jump(this.measuredOrigin,!1);const i=r.length-1,o=r[i];r[i]=oi[n](t.measureViewportBox(),window.getComputedStyle(t.current)),o!==null&&this.finalKeyframe===void 0&&(this.finalKeyframe=o),this.removedTransforms?.length&&this.removedTransforms.forEach(([a,l])=>{t.getValue(a).set(l)}),this.resolveNoneKeyframes()}}function c4(e,t,n){if(e instanceof EventTarget)return[e];if(typeof e=="string"){let r=document;const s=n?.[e]??r.querySelectorAll(e);return s?Array.from(s):[]}return Array.from(e)}const vC=(e,t)=>t&&typeof e=="number"?t.transform(e):e;function MC(e){return $2(e)&&"offsetHeight"in e}const{schedule:og}=K2(queueMicrotask,!1),ss={x:!1,y:!1};function AC(){return ss.x||ss.y}function u4(e){return e==="x"||e==="y"?ss[e]?null:(ss[e]=!0,()=>{ss[e]=!1}):ss.x||ss.y?null:(ss.x=ss.y=!0,()=>{ss.x=ss.y=!1})}function TC(e,t){const n=c4(e),r=new AbortController,s={passive:!0,...t,signal:r.signal};return[n,s,()=>r.abort()]}function x1(e){return!(e.pointerType==="touch"||AC())}function d4(e,t,n={}){const[r,s,i]=TC(e,n),o=a=>{if(!x1(a))return;const{target:l}=a,c=t(l,a);if(typeof c!="function"||!l)return;const d=u=>{x1(u)&&(c(u),l.removeEventListener("pointerleave",d))};l.addEventListener("pointerleave",d,s)};return r.forEach(a=>{a.addEventListener("pointerenter",o,s)}),i}const PC=(e,t)=>t?e===t?!0:PC(e,t.parentElement):!1,ag=e=>e.pointerType==="mouse"?typeof e.button!="number"||e.button<=0:e.isPrimary!==!1,p4=new Set(["BUTTON","INPUT","SELECT","TEXTAREA","A"]);function CC(e){return p4.has(e.tagName)||e.isContentEditable===!0}const Yc=new WeakSet;function b1(e){return t=>{t.key==="Enter"&&e(t)}}function qp(e,t){e.dispatchEvent(new PointerEvent("pointer"+t,{isPrimary:!0,bubbles:!0}))}const h4=(e,t)=>{const n=e.currentTarget;if(!n)return;const r=b1(()=>{if(Yc.has(n))return;qp(n,"down");const s=b1(()=>{qp(n,"up")}),i=()=>qp(n,"cancel");n.addEventListener("keyup",s,t),n.addEventListener("blur",i,t)});n.addEventListener("keydown",r,t),n.addEventListener("blur",()=>n.removeEventListener("keydown",r),t)};function w1(e){return ag(e)&&!AC()}function m4(e,t,n={}){const[r,s,i]=TC(e,n),o=a=>{const l=a.currentTarget;if(!w1(a))return;Yc.add(l);const c=t(l,a),d=(h,m)=>{window.removeEventListener("pointerup",u),window.removeEventListener("pointercancel",p),Yc.has(l)&&Yc.delete(l),w1(h)&&typeof c=="function"&&c(h,{success:m})},u=h=>{d(h,l===window||l===document||n.useGlobalTarget||PC(l,h.target))},p=h=>{d(h,!1)};window.addEventListener("pointerup",u,s),window.addEventListener("pointercancel",p,s)};return r.forEach(a=>{(n.useGlobalTarget?window:a).addEventListener("pointerdown",o,s),MC(a)&&(a.addEventListener("focus",c=>h4(c,s)),!CC(a)&&!a.hasAttribute("tabindex")&&(a.tabIndex=0))}),i}function IC(e){return $2(e)&&"ownerSVGElement"in e}function f4(e){return IC(e)&&e.tagName==="svg"}const g4=[...xC,Hn,ci],_4=e=>g4.find(yC(e)),v1=()=>({translate:0,scale:1,origin:0,originPoint:0}),Eo=()=>({x:v1(),y:v1()}),M1=()=>({min:0,max:0}),Yn=()=>({x:M1(),y:M1()}),zm={current:null},kC={current:!1},y4=typeof window<"u";function x4(){if(kC.current=!0,!!y4)if(window.matchMedia){const e=window.matchMedia("(prefers-reduced-motion)"),t=()=>zm.current=e.matches;e.addEventListener("change",t),t()}else zm.current=!1}const b4=new WeakMap;function Wu(e){return e!==null&&typeof e=="object"&&typeof e.start=="function"}function pl(e){return typeof e=="string"||Array.isArray(e)}const lg=["animate","whileInView","whileFocus","whileHover","whileTap","whileDrag","exit"],cg=["initial",...lg];function Hu(e){return Wu(e.animate)||cg.some(t=>pl(e[t]))}function EC(e){return!!(Hu(e)||e.variants)}function w4(e,t,n){for(const r in t){const s=t[r],i=n[r];if(cr(s))e.addValue(r,s);else if(cr(i))e.addValue(r,Go(s,{owner:e}));else if(i!==s)if(e.hasValue(r)){const o=e.getValue(r);o.liveStyle===!0?o.jump(s):o.hasAnimated||o.set(s)}else{const o=e.getStaticValue(r);e.addValue(r,Go(o!==void 0?o:s,{owner:e}))}}for(const r in n)t[r]===void 0&&e.removeValue(r);return t}const A1=["AnimationStart","AnimationComplete","Update","BeforeLayoutMeasure","LayoutMeasure","LayoutAnimationStart","LayoutAnimationComplete"];let mu={};function SC(e){mu=e}function v4(){return mu}class M4{scrapeMotionValuesFromProps(t,n,r){return{}}constructor({parent:t,props:n,presenceContext:r,reducedMotionConfig:s,blockInitialAnimation:i,visualState:o},a={}){this.current=null,this.children=new Set,this.isVariantNode=!1,this.isControllingVariants=!1,this.shouldReduceMotion=null,this.values=new Map,this.KeyframeResolver=eg,this.features={},this.valueSubscriptions=new Map,this.prevMotionValues={},this.events={},this.propEventSubscriptions={},this.notifyUpdate=()=>this.notify("Update",this.latestValues),this.render=()=>{this.current&&(this.triggerBuild(),this.renderInstance(this.current,this.renderState,this.props.style,this.projection))},this.renderScheduledAt=0,this.scheduleRender=()=>{const p=fr.now();this.renderScheduledAt<p&&(this.renderScheduledAt=p,yn.render(this.render,!1,!0))};const{latestValues:l,renderState:c}=o;this.latestValues=l,this.baseTarget={...l},this.initialValues=n.initial?{...l}:{},this.renderState=c,this.parent=t,this.props=n,this.presenceContext=r,this.depth=t?t.depth+1:0,this.reducedMotionConfig=s,this.options=a,this.blockInitialAnimation=!!i,this.isControllingVariants=Hu(n),this.isVariantNode=EC(n),this.isVariantNode&&(this.variantChildren=new Set),this.manuallyAnimateOnMount=!!(t&&t.current);const{willChange:d,...u}=this.scrapeMotionValuesFromProps(n,{},this);for(const p in u){const h=u[p];l[p]!==void 0&&cr(h)&&h.set(l[p])}}mount(t){this.current=t,b4.set(t,this),this.projection&&!this.projection.instance&&this.projection.mount(t),this.parent&&this.isVariantNode&&!this.isControllingVariants&&(this.removeFromVariantTree=this.parent.addVariantChild(this)),this.values.forEach((n,r)=>this.bindToMotionValue(r,n)),this.reducedMotionConfig==="never"?this.shouldReduceMotion=!1:this.reducedMotionConfig==="always"?this.shouldReduceMotion=!0:(kC.current||x4(),this.shouldReduceMotion=zm.current),this.parent?.addChild(this),this.update(this.props,this.presenceContext)}unmount(){this.projection&&this.projection.unmount(),li(this.notifyUpdate),li(this.render),this.valueSubscriptions.forEach(t=>t()),this.valueSubscriptions.clear(),this.removeFromVariantTree&&this.removeFromVariantTree(),this.parent?.removeChild(this);for(const t in this.events)this.events[t].clear();for(const t in this.features){const n=this.features[t];n&&(n.unmount(),n.isMounted=!1)}this.current=null}addChild(t){this.children.add(t),this.enteringChildren??(this.enteringChildren=new Set),this.enteringChildren.add(t)}removeChild(t){this.children.delete(t),this.enteringChildren&&this.enteringChildren.delete(t)}bindToMotionValue(t,n){this.valueSubscriptions.has(t)&&this.valueSubscriptions.get(t)();const r=Jo.has(t);r&&this.onBindTransform&&this.onBindTransform();const s=n.on("change",o=>{this.latestValues[t]=o,this.props.onUpdate&&yn.preRender(this.notifyUpdate),r&&this.projection&&(this.projection.isTransformDirty=!0),this.scheduleRender()});let i;typeof window<"u"&&window.MotionCheckAppearSync&&(i=window.MotionCheckAppearSync(this,t,n)),this.valueSubscriptions.set(t,()=>{s(),i&&i(),n.owner&&n.stop()})}sortNodePosition(t){return!this.current||!this.sortInstanceNodePosition||this.type!==t.type?0:this.sortInstanceNodePosition(this.current,t.current)}updateFeatures(){let t="animation";for(t in mu){const n=mu[t];if(!n)continue;const{isEnabled:r,Feature:s}=n;if(!this.features[t]&&s&&r(this.props)&&(this.features[t]=new s(this)),this.features[t]){const i=this.features[t];i.isMounted?i.update():(i.mount(),i.isMounted=!0)}}}triggerBuild(){this.build(this.renderState,this.latestValues,this.props)}measureViewportBox(){return this.current?this.measureInstanceViewportBox(this.current,this.props):Yn()}getStaticValue(t){return this.latestValues[t]}setStaticValue(t,n){this.latestValues[t]=n}update(t,n){(t.transformTemplate||this.props.transformTemplate)&&this.scheduleRender(),this.prevProps=this.props,this.props=t,this.prevPresenceContext=this.presenceContext,this.presenceContext=n;for(let r=0;r<A1.length;r++){const s=A1[r];this.propEventSubscriptions[s]&&(this.propEventSubscriptions[s](),delete this.propEventSubscriptions[s]);const i="on"+s,o=t[i];o&&(this.propEventSubscriptions[s]=this.on(s,o))}this.prevMotionValues=w4(this,this.scrapeMotionValuesFromProps(t,this.prevProps||{},this),this.prevMotionValues),this.handleChildMotionValue&&this.handleChildMotionValue()}getProps(){return this.props}getVariant(t){return this.props.variants?this.props.variants[t]:void 0}getDefaultTransition(){return this.props.transition}getTransformPagePoint(){return this.props.transformPagePoint}getClosestVariantNode(){return this.isVariantNode?this:this.parent?this.parent.getClosestVariantNode():void 0}addVariantChild(t){const n=this.getClosestVariantNode();if(n)return n.variantChildren&&n.variantChildren.add(t),()=>n.variantChildren.delete(t)}addValue(t,n){const r=this.values.get(t);n!==r&&(r&&this.removeValue(t),this.bindToMotionValue(t,n),this.values.set(t,n),this.latestValues[t]=n.get())}removeValue(t){this.values.delete(t);const n=this.valueSubscriptions.get(t);n&&(n(),this.valueSubscriptions.delete(t)),delete this.latestValues[t],this.removeValueFromRenderState(t,this.renderState)}hasValue(t){return this.values.has(t)}getValue(t,n){if(this.props.values&&this.props.values[t])return this.props.values[t];let r=this.values.get(t);return r===void 0&&n!==void 0&&(r=Go(n===null?void 0:n,{owner:this}),this.addValue(t,r)),r}readValue(t,n){let r=this.latestValues[t]!==void 0||!this.current?this.latestValues[t]:this.getBaseTargetFromProps(this.props,t)??this.readValueFromInstance(this.current,t,this.options);return r!=null&&(typeof r=="string"&&(D2(r)||F2(r))?r=parseFloat(r):!_4(r)&&ci.test(n)&&(r=wC(t,n)),this.setBaseTarget(t,cr(r)?r.get():r)),cr(r)?r.get():r}setBaseTarget(t,n){this.baseTarget[t]=n}getBaseTarget(t){const{initial:n}=this.props;let r;if(typeof n=="string"||typeof n=="object"){const i=rg(this.props,n,this.presenceContext?.custom);i&&(r=i[t])}if(n&&r!==void 0)return r;const s=this.getBaseTargetFromProps(this.props,t);return s!==void 0&&!cr(s)?s:this.initialValues[t]!==void 0&&r===void 0?void 0:this.baseTarget[t]}on(t,n){return this.events[t]||(this.events[t]=new Vf),this.events[t].add(n)}notify(t,...n){this.events[t]&&this.events[t].notify(...n)}scheduleRenderMicrotask(){og.render(this.render)}}class pi{constructor(t){this.isMounted=!1,this.node=t}update(){}}class LC extends M4{constructor(){super(...arguments),this.KeyframeResolver=l4}sortInstanceNodePosition(t,n){return t.compareDocumentPosition(n)&2?1:-1}getBaseTargetFromProps(t,n){const r=t.style;return r?r[n]:void 0}removeValueFromRenderState(t,{vars:n,style:r}){delete n[t],delete r[t]}handleChildMotionValue(){this.childSubscription&&(this.childSubscription(),delete this.childSubscription);const{children:t}=this.props;cr(t)&&(this.childSubscription=t.on("change",n=>{this.current&&(this.current.textContent=`${n}`)}))}}function DC({top:e,left:t,right:n,bottom:r}){return{x:{min:t,max:n},y:{min:e,max:r}}}function A4({x:e,y:t}){return{top:t.min,right:e.max,bottom:t.max,left:e.min}}function T4(e,t){if(!t)return e;const n=t({x:e.left,y:e.top}),r=t({x:e.right,y:e.bottom});return{top:n.y,left:n.x,bottom:r.y,right:r.x}}function Qp(e){return e===void 0||e===1}function Bm({scale:e,scaleX:t,scaleY:n}){return!Qp(e)||!Qp(t)||!Qp(n)}function Li(e){return Bm(e)||$C(e)||e.z||e.rotate||e.rotateX||e.rotateY||e.skewX||e.skewY}function $C(e){return T1(e.x)||T1(e.y)}function T1(e){return e&&e!=="0%"}function fu(e,t,n){const r=e-n,s=t*r;return n+s}function P1(e,t,n,r,s){return s!==void 0&&(e=fu(e,s,r)),fu(e,n,r)+t}function Gm(e,t=0,n=1,r,s){e.min=P1(e.min,t,n,r,s),e.max=P1(e.max,t,n,r,s)}function FC(e,{x:t,y:n}){Gm(e.x,t.translate,t.scale,t.originPoint),Gm(e.y,n.translate,n.scale,n.originPoint)}const C1=.999999999999,I1=1.0000000000001;function P4(e,t,n,r=!1){const s=n.length;if(!s)return;t.x=t.y=1;let i,o;for(let a=0;a<s;a++){i=n[a],o=i.projectionDelta;const{visualElement:l}=i.options;l&&l.props.style&&l.props.style.display==="contents"||(r&&i.options.layoutScroll&&i.scroll&&i!==i.root&&Lo(e,{x:-i.scroll.offset.x,y:-i.scroll.offset.y}),o&&(t.x*=o.x.scale,t.y*=o.y.scale,FC(e,o)),r&&Li(i.latestValues)&&Lo(e,i.latestValues))}t.x<I1&&t.x>C1&&(t.x=1),t.y<I1&&t.y>C1&&(t.y=1)}function So(e,t){e.min=e.min+t,e.max=e.max+t}function k1(e,t,n,r,s=.5){const i=kn(e.min,e.max,s);Gm(e,t,n,i,r)}function Lo(e,t){k1(e.x,t.x,t.scaleX,t.scale,t.originX),k1(e.y,t.y,t.scaleY,t.scale,t.originY)}function OC(e,t){return DC(T4(e.getBoundingClientRect(),t))}function C4(e,t,n){const r=OC(e,n),{scroll:s}=t;return s&&(So(r.x,s.offset.x),So(r.y,s.offset.y)),r}const I4={x:"translateX",y:"translateY",z:"translateZ",transformPerspective:"perspective"},k4=Yo.length;function E4(e,t,n){let r="",s=!0;for(let i=0;i<k4;i++){const o=Yo[i],a=e[o];if(a===void 0)continue;let l=!0;if(typeof a=="number"?l=a===(o.startsWith("scale")?1:0):l=parseFloat(a)===0,!l||n){const c=vC(a,ig[o]);if(!l){s=!1;const d=I4[o]||o;r+=`${d}(${c}) `}n&&(t[o]=c)}}return r=r.trim(),n?r=n(t,s?"":r):s&&(r="none"),r}function ug(e,t,n){const{style:r,vars:s,transformOrigin:i}=e;let o=!1,a=!1;for(const l in t){const c=t[l];if(Jo.has(l)){o=!0;continue}else if(Q2(l)){s[l]=c;continue}else{const d=vC(c,ig[l]);l.startsWith("origin")?(a=!0,i[l]=d):r[l]=d}}if(t.transform||(o||n?r.transform=E4(t,e.transform,n):r.transform&&(r.transform="none")),a){const{originX:l="50%",originY:c="50%",originZ:d=0}=i;r.transformOrigin=`${l} ${c} ${d}`}}function jC(e,{style:t,vars:n},r,s){const i=e.style;let o;for(o in t)i[o]=t[o];s?.applyProjectionStyles(i,r);for(o in n)i.setProperty(o,n[o])}function E1(e,t){return t.max===t.min?0:e/(t.max-t.min)*100}const Ia={correct:(e,t)=>{if(!t.target)return e;if(typeof e=="string")if(pt.test(e))e=parseFloat(e);else return e;const n=E1(e,t.target.x),r=E1(e,t.target.y);return`${n}% ${r}%`}},S4={correct:(e,{treeScale:t,projectionDelta:n})=>{const r=e,s=ci.parse(e);if(s.length>5)return r;const i=ci.createTransformer(e),o=typeof s[0]!="number"?1:0,a=n.x.scale*t.x,l=n.y.scale*t.y;s[0+o]/=a,s[1+o]/=l;const c=kn(a,l,.5);return typeof s[2+o]=="number"&&(s[2+o]/=c),typeof s[3+o]=="number"&&(s[3+o]/=c),i(s)}},Vm={borderRadius:{...Ia,applyTo:["borderTopLeftRadius","borderTopRightRadius","borderBottomLeftRadius","borderBottomRightRadius"]},borderTopLeftRadius:Ia,borderTopRightRadius:Ia,borderBottomLeftRadius:Ia,borderBottomRightRadius:Ia,boxShadow:S4};function RC(e,{layout:t,layoutId:n}){return Jo.has(e)||e.startsWith("origin")||(t||n!==void 0)&&(!!Vm[e]||e==="opacity")}function dg(e,t,n){const r=e.style,s=t?.style,i={};if(!r)return i;for(const o in r)(cr(r[o])||s&&cr(s[o])||RC(o,e)||n?.getValue(o)?.liveStyle!==void 0)&&(i[o]=r[o]);return i}function L4(e){return window.getComputedStyle(e)}class D4 extends LC{constructor(){super(...arguments),this.type="html",this.renderInstance=jC}readValueFromInstance(t,n){if(Jo.has(n))return this.projection?.isProjecting?Em(n):cz(t,n);{const r=L4(t),s=(Q2(n)?r.getPropertyValue(n):r[n])||0;return typeof s=="string"?s.trim():s}}measureInstanceViewportBox(t,{transformPagePoint:n}){return OC(t,n)}build(t,n,r){ug(t,n,r.transformTemplate)}scrapeMotionValuesFromProps(t,n,r){return dg(t,n,r)}}const $4={offset:"stroke-dashoffset",array:"stroke-dasharray"},F4={offset:"strokeDashoffset",array:"strokeDasharray"};function O4(e,t,n=1,r=0,s=!0){e.pathLength=1;const i=s?$4:F4;e[i.offset]=pt.transform(-r);const o=pt.transform(t),a=pt.transform(n);e[i.array]=`${o} ${a}`}const j4=["offsetDistance","offsetPath","offsetRotate","offsetAnchor"];function NC(e,{attrX:t,attrY:n,attrScale:r,pathLength:s,pathSpacing:i=1,pathOffset:o=0,...a},l,c,d){if(ug(e,a,c),l){e.style.viewBox&&(e.attrs.viewBox=e.style.viewBox);return}e.attrs=e.style,e.style={};const{attrs:u,style:p}=e;u.transform&&(p.transform=u.transform,delete u.transform),(p.transform||u.transformOrigin)&&(p.transformOrigin=u.transformOrigin??"50% 50%",delete u.transformOrigin),p.transform&&(p.transformBox=d?.transformBox??"fill-box",delete u.transformBox);for(const h of j4)u[h]!==void 0&&(p[h]=u[h],delete u[h]);t!==void 0&&(u.x=t),n!==void 0&&(u.y=n),r!==void 0&&(u.scale=r),s!==void 0&&O4(u,s,i,o,!1)}const zC=new Set(["baseFrequency","diffuseConstant","kernelMatrix","kernelUnitLength","keySplines","keyTimes","limitingConeAngle","markerHeight","markerWidth","numOctaves","targetX","targetY","surfaceScale","specularConstant","specularExponent","stdDeviation","tableValues","viewBox","gradientTransform","pathLength","startOffset","textLength","lengthAdjust"]),BC=e=>typeof e=="string"&&e.toLowerCase()==="svg";function R4(e,t,n,r){jC(e,t,void 0,r);for(const s in t.attrs)e.setAttribute(zC.has(s)?s:sg(s),t.attrs[s])}function GC(e,t,n){const r=dg(e,t,n);for(const s in e)if(cr(e[s])||cr(t[s])){const i=Yo.indexOf(s)!==-1?"attr"+s.charAt(0).toUpperCase()+s.substring(1):s;r[i]=e[s]}return r}class N4 extends LC{constructor(){super(...arguments),this.type="svg",this.isSVGTag=!1,this.measureInstanceViewportBox=Yn}getBaseTargetFromProps(t,n){return t[n]}readValueFromInstance(t,n){if(Jo.has(n)){const r=bC(n);return r&&r.default||0}return n=zC.has(n)?n:sg(n),t.getAttribute(n)}scrapeMotionValuesFromProps(t,n,r){return GC(t,n,r)}build(t,n,r){NC(t,n,this.isSVGTag,r.transformTemplate,r.style)}renderInstance(t,n,r,s){R4(t,n,r,s)}mount(t){this.isSVGTag=BC(t.tagName),super.mount(t)}}const z4=cg.length;function VC(e){if(!e)return;if(!e.isControllingVariants){const n=e.parent?VC(e.parent)||{}:{};return e.props.initial!==void 0&&(n.initial=e.props.initial),n}const t={};for(let n=0;n<z4;n++){const r=cg[n],s=e.props[r];(pl(s)||s===!1)&&(t[r]=s)}return t}function UC(e,t){if(!Array.isArray(t))return!1;const n=t.length;if(n!==e.length)return!1;for(let r=0;r<n;r++)if(t[r]!==e[r])return!1;return!0}const B4=[...lg].reverse(),G4=lg.length;function V4(e){return t=>Promise.all(t.map(({animation:n,options:r})=>Jz(e,n,r)))}function U4(e){let t=V4(e),n=S1(),r=!0;const s=l=>(c,d)=>{const u=No(e,d,l==="exit"?e.presenceContext?.custom:void 0);if(u){const{transition:p,transitionEnd:h,...m}=u;c={...c,...m,...h}}return c};function i(l){t=l(e)}function o(l){const{props:c}=e,d=VC(e.parent)||{},u=[],p=new Set;let h={},m=1/0;for(let I=0;I<G4;I++){const f=B4[I],_=n[f],T=c[f]!==void 0?c[f]:d[f],M=pl(T),v=f===l?_.isActive:null;v===!1&&(m=I);let b=T===d[f]&&T!==c[f]&&M;if(b&&r&&e.manuallyAnimateOnMount&&(b=!1),_.protectedKeys={...h},!_.isActive&&v===null||!T&&!_.prevProp||Wu(T)||typeof T=="boolean")continue;const A=W4(_.prevProp,T);let k=A||f===l&&_.isActive&&!b&&M||I>m&&M,F=!1;const L=Array.isArray(T)?T:[T];let G=L.reduce(s(f),{});v===!1&&(G={});const{prevResolvedValues:j={}}=_,R={...j,...G},K=te=>{k=!0,p.has(te)&&(F=!0,p.delete(te)),_.needsAnimating[te]=!0;const ne=e.getValue(te);ne&&(ne.liveStyle=!1)};for(const te in R){const ne=G[te],le=j[te];if(h.hasOwnProperty(te))continue;let N=!1;Om(ne)&&Om(le)?N=!UC(ne,le):N=ne!==le,N?ne!=null?K(te):p.add(te):ne!==void 0&&p.has(te)?K(te):_.protectedKeys[te]=!0}_.prevProp=T,_.prevResolvedValues=G,_.isActive&&(h={...h,...G}),r&&e.blockInitialAnimation&&(k=!1);const U=b&&A;k&&(!U||F)&&u.push(...L.map(te=>{const ne={type:f};if(typeof te=="string"&&r&&!U&&e.manuallyAnimateOnMount&&e.parent){const{parent:le}=e,N=No(le,te);if(le.enteringChildren&&N){const{delayChildren:oe}=N.transition||{};ne.delay=hC(le.enteringChildren,e,oe)}}return{animation:te,options:ne}}))}if(p.size){const I={};if(typeof c.initial!="boolean"){const f=No(e,Array.isArray(c.initial)?c.initial[0]:c.initial);f&&f.transition&&(I.transition=f.transition)}p.forEach(f=>{const _=e.getBaseTarget(f),T=e.getValue(f);T&&(T.liveStyle=!0),I[f]=_??null}),u.push({animation:I})}let g=!!u.length;return r&&(c.initial===!1||c.initial===c.animate)&&!e.manuallyAnimateOnMount&&(g=!1),r=!1,g?t(u):Promise.resolve()}function a(l,c){if(n[l].isActive===c)return Promise.resolve();e.variantChildren?.forEach(u=>u.animationState?.setActive(l,c)),n[l].isActive=c;const d=o(l);for(const u in n)n[u].protectedKeys={};return d}return{animateChanges:o,setActive:a,setAnimateFunction:i,getState:()=>n,reset:()=>{n=S1()}}}function W4(e,t){return typeof t=="string"?t!==e:Array.isArray(t)?!UC(t,e):!1}function Ci(e=!1){return{isActive:e,protectedKeys:{},needsAnimating:{},prevResolvedValues:{}}}function S1(){return{animate:Ci(!0),whileInView:Ci(),whileHover:Ci(),whileTap:Ci(),whileDrag:Ci(),whileFocus:Ci(),exit:Ci()}}const WC=1e-4,H4=1-WC,K4=1+WC,HC=.01,q4=0-HC,Q4=0+HC;function gr(e){return e.max-e.min}function X4(e,t,n){return Math.abs(e-t)<=n}function L1(e,t,n,r=.5){e.origin=r,e.originPoint=kn(t.min,t.max,e.origin),e.scale=gr(n)/gr(t),e.translate=kn(n.min,n.max,e.origin)-e.originPoint,(e.scale>=H4&&e.scale<=K4||isNaN(e.scale))&&(e.scale=1),(e.translate>=q4&&e.translate<=Q4||isNaN(e.translate))&&(e.translate=0)}function sl(e,t,n,r){L1(e.x,t.x,n.x,r?r.originX:void 0),L1(e.y,t.y,n.y,r?r.originY:void 0)}function D1(e,t,n){e.min=n.min+t.min,e.max=e.min+gr(t)}function Y4(e,t,n){D1(e.x,t.x,n.x),D1(e.y,t.y,n.y)}function $1(e,t,n){e.min=t.min-n.min,e.max=e.min+gr(t)}function gu(e,t,n){$1(e.x,t.x,n.x),$1(e.y,t.y,n.y)}function F1(e,t,n,r,s){return e-=t,e=fu(e,1/n,r),s!==void 0&&(e=fu(e,1/s,r)),e}function J4(e,t=0,n=1,r=.5,s,i=e,o=e){if(Cs.test(t)&&(t=parseFloat(t),t=kn(o.min,o.max,t/100)-o.min),typeof t!="number")return;let a=kn(i.min,i.max,r);e===i&&(a-=t),e.min=F1(e.min,t,n,a,s),e.max=F1(e.max,t,n,a,s)}function O1(e,t,[n,r,s],i,o){J4(e,t[n],t[r],t[s],t.scale,i,o)}const Z4=["x","scaleX","originX"],eB=["y","scaleY","originY"];function j1(e,t,n,r){O1(e.x,t,Z4,n?n.x:void 0,r?r.x:void 0),O1(e.y,t,eB,n?n.y:void 0,r?r.y:void 0)}function R1(e,t){e.min=t.min,e.max=t.max}function Jr(e,t){R1(e.x,t.x),R1(e.y,t.y)}function N1(e,t){e.translate=t.translate,e.scale=t.scale,e.originPoint=t.originPoint,e.origin=t.origin}function z1(e){return e.translate===0&&e.scale===1}function KC(e){return z1(e.x)&&z1(e.y)}function B1(e,t){return e.min===t.min&&e.max===t.max}function tB(e,t){return B1(e.x,t.x)&&B1(e.y,t.y)}function G1(e,t){return Math.round(e.min)===Math.round(t.min)&&Math.round(e.max)===Math.round(t.max)}function qC(e,t){return G1(e.x,t.x)&&G1(e.y,t.y)}function V1(e){return gr(e.x)/gr(e.y)}function U1(e,t){return e.translate===t.translate&&e.scale===t.scale&&e.originPoint===t.originPoint}function zr(e){return[e("x"),e("y")]}function nB(e,t,n){let r="";const s=e.x.translate/t.x,i=e.y.translate/t.y,o=n?.z||0;if((s||i||o)&&(r=`translate3d(${s}px, ${i}px, ${o}px) `),(t.x!==1||t.y!==1)&&(r+=`scale(${1/t.x}, ${1/t.y}) `),n){const{transformPerspective:c,rotate:d,rotateX:u,rotateY:p,skewX:h,skewY:m}=n;c&&(r=`perspective(${c}px) ${r}`),d&&(r+=`rotate(${d}deg) `),u&&(r+=`rotateX(${u}deg) `),p&&(r+=`rotateY(${p}deg) `),h&&(r+=`skewX(${h}deg) `),m&&(r+=`skewY(${m}deg) `)}const a=e.x.scale*t.x,l=e.y.scale*t.y;return(a!==1||l!==1)&&(r+=`scale(${a}, ${l})`),r||"none"}const QC=["TopLeft","TopRight","BottomLeft","BottomRight"],rB=QC.length,W1=e=>typeof e=="string"?parseFloat(e):e,H1=e=>typeof e=="number"||pt.test(e);function sB(e,t,n,r,s,i){s?(e.opacity=kn(0,n.opacity??1,iB(r)),e.opacityExit=kn(t.opacity??1,0,oB(r))):i&&(e.opacity=kn(t.opacity??1,n.opacity??1,r));for(let o=0;o<rB;o++){const a=`border${QC[o]}Radius`;let l=K1(t,a),c=K1(n,a);if(l===void 0&&c===void 0)continue;l||(l=0),c||(c=0),l===0||c===0||H1(l)===H1(c)?(e[a]=Math.max(kn(W1(l),W1(c),r),0),(Cs.test(c)||Cs.test(l))&&(e[a]+="%")):e[a]=c}(t.rotate||n.rotate)&&(e.rotate=kn(t.rotate||0,n.rotate||0,r))}function K1(e,t){return e[t]!==void 0?e[t]:e.borderRadius}const iB=XC(0,.5,V2),oB=XC(.5,.95,Hr);function XC(e,t,n){return r=>r<e?0:r>t?1:n(cl(e,t,r))}function aB(e,t){const n=fr.now(),r=({timestamp:s})=>{const i=s-n;i>=t&&(li(r),e(i-t))};return yn.setup(r,!0),()=>li(r)}function hl(e,t,n,r={passive:!0}){return e.addEventListener(t,n,r),()=>e.removeEventListener(t,n)}function Jc(e){return cr(e)?e.get():e}function lB(e,t,n){const r=cr(e)?e:Go(e);return r.start(ng("",r,t,n)),r.animation}const cB=(e,t)=>e.depth-t.depth;class uB{constructor(){this.children=[],this.isDirty=!1}add(t){Nf(this.children,t),this.isDirty=!0}remove(t){zf(this.children,t),this.isDirty=!0}forEach(t){this.isDirty&&this.children.sort(cB),this.isDirty=!1,this.children.forEach(t)}}class dB{constructor(){this.members=[]}add(t){Nf(this.members,t),t.scheduleRender()}remove(t){if(zf(this.members,t),t===this.prevLead&&(this.prevLead=void 0),t===this.lead){const n=this.members[this.members.length-1];n&&this.promote(n)}}relegate(t){const n=this.members.findIndex(s=>t===s);if(n===0)return!1;let r;for(let s=n;s>=0;s--){const i=this.members[s];if(i.isPresent!==!1){r=i;break}}return r?(this.promote(r),!0):!1}promote(t,n){const r=this.lead;if(t!==r&&(this.prevLead=r,this.lead=t,t.show(),r)){r.instance&&r.scheduleRender(),t.scheduleRender(),t.resumeFrom=r,n&&(t.resumeFrom.preserveOpacity=!0),r.snapshot&&(t.snapshot=r.snapshot,t.snapshot.latestValues=r.animationValues||r.latestValues),t.root&&t.root.isUpdating&&(t.isLayoutDirty=!0);const{crossfade:s}=t.options;s===!1&&r.hide()}}exitAnimationComplete(){this.members.forEach(t=>{const{options:n,resumingFrom:r}=t;n.onExitComplete&&n.onExitComplete(),r&&r.options.onExitComplete&&r.options.onExitComplete()})}scheduleRender(){this.members.forEach(t=>{t.instance&&t.scheduleRender(!1)})}removeLeadSnapshot(){this.lead&&this.lead.snapshot&&(this.lead.snapshot=void 0)}}const Zc={hasAnimatedSinceResize:!0,hasEverUpdated:!1},Xp=["","X","Y","Z"],pB=1e3;let hB=0;function Yp(e,t,n,r){const{latestValues:s}=t;s[e]&&(n[e]=s[e],t.setStaticValue(e,0),r&&(r[e]=0))}function YC(e){if(e.hasCheckedOptimisedAppear=!0,e.root===e)return;const{visualElement:t}=e.options;if(!t)return;const n=gC(t);if(window.MotionHasOptimisedAnimation(n,"transform")){const{layout:s,layoutId:i}=e.options;window.MotionCancelOptimisedAnimation(n,"transform",yn,!(s||i))}const{parent:r}=e;r&&!r.hasCheckedOptimisedAppear&&YC(r)}function JC({attachResizeListener:e,defaultParent:t,measureScroll:n,checkIsScrollRoot:r,resetTransform:s}){return class{constructor(o={},a=t?.()){this.id=hB++,this.animationId=0,this.animationCommitId=0,this.children=new Set,this.options={},this.isTreeAnimating=!1,this.isAnimationBlocked=!1,this.isLayoutDirty=!1,this.isProjectionDirty=!1,this.isSharedProjectionDirty=!1,this.isTransformDirty=!1,this.updateManuallyBlocked=!1,this.updateBlockedByResize=!1,this.isUpdating=!1,this.isSVG=!1,this.needsReset=!1,this.shouldResetTransform=!1,this.hasCheckedOptimisedAppear=!1,this.treeScale={x:1,y:1},this.eventHandlers=new Map,this.hasTreeAnimated=!1,this.layoutVersion=0,this.updateScheduled=!1,this.scheduleUpdate=()=>this.update(),this.projectionUpdateScheduled=!1,this.checkUpdateFailed=()=>{this.isUpdating&&(this.isUpdating=!1,this.clearAllSnapshots())},this.updateProjection=()=>{this.projectionUpdateScheduled=!1,this.nodes.forEach(gB),this.nodes.forEach(bB),this.nodes.forEach(wB),this.nodes.forEach(_B)},this.resolvedRelativeTargetAt=0,this.linkedParentVersion=0,this.hasProjected=!1,this.isVisible=!0,this.animationProgress=0,this.sharedNodes=new Map,this.latestValues=o,this.root=a?a.root||a:this,this.path=a?[...a.path,a]:[],this.parent=a,this.depth=a?a.depth+1:0;for(let l=0;l<this.path.length;l++)this.path[l].shouldResetTransform=!0;this.root===this&&(this.nodes=new uB)}addEventListener(o,a){return this.eventHandlers.has(o)||this.eventHandlers.set(o,new Vf),this.eventHandlers.get(o).add(a)}notifyListeners(o,...a){const l=this.eventHandlers.get(o);l&&l.notify(...a)}hasListeners(o){return this.eventHandlers.has(o)}mount(o){if(this.instance)return;this.isSVG=IC(o)&&!f4(o),this.instance=o;const{layoutId:a,layout:l,visualElement:c}=this.options;if(c&&!c.current&&c.mount(o),this.root.nodes.add(this),this.parent&&this.parent.children.add(this),this.root.hasTreeAnimated&&(l||a)&&(this.isLayoutDirty=!0),e){let d,u=0;const p=()=>this.root.updateBlockedByResize=!1;yn.read(()=>{u=window.innerWidth}),e(o,()=>{const h=window.innerWidth;h!==u&&(u=h,this.root.updateBlockedByResize=!0,d&&d(),d=aB(p,250),Zc.hasAnimatedSinceResize&&(Zc.hasAnimatedSinceResize=!1,this.nodes.forEach(X1)))})}a&&this.root.registerSharedNode(a,this),this.options.animate!==!1&&c&&(a||l)&&this.addEventListener("didUpdate",({delta:d,hasLayoutChanged:u,hasRelativeLayoutChanged:p,layout:h})=>{if(this.isTreeAnimationBlocked()){this.target=void 0,this.relativeTarget=void 0;return}const m=this.options.transition||c.getDefaultTransition()||PB,{onLayoutAnimationStart:g,onLayoutAnimationComplete:I}=c.getProps(),f=!this.targetLayout||!qC(this.targetLayout,h),_=!u&&p;if(this.options.layoutRoot||this.resumeFrom||_||u&&(f||!this.currentAnimation)){this.resumeFrom&&(this.resumingFrom=this.resumeFrom,this.resumingFrom.resumingFrom=void 0);const T={...tg(m,"layout"),onPlay:g,onComplete:I};(c.shouldReduceMotion||this.options.layoutRoot)&&(T.delay=0,T.type=!1),this.startAnimation(T),this.setAnimationOrigin(d,_)}else u||X1(this),this.isLead()&&this.options.onExitComplete&&this.options.onExitComplete();this.targetLayout=h})}unmount(){this.options.layoutId&&this.willUpdate(),this.root.nodes.remove(this);const o=this.getStack();o&&o.remove(this),this.parent&&this.parent.children.delete(this),this.instance=void 0,this.eventHandlers.clear(),li(this.updateProjection)}blockUpdate(){this.updateManuallyBlocked=!0}unblockUpdate(){this.updateManuallyBlocked=!1}isUpdateBlocked(){return this.updateManuallyBlocked||this.updateBlockedByResize}isTreeAnimationBlocked(){return this.isAnimationBlocked||this.parent&&this.parent.isTreeAnimationBlocked()||!1}startUpdate(){this.isUpdateBlocked()||(this.isUpdating=!0,this.nodes&&this.nodes.forEach(vB),this.animationId++)}getTransformTemplate(){const{visualElement:o}=this.options;return o&&o.getProps().transformTemplate}willUpdate(o=!0){if(this.root.hasTreeAnimated=!0,this.root.isUpdateBlocked()){this.options.onExitComplete&&this.options.onExitComplete();return}if(window.MotionCancelOptimisedAnimation&&!this.hasCheckedOptimisedAppear&&YC(this),!this.root.isUpdating&&this.root.startUpdate(),this.isLayoutDirty)return;this.isLayoutDirty=!0;for(let d=0;d<this.path.length;d++){const u=this.path[d];u.shouldResetTransform=!0,u.updateScroll("snapshot"),u.options.layoutRoot&&u.willUpdate(!1)}const{layoutId:a,layout:l}=this.options;if(a===void 0&&!l)return;const c=this.getTransformTemplate();this.prevTransformTemplateValue=c?c(this.latestValues,""):void 0,this.updateSnapshot(),o&&this.notifyListeners("willUpdate")}update(){if(this.updateScheduled=!1,this.isUpdateBlocked()){this.unblockUpdate(),this.clearAllSnapshots(),this.nodes.forEach(q1);return}if(this.animationId<=this.animationCommitId){this.nodes.forEach(Q1);return}this.animationCommitId=this.animationId,this.isUpdating?(this.isUpdating=!1,this.nodes.forEach(xB),this.nodes.forEach(mB),this.nodes.forEach(fB)):this.nodes.forEach(Q1),this.clearAllSnapshots();const a=fr.now();ir.delta=ks(0,1e3/60,a-ir.timestamp),ir.timestamp=a,ir.isProcessing=!0,Gp.update.process(ir),Gp.preRender.process(ir),Gp.render.process(ir),ir.isProcessing=!1}didUpdate(){this.updateScheduled||(this.updateScheduled=!0,og.read(this.scheduleUpdate))}clearAllSnapshots(){this.nodes.forEach(yB),this.sharedNodes.forEach(MB)}scheduleUpdateProjection(){this.projectionUpdateScheduled||(this.projectionUpdateScheduled=!0,yn.preRender(this.updateProjection,!1,!0))}scheduleCheckAfterUnmount(){yn.postRender(()=>{this.isLayoutDirty?this.root.didUpdate():this.root.checkUpdateFailed()})}updateSnapshot(){this.snapshot||!this.instance||(this.snapshot=this.measure(),this.snapshot&&!gr(this.snapshot.measuredBox.x)&&!gr(this.snapshot.measuredBox.y)&&(this.snapshot=void 0))}updateLayout(){if(!this.instance||(this.updateScroll(),!(this.options.alwaysMeasureLayout&&this.isLead())&&!this.isLayoutDirty))return;if(this.resumeFrom&&!this.resumeFrom.instance)for(let l=0;l<this.path.length;l++)this.path[l].updateScroll();const o=this.layout;this.layout=this.measure(!1),this.layoutVersion++,this.layoutCorrected=Yn(),this.isLayoutDirty=!1,this.projectionDelta=void 0,this.notifyListeners("measure",this.layout.layoutBox);const{visualElement:a}=this.options;a&&a.notify("LayoutMeasure",this.layout.layoutBox,o?o.layoutBox:void 0)}updateScroll(o="measure"){let a=!!(this.options.layoutScroll&&this.instance);if(this.scroll&&this.scroll.animationId===this.root.animationId&&this.scroll.phase===o&&(a=!1),a&&this.instance){const l=r(this.instance);this.scroll={animationId:this.root.animationId,phase:o,isRoot:l,offset:n(this.instance),wasRoot:this.scroll?this.scroll.isRoot:l}}}resetTransform(){if(!s)return;const o=this.isLayoutDirty||this.shouldResetTransform||this.options.alwaysMeasureLayout,a=this.projectionDelta&&!KC(this.projectionDelta),l=this.getTransformTemplate(),c=l?l(this.latestValues,""):void 0,d=c!==this.prevTransformTemplateValue;o&&this.instance&&(a||Li(this.latestValues)||d)&&(s(this.instance,c),this.shouldResetTransform=!1,this.scheduleRender())}measure(o=!0){const a=this.measurePageBox();let l=this.removeElementScroll(a);return o&&(l=this.removeTransform(l)),CB(l),{animationId:this.root.animationId,measuredBox:a,layoutBox:l,latestValues:{},source:this.id}}measurePageBox(){const{visualElement:o}=this.options;if(!o)return Yn();const a=o.measureViewportBox();if(!(this.scroll?.wasRoot||this.path.some(IB))){const{scroll:c}=this.root;c&&(So(a.x,c.offset.x),So(a.y,c.offset.y))}return a}removeElementScroll(o){const a=Yn();if(Jr(a,o),this.scroll?.wasRoot)return a;for(let l=0;l<this.path.length;l++){const c=this.path[l],{scroll:d,options:u}=c;c!==this.root&&d&&u.layoutScroll&&(d.wasRoot&&Jr(a,o),So(a.x,d.offset.x),So(a.y,d.offset.y))}return a}applyTransform(o,a=!1){const l=Yn();Jr(l,o);for(let c=0;c<this.path.length;c++){const d=this.path[c];!a&&d.options.layoutScroll&&d.scroll&&d!==d.root&&Lo(l,{x:-d.scroll.offset.x,y:-d.scroll.offset.y}),Li(d.latestValues)&&Lo(l,d.latestValues)}return Li(this.latestValues)&&Lo(l,this.latestValues),l}removeTransform(o){const a=Yn();Jr(a,o);for(let l=0;l<this.path.length;l++){const c=this.path[l];if(!c.instance||!Li(c.latestValues))continue;Bm(c.latestValues)&&c.updateSnapshot();const d=Yn(),u=c.measurePageBox();Jr(d,u),j1(a,c.latestValues,c.snapshot?c.snapshot.layoutBox:void 0,d)}return Li(this.latestValues)&&j1(a,this.latestValues),a}setTargetDelta(o){this.targetDelta=o,this.root.scheduleUpdateProjection(),this.isProjectionDirty=!0}setOptions(o){this.options={...this.options,...o,crossfade:o.crossfade!==void 0?o.crossfade:!0}}clearMeasurements(){this.scroll=void 0,this.layout=void 0,this.snapshot=void 0,this.prevTransformTemplateValue=void 0,this.targetDelta=void 0,this.target=void 0,this.isLayoutDirty=!1}forceRelativeParentToResolveTarget(){this.relativeParent&&this.relativeParent.resolvedRelativeTargetAt!==ir.timestamp&&this.relativeParent.resolveTargetDelta(!0)}resolveTargetDelta(o=!1){const a=this.getLead();this.isProjectionDirty||(this.isProjectionDirty=a.isProjectionDirty),this.isTransformDirty||(this.isTransformDirty=a.isTransformDirty),this.isSharedProjectionDirty||(this.isSharedProjectionDirty=a.isSharedProjectionDirty);const l=!!this.resumingFrom||this!==a;if(!(o||l&&this.isSharedProjectionDirty||this.isProjectionDirty||this.parent?.isProjectionDirty||this.attemptToResolveRelativeTarget||this.root.updateBlockedByResize))return;const{layout:d,layoutId:u}=this.options;if(!this.layout||!(d||u))return;this.resolvedRelativeTargetAt=ir.timestamp;const p=this.getClosestProjectingParent();p&&this.linkedParentVersion!==p.layoutVersion&&!p.options.layoutRoot&&this.removeRelativeTarget(),!this.targetDelta&&!this.relativeTarget&&(p&&p.layout?this.createRelativeTarget(p,this.layout.layoutBox,p.layout.layoutBox):this.removeRelativeTarget()),!(!this.relativeTarget&&!this.targetDelta)&&(this.target||(this.target=Yn(),this.targetWithTransforms=Yn()),this.relativeTarget&&this.relativeTargetOrigin&&this.relativeParent&&this.relativeParent.target?(this.forceRelativeParentToResolveTarget(),Y4(this.target,this.relativeTarget,this.relativeParent.target)):this.targetDelta?(this.resumingFrom?this.target=this.applyTransform(this.layout.layoutBox):Jr(this.target,this.layout.layoutBox),FC(this.target,this.targetDelta)):Jr(this.target,this.layout.layoutBox),this.attemptToResolveRelativeTarget&&(this.attemptToResolveRelativeTarget=!1,p&&!!p.resumingFrom==!!this.resumingFrom&&!p.options.layoutScroll&&p.target&&this.animationProgress!==1?this.createRelativeTarget(p,this.target,p.target):this.relativeParent=this.relativeTarget=void 0))}getClosestProjectingParent(){if(!(!this.parent||Bm(this.parent.latestValues)||$C(this.parent.latestValues)))return this.parent.isProjecting()?this.parent:this.parent.getClosestProjectingParent()}isProjecting(){return!!((this.relativeTarget||this.targetDelta||this.options.layoutRoot)&&this.layout)}createRelativeTarget(o,a,l){this.relativeParent=o,this.linkedParentVersion=o.layoutVersion,this.forceRelativeParentToResolveTarget(),this.relativeTarget=Yn(),this.relativeTargetOrigin=Yn(),gu(this.relativeTargetOrigin,a,l),Jr(this.relativeTarget,this.relativeTargetOrigin)}removeRelativeTarget(){this.relativeParent=this.relativeTarget=void 0}calcProjection(){const o=this.getLead(),a=!!this.resumingFrom||this!==o;let l=!0;if((this.isProjectionDirty||this.parent?.isProjectionDirty)&&(l=!1),a&&(this.isSharedProjectionDirty||this.isTransformDirty)&&(l=!1),this.resolvedRelativeTargetAt===ir.timestamp&&(l=!1),l)return;const{layout:c,layoutId:d}=this.options;if(this.isTreeAnimating=!!(this.parent&&this.parent.isTreeAnimating||this.currentAnimation||this.pendingAnimation),this.isTreeAnimating||(this.targetDelta=this.relativeTarget=void 0),!this.layout||!(c||d))return;Jr(this.layoutCorrected,this.layout.layoutBox);const u=this.treeScale.x,p=this.treeScale.y;P4(this.layoutCorrected,this.treeScale,this.path,a),o.layout&&!o.target&&(this.treeScale.x!==1||this.treeScale.y!==1)&&(o.target=o.layout.layoutBox,o.targetWithTransforms=Yn());const{target:h}=o;if(!h){this.prevProjectionDelta&&(this.createProjectionDeltas(),this.scheduleRender());return}!this.projectionDelta||!this.prevProjectionDelta?this.createProjectionDeltas():(N1(this.prevProjectionDelta.x,this.projectionDelta.x),N1(this.prevProjectionDelta.y,this.projectionDelta.y)),sl(this.projectionDelta,this.layoutCorrected,h,this.latestValues),(this.treeScale.x!==u||this.treeScale.y!==p||!U1(this.projectionDelta.x,this.prevProjectionDelta.x)||!U1(this.projectionDelta.y,this.prevProjectionDelta.y))&&(this.hasProjected=!0,this.scheduleRender(),this.notifyListeners("projectionUpdate",h))}hide(){this.isVisible=!1}show(){this.isVisible=!0}scheduleRender(o=!0){if(this.options.visualElement?.scheduleRender(),o){const a=this.getStack();a&&a.scheduleRender()}this.resumingFrom&&!this.resumingFrom.instance&&(this.resumingFrom=void 0)}createProjectionDeltas(){this.prevProjectionDelta=Eo(),this.projectionDelta=Eo(),this.projectionDeltaWithTransform=Eo()}setAnimationOrigin(o,a=!1){const l=this.snapshot,c=l?l.latestValues:{},d={...this.latestValues},u=Eo();(!this.relativeParent||!this.relativeParent.options.layoutRoot)&&(this.relativeTarget=this.relativeTargetOrigin=void 0),this.attemptToResolveRelativeTarget=!a;const p=Yn(),h=l?l.source:void 0,m=this.layout?this.layout.source:void 0,g=h!==m,I=this.getStack(),f=!I||I.members.length<=1,_=!!(g&&!f&&this.options.crossfade===!0&&!this.path.some(TB));this.animationProgress=0;let T;this.mixTargetDelta=M=>{const v=M/1e3;Y1(u.x,o.x,v),Y1(u.y,o.y,v),this.setTargetDelta(u),this.relativeTarget&&this.relativeTargetOrigin&&this.layout&&this.relativeParent&&this.relativeParent.layout&&(gu(p,this.layout.layoutBox,this.relativeParent.layout.layoutBox),AB(this.relativeTarget,this.relativeTargetOrigin,p,v),T&&tB(this.relativeTarget,T)&&(this.isProjectionDirty=!1),T||(T=Yn()),Jr(T,this.relativeTarget)),g&&(this.animationValues=d,sB(d,c,this.latestValues,v,_,f)),this.root.scheduleUpdateProjection(),this.scheduleRender(),this.animationProgress=v},this.mixTargetDelta(this.options.layoutRoot?1e3:0)}startAnimation(o){this.notifyListeners("animationStart"),this.currentAnimation?.stop(),this.resumingFrom?.currentAnimation?.stop(),this.pendingAnimation&&(li(this.pendingAnimation),this.pendingAnimation=void 0),this.pendingAnimation=yn.update(()=>{Zc.hasAnimatedSinceResize=!0,this.motionValue||(this.motionValue=Go(0)),this.currentAnimation=lB(this.motionValue,[0,1e3],{...o,velocity:0,isSync:!0,onUpdate:a=>{this.mixTargetDelta(a),o.onUpdate&&o.onUpdate(a)},onStop:()=>{},onComplete:()=>{o.onComplete&&o.onComplete(),this.completeAnimation()}}),this.resumingFrom&&(this.resumingFrom.currentAnimation=this.currentAnimation),this.pendingAnimation=void 0})}completeAnimation(){this.resumingFrom&&(this.resumingFrom.currentAnimation=void 0,this.resumingFrom.preserveOpacity=void 0);const o=this.getStack();o&&o.exitAnimationComplete(),this.resumingFrom=this.currentAnimation=this.animationValues=void 0,this.notifyListeners("animationComplete")}finishAnimation(){this.currentAnimation&&(this.mixTargetDelta&&this.mixTargetDelta(pB),this.currentAnimation.stop()),this.completeAnimation()}applyTransformsToTarget(){const o=this.getLead();let{targetWithTransforms:a,target:l,layout:c,latestValues:d}=o;if(!(!a||!l||!c)){if(this!==o&&this.layout&&c&&ZC(this.options.animationType,this.layout.layoutBox,c.layoutBox)){l=this.target||Yn();const u=gr(this.layout.layoutBox.x);l.x.min=o.target.x.min,l.x.max=l.x.min+u;const p=gr(this.layout.layoutBox.y);l.y.min=o.target.y.min,l.y.max=l.y.min+p}Jr(a,l),Lo(a,d),sl(this.projectionDeltaWithTransform,this.layoutCorrected,a,d)}}registerSharedNode(o,a){this.sharedNodes.has(o)||this.sharedNodes.set(o,new dB),this.sharedNodes.get(o).add(a);const c=a.options.initialPromotionConfig;a.promote({transition:c?c.transition:void 0,preserveFollowOpacity:c&&c.shouldPreserveFollowOpacity?c.shouldPreserveFollowOpacity(a):void 0})}isLead(){const o=this.getStack();return o?o.lead===this:!0}getLead(){const{layoutId:o}=this.options;return o?this.getStack()?.lead||this:this}getPrevLead(){const{layoutId:o}=this.options;return o?this.getStack()?.prevLead:void 0}getStack(){const{layoutId:o}=this.options;if(o)return this.root.sharedNodes.get(o)}promote({needsReset:o,transition:a,preserveFollowOpacity:l}={}){const c=this.getStack();c&&c.promote(this,l),o&&(this.projectionDelta=void 0,this.needsReset=!0),a&&this.setOptions({transition:a})}relegate(){const o=this.getStack();return o?o.relegate(this):!1}resetSkewAndRotation(){const{visualElement:o}=this.options;if(!o)return;let a=!1;const{latestValues:l}=o;if((l.z||l.rotate||l.rotateX||l.rotateY||l.rotateZ||l.skewX||l.skewY)&&(a=!0),!a)return;const c={};l.z&&Yp("z",o,c,this.animationValues);for(let d=0;d<Xp.length;d++)Yp(`rotate${Xp[d]}`,o,c,this.animationValues),Yp(`skew${Xp[d]}`,o,c,this.animationValues);o.render();for(const d in c)o.setStaticValue(d,c[d]),this.animationValues&&(this.animationValues[d]=c[d]);o.scheduleRender()}applyProjectionStyles(o,a){if(!this.instance||this.isSVG)return;if(!this.isVisible){o.visibility="hidden";return}const l=this.getTransformTemplate();if(this.needsReset){this.needsReset=!1,o.visibility="",o.opacity="",o.pointerEvents=Jc(a?.pointerEvents)||"",o.transform=l?l(this.latestValues,""):"none";return}const c=this.getLead();if(!this.projectionDelta||!this.layout||!c.target){this.options.layoutId&&(o.opacity=this.latestValues.opacity!==void 0?this.latestValues.opacity:1,o.pointerEvents=Jc(a?.pointerEvents)||""),this.hasProjected&&!Li(this.latestValues)&&(o.transform=l?l({},""):"none",this.hasProjected=!1);return}o.visibility="";const d=c.animationValues||c.latestValues;this.applyTransformsToTarget();let u=nB(this.projectionDeltaWithTransform,this.treeScale,d);l&&(u=l(d,u)),o.transform=u;const{x:p,y:h}=this.projectionDelta;o.transformOrigin=`${p.origin*100}% ${h.origin*100}% 0`,c.animationValues?o.opacity=c===this?d.opacity??this.latestValues.opacity??1:this.preserveOpacity?this.latestValues.opacity:d.opacityExit:o.opacity=c===this?d.opacity!==void 0?d.opacity:"":d.opacityExit!==void 0?d.opacityExit:0;for(const m in Vm){if(d[m]===void 0)continue;const{correct:g,applyTo:I,isCSSVariable:f}=Vm[m],_=u==="none"?d[m]:g(d[m],c);if(I){const T=I.length;for(let M=0;M<T;M++)o[I[M]]=_}else f?this.options.visualElement.renderState.vars[m]=_:o[m]=_}this.options.layoutId&&(o.pointerEvents=c===this?Jc(a?.pointerEvents)||"":"none")}clearSnapshot(){this.resumeFrom=this.snapshot=void 0}resetTree(){this.root.nodes.forEach(o=>o.currentAnimation?.stop()),this.root.nodes.forEach(q1),this.root.sharedNodes.clear()}}}function mB(e){e.updateLayout()}function fB(e){const t=e.resumeFrom?.snapshot||e.snapshot;if(e.isLead()&&e.layout&&t&&e.hasListeners("didUpdate")){const{layoutBox:n,measuredBox:r}=e.layout,{animationType:s}=e.options,i=t.source!==e.layout.source;s==="size"?zr(d=>{const u=i?t.measuredBox[d]:t.layoutBox[d],p=gr(u);u.min=n[d].min,u.max=u.min+p}):ZC(s,t.layoutBox,n)&&zr(d=>{const u=i?t.measuredBox[d]:t.layoutBox[d],p=gr(n[d]);u.max=u.min+p,e.relativeTarget&&!e.currentAnimation&&(e.isProjectionDirty=!0,e.relativeTarget[d].max=e.relativeTarget[d].min+p)});const o=Eo();sl(o,n,t.layoutBox);const a=Eo();i?sl(a,e.applyTransform(r,!0),t.measuredBox):sl(a,n,t.layoutBox);const l=!KC(o);let c=!1;if(!e.resumeFrom){const d=e.getClosestProjectingParent();if(d&&!d.resumeFrom){const{snapshot:u,layout:p}=d;if(u&&p){const h=Yn();gu(h,t.layoutBox,u.layoutBox);const m=Yn();gu(m,n,p.layoutBox),qC(h,m)||(c=!0),d.options.layoutRoot&&(e.relativeTarget=m,e.relativeTargetOrigin=h,e.relativeParent=d)}}}e.notifyListeners("didUpdate",{layout:n,snapshot:t,delta:a,layoutDelta:o,hasLayoutChanged:l,hasRelativeLayoutChanged:c})}else if(e.isLead()){const{onExitComplete:n}=e.options;n&&n()}e.options.transition=void 0}function gB(e){e.parent&&(e.isProjecting()||(e.isProjectionDirty=e.parent.isProjectionDirty),e.isSharedProjectionDirty||(e.isSharedProjectionDirty=!!(e.isProjectionDirty||e.parent.isProjectionDirty||e.parent.isSharedProjectionDirty)),e.isTransformDirty||(e.isTransformDirty=e.parent.isTransformDirty))}function _B(e){e.isProjectionDirty=e.isSharedProjectionDirty=e.isTransformDirty=!1}function yB(e){e.clearSnapshot()}function q1(e){e.clearMeasurements()}function Q1(e){e.isLayoutDirty=!1}function xB(e){const{visualElement:t}=e.options;t&&t.getProps().onBeforeLayoutMeasure&&t.notify("BeforeLayoutMeasure"),e.resetTransform()}function X1(e){e.finishAnimation(),e.targetDelta=e.relativeTarget=e.target=void 0,e.isProjectionDirty=!0}function bB(e){e.resolveTargetDelta()}function wB(e){e.calcProjection()}function vB(e){e.resetSkewAndRotation()}function MB(e){e.removeLeadSnapshot()}function Y1(e,t,n){e.translate=kn(t.translate,0,n),e.scale=kn(t.scale,1,n),e.origin=t.origin,e.originPoint=t.originPoint}function J1(e,t,n,r){e.min=kn(t.min,n.min,r),e.max=kn(t.max,n.max,r)}function AB(e,t,n,r){J1(e.x,t.x,n.x,r),J1(e.y,t.y,n.y,r)}function TB(e){return e.animationValues&&e.animationValues.opacityExit!==void 0}const PB={duration:.45,ease:[.4,0,.1,1]},Z1=e=>typeof navigator<"u"&&navigator.userAgent&&navigator.userAgent.toLowerCase().includes(e),ev=Z1("applewebkit/")&&!Z1("chrome/")?Math.round:Hr;function tv(e){e.min=ev(e.min),e.max=ev(e.max)}function CB(e){tv(e.x),tv(e.y)}function ZC(e,t,n){return e==="position"||e==="preserve-aspect"&&!X4(V1(t),V1(n),.2)}function IB(e){return e!==e.root&&e.scroll?.wasRoot}const kB=JC({attachResizeListener:(e,t)=>hl(e,"resize",t),measureScroll:()=>({x:document.documentElement.scrollLeft||document.body.scrollLeft,y:document.documentElement.scrollTop||document.body.scrollTop}),checkIsScrollRoot:()=>!0}),Jp={current:void 0},eI=JC({measureScroll:e=>({x:e.scrollLeft,y:e.scrollTop}),defaultParent:()=>{if(!Jp.current){const e=new kB({});e.mount(window),e.setOptions({layoutScroll:!0}),Jp.current=e}return Jp.current},resetTransform:(e,t)=>{e.style.transform=t!==void 0?t:"none"},checkIsScrollRoot:e=>window.getComputedStyle(e).position==="fixed"}),pg=Me.createContext({transformPagePoint:e=>e,isStatic:!1,reducedMotion:"never"});function nv(e,t){if(typeof e=="function")return e(t);e!=null&&(e.current=t)}function EB(...e){return t=>{let n=!1;const r=e.map(s=>{const i=nv(s,t);return!n&&typeof i=="function"&&(n=!0),i});if(n)return()=>{for(let s=0;s<r.length;s++){const i=r[s];typeof i=="function"?i():nv(e[s],null)}}}}function SB(...e){return Me.useCallback(EB(...e),e)}class LB extends Me.Component{getSnapshotBeforeUpdate(t){const n=this.props.childRef.current;if(n&&t.isPresent&&!this.props.isPresent){const r=n.offsetParent,s=MC(r)&&r.offsetWidth||0,i=this.props.sizeRef.current;i.height=n.offsetHeight||0,i.width=n.offsetWidth||0,i.top=n.offsetTop,i.left=n.offsetLeft,i.right=s-i.width-i.left}return null}componentDidUpdate(){}render(){return this.props.children}}function DB({children:e,isPresent:t,anchorX:n,root:r}){const s=Me.useId(),i=Me.useRef(null),o=Me.useRef({width:0,height:0,top:0,left:0,right:0}),{nonce:a}=Me.useContext(pg),l=e.props?.ref??e?.ref,c=SB(i,l);return Me.useInsertionEffect(()=>{const{width:d,height:u,top:p,left:h,right:m}=o.current;if(t||!i.current||!d||!u)return;const g=n==="left"?`left: ${h}`:`right: ${m}`;i.current.dataset.motionPopId=s;const I=document.createElement("style");a&&(I.nonce=a);const f=r??document.head;return f.appendChild(I),I.sheet&&I.sheet.insertRule(`
          [data-motion-pop-id="${s}"] {
            position: absolute !important;
            width: ${d}px !important;
            height: ${u}px !important;
            ${g}px !important;
            top: ${p}px !important;
          }
        `),()=>{f.contains(I)&&f.removeChild(I)}},[t]),w.jsx(LB,{isPresent:t,childRef:i,sizeRef:o,children:Me.cloneElement(e,{ref:c})})}const $B=({children:e,initial:t,isPresent:n,onExitComplete:r,custom:s,presenceAffectsLayout:i,mode:o,anchorX:a,root:l})=>{const c=Rf(FB),d=Me.useId();let u=!0,p=Me.useMemo(()=>(u=!1,{id:d,initial:t,isPresent:n,custom:s,onExitComplete:h=>{c.set(h,!0);for(const m of c.values())if(!m)return;r&&r()},register:h=>(c.set(h,!1),()=>c.delete(h))}),[n,c,r]);return i&&u&&(p={...p}),Me.useMemo(()=>{c.forEach((h,m)=>c.set(m,!1))},[n]),Me.useEffect(()=>{!n&&!c.size&&r&&r()},[n]),o==="popLayout"&&(e=w.jsx(DB,{isPresent:n,anchorX:a,root:l,children:e})),w.jsx(Uu.Provider,{value:p,children:e})};function FB(){return new Map}function tI(e=!0){const t=Me.useContext(Uu);if(t===null)return[!0,null];const{isPresent:n,onExitComplete:r,register:s}=t,i=Me.useId();Me.useEffect(()=>{if(e)return s(i)},[e]);const o=Me.useCallback(()=>e&&r&&r(i),[i,r,e]);return!n&&r?[!1,o]:[!0]}const Pc=e=>e.key||"";function rv(e){const t=[];return Me.Children.forEach(e,n=>{Me.isValidElement(n)&&t.push(n)}),t}const vr=({children:e,custom:t,initial:n=!0,onExitComplete:r,presenceAffectsLayout:s=!0,mode:i="sync",propagate:o=!1,anchorX:a="left",root:l})=>{const[c,d]=tI(o),u=Me.useMemo(()=>rv(e),[e]),p=o&&!c?[]:u.map(Pc),h=Me.useRef(!0),m=Me.useRef(u),g=Rf(()=>new Map),I=Me.useRef(new Set),[f,_]=Me.useState(u),[T,M]=Me.useState(u);L2(()=>{h.current=!1,m.current=u;for(let A=0;A<T.length;A++){const k=Pc(T[A]);p.includes(k)?(g.delete(k),I.current.delete(k)):g.get(k)!==!0&&g.set(k,!1)}},[T,p.length,p.join("-")]);const v=[];if(u!==f){let A=[...u];for(let k=0;k<T.length;k++){const F=T[k],L=Pc(F);p.includes(L)||(A.splice(k,0,F),v.push(F))}return i==="wait"&&v.length&&(A=v),M(rv(A)),_(u),null}const{forceRender:b}=Me.useContext(jf);return w.jsx(w.Fragment,{children:T.map(A=>{const k=Pc(A),F=o&&!c?!1:u===T||p.includes(k),L=()=>{if(I.current.has(k))return;if(I.current.add(k),g.has(k))g.set(k,!0);else return;let G=!0;g.forEach(j=>{j||(G=!1)}),G&&(b?.(),M(m.current),o&&d?.(),r&&r())};return w.jsx($B,{isPresent:F,initial:!h.current||n?void 0:!1,custom:t,presenceAffectsLayout:s,mode:i,root:l,onExitComplete:F?void 0:L,anchorX:a,children:A},k)})})},nI=Me.createContext({strict:!1}),sv={animation:["animate","variants","whileHover","whileTap","exit","whileInView","whileFocus","whileDrag"],exit:["exit"],drag:["drag","dragControls"],focus:["whileFocus"],hover:["whileHover","onHoverStart","onHoverEnd"],tap:["whileTap","onTap","onTapStart","onTapCancel"],pan:["onPan","onPanStart","onPanSessionStart","onPanEnd"],inView:["whileInView","onViewportEnter","onViewportLeave"],layout:["layout","layoutId"]};let iv=!1;function OB(){if(iv)return;const e={};for(const t in sv)e[t]={isEnabled:n=>sv[t].some(r=>!!n[r])};SC(e),iv=!0}function rI(){return OB(),v4()}function jB(e){const t=rI();for(const n in e)t[n]={...t[n],...e[n]};SC(t)}const RB=new Set(["animate","exit","variants","initial","style","values","variants","transition","transformTemplate","custom","inherit","onBeforeLayoutMeasure","onAnimationStart","onAnimationComplete","onUpdate","onDragStart","onDrag","onDragEnd","onMeasureDragConstraints","onDirectionLock","onDragTransitionEnd","_dragX","_dragY","onHoverStart","onHoverEnd","onViewportEnter","onViewportLeave","globalTapTarget","ignoreStrict","viewport"]);function _u(e){return e.startsWith("while")||e.startsWith("drag")&&e!=="draggable"||e.startsWith("layout")||e.startsWith("onTap")||e.startsWith("onPan")||e.startsWith("onLayout")||RB.has(e)}let sI=e=>!_u(e);function NB(e){typeof e=="function"&&(sI=t=>t.startsWith("on")?!_u(t):e(t))}try{NB(require("@emotion/is-prop-valid").default)}catch{}function zB(e,t,n){const r={};for(const s in e)s==="values"&&typeof e.values=="object"||(sI(s)||n===!0&&_u(s)||!t&&!_u(s)||e.draggable&&s.startsWith("onDrag"))&&(r[s]=e[s]);return r}const Ku=Me.createContext({});function BB(e,t){if(Hu(e)){const{initial:n,animate:r}=e;return{initial:n===!1||pl(n)?n:void 0,animate:pl(r)?r:void 0}}return e.inherit!==!1?t:{}}function GB(e){const{initial:t,animate:n}=BB(e,Me.useContext(Ku));return Me.useMemo(()=>({initial:t,animate:n}),[ov(t),ov(n)])}function ov(e){return Array.isArray(e)?e.join(" "):e}const hg=()=>({style:{},transform:{},transformOrigin:{},vars:{}});function iI(e,t,n){for(const r in t)!cr(t[r])&&!RC(r,n)&&(e[r]=t[r])}function VB({transformTemplate:e},t){return Me.useMemo(()=>{const n=hg();return ug(n,t,e),Object.assign({},n.vars,n.style)},[t])}function UB(e,t){const n=e.style||{},r={};return iI(r,n,e),Object.assign(r,VB(e,t)),r}function WB(e,t){const n={},r=UB(e,t);return e.drag&&e.dragListener!==!1&&(n.draggable=!1,r.userSelect=r.WebkitUserSelect=r.WebkitTouchCallout="none",r.touchAction=e.drag===!0?"none":`pan-${e.drag==="x"?"y":"x"}`),e.tabIndex===void 0&&(e.onTap||e.onTapStart||e.whileTap)&&(n.tabIndex=0),n.style=r,n}const oI=()=>({...hg(),attrs:{}});function HB(e,t,n,r){const s=Me.useMemo(()=>{const i=oI();return NC(i,t,BC(r),e.transformTemplate,e.style),{...i.attrs,style:{...i.style}}},[t]);if(e.style){const i={};iI(i,e.style,e),s.style={...i,...s.style}}return s}const KB=["animate","circle","defs","desc","ellipse","g","image","line","filter","marker","mask","metadata","path","pattern","polygon","polyline","rect","stop","switch","symbol","svg","text","tspan","use","view"];function mg(e){return typeof e!="string"||e.includes("-")?!1:!!(KB.indexOf(e)>-1||/[A-Z]/u.test(e))}function qB(e,t,n,{latestValues:r},s,i=!1,o){const l=(o??mg(e)?HB:WB)(t,r,s,e),c=zB(t,typeof e=="string",i),d=e!==Me.Fragment?{...c,...l,ref:n}:{},{children:u}=t,p=Me.useMemo(()=>cr(u)?u.get():u,[u]);return Me.createElement(e,{...d,children:p})}function QB({scrapeMotionValuesFromProps:e,createRenderState:t},n,r,s){return{latestValues:XB(n,r,s,e),renderState:t()}}function XB(e,t,n,r){const s={},i=r(e,{});for(const p in i)s[p]=Jc(i[p]);let{initial:o,animate:a}=e;const l=Hu(e),c=EC(e);t&&c&&!l&&e.inherit!==!1&&(o===void 0&&(o=t.initial),a===void 0&&(a=t.animate));let d=n?n.initial===!1:!1;d=d||o===!1;const u=d?a:o;if(u&&typeof u!="boolean"&&!Wu(u)){const p=Array.isArray(u)?u:[u];for(let h=0;h<p.length;h++){const m=rg(e,p[h]);if(m){const{transitionEnd:g,transition:I,...f}=m;for(const _ in f){let T=f[_];if(Array.isArray(T)){const M=d?T.length-1:0;T=T[M]}T!==null&&(s[_]=T)}for(const _ in g)s[_]=g[_]}}}return s}const aI=e=>(t,n)=>{const r=Me.useContext(Ku),s=Me.useContext(Uu),i=()=>QB(e,t,r,s);return n?i():Rf(i)},YB=aI({scrapeMotionValuesFromProps:dg,createRenderState:hg}),JB=aI({scrapeMotionValuesFromProps:GC,createRenderState:oI}),ZB=Symbol.for("motionComponentSymbol");function e5(e,t,n){const r=Me.useRef(n);Me.useInsertionEffect(()=>{r.current=n});const s=Me.useRef(null);return Me.useCallback(i=>{i&&e.onMount?.(i),t&&(i?t.mount(i):t.unmount());const o=r.current;if(typeof o=="function")if(i){const a=o(i);typeof a=="function"&&(s.current=a)}else s.current?(s.current(),s.current=null):o(i);else o&&(o.current=i)},[t])}const lI=Me.createContext({});function Wa(e){return e&&typeof e=="object"&&Object.prototype.hasOwnProperty.call(e,"current")}function t5(e,t,n,r,s,i){const{visualElement:o}=Me.useContext(Ku),a=Me.useContext(nI),l=Me.useContext(Uu),c=Me.useContext(pg).reducedMotion,d=Me.useRef(null);r=r||a.renderer,!d.current&&r&&(d.current=r(e,{visualState:t,parent:o,props:n,presenceContext:l,blockInitialAnimation:l?l.initial===!1:!1,reducedMotionConfig:c,isSVG:i}));const u=d.current,p=Me.useContext(lI);u&&!u.projection&&s&&(u.type==="html"||u.type==="svg")&&n5(d.current,n,s,p);const h=Me.useRef(!1);Me.useInsertionEffect(()=>{u&&h.current&&u.update(n,l)});const m=n[fC],g=Me.useRef(!!m&&!window.MotionHandoffIsComplete?.(m)&&window.MotionHasOptimisedAnimation?.(m));return L2(()=>{u&&(h.current=!0,window.MotionIsMounted=!0,u.updateFeatures(),u.scheduleRenderMicrotask(),g.current&&u.animationState&&u.animationState.animateChanges())}),Me.useEffect(()=>{u&&(!g.current&&u.animationState&&u.animationState.animateChanges(),g.current&&(queueMicrotask(()=>{window.MotionHandoffMarkAsComplete?.(m)}),g.current=!1),u.enteringChildren=void 0)}),u}function n5(e,t,n,r){const{layoutId:s,layout:i,drag:o,dragConstraints:a,layoutScroll:l,layoutRoot:c,layoutCrossfade:d}=t;e.projection=new n(e.latestValues,t["data-framer-portal-id"]?void 0:cI(e.parent)),e.projection.setOptions({layoutId:s,layout:i,alwaysMeasureLayout:!!o||a&&Wa(a),visualElement:e,animationType:typeof i=="string"?i:"both",initialPromotionConfig:r,crossfade:d,layoutScroll:l,layoutRoot:c})}function cI(e){if(e)return e.options.allowProjection!==!1?e.projection:cI(e.parent)}function Zp(e,{forwardMotionProps:t=!1,type:n}={},r,s){r&&jB(r);const i=n?n==="svg":mg(e),o=i?JB:YB;function a(c,d){let u;const p={...Me.useContext(pg),...c,layoutId:r5(c)},{isStatic:h}=p,m=GB(c),g=o(c,h);if(!h&&S2){s5();const I=i5(p);u=I.MeasureLayout,m.visualElement=t5(e,g,p,s,I.ProjectionNode,i)}return w.jsxs(Ku.Provider,{value:m,children:[u&&m.visualElement?w.jsx(u,{visualElement:m.visualElement,...p}):null,qB(e,c,e5(g,m.visualElement,d),g,h,t,i)]})}a.displayName=`motion.${typeof e=="string"?e:`create(${e.displayName??e.name??""})`}`;const l=Me.forwardRef(a);return l[ZB]=e,l}function r5({layoutId:e}){const t=Me.useContext(jf).id;return t&&e!==void 0?t+"-"+e:e}function s5(e,t){Me.useContext(nI).strict}function i5(e){const t=rI(),{drag:n,layout:r}=t;if(!n&&!r)return{};const s={...n,...r};return{MeasureLayout:n?.isEnabled(e)||r?.isEnabled(e)?s.MeasureLayout:void 0,ProjectionNode:s.ProjectionNode}}function o5(e,t){if(typeof Proxy>"u")return Zp;const n=new Map,r=(i,o)=>Zp(i,o,e,t),s=(i,o)=>r(i,o);return new Proxy(s,{get:(i,o)=>o==="create"?r:(n.has(o)||n.set(o,Zp(o,void 0,e,t)),n.get(o))})}const a5=(e,t)=>t.isSVG??mg(e)?new N4(t):new D4(t,{allowProjection:e!==Me.Fragment});class l5 extends pi{constructor(t){super(t),t.animationState||(t.animationState=U4(t))}updateAnimationControlsSubscription(){const{animate:t}=this.node.getProps();Wu(t)&&(this.unmountControls=t.subscribe(this.node))}mount(){this.updateAnimationControlsSubscription()}update(){const{animate:t}=this.node.getProps(),{animate:n}=this.node.prevProps||{};t!==n&&this.updateAnimationControlsSubscription()}unmount(){this.node.animationState.reset(),this.unmountControls?.()}}let c5=0;class u5 extends pi{constructor(){super(...arguments),this.id=c5++}update(){if(!this.node.presenceContext)return;const{isPresent:t,onExitComplete:n}=this.node.presenceContext,{isPresent:r}=this.node.prevPresenceContext||{};if(!this.node.animationState||t===r)return;const s=this.node.animationState.setActive("exit",!t);n&&!t&&s.then(()=>{n(this.id)})}mount(){const{register:t,onExitComplete:n}=this.node.presenceContext||{};n&&n(this.id),t&&(this.unmount=t(this.id))}unmount(){}}const d5={animation:{Feature:l5},exit:{Feature:u5}};function Pl(e){return{point:{x:e.pageX,y:e.pageY}}}const p5=e=>t=>ag(t)&&e(t,Pl(t));function il(e,t,n,r){return hl(e,t,p5(n),r)}const uI=({current:e})=>e?e.ownerDocument.defaultView:null,av=(e,t)=>Math.abs(e-t);function h5(e,t){const n=av(e.x,t.x),r=av(e.y,t.y);return Math.sqrt(n**2+r**2)}const lv=new Set(["auto","scroll"]);class dI{constructor(t,n,{transformPagePoint:r,contextWindow:s=window,dragSnapToOrigin:i=!1,distanceThreshold:o=3,element:a}={}){if(this.startEvent=null,this.lastMoveEvent=null,this.lastMoveEventInfo=null,this.handlers={},this.contextWindow=window,this.scrollPositions=new Map,this.removeScrollListeners=null,this.onElementScroll=h=>{this.handleScroll(h.target)},this.onWindowScroll=()=>{this.handleScroll(window)},this.updatePoint=()=>{if(!(this.lastMoveEvent&&this.lastMoveEventInfo))return;const h=th(this.lastMoveEventInfo,this.history),m=this.startEvent!==null,g=h5(h.offset,{x:0,y:0})>=this.distanceThreshold;if(!m&&!g)return;const{point:I}=h,{timestamp:f}=ir;this.history.push({...I,timestamp:f});const{onStart:_,onMove:T}=this.handlers;m||(_&&_(this.lastMoveEvent,h),this.startEvent=this.lastMoveEvent),T&&T(this.lastMoveEvent,h)},this.handlePointerMove=(h,m)=>{this.lastMoveEvent=h,this.lastMoveEventInfo=eh(m,this.transformPagePoint),yn.update(this.updatePoint,!0)},this.handlePointerUp=(h,m)=>{this.end();const{onEnd:g,onSessionEnd:I,resumeAnimation:f}=this.handlers;if((this.dragSnapToOrigin||!this.startEvent)&&f&&f(),!(this.lastMoveEvent&&this.lastMoveEventInfo))return;const _=th(h.type==="pointercancel"?this.lastMoveEventInfo:eh(m,this.transformPagePoint),this.history);this.startEvent&&g&&g(h,_),I&&I(h,_)},!ag(t))return;this.dragSnapToOrigin=i,this.handlers=n,this.transformPagePoint=r,this.distanceThreshold=o,this.contextWindow=s||window;const l=Pl(t),c=eh(l,this.transformPagePoint),{point:d}=c,{timestamp:u}=ir;this.history=[{...d,timestamp:u}];const{onSessionStart:p}=n;p&&p(t,th(c,this.history)),this.removeListeners=Ml(il(this.contextWindow,"pointermove",this.handlePointerMove),il(this.contextWindow,"pointerup",this.handlePointerUp),il(this.contextWindow,"pointercancel",this.handlePointerUp)),a&&this.startScrollTracking(a)}startScrollTracking(t){let n=t.parentElement;for(;n;){const r=getComputedStyle(n);(lv.has(r.overflowX)||lv.has(r.overflowY))&&this.scrollPositions.set(n,{x:n.scrollLeft,y:n.scrollTop}),n=n.parentElement}this.scrollPositions.set(window,{x:window.scrollX,y:window.scrollY}),window.addEventListener("scroll",this.onElementScroll,{capture:!0,passive:!0}),window.addEventListener("scroll",this.onWindowScroll,{passive:!0}),this.removeScrollListeners=()=>{window.removeEventListener("scroll",this.onElementScroll,{capture:!0}),window.removeEventListener("scroll",this.onWindowScroll)}}handleScroll(t){const n=this.scrollPositions.get(t);if(!n)return;const r=t===window,s=r?{x:window.scrollX,y:window.scrollY}:{x:t.scrollLeft,y:t.scrollTop},i={x:s.x-n.x,y:s.y-n.y};i.x===0&&i.y===0||(r?this.lastMoveEventInfo&&(this.lastMoveEventInfo.point.x+=i.x,this.lastMoveEventInfo.point.y+=i.y):this.history.length>0&&(this.history[0].x-=i.x,this.history[0].y-=i.y),this.scrollPositions.set(t,s),yn.update(this.updatePoint,!0))}updateHandlers(t){this.handlers=t}end(){this.removeListeners&&this.removeListeners(),this.removeScrollListeners&&this.removeScrollListeners(),this.scrollPositions.clear(),li(this.updatePoint)}}function eh(e,t){return t?{point:t(e.point)}:e}function cv(e,t){return{x:e.x-t.x,y:e.y-t.y}}function th({point:e},t){return{point:e,delta:cv(e,pI(t)),offset:cv(e,m5(t)),velocity:f5(t,.1)}}function m5(e){return e[0]}function pI(e){return e[e.length-1]}function f5(e,t){if(e.length<2)return{x:0,y:0};let n=e.length-1,r=null;const s=pI(e);for(;n>=0&&(r=e[n],!(s.timestamp-r.timestamp>Bs(t)));)n--;if(!r)return{x:0,y:0};const i=Wr(s.timestamp-r.timestamp);if(i===0)return{x:0,y:0};const o={x:(s.x-r.x)/i,y:(s.y-r.y)/i};return o.x===1/0&&(o.x=0),o.y===1/0&&(o.y=0),o}function g5(e,{min:t,max:n},r){return t!==void 0&&e<t?e=r?kn(t,e,r.min):Math.max(e,t):n!==void 0&&e>n&&(e=r?kn(n,e,r.max):Math.min(e,n)),e}function uv(e,t,n){return{min:t!==void 0?e.min+t:void 0,max:n!==void 0?e.max+n-(e.max-e.min):void 0}}function _5(e,{top:t,left:n,bottom:r,right:s}){return{x:uv(e.x,n,s),y:uv(e.y,t,r)}}function dv(e,t){let n=t.min-e.min,r=t.max-e.max;return t.max-t.min<e.max-e.min&&([n,r]=[r,n]),{min:n,max:r}}function y5(e,t){return{x:dv(e.x,t.x),y:dv(e.y,t.y)}}function x5(e,t){let n=.5;const r=gr(e),s=gr(t);return s>r?n=cl(t.min,t.max-r,e.min):r>s&&(n=cl(e.min,e.max-s,t.min)),ks(0,1,n)}function b5(e,t){const n={};return t.min!==void 0&&(n.min=t.min-e.min),t.max!==void 0&&(n.max=t.max-e.min),n}const Um=.35;function w5(e=Um){return e===!1?e=0:e===!0&&(e=Um),{x:pv(e,"left","right"),y:pv(e,"top","bottom")}}function pv(e,t,n){return{min:hv(e,t),max:hv(e,n)}}function hv(e,t){return typeof e=="number"?e:e[t]||0}const v5=new WeakMap;class M5{constructor(t){this.openDragLock=null,this.isDragging=!1,this.currentDirection=null,this.originPoint={x:0,y:0},this.constraints=!1,this.hasMutatedConstraints=!1,this.elastic=Yn(),this.latestPointerEvent=null,this.latestPanInfo=null,this.visualElement=t}start(t,{snapToCursor:n=!1,distanceThreshold:r}={}){const{presenceContext:s}=this.visualElement;if(s&&s.isPresent===!1)return;const i=u=>{n?(this.stopAnimation(),this.snapToCursor(Pl(u).point)):this.pauseAnimation()},o=(u,p)=>{this.stopAnimation();const{drag:h,dragPropagation:m,onDragStart:g}=this.getProps();if(h&&!m&&(this.openDragLock&&this.openDragLock(),this.openDragLock=u4(h),!this.openDragLock))return;this.latestPointerEvent=u,this.latestPanInfo=p,this.isDragging=!0,this.currentDirection=null,this.resolveConstraints(),this.visualElement.projection&&(this.visualElement.projection.isAnimationBlocked=!0,this.visualElement.projection.target=void 0),zr(f=>{let _=this.getAxisMotionValue(f).get()||0;if(Cs.test(_)){const{projection:T}=this.visualElement;if(T&&T.layout){const M=T.layout.layoutBox[f];M&&(_=gr(M)*(parseFloat(_)/100))}}this.originPoint[f]=_}),g&&yn.postRender(()=>g(u,p)),jm(this.visualElement,"transform");const{animationState:I}=this.visualElement;I&&I.setActive("whileDrag",!0)},a=(u,p)=>{this.latestPointerEvent=u,this.latestPanInfo=p;const{dragPropagation:h,dragDirectionLock:m,onDirectionLock:g,onDrag:I}=this.getProps();if(!h&&!this.openDragLock)return;const{offset:f}=p;if(m&&this.currentDirection===null){this.currentDirection=A5(f),this.currentDirection!==null&&g&&g(this.currentDirection);return}this.updateAxis("x",p.point,f),this.updateAxis("y",p.point,f),this.visualElement.render(),I&&I(u,p)},l=(u,p)=>{this.latestPointerEvent=u,this.latestPanInfo=p,this.stop(u,p),this.latestPointerEvent=null,this.latestPanInfo=null},c=()=>zr(u=>this.getAnimationState(u)==="paused"&&this.getAxisMotionValue(u).animation?.play()),{dragSnapToOrigin:d}=this.getProps();this.panSession=new dI(t,{onSessionStart:i,onStart:o,onMove:a,onSessionEnd:l,resumeAnimation:c},{transformPagePoint:this.visualElement.getTransformPagePoint(),dragSnapToOrigin:d,distanceThreshold:r,contextWindow:uI(this.visualElement),element:this.visualElement.current})}stop(t,n){const r=t||this.latestPointerEvent,s=n||this.latestPanInfo,i=this.isDragging;if(this.cancel(),!i||!s||!r)return;const{velocity:o}=s;this.startAnimation(o);const{onDragEnd:a}=this.getProps();a&&yn.postRender(()=>a(r,s))}cancel(){this.isDragging=!1;const{projection:t,animationState:n}=this.visualElement;t&&(t.isAnimationBlocked=!1),this.panSession&&this.panSession.end(),this.panSession=void 0;const{dragPropagation:r}=this.getProps();!r&&this.openDragLock&&(this.openDragLock(),this.openDragLock=null),n&&n.setActive("whileDrag",!1)}updateAxis(t,n,r){const{drag:s}=this.getProps();if(!r||!Cc(t,s,this.currentDirection))return;const i=this.getAxisMotionValue(t);let o=this.originPoint[t]+r[t];this.constraints&&this.constraints[t]&&(o=g5(o,this.constraints[t],this.elastic[t])),i.set(o)}resolveConstraints(){const{dragConstraints:t,dragElastic:n}=this.getProps(),r=this.visualElement.projection&&!this.visualElement.projection.layout?this.visualElement.projection.measure(!1):this.visualElement.projection?.layout,s=this.constraints;t&&Wa(t)?this.constraints||(this.constraints=this.resolveRefConstraints()):t&&r?this.constraints=_5(r.layoutBox,t):this.constraints=!1,this.elastic=w5(n),s!==this.constraints&&r&&this.constraints&&!this.hasMutatedConstraints&&zr(i=>{this.constraints!==!1&&this.getAxisMotionValue(i)&&(this.constraints[i]=b5(r.layoutBox[i],this.constraints[i]))})}resolveRefConstraints(){const{dragConstraints:t,onMeasureDragConstraints:n}=this.getProps();if(!t||!Wa(t))return!1;const r=t.current,{projection:s}=this.visualElement;if(!s||!s.layout)return!1;const i=C4(r,s.root,this.visualElement.getTransformPagePoint());let o=y5(s.layout.layoutBox,i);if(n){const a=n(A4(o));this.hasMutatedConstraints=!!a,a&&(o=DC(a))}return o}startAnimation(t){const{drag:n,dragMomentum:r,dragElastic:s,dragTransition:i,dragSnapToOrigin:o,onDragTransitionEnd:a}=this.getProps(),l=this.constraints||{},c=zr(d=>{if(!Cc(d,n,this.currentDirection))return;let u=l&&l[d]||{};o&&(u={min:0,max:0});const p=s?200:1e6,h=s?40:1e7,m={type:"inertia",velocity:r?t[d]:0,bounceStiffness:p,bounceDamping:h,timeConstant:750,restDelta:1,restSpeed:10,...i,...u};return this.startAxisValueAnimation(d,m)});return Promise.all(c).then(a)}startAxisValueAnimation(t,n){const r=this.getAxisMotionValue(t);return jm(this.visualElement,t),r.start(ng(t,r,0,n,this.visualElement,!1))}stopAnimation(){zr(t=>this.getAxisMotionValue(t).stop())}pauseAnimation(){zr(t=>this.getAxisMotionValue(t).animation?.pause())}getAnimationState(t){return this.getAxisMotionValue(t).animation?.state}getAxisMotionValue(t){const n=`_drag${t.toUpperCase()}`,r=this.visualElement.getProps(),s=r[n];return s||this.visualElement.getValue(t,(r.initial?r.initial[t]:void 0)||0)}snapToCursor(t){zr(n=>{const{drag:r}=this.getProps();if(!Cc(n,r,this.currentDirection))return;const{projection:s}=this.visualElement,i=this.getAxisMotionValue(n);if(s&&s.layout){const{min:o,max:a}=s.layout.layoutBox[n],l=i.get()||0;i.set(t[n]-kn(o,a,.5)+l)}})}scalePositionWithinConstraints(){if(!this.visualElement.current)return;const{drag:t,dragConstraints:n}=this.getProps(),{projection:r}=this.visualElement;if(!Wa(n)||!r||!this.constraints)return;this.stopAnimation();const s={x:0,y:0};zr(o=>{const a=this.getAxisMotionValue(o);if(a&&this.constraints!==!1){const l=a.get();s[o]=x5({min:l,max:l},this.constraints[o])}});const{transformTemplate:i}=this.visualElement.getProps();this.visualElement.current.style.transform=i?i({},""):"none",r.root&&r.root.updateScroll(),r.updateLayout(),this.resolveConstraints(),zr(o=>{if(!Cc(o,t,null))return;const a=this.getAxisMotionValue(o),{min:l,max:c}=this.constraints[o];a.set(kn(l,c,s[o]))})}addListeners(){if(!this.visualElement.current)return;v5.set(this.visualElement,this);const t=this.visualElement.current,n=il(t,"pointerdown",l=>{const{drag:c,dragListener:d=!0}=this.getProps();c&&d&&!CC(l.target)&&this.start(l)}),r=()=>{const{dragConstraints:l}=this.getProps();Wa(l)&&l.current&&(this.constraints=this.resolveRefConstraints())},{projection:s}=this.visualElement,i=s.addEventListener("measure",r);s&&!s.layout&&(s.root&&s.root.updateScroll(),s.updateLayout()),yn.read(r);const o=hl(window,"resize",()=>this.scalePositionWithinConstraints()),a=s.addEventListener("didUpdate",(({delta:l,hasLayoutChanged:c})=>{this.isDragging&&c&&(zr(d=>{const u=this.getAxisMotionValue(d);u&&(this.originPoint[d]+=l[d].translate,u.set(u.get()+l[d].translate))}),this.visualElement.render())}));return()=>{o(),n(),i(),a&&a()}}getProps(){const t=this.visualElement.getProps(),{drag:n=!1,dragDirectionLock:r=!1,dragPropagation:s=!1,dragConstraints:i=!1,dragElastic:o=Um,dragMomentum:a=!0}=t;return{...t,drag:n,dragDirectionLock:r,dragPropagation:s,dragConstraints:i,dragElastic:o,dragMomentum:a}}}function Cc(e,t,n){return(t===!0||t===e)&&(n===null||n===e)}function A5(e,t=10){let n=null;return Math.abs(e.y)>t?n="y":Math.abs(e.x)>t&&(n="x"),n}class T5 extends pi{constructor(t){super(t),this.removeGroupControls=Hr,this.removeListeners=Hr,this.controls=new M5(t)}mount(){const{dragControls:t}=this.node.getProps();t&&(this.removeGroupControls=t.subscribe(this.controls)),this.removeListeners=this.controls.addListeners()||Hr}update(){const{dragControls:t}=this.node.getProps(),{dragControls:n}=this.node.prevProps||{};t!==n&&(this.removeGroupControls(),t&&(this.removeGroupControls=t.subscribe(this.controls)))}unmount(){this.removeGroupControls(),this.removeListeners()}}const mv=e=>(t,n)=>{e&&yn.postRender(()=>e(t,n))};class P5 extends pi{constructor(){super(...arguments),this.removePointerDownListener=Hr}onPointerDown(t){this.session=new dI(t,this.createPanHandlers(),{transformPagePoint:this.node.getTransformPagePoint(),contextWindow:uI(this.node)})}createPanHandlers(){const{onPanSessionStart:t,onPanStart:n,onPan:r,onPanEnd:s}=this.node.getProps();return{onSessionStart:mv(t),onStart:mv(n),onMove:r,onEnd:(i,o)=>{delete this.session,s&&yn.postRender(()=>s(i,o))}}}mount(){this.removePointerDownListener=il(this.node.current,"pointerdown",t=>this.onPointerDown(t))}update(){this.session&&this.session.updateHandlers(this.createPanHandlers())}unmount(){this.removePointerDownListener(),this.session&&this.session.end()}}let nh=!1;class C5 extends Me.Component{componentDidMount(){const{visualElement:t,layoutGroup:n,switchLayoutGroup:r,layoutId:s}=this.props,{projection:i}=t;i&&(n.group&&n.group.add(i),r&&r.register&&s&&r.register(i),nh&&i.root.didUpdate(),i.addEventListener("animationComplete",()=>{this.safeToRemove()}),i.setOptions({...i.options,onExitComplete:()=>this.safeToRemove()})),Zc.hasEverUpdated=!0}getSnapshotBeforeUpdate(t){const{layoutDependency:n,visualElement:r,drag:s,isPresent:i}=this.props,{projection:o}=r;return o&&(o.isPresent=i,nh=!0,s||t.layoutDependency!==n||n===void 0||t.isPresent!==i?o.willUpdate():this.safeToRemove(),t.isPresent!==i&&(i?o.promote():o.relegate()||yn.postRender(()=>{const a=o.getStack();(!a||!a.members.length)&&this.safeToRemove()}))),null}componentDidUpdate(){const{projection:t}=this.props.visualElement;t&&(t.root.didUpdate(),og.postRender(()=>{!t.currentAnimation&&t.isLead()&&this.safeToRemove()}))}componentWillUnmount(){const{visualElement:t,layoutGroup:n,switchLayoutGroup:r}=this.props,{projection:s}=t;nh=!0,s&&(s.scheduleCheckAfterUnmount(),n&&n.group&&n.group.remove(s),r&&r.deregister&&r.deregister(s))}safeToRemove(){const{safeToRemove:t}=this.props;t&&t()}render(){return null}}function hI(e){const[t,n]=tI(),r=Me.useContext(jf);return w.jsx(C5,{...e,layoutGroup:r,switchLayoutGroup:Me.useContext(lI),isPresent:t,safeToRemove:n})}const I5={pan:{Feature:P5},drag:{Feature:T5,ProjectionNode:eI,MeasureLayout:hI}};function fv(e,t,n){const{props:r}=e;e.animationState&&r.whileHover&&e.animationState.setActive("whileHover",n==="Start");const s="onHover"+n,i=r[s];i&&yn.postRender(()=>i(t,Pl(t)))}class k5 extends pi{mount(){const{current:t}=this.node;t&&(this.unmount=d4(t,(n,r)=>(fv(this.node,r,"Start"),s=>fv(this.node,s,"End"))))}unmount(){}}class E5 extends pi{constructor(){super(...arguments),this.isActive=!1}onFocus(){let t=!1;try{t=this.node.current.matches(":focus-visible")}catch{t=!0}!t||!this.node.animationState||(this.node.animationState.setActive("whileFocus",!0),this.isActive=!0)}onBlur(){!this.isActive||!this.node.animationState||(this.node.animationState.setActive("whileFocus",!1),this.isActive=!1)}mount(){this.unmount=Ml(hl(this.node.current,"focus",()=>this.onFocus()),hl(this.node.current,"blur",()=>this.onBlur()))}unmount(){}}function gv(e,t,n){const{props:r}=e;if(e.current instanceof HTMLButtonElement&&e.current.disabled)return;e.animationState&&r.whileTap&&e.animationState.setActive("whileTap",n==="Start");const s="onTap"+(n==="End"?"":n),i=r[s];i&&yn.postRender(()=>i(t,Pl(t)))}class S5 extends pi{mount(){const{current:t}=this.node;t&&(this.unmount=m4(t,(n,r)=>(gv(this.node,r,"Start"),(s,{success:i})=>gv(this.node,s,i?"End":"Cancel")),{useGlobalTarget:this.node.props.globalTapTarget}))}unmount(){}}const Wm=new WeakMap,rh=new WeakMap,L5=e=>{const t=Wm.get(e.target);t&&t(e)},D5=e=>{e.forEach(L5)};function $5({root:e,...t}){const n=e||document;rh.has(n)||rh.set(n,{});const r=rh.get(n),s=JSON.stringify(t);return r[s]||(r[s]=new IntersectionObserver(D5,{root:e,...t})),r[s]}function F5(e,t,n){const r=$5(t);return Wm.set(e,n),r.observe(e),()=>{Wm.delete(e),r.unobserve(e)}}const O5={some:0,all:1};class j5 extends pi{constructor(){super(...arguments),this.hasEnteredView=!1,this.isInView=!1}startObserver(){this.unmount();const{viewport:t={}}=this.node.getProps(),{root:n,margin:r,amount:s="some",once:i}=t,o={root:n?n.current:void 0,rootMargin:r,threshold:typeof s=="number"?s:O5[s]},a=l=>{const{isIntersecting:c}=l;if(this.isInView===c||(this.isInView=c,i&&!c&&this.hasEnteredView))return;c&&(this.hasEnteredView=!0),this.node.animationState&&this.node.animationState.setActive("whileInView",c);const{onViewportEnter:d,onViewportLeave:u}=this.node.getProps(),p=c?d:u;p&&p(l)};return F5(this.node.current,o,a)}mount(){this.startObserver()}update(){if(typeof IntersectionObserver>"u")return;const{props:t,prevProps:n}=this.node;["amount","margin","root"].some(R5(t,n))&&this.startObserver()}unmount(){}}function R5({viewport:e={}},{viewport:t={}}={}){return n=>e[n]!==t[n]}const N5={inView:{Feature:j5},tap:{Feature:S5},focus:{Feature:E5},hover:{Feature:k5}},z5={layout:{ProjectionNode:eI,MeasureLayout:hI}},B5={...d5,...N5,...I5,...z5},lt=o5(B5,a5),eu=new Map,Di=[],G5=(e,t,n)=>{if(t&&typeof t.init=="function"&&typeof t.createInferenceSessionHandler=="function"){const r=eu.get(e);if(r===void 0)eu.set(e,{backend:t,priority:n});else{if(r.priority>n)return;if(r.priority===n&&r.backend!==t)throw new Error(`cannot register backend "${e}" using priority ${n}`)}if(n>=0){const s=Di.indexOf(e);s!==-1&&Di.splice(s,1);for(let i=0;i<Di.length;i++)if(eu.get(Di[i]).priority<=n){Di.splice(i,0,e);return}Di.push(e)}return}throw new TypeError("not a valid backend")},V5=async e=>{const t=eu.get(e);if(!t)return"backend not found.";if(t.initialized)return t.backend;if(t.aborted)return t.error;{const n=!!t.initPromise;try{return n||(t.initPromise=t.backend.init(e)),await t.initPromise,t.initialized=!0,t.backend}catch(r){return n||(t.error=`${r}`,t.aborted=!0),t.error}finally{delete t.initPromise}}},U5=async e=>{const t=e.executionProviders||[],n=t.map(l=>typeof l=="string"?l:l.name),r=n.length===0?Di:n;let s;const i=[],o=new Set;for(const l of r){const c=await V5(l);typeof c=="string"?i.push({name:l,err:c}):(s||(s=c),s===c&&o.add(l))}if(!s)throw new Error(`no available backend found. ERR: ${i.map(l=>`[${l.name}] ${l.err}`).join(", ")}`);for(const{name:l,err:c}of i)n.includes(l)&&console.warn(`removing requested execution provider "${l}" from session options because it is not available: ${c}`);const a=t.filter(l=>o.has(typeof l=="string"?l:l.name));return[s,new Proxy(e,{get:(l,c)=>c==="executionProviders"?a:Reflect.get(l,c)})]},W5="1.21.0";let _v="warning";const cs={wasm:{},webgl:{},webgpu:{},versions:{common:W5},set logLevel(e){if(e!==void 0){if(typeof e!="string"||["verbose","info","warning","error","fatal"].indexOf(e)===-1)throw new Error(`Unsupported logging level: ${e}`);_v=e}},get logLevel(){return _v}};Object.defineProperty(cs,"logLevel",{enumerable:!0});const H5=cs,K5=(e,t)=>{const n=typeof document<"u"?document.createElement("canvas"):new OffscreenCanvas(1,1);n.width=e.dims[3],n.height=e.dims[2];const r=n.getContext("2d");if(r!=null){let s,i;t?.tensorLayout!==void 0&&t.tensorLayout==="NHWC"?(s=e.dims[2],i=e.dims[3]):(s=e.dims[3],i=e.dims[2]);const o=t?.format!==void 0?t.format:"RGB",a=t?.norm;let l,c;a===void 0||a.mean===void 0?l=[255,255,255,255]:typeof a.mean=="number"?l=[a.mean,a.mean,a.mean,a.mean]:(l=[a.mean[0],a.mean[1],a.mean[2],0],a.mean[3]!==void 0&&(l[3]=a.mean[3])),a===void 0||a.bias===void 0?c=[0,0,0,0]:typeof a.bias=="number"?c=[a.bias,a.bias,a.bias,a.bias]:(c=[a.bias[0],a.bias[1],a.bias[2],0],a.bias[3]!==void 0&&(c[3]=a.bias[3]));const d=i*s;let u=0,p=d,h=d*2,m=-1;o==="RGBA"?(u=0,p=d,h=d*2,m=d*3):o==="RGB"?(u=0,p=d,h=d*2):o==="RBG"&&(u=0,h=d,p=d*2);for(let g=0;g<i;g++)for(let I=0;I<s;I++){const f=(e.data[u++]-c[0])*l[0],_=(e.data[p++]-c[1])*l[1],T=(e.data[h++]-c[2])*l[2],M=m===-1?255:(e.data[m++]-c[3])*l[3];r.fillStyle="rgba("+f+","+_+","+T+","+M+")",r.fillRect(I,g,1,1)}if("toDataURL"in n)return n.toDataURL();throw new Error("toDataURL is not supported")}else throw new Error("Can not access image data")},q5=(e,t)=>{const n=typeof document<"u"?document.createElement("canvas").getContext("2d"):new OffscreenCanvas(1,1).getContext("2d");let r;if(n!=null){let s,i,o;t?.tensorLayout!==void 0&&t.tensorLayout==="NHWC"?(s=e.dims[2],i=e.dims[1],o=e.dims[3]):(s=e.dims[3],i=e.dims[2],o=e.dims[1]);const a=t!==void 0&&t.format!==void 0?t.format:"RGB",l=t?.norm;let c,d;l===void 0||l.mean===void 0?c=[255,255,255,255]:typeof l.mean=="number"?c=[l.mean,l.mean,l.mean,l.mean]:(c=[l.mean[0],l.mean[1],l.mean[2],255],l.mean[3]!==void 0&&(c[3]=l.mean[3])),l===void 0||l.bias===void 0?d=[0,0,0,0]:typeof l.bias=="number"?d=[l.bias,l.bias,l.bias,l.bias]:(d=[l.bias[0],l.bias[1],l.bias[2],0],l.bias[3]!==void 0&&(d[3]=l.bias[3]));const u=i*s;if(t!==void 0&&(t.format!==void 0&&o===4&&t.format!=="RGBA"||o===3&&t.format!=="RGB"&&t.format!=="BGR"))throw new Error("Tensor format doesn't match input tensor dims");const p=4;let h=0,m=1,g=2,I=3,f=0,_=u,T=u*2,M=-1;a==="RGBA"?(f=0,_=u,T=u*2,M=u*3):a==="RGB"?(f=0,_=u,T=u*2):a==="RBG"&&(f=0,T=u,_=u*2),r=n.createImageData(s,i);for(let v=0;v<i*s;h+=p,m+=p,g+=p,I+=p,v++)r.data[h]=(e.data[f++]-d[0])*c[0],r.data[m]=(e.data[_++]-d[1])*c[1],r.data[g]=(e.data[T++]-d[2])*c[2],r.data[I]=M===-1?255:(e.data[M++]-d[3])*c[3]}else throw new Error("Can not access image data");return r},sh=(e,t)=>{if(e===void 0)throw new Error("Image buffer must be defined");if(t.height===void 0||t.width===void 0)throw new Error("Image height and width must be defined");if(t.tensorLayout==="NHWC")throw new Error("NHWC Tensor layout is not supported yet");const{height:n,width:r}=t,s=t.norm??{mean:255,bias:0};let i,o;typeof s.mean=="number"?i=[s.mean,s.mean,s.mean,s.mean]:i=[s.mean[0],s.mean[1],s.mean[2],s.mean[3]??255],typeof s.bias=="number"?o=[s.bias,s.bias,s.bias,s.bias]:o=[s.bias[0],s.bias[1],s.bias[2],s.bias[3]??0];const a=t.format!==void 0?t.format:"RGBA",l=t.tensorFormat!==void 0&&t.tensorFormat!==void 0?t.tensorFormat:"RGB",c=n*r,d=l==="RGBA"?new Float32Array(c*4):new Float32Array(c*3);let u=4,p=0,h=1,m=2,g=3,I=0,f=c,_=c*2,T=-1;a==="RGB"&&(u=3,p=0,h=1,m=2,g=-1),l==="RGBA"?T=c*3:l==="RBG"?(I=0,_=c,f=c*2):l==="BGR"&&(_=0,f=c,I=c*2);for(let v=0;v<c;v++,p+=u,m+=u,h+=u,g+=u)d[I++]=(e[p]+o[0])/i[0],d[f++]=(e[h]+o[1])/i[1],d[_++]=(e[m]+o[2])/i[2],T!==-1&&g!==-1&&(d[T++]=(e[g]+o[3])/i[3]);return l==="RGBA"?new Vr("float32",d,[1,4,n,r]):new Vr("float32",d,[1,3,n,r])},Q5=async(e,t)=>{const n=typeof HTMLImageElement<"u"&&e instanceof HTMLImageElement,r=typeof ImageData<"u"&&e instanceof ImageData,s=typeof ImageBitmap<"u"&&e instanceof ImageBitmap,i=typeof e=="string";let o,a=t??{};const l=()=>{if(typeof document<"u")return document.createElement("canvas");if(typeof OffscreenCanvas<"u")return new OffscreenCanvas(1,1);throw new Error("Canvas is not supported")},c=d=>typeof HTMLCanvasElement<"u"&&d instanceof HTMLCanvasElement||d instanceof OffscreenCanvas?d.getContext("2d"):null;if(n){const d=l();d.width=e.width,d.height=e.height;const u=c(d);if(u!=null){let p=e.height,h=e.width;if(t!==void 0&&t.resizedHeight!==void 0&&t.resizedWidth!==void 0&&(p=t.resizedHeight,h=t.resizedWidth),t!==void 0){if(a=t,t.tensorFormat!==void 0)throw new Error("Image input config format must be RGBA for HTMLImageElement");a.tensorFormat="RGBA",a.height=p,a.width=h}else a.tensorFormat="RGBA",a.height=p,a.width=h;u.drawImage(e,0,0),o=u.getImageData(0,0,h,p).data}else throw new Error("Can not access image data")}else if(r){let d,u;if(t!==void 0&&t.resizedWidth!==void 0&&t.resizedHeight!==void 0?(d=t.resizedHeight,u=t.resizedWidth):(d=e.height,u=e.width),t!==void 0&&(a=t),a.format="RGBA",a.height=d,a.width=u,t!==void 0){const p=l();p.width=u,p.height=d;const h=c(p);if(h!=null)h.putImageData(e,0,0),o=h.getImageData(0,0,u,d).data;else throw new Error("Can not access image data")}else o=e.data}else if(s){if(t===void 0)throw new Error("Please provide image config with format for Imagebitmap");const d=l();d.width=e.width,d.height=e.height;const u=c(d);if(u!=null){const p=e.height,h=e.width;return u.drawImage(e,0,0,h,p),o=u.getImageData(0,0,h,p).data,a.height=p,a.width=h,sh(o,a)}else throw new Error("Can not access image data")}else{if(i)return new Promise((d,u)=>{const p=l(),h=c(p);if(!e||!h)return u();const m=new Image;m.crossOrigin="Anonymous",m.src=e,m.onload=()=>{p.width=m.width,p.height=m.height,h.drawImage(m,0,0,p.width,p.height);const g=h.getImageData(0,0,p.width,p.height);a.height=p.height,a.width=p.width,d(sh(g.data,a))}});throw new Error("Input data provided is not supported - aborted tensor creation")}if(o!==void 0)return sh(o,a);throw new Error("Input data provided is not supported - aborted tensor creation")},X5=(e,t)=>{const{width:n,height:r,download:s,dispose:i}=t,o=[1,r,n,4];return new Vr({location:"texture",type:"float32",texture:e,dims:o,download:s,dispose:i})},Y5=(e,t)=>{const{dataType:n,dims:r,download:s,dispose:i}=t;return new Vr({location:"gpu-buffer",type:n??"float32",gpuBuffer:e,dims:r,download:s,dispose:i})},J5=(e,t)=>{const{dataType:n,dims:r,download:s,dispose:i}=t;return new Vr({location:"ml-tensor",type:n??"float32",mlTensor:e,dims:r,download:s,dispose:i})},Z5=(e,t,n)=>new Vr({location:"cpu-pinned",type:e,data:t,dims:n??[t.length]}),Do=new Map([["float32",Float32Array],["uint8",Uint8Array],["int8",Int8Array],["uint16",Uint16Array],["int16",Int16Array],["int32",Int32Array],["bool",Uint8Array],["float64",Float64Array],["uint32",Uint32Array],["int4",Uint8Array],["uint4",Uint8Array]]),tu=new Map([[Float32Array,"float32"],[Uint8Array,"uint8"],[Int8Array,"int8"],[Uint16Array,"uint16"],[Int16Array,"int16"],[Int32Array,"int32"],[Float64Array,"float64"],[Uint32Array,"uint32"]]);let yv=!1;const eG=()=>{if(!yv){yv=!0;const e=typeof BigInt64Array<"u"&&BigInt64Array.from,t=typeof BigUint64Array<"u"&&BigUint64Array.from,n=globalThis.Float16Array,r=typeof n<"u"&&n.from;e&&(Do.set("int64",BigInt64Array),tu.set(BigInt64Array,"int64")),t&&(Do.set("uint64",BigUint64Array),tu.set(BigUint64Array,"uint64")),r?(Do.set("float16",n),tu.set(n,"float16")):Do.set("float16",Uint16Array)}},tG=e=>{let t=1;for(let n=0;n<e.length;n++){const r=e[n];if(typeof r!="number"||!Number.isSafeInteger(r))throw new TypeError(`dims[${n}] must be an integer, got: ${r}`);if(r<0)throw new RangeError(`dims[${n}] must be a non-negative integer, got: ${r}`);t*=r}return t},nG=(e,t)=>{switch(e.location){case"cpu":return new Vr(e.type,e.data,t);case"cpu-pinned":return new Vr({location:"cpu-pinned",data:e.data,type:e.type,dims:t});case"texture":return new Vr({location:"texture",texture:e.texture,type:e.type,dims:t});case"gpu-buffer":return new Vr({location:"gpu-buffer",gpuBuffer:e.gpuBuffer,type:e.type,dims:t});case"ml-tensor":return new Vr({location:"ml-tensor",mlTensor:e.mlTensor,type:e.type,dims:t});default:throw new Error(`tensorReshape: tensor location ${e.location} is not supported`)}};let Vr=class{constructor(t,n,r){eG();let s,i;if(typeof t=="object"&&"location"in t)switch(this.dataLocation=t.location,s=t.type,i=t.dims,t.location){case"cpu-pinned":{const a=Do.get(s);if(!a)throw new TypeError(`unsupported type "${s}" to create tensor from pinned buffer`);if(!(t.data instanceof a))throw new TypeError(`buffer should be of type ${a.name}`);this.cpuData=t.data;break}case"texture":{if(s!=="float32")throw new TypeError(`unsupported type "${s}" to create tensor from texture`);this.gpuTextureData=t.texture,this.downloader=t.download,this.disposer=t.dispose;break}case"gpu-buffer":{if(s!=="float32"&&s!=="float16"&&s!=="int32"&&s!=="int64"&&s!=="uint32"&&s!=="uint8"&&s!=="bool"&&s!=="uint4"&&s!=="int4")throw new TypeError(`unsupported type "${s}" to create tensor from gpu buffer`);this.gpuBufferData=t.gpuBuffer,this.downloader=t.download,this.disposer=t.dispose;break}case"ml-tensor":{if(s!=="float32"&&s!=="float16"&&s!=="int32"&&s!=="int64"&&s!=="uint32"&&s!=="uint64"&&s!=="int8"&&s!=="uint8"&&s!=="bool"&&s!=="uint4"&&s!=="int4")throw new TypeError(`unsupported type "${s}" to create tensor from MLTensor`);this.mlTensorData=t.mlTensor,this.downloader=t.download,this.disposer=t.dispose;break}default:throw new Error(`Tensor constructor: unsupported location '${this.dataLocation}'`)}else{let a,l;if(typeof t=="string")if(s=t,l=r,t==="string"){if(!Array.isArray(n))throw new TypeError("A string tensor's data must be a string array.");a=n}else{const c=Do.get(t);if(c===void 0)throw new TypeError(`Unsupported tensor type: ${t}.`);if(Array.isArray(n)){if(t==="float16"&&c===Uint16Array||t==="uint4"||t==="int4")throw new TypeError(`Creating a ${t} tensor from number array is not supported. Please use ${c.name} as data.`);t==="uint64"||t==="int64"?a=c.from(n,BigInt):a=c.from(n)}else if(n instanceof c)a=n;else if(n instanceof Uint8ClampedArray)if(t==="uint8")a=Uint8Array.from(n);else throw new TypeError("A Uint8ClampedArray tensor's data must be type of uint8");else if(t==="float16"&&n instanceof Uint16Array&&c!==Uint16Array)a=new globalThis.Float16Array(n.buffer,n.byteOffset,n.length);else throw new TypeError(`A ${s} tensor's data must be type of ${c}`)}else if(l=n,Array.isArray(t)){if(t.length===0)throw new TypeError("Tensor type cannot be inferred from an empty array.");const c=typeof t[0];if(c==="string")s="string",a=t;else if(c==="boolean")s="bool",a=Uint8Array.from(t);else throw new TypeError(`Invalid element type of data array: ${c}.`)}else if(t instanceof Uint8ClampedArray)s="uint8",a=Uint8Array.from(t);else{const c=tu.get(t.constructor);if(c===void 0)throw new TypeError(`Unsupported type for tensor data: ${t.constructor}.`);s=c,a=t}if(l===void 0)l=[a.length];else if(!Array.isArray(l))throw new TypeError("A tensor's dims must be a number array");i=l,this.cpuData=a,this.dataLocation="cpu"}const o=tG(i);if(this.cpuData&&o!==this.cpuData.length&&!((s==="uint4"||s==="int4")&&Math.ceil(o/2)===this.cpuData.length))throw new Error(`Tensor's size(${o}) does not match data length(${this.cpuData.length}).`);this.type=s,this.dims=i,this.size=o}static async fromImage(t,n){return Q5(t,n)}static fromTexture(t,n){return X5(t,n)}static fromGpuBuffer(t,n){return Y5(t,n)}static fromMLTensor(t,n){return J5(t,n)}static fromPinnedBuffer(t,n,r){return Z5(t,n,r)}toDataURL(t){return K5(this,t)}toImageData(t){return q5(this,t)}get data(){if(this.ensureValid(),!this.cpuData)throw new Error("The data is not on CPU. Use `getData()` to download GPU data to CPU, or use `texture` or `gpuBuffer` property to access the GPU data directly.");return this.cpuData}get location(){return this.dataLocation}get texture(){if(this.ensureValid(),!this.gpuTextureData)throw new Error("The data is not stored as a WebGL texture.");return this.gpuTextureData}get gpuBuffer(){if(this.ensureValid(),!this.gpuBufferData)throw new Error("The data is not stored as a WebGPU buffer.");return this.gpuBufferData}get mlTensor(){if(this.ensureValid(),!this.mlTensorData)throw new Error("The data is not stored as a WebNN MLTensor.");return this.mlTensorData}async getData(t){switch(this.ensureValid(),this.dataLocation){case"cpu":case"cpu-pinned":return this.data;case"texture":case"gpu-buffer":case"ml-tensor":{if(!this.downloader)throw new Error("The current tensor is not created with a specified data downloader.");if(this.isDownloading)throw new Error("The current tensor is being downloaded.");try{this.isDownloading=!0;const n=await this.downloader();return this.downloader=void 0,this.dataLocation="cpu",this.cpuData=n,t&&this.disposer&&(this.disposer(),this.disposer=void 0),n}finally{this.isDownloading=!1}}default:throw new Error(`cannot get data from location: ${this.dataLocation}`)}}dispose(){if(this.isDownloading)throw new Error("The current tensor is being downloaded.");this.disposer&&(this.disposer(),this.disposer=void 0),this.cpuData=void 0,this.gpuTextureData=void 0,this.gpuBufferData=void 0,this.mlTensorData=void 0,this.downloader=void 0,this.isDownloading=void 0,this.dataLocation="none"}ensureValid(){if(this.dataLocation==="none")throw new Error("The tensor is disposed.")}reshape(t){if(this.ensureValid(),this.downloader||this.disposer)throw new Error("Cannot reshape a tensor that owns GPU resource.");return nG(this,t)}};const Co=Vr,mI=(e,t)=>{(typeof cs.trace>"u"?!cs.wasm.trace:!cs.trace)||console.timeStamp(`${e}::ORT::${t}`)},fI=(e,t)=>{const n=new Error().stack?.split(/\r\n|\r|\n/g)||[];let r=!1;for(let s=0;s<n.length;s++){if(r&&!n[s].includes("TRACE_FUNC")){let i=`FUNC_${e}::${n[s].trim().split(" ")[1]}`;t&&(i+=`::${t}`),mI("CPU",i);return}n[s].includes("TRACE_FUNC")&&(r=!0)}},Hm=e=>{(typeof cs.trace>"u"?!cs.wasm.trace:!cs.trace)||fI("BEGIN",e)},Km=e=>{(typeof cs.trace>"u"?!cs.wasm.trace:!cs.trace)||fI("END",e)};let rG=class gI{constructor(t){this.handler=t}async run(t,n,r){Hm();const s={};let i={};if(typeof t!="object"||t===null||t instanceof Co||Array.isArray(t))throw new TypeError("'feeds' must be an object that use input names as keys and OnnxValue as corresponding values.");let o=!0;if(typeof n=="object"){if(n===null)throw new TypeError("Unexpected argument[1]: cannot be null.");if(n instanceof Co)throw new TypeError("'fetches' cannot be a Tensor");if(Array.isArray(n)){if(n.length===0)throw new TypeError("'fetches' cannot be an empty array.");o=!1;for(const c of n){if(typeof c!="string")throw new TypeError("'fetches' must be a string array or an object.");if(this.outputNames.indexOf(c)===-1)throw new RangeError(`'fetches' contains invalid output name: ${c}.`);s[c]=null}if(typeof r=="object"&&r!==null)i=r;else if(typeof r<"u")throw new TypeError("'options' must be an object.")}else{let c=!1;const d=Object.getOwnPropertyNames(n);for(const u of this.outputNames)if(d.indexOf(u)!==-1){const p=n[u];(p===null||p instanceof Co)&&(c=!0,o=!1,s[u]=p)}if(c){if(typeof r=="object"&&r!==null)i=r;else if(typeof r<"u")throw new TypeError("'options' must be an object.")}else i=n}}else if(typeof n<"u")throw new TypeError("Unexpected argument[1]: must be 'fetches' or 'options'.");for(const c of this.inputNames)if(typeof t[c]>"u")throw new Error(`input '${c}' is missing in 'feeds'.`);if(o)for(const c of this.outputNames)s[c]=null;const a=await this.handler.run(t,s,i),l={};for(const c in a)if(Object.hasOwnProperty.call(a,c)){const d=a[c];d instanceof Co?l[c]=d:l[c]=new Co(d.type,d.data,d.dims)}return Km(),l}async release(){return this.handler.dispose()}static async create(t,n,r,s){Hm();let i,o={};if(typeof t=="string"){if(i=t,typeof n=="object"&&n!==null)o=n;else if(typeof n<"u")throw new TypeError("'options' must be an object.")}else if(t instanceof Uint8Array){if(i=t,typeof n=="object"&&n!==null)o=n;else if(typeof n<"u")throw new TypeError("'options' must be an object.")}else if(t instanceof ArrayBuffer||typeof SharedArrayBuffer<"u"&&t instanceof SharedArrayBuffer){const d=t;let u=0,p=t.byteLength;if(typeof n=="object"&&n!==null)o=n;else if(typeof n=="number"){if(u=n,!Number.isSafeInteger(u))throw new RangeError("'byteOffset' must be an integer.");if(u<0||u>=d.byteLength)throw new RangeError(`'byteOffset' is out of range [0, ${d.byteLength}).`);if(p=t.byteLength-u,typeof r=="number"){if(p=r,!Number.isSafeInteger(p))throw new RangeError("'byteLength' must be an integer.");if(p<=0||u+p>d.byteLength)throw new RangeError(`'byteLength' is out of range (0, ${d.byteLength-u}].`);if(typeof s=="object"&&s!==null)o=s;else if(typeof s<"u")throw new TypeError("'options' must be an object.")}else if(typeof r<"u")throw new TypeError("'byteLength' must be a number.")}else if(typeof n<"u")throw new TypeError("'options' must be an object.");i=new Uint8Array(d,u,p)}else throw new TypeError("Unexpected argument[0]: must be 'path' or 'buffer'.");const[a,l]=await U5(o),c=await a.createInferenceSessionHandler(i,l);return Km(),new gI(c)}startProfiling(){this.handler.startProfiling()}endProfiling(){this.handler.endProfiling()}get inputNames(){return this.handler.inputNames}get outputNames(){return this.handler.outputNames}};const sG=rG,iG=Object.freeze(Object.defineProperty({__proto__:null,InferenceSession:sG,TRACE:mI,TRACE_FUNC_BEGIN:Hm,TRACE_FUNC_END:Km,Tensor:Co,env:H5,registerBackend:G5},Symbol.toStringTag,{value:"Module"}));var fg=Object.defineProperty,oG=Object.getOwnPropertyDescriptor,aG=Object.getOwnPropertyNames,lG=Object.prototype.hasOwnProperty,cG=(e=>typeof require<"u"?require:typeof Proxy<"u"?new Proxy(e,{get:(t,n)=>(typeof require<"u"?require:t)[n]}):e)(function(e){if(typeof require<"u")return require.apply(this,arguments);throw Error('Dynamic require of "'+e+'" is not supported')}),He=(e,t)=>()=>(e&&(t=e(e=0)),t),Zo=(e,t)=>{for(var n in t)fg(e,n,{get:t[n],enumerable:!0})},uG=(e,t,n,r)=>{if(t&&typeof t=="object"||typeof t=="function")for(let s of aG(t))!lG.call(e,s)&&s!==n&&fg(e,s,{get:()=>t[s],enumerable:!(r=oG(t,s))||r.enumerable});return e},ml=e=>uG(fg({},"__esModule",{value:!0}),e),ka,ni,Ri,xv,_I,yI=He(()=>{ka=new Map,ni=[],Ri=(e,t,n)=>{if(t&&typeof t.init=="function"&&typeof t.createInferenceSessionHandler=="function"){let r=ka.get(e);if(r===void 0)ka.set(e,{backend:t,priority:n});else{if(r.priority>n)return;if(r.priority===n&&r.backend!==t)throw new Error(`cannot register backend "${e}" using priority ${n}`)}if(n>=0){let s=ni.indexOf(e);s!==-1&&ni.splice(s,1);for(let i=0;i<ni.length;i++)if(ka.get(ni[i]).priority<=n){ni.splice(i,0,e);return}ni.push(e)}return}throw new TypeError("not a valid backend")},xv=async e=>{let t=ka.get(e);if(!t)return"backend not found.";if(t.initialized)return t.backend;if(t.aborted)return t.error;{let n=!!t.initPromise;try{return n||(t.initPromise=t.backend.init(e)),await t.initPromise,t.initialized=!0,t.backend}catch(r){return n||(t.error=`${r}`,t.aborted=!0),t.error}finally{delete t.initPromise}}},_I=async e=>{let t=e.executionProviders||[],n=t.map(l=>typeof l=="string"?l:l.name),r=n.length===0?ni:n,s,i=[],o=new Set;for(let l of r){let c=await xv(l);typeof c=="string"?i.push({name:l,err:c}):(s||(s=c),s===c&&o.add(l))}if(!s)throw new Error(`no available backend found. ERR: ${i.map(l=>`[${l.name}] ${l.err}`).join(", ")}`);for(let{name:l,err:c}of i)n.includes(l)&&console.warn(`removing requested execution provider "${l}" from session options because it is not available: ${c}`);let a=t.filter(l=>o.has(typeof l=="string"?l:l.name));return[s,new Proxy(e,{get:(l,c)=>c==="executionProviders"?a:Reflect.get(l,c)})]}}),dG=He(()=>{yI()}),xI,pG=He(()=>{xI="1.22.0-dev.20250409-89f8206ba4"}),ih,Br,bI=He(()=>{pG(),ih="warning",Br={wasm:{},webgl:{},webgpu:{},versions:{common:xI},set logLevel(e){if(e!==void 0){if(typeof e!="string"||["verbose","info","warning","error","fatal"].indexOf(e)===-1)throw new Error(`Unsupported logging level: ${e}`);ih=e}},get logLevel(){return ih}},Object.defineProperty(Br,"logLevel",{enumerable:!0})}),ln,hG=He(()=>{bI(),ln=Br}),wI,vI,mG=He(()=>{wI=(e,t)=>{let n=typeof document<"u"?document.createElement("canvas"):new OffscreenCanvas(1,1);n.width=e.dims[3],n.height=e.dims[2];let r=n.getContext("2d");if(r!=null){let s,i;t?.tensorLayout!==void 0&&t.tensorLayout==="NHWC"?(s=e.dims[2],i=e.dims[3]):(s=e.dims[3],i=e.dims[2]);let o=t?.format!==void 0?t.format:"RGB",a=t?.norm,l,c;a===void 0||a.mean===void 0?l=[255,255,255,255]:typeof a.mean=="number"?l=[a.mean,a.mean,a.mean,a.mean]:(l=[a.mean[0],a.mean[1],a.mean[2],0],a.mean[3]!==void 0&&(l[3]=a.mean[3])),a===void 0||a.bias===void 0?c=[0,0,0,0]:typeof a.bias=="number"?c=[a.bias,a.bias,a.bias,a.bias]:(c=[a.bias[0],a.bias[1],a.bias[2],0],a.bias[3]!==void 0&&(c[3]=a.bias[3]));let d=i*s,u=0,p=d,h=d*2,m=-1;o==="RGBA"?(u=0,p=d,h=d*2,m=d*3):o==="RGB"?(u=0,p=d,h=d*2):o==="RBG"&&(u=0,h=d,p=d*2);for(let g=0;g<i;g++)for(let I=0;I<s;I++){let f=(e.data[u++]-c[0])*l[0],_=(e.data[p++]-c[1])*l[1],T=(e.data[h++]-c[2])*l[2],M=m===-1?255:(e.data[m++]-c[3])*l[3];r.fillStyle="rgba("+f+","+_+","+T+","+M+")",r.fillRect(I,g,1,1)}if("toDataURL"in n)return n.toDataURL();throw new Error("toDataURL is not supported")}else throw new Error("Can not access image data")},vI=(e,t)=>{let n=typeof document<"u"?document.createElement("canvas").getContext("2d"):new OffscreenCanvas(1,1).getContext("2d"),r;if(n!=null){let s,i,o;t?.tensorLayout!==void 0&&t.tensorLayout==="NHWC"?(s=e.dims[2],i=e.dims[1],o=e.dims[3]):(s=e.dims[3],i=e.dims[2],o=e.dims[1]);let a=t!==void 0&&t.format!==void 0?t.format:"RGB",l=t?.norm,c,d;l===void 0||l.mean===void 0?c=[255,255,255,255]:typeof l.mean=="number"?c=[l.mean,l.mean,l.mean,l.mean]:(c=[l.mean[0],l.mean[1],l.mean[2],255],l.mean[3]!==void 0&&(c[3]=l.mean[3])),l===void 0||l.bias===void 0?d=[0,0,0,0]:typeof l.bias=="number"?d=[l.bias,l.bias,l.bias,l.bias]:(d=[l.bias[0],l.bias[1],l.bias[2],0],l.bias[3]!==void 0&&(d[3]=l.bias[3]));let u=i*s;if(t!==void 0&&(t.format!==void 0&&o===4&&t.format!=="RGBA"||o===3&&t.format!=="RGB"&&t.format!=="BGR"))throw new Error("Tensor format doesn't match input tensor dims");let p=4,h=0,m=1,g=2,I=3,f=0,_=u,T=u*2,M=-1;a==="RGBA"?(f=0,_=u,T=u*2,M=u*3):a==="RGB"?(f=0,_=u,T=u*2):a==="RBG"&&(f=0,T=u,_=u*2),r=n.createImageData(s,i);for(let v=0;v<i*s;h+=p,m+=p,g+=p,I+=p,v++)r.data[h]=(e.data[f++]-d[0])*c[0],r.data[m]=(e.data[_++]-d[1])*c[1],r.data[g]=(e.data[T++]-d[2])*c[2],r.data[I]=M===-1?255:(e.data[M++]-d[3])*c[3]}else throw new Error("Can not access image data");return r}}),Ic,MI,AI,TI,PI,CI,fG=He(()=>{gg(),Ic=(e,t)=>{if(e===void 0)throw new Error("Image buffer must be defined");if(t.height===void 0||t.width===void 0)throw new Error("Image height and width must be defined");if(t.tensorLayout==="NHWC")throw new Error("NHWC Tensor layout is not supported yet");let{height:n,width:r}=t,s=t.norm??{mean:255,bias:0},i,o;typeof s.mean=="number"?i=[s.mean,s.mean,s.mean,s.mean]:i=[s.mean[0],s.mean[1],s.mean[2],s.mean[3]??255],typeof s.bias=="number"?o=[s.bias,s.bias,s.bias,s.bias]:o=[s.bias[0],s.bias[1],s.bias[2],s.bias[3]??0];let a=t.format!==void 0?t.format:"RGBA",l=t.tensorFormat!==void 0&&t.tensorFormat!==void 0?t.tensorFormat:"RGB",c=n*r,d=l==="RGBA"?new Float32Array(c*4):new Float32Array(c*3),u=4,p=0,h=1,m=2,g=3,I=0,f=c,_=c*2,T=-1;a==="RGB"&&(u=3,p=0,h=1,m=2,g=-1),l==="RGBA"?T=c*3:l==="RBG"?(I=0,_=c,f=c*2):l==="BGR"&&(_=0,f=c,I=c*2);for(let M=0;M<c;M++,p+=u,m+=u,h+=u,g+=u)d[I++]=(e[p]+o[0])/i[0],d[f++]=(e[h]+o[1])/i[1],d[_++]=(e[m]+o[2])/i[2],T!==-1&&g!==-1&&(d[T++]=(e[g]+o[3])/i[3]);return l==="RGBA"?new kr("float32",d,[1,4,n,r]):new kr("float32",d,[1,3,n,r])},MI=async(e,t)=>{let n=typeof HTMLImageElement<"u"&&e instanceof HTMLImageElement,r=typeof ImageData<"u"&&e instanceof ImageData,s=typeof ImageBitmap<"u"&&e instanceof ImageBitmap,i=typeof e=="string",o,a=t??{},l=()=>{if(typeof document<"u")return document.createElement("canvas");if(typeof OffscreenCanvas<"u")return new OffscreenCanvas(1,1);throw new Error("Canvas is not supported")},c=d=>typeof HTMLCanvasElement<"u"&&d instanceof HTMLCanvasElement||d instanceof OffscreenCanvas?d.getContext("2d"):null;if(n){let d=l();d.width=e.width,d.height=e.height;let u=c(d);if(u!=null){let p=e.height,h=e.width;if(t!==void 0&&t.resizedHeight!==void 0&&t.resizedWidth!==void 0&&(p=t.resizedHeight,h=t.resizedWidth),t!==void 0){if(a=t,t.tensorFormat!==void 0)throw new Error("Image input config format must be RGBA for HTMLImageElement");a.tensorFormat="RGBA",a.height=p,a.width=h}else a.tensorFormat="RGBA",a.height=p,a.width=h;u.drawImage(e,0,0),o=u.getImageData(0,0,h,p).data}else throw new Error("Can not access image data")}else if(r){let d,u;if(t!==void 0&&t.resizedWidth!==void 0&&t.resizedHeight!==void 0?(d=t.resizedHeight,u=t.resizedWidth):(d=e.height,u=e.width),t!==void 0&&(a=t),a.format="RGBA",a.height=d,a.width=u,t!==void 0){let p=l();p.width=u,p.height=d;let h=c(p);if(h!=null)h.putImageData(e,0,0),o=h.getImageData(0,0,u,d).data;else throw new Error("Can not access image data")}else o=e.data}else if(s){if(t===void 0)throw new Error("Please provide image config with format for Imagebitmap");let d=l();d.width=e.width,d.height=e.height;let u=c(d);if(u!=null){let p=e.height,h=e.width;return u.drawImage(e,0,0,h,p),o=u.getImageData(0,0,h,p).data,a.height=p,a.width=h,Ic(o,a)}else throw new Error("Can not access image data")}else{if(i)return new Promise((d,u)=>{let p=l(),h=c(p);if(!e||!h)return u();let m=new Image;m.crossOrigin="Anonymous",m.src=e,m.onload=()=>{p.width=m.width,p.height=m.height,h.drawImage(m,0,0,p.width,p.height);let g=h.getImageData(0,0,p.width,p.height);a.height=p.height,a.width=p.width,d(Ic(g.data,a))}});throw new Error("Input data provided is not supported - aborted tensor creation")}if(o!==void 0)return Ic(o,a);throw new Error("Input data provided is not supported - aborted tensor creation")},AI=(e,t)=>{let{width:n,height:r,download:s,dispose:i}=t,o=[1,r,n,4];return new kr({location:"texture",type:"float32",texture:e,dims:o,download:s,dispose:i})},TI=(e,t)=>{let{dataType:n,dims:r,download:s,dispose:i}=t;return new kr({location:"gpu-buffer",type:n??"float32",gpuBuffer:e,dims:r,download:s,dispose:i})},PI=(e,t)=>{let{dataType:n,dims:r,download:s,dispose:i}=t;return new kr({location:"ml-tensor",type:n??"float32",mlTensor:e,dims:r,download:s,dispose:i})},CI=(e,t,n)=>new kr({location:"cpu-pinned",type:e,data:t,dims:n??[t.length]})}),$i,Ha,oh,II,gG=He(()=>{$i=new Map([["float32",Float32Array],["uint8",Uint8Array],["int8",Int8Array],["uint16",Uint16Array],["int16",Int16Array],["int32",Int32Array],["bool",Uint8Array],["float64",Float64Array],["uint32",Uint32Array],["int4",Uint8Array],["uint4",Uint8Array]]),Ha=new Map([[Float32Array,"float32"],[Uint8Array,"uint8"],[Int8Array,"int8"],[Uint16Array,"uint16"],[Int16Array,"int16"],[Int32Array,"int32"],[Float64Array,"float64"],[Uint32Array,"uint32"]]),oh=!1,II=()=>{if(!oh){oh=!0;let e=typeof BigInt64Array<"u"&&BigInt64Array.from,t=typeof BigUint64Array<"u"&&BigUint64Array.from,n=globalThis.Float16Array,r=typeof n<"u"&&n.from;e&&($i.set("int64",BigInt64Array),Ha.set(BigInt64Array,"int64")),t&&($i.set("uint64",BigUint64Array),Ha.set(BigUint64Array,"uint64")),r?($i.set("float16",n),Ha.set(n,"float16")):$i.set("float16",Uint16Array)}}}),kI,EI,_G=He(()=>{gg(),kI=e=>{let t=1;for(let n=0;n<e.length;n++){let r=e[n];if(typeof r!="number"||!Number.isSafeInteger(r))throw new TypeError(`dims[${n}] must be an integer, got: ${r}`);if(r<0)throw new RangeError(`dims[${n}] must be a non-negative integer, got: ${r}`);t*=r}return t},EI=(e,t)=>{switch(e.location){case"cpu":return new kr(e.type,e.data,t);case"cpu-pinned":return new kr({location:"cpu-pinned",data:e.data,type:e.type,dims:t});case"texture":return new kr({location:"texture",texture:e.texture,type:e.type,dims:t});case"gpu-buffer":return new kr({location:"gpu-buffer",gpuBuffer:e.gpuBuffer,type:e.type,dims:t});case"ml-tensor":return new kr({location:"ml-tensor",mlTensor:e.mlTensor,type:e.type,dims:t});default:throw new Error(`tensorReshape: tensor location ${e.location} is not supported`)}}}),kr,gg=He(()=>{mG(),fG(),gG(),_G(),kr=class{constructor(e,t,n){II();let r,s;if(typeof e=="object"&&"location"in e)switch(this.dataLocation=e.location,r=e.type,s=e.dims,e.location){case"cpu-pinned":{let o=$i.get(r);if(!o)throw new TypeError(`unsupported type "${r}" to create tensor from pinned buffer`);if(!(e.data instanceof o))throw new TypeError(`buffer should be of type ${o.name}`);this.cpuData=e.data;break}case"texture":{if(r!=="float32")throw new TypeError(`unsupported type "${r}" to create tensor from texture`);this.gpuTextureData=e.texture,this.downloader=e.download,this.disposer=e.dispose;break}case"gpu-buffer":{if(r!=="float32"&&r!=="float16"&&r!=="int32"&&r!=="int64"&&r!=="uint32"&&r!=="uint8"&&r!=="bool"&&r!=="uint4"&&r!=="int4")throw new TypeError(`unsupported type "${r}" to create tensor from gpu buffer`);this.gpuBufferData=e.gpuBuffer,this.downloader=e.download,this.disposer=e.dispose;break}case"ml-tensor":{if(r!=="float32"&&r!=="float16"&&r!=="int32"&&r!=="int64"&&r!=="uint32"&&r!=="uint64"&&r!=="int8"&&r!=="uint8"&&r!=="bool"&&r!=="uint4"&&r!=="int4")throw new TypeError(`unsupported type "${r}" to create tensor from MLTensor`);this.mlTensorData=e.mlTensor,this.downloader=e.download,this.disposer=e.dispose;break}default:throw new Error(`Tensor constructor: unsupported location '${this.dataLocation}'`)}else{let o,a;if(typeof e=="string")if(r=e,a=n,e==="string"){if(!Array.isArray(t))throw new TypeError("A string tensor's data must be a string array.");o=t}else{let l=$i.get(e);if(l===void 0)throw new TypeError(`Unsupported tensor type: ${e}.`);if(Array.isArray(t)){if(e==="float16"&&l===Uint16Array||e==="uint4"||e==="int4")throw new TypeError(`Creating a ${e} tensor from number array is not supported. Please use ${l.name} as data.`);e==="uint64"||e==="int64"?o=l.from(t,BigInt):o=l.from(t)}else if(t instanceof l)o=t;else if(t instanceof Uint8ClampedArray)if(e==="uint8")o=Uint8Array.from(t);else throw new TypeError("A Uint8ClampedArray tensor's data must be type of uint8");else if(e==="float16"&&t instanceof Uint16Array&&l!==Uint16Array)o=new globalThis.Float16Array(t.buffer,t.byteOffset,t.length);else throw new TypeError(`A ${r} tensor's data must be type of ${l}`)}else if(a=t,Array.isArray(e)){if(e.length===0)throw new TypeError("Tensor type cannot be inferred from an empty array.");let l=typeof e[0];if(l==="string")r="string",o=e;else if(l==="boolean")r="bool",o=Uint8Array.from(e);else throw new TypeError(`Invalid element type of data array: ${l}.`)}else if(e instanceof Uint8ClampedArray)r="uint8",o=Uint8Array.from(e);else{let l=Ha.get(e.constructor);if(l===void 0)throw new TypeError(`Unsupported type for tensor data: ${e.constructor}.`);r=l,o=e}if(a===void 0)a=[o.length];else if(!Array.isArray(a))throw new TypeError("A tensor's dims must be a number array");s=a,this.cpuData=o,this.dataLocation="cpu"}let i=kI(s);if(this.cpuData&&i!==this.cpuData.length&&!((r==="uint4"||r==="int4")&&Math.ceil(i/2)===this.cpuData.length))throw new Error(`Tensor's size(${i}) does not match data length(${this.cpuData.length}).`);this.type=r,this.dims=s,this.size=i}static async fromImage(e,t){return MI(e,t)}static fromTexture(e,t){return AI(e,t)}static fromGpuBuffer(e,t){return TI(e,t)}static fromMLTensor(e,t){return PI(e,t)}static fromPinnedBuffer(e,t,n){return CI(e,t,n)}toDataURL(e){return wI(this,e)}toImageData(e){return vI(this,e)}get data(){if(this.ensureValid(),!this.cpuData)throw new Error("The data is not on CPU. Use `getData()` to download GPU data to CPU, or use `texture` or `gpuBuffer` property to access the GPU data directly.");return this.cpuData}get location(){return this.dataLocation}get texture(){if(this.ensureValid(),!this.gpuTextureData)throw new Error("The data is not stored as a WebGL texture.");return this.gpuTextureData}get gpuBuffer(){if(this.ensureValid(),!this.gpuBufferData)throw new Error("The data is not stored as a WebGPU buffer.");return this.gpuBufferData}get mlTensor(){if(this.ensureValid(),!this.mlTensorData)throw new Error("The data is not stored as a WebNN MLTensor.");return this.mlTensorData}async getData(e){switch(this.ensureValid(),this.dataLocation){case"cpu":case"cpu-pinned":return this.data;case"texture":case"gpu-buffer":case"ml-tensor":{if(!this.downloader)throw new Error("The current tensor is not created with a specified data downloader.");if(this.isDownloading)throw new Error("The current tensor is being downloaded.");try{this.isDownloading=!0;let t=await this.downloader();return this.downloader=void 0,this.dataLocation="cpu",this.cpuData=t,e&&this.disposer&&(this.disposer(),this.disposer=void 0),t}finally{this.isDownloading=!1}}default:throw new Error(`cannot get data from location: ${this.dataLocation}`)}}dispose(){if(this.isDownloading)throw new Error("The current tensor is being downloaded.");this.disposer&&(this.disposer(),this.disposer=void 0),this.cpuData=void 0,this.gpuTextureData=void 0,this.gpuBufferData=void 0,this.mlTensorData=void 0,this.downloader=void 0,this.isDownloading=void 0,this.dataLocation="none"}ensureValid(){if(this.dataLocation==="none")throw new Error("The tensor is disposed.")}reshape(e){if(this.ensureValid(),this.downloader||this.disposer)throw new Error("Cannot reshape a tensor that owns GPU resource.");return EI(this,e)}}}),os,SI=He(()=>{gg(),os=kr}),fl,ah,us,Kr,LI=He(()=>{bI(),fl=(e,t)=>{(typeof Br.trace>"u"?!Br.wasm.trace:!Br.trace)||console.timeStamp(`${e}::ORT::${t}`)},ah=(e,t)=>{let n=new Error().stack?.split(/\r\n|\r|\n/g)||[],r=!1;for(let s=0;s<n.length;s++){if(r&&!n[s].includes("TRACE_FUNC")){let i=`FUNC_${e}::${n[s].trim().split(" ")[1]}`;t&&(i+=`::${t}`),fl("CPU",i);return}n[s].includes("TRACE_FUNC")&&(r=!0)}},us=e=>{(typeof Br.trace>"u"?!Br.wasm.trace:!Br.trace)||ah("BEGIN",e)},Kr=e=>{(typeof Br.trace>"u"?!Br.wasm.trace:!Br.trace)||ah("END",e)}}),DI,yG=He(()=>{yI(),SI(),LI(),DI=class $I{constructor(t){this.handler=t}async run(t,n,r){us();let s={},i={};if(typeof t!="object"||t===null||t instanceof os||Array.isArray(t))throw new TypeError("'feeds' must be an object that use input names as keys and OnnxValue as corresponding values.");let o=!0;if(typeof n=="object"){if(n===null)throw new TypeError("Unexpected argument[1]: cannot be null.");if(n instanceof os)throw new TypeError("'fetches' cannot be a Tensor");if(Array.isArray(n)){if(n.length===0)throw new TypeError("'fetches' cannot be an empty array.");o=!1;for(let c of n){if(typeof c!="string")throw new TypeError("'fetches' must be a string array or an object.");if(this.outputNames.indexOf(c)===-1)throw new RangeError(`'fetches' contains invalid output name: ${c}.`);s[c]=null}if(typeof r=="object"&&r!==null)i=r;else if(typeof r<"u")throw new TypeError("'options' must be an object.")}else{let c=!1,d=Object.getOwnPropertyNames(n);for(let u of this.outputNames)if(d.indexOf(u)!==-1){let p=n[u];(p===null||p instanceof os)&&(c=!0,o=!1,s[u]=p)}if(c){if(typeof r=="object"&&r!==null)i=r;else if(typeof r<"u")throw new TypeError("'options' must be an object.")}else i=n}}else if(typeof n<"u")throw new TypeError("Unexpected argument[1]: must be 'fetches' or 'options'.");for(let c of this.inputNames)if(typeof t[c]>"u")throw new Error(`input '${c}' is missing in 'feeds'.`);if(o)for(let c of this.outputNames)s[c]=null;let a=await this.handler.run(t,s,i),l={};for(let c in a)if(Object.hasOwnProperty.call(a,c)){let d=a[c];d instanceof os?l[c]=d:l[c]=new os(d.type,d.data,d.dims)}return Kr(),l}async release(){return this.handler.dispose()}static async create(t,n,r,s){us();let i,o={};if(typeof t=="string"){if(i=t,typeof n=="object"&&n!==null)o=n;else if(typeof n<"u")throw new TypeError("'options' must be an object.")}else if(t instanceof Uint8Array){if(i=t,typeof n=="object"&&n!==null)o=n;else if(typeof n<"u")throw new TypeError("'options' must be an object.")}else if(t instanceof ArrayBuffer||typeof SharedArrayBuffer<"u"&&t instanceof SharedArrayBuffer){let d=t,u=0,p=t.byteLength;if(typeof n=="object"&&n!==null)o=n;else if(typeof n=="number"){if(u=n,!Number.isSafeInteger(u))throw new RangeError("'byteOffset' must be an integer.");if(u<0||u>=d.byteLength)throw new RangeError(`'byteOffset' is out of range [0, ${d.byteLength}).`);if(p=t.byteLength-u,typeof r=="number"){if(p=r,!Number.isSafeInteger(p))throw new RangeError("'byteLength' must be an integer.");if(p<=0||u+p>d.byteLength)throw new RangeError(`'byteLength' is out of range (0, ${d.byteLength-u}].`);if(typeof s=="object"&&s!==null)o=s;else if(typeof s<"u")throw new TypeError("'options' must be an object.")}else if(typeof r<"u")throw new TypeError("'byteLength' must be a number.")}else if(typeof n<"u")throw new TypeError("'options' must be an object.");i=new Uint8Array(d,u,p)}else throw new TypeError("Unexpected argument[0]: must be 'path' or 'buffer'.");let[a,l]=await _I(o),c=await a.createInferenceSessionHandler(i,l);return Kr(),new $I(c)}startProfiling(){this.handler.startProfiling()}endProfiling(){this.handler.endProfiling()}get inputNames(){return this.handler.inputNames}get outputNames(){return this.handler.outputNames}get inputMetadata(){return this.handler.inputMetadata}get outputMetadata(){return this.handler.outputMetadata}}}),_g,xG=He(()=>{yG(),_g=DI}),bG=He(()=>{}),wG=He(()=>{}),vG=He(()=>{}),MG=He(()=>{}),FI={};Zo(FI,{InferenceSession:()=>_g,TRACE:()=>fl,TRACE_FUNC_BEGIN:()=>us,TRACE_FUNC_END:()=>Kr,Tensor:()=>os,env:()=>ln,registerBackend:()=>Ri});var ds=He(()=>{dG(),hG(),xG(),SI(),bG(),wG(),LI(),vG(),MG()}),yg=He(()=>{}),OI={};Zo(OI,{default:()=>jI});var lh,ch,jI,AG=He(()=>{GS(),Yi(),xg(),lh="ort-wasm-proxy-worker",ch=globalThis.self?.name===lh,ch&&(self.onmessage=e=>{let{type:t,in:n}=e.data;try{switch(t){case"init-wasm":bg(n.wasm).then(()=>{Rg(n).then(()=>{postMessage({type:t})},r=>{postMessage({type:t,err:r})})},r=>{postMessage({type:t,err:r})});break;case"init-ep":{let{epName:r,env:s}=n;Ng(s,r).then(()=>{postMessage({type:t})},i=>{postMessage({type:t,err:i})});break}case"copy-from":{let{buffer:r}=n,s=Au(r);postMessage({type:t,out:s});break}case"create":{let{model:r,options:s}=n;zg(r,s).then(i=>{postMessage({type:t,out:i})},i=>{postMessage({type:t,err:i})});break}case"release":Bg(n),postMessage({type:t});break;case"run":{let{sessionId:r,inputIndices:s,inputs:i,outputIndices:o,options:a}=n;Gg(r,s,i,o,new Array(o.length).fill(null),a).then(l=>{l.some(c=>c[3]!=="cpu")?postMessage({type:t,err:"Proxy does not support non-cpu tensor location."}):postMessage({type:t,out:l},Ug([...i,...l]))},l=>{postMessage({type:t,err:l})});break}case"end-profiling":Vg(n),postMessage({type:t});break;default:}}catch(r){postMessage({type:t,err:r})}}),jI=ch?null:e=>new Worker(e??Ir,{type:"module",name:lh})}),RI={};Zo(RI,{default:()=>NI});var uh,dh,NI,bv,TG=He(()=>{dh=(uh=import.meta.url,async function(e={}){var t,n,r=e,s=new Promise((x,E)=>{t=x,n=E}),i=typeof window=="object",o=typeof WorkerGlobalScope<"u",a=o&&self.name?.startsWith("em-pthread");r.mountExternalData=(x,E)=>{x.startsWith("./")&&(x=x.substring(2)),(r.Eb||(r.Eb=new Map)).set(x,E)},r.unmountExternalData=()=>{delete r.Eb};var l=globalThis.SharedArrayBuffer??new WebAssembly.Memory({initial:0,maximum:0,pc:!0}).buffer.constructor;let c=x=>async(...E)=>{try{if(r.Fb)throw Error("Session already started");let O=r.Fb={dc:E[0],errors:[]},B=await x(...E);if(r.Fb!==O)throw Error("Session mismatch");r.Jb?.flush();let q=O.errors;if(0<q.length){let ue=await Promise.all(q);if(ue=ue.filter(Fe=>Fe),0<ue.length)throw Error(ue.join(`
`))}return B}finally{r.Fb=null}};r.jsepInit=(x,E)=>{if(x==="webgpu"){[r.Jb,r.Ub,r.Yb,r.Kb,r.Xb,r.jb,r.Zb,r.ac,r.Vb,r.Wb,r.$b]=E;let O=r.Jb;r.jsepRegisterBuffer=(B,q,ue,Fe)=>O.registerBuffer(B,q,ue,Fe),r.jsepGetBuffer=B=>O.getBuffer(B),r.jsepCreateDownloader=(B,q,ue)=>O.createDownloader(B,q,ue),r.jsepOnCreateSession=B=>{O.onCreateSession(B)},r.jsepOnReleaseSession=B=>{O.onReleaseSession(B)},r.jsepOnRunStart=B=>O.onRunStart(B),r.bc=(B,q)=>{O.upload(B,q)}}else if(x==="webnn"){let O=E[0];[r.nc,r.Nb,r.webnnEnsureTensor,r.Ob,r.webnnDownloadTensor]=E.slice(1),r.webnnReleaseTensorId=r.Nb,r.webnnUploadTensor=r.Ob,r.webnnOnRunStart=B=>O.onRunStart(B),r.webnnOnRunEnd=O.onRunEnd.bind(O),r.webnnRegisterMLContext=(B,q)=>{O.registerMLContext(B,q)},r.webnnOnReleaseSession=B=>{O.onReleaseSession(B)},r.webnnCreateMLTensorDownloader=(B,q)=>O.createMLTensorDownloader(B,q),r.webnnRegisterMLTensor=(B,q,ue,Fe)=>O.registerMLTensor(B,q,ue,Fe),r.webnnCreateMLContext=B=>O.createMLContext(B),r.webnnRegisterMLConstant=(B,q,ue,Fe,We,tt)=>O.registerMLConstant(B,q,ue,Fe,We,r.Eb,tt),r.webnnRegisterGraphInput=O.registerGraphInput.bind(O),r.webnnIsGraphInput=O.isGraphInput.bind(O),r.webnnCreateTemporaryTensor=O.createTemporaryTensor.bind(O),r.webnnIsInt64Supported=O.isInt64Supported.bind(O)}};let d=()=>{let x=(E,O,B)=>(...q)=>{let ue=nn,Fe=O?.();q=E(...q);let We=O?.();return Fe!==We&&(E=We,B(Fe),O=B=null),nn!=ue?new Promise((tt,dt)=>{Sn={resolve:tt,reject:dt}}):q};(()=>{for(let E of["_OrtAppendExecutionProvider","_OrtCreateSession","_OrtRun","_OrtRunWithBinding","_OrtBindInput"])r[E]=x(r[E],()=>r[E],O=>r[E]=O)})(),c!==void 0&&(r._OrtRun=c(r._OrtRun),r._OrtRunWithBinding=c(r._OrtRunWithBinding)),d=void 0};r.asyncInit=()=>{d?.()};var u,p,h=Object.assign({},r),m=(x,E)=>{throw E},g="";(i||o)&&(o?g=self.location.href:typeof document<"u"&&document.currentScript&&(g=document.currentScript.src),uh&&(g=uh),g=g.startsWith("blob:")?"":g.slice(0,g.replace(/[?#].*/,"").lastIndexOf("/")+1),o&&(p=x=>{var E=new XMLHttpRequest;return E.open("GET",x,!1),E.responseType="arraybuffer",E.send(null),new Uint8Array(E.response)}),u=async x=>{if(le(x))return new Promise((O,B)=>{var q=new XMLHttpRequest;q.open("GET",x,!0),q.responseType="arraybuffer",q.onload=()=>{q.status==200||q.status==0&&q.response?O(q.response):B(q.status)},q.onerror=B,q.send(null)});var E=await fetch(x,{credentials:"same-origin"});if(E.ok)return E.arrayBuffer();throw Error(E.status+" : "+E.url)});var I=console.log.bind(console),f=console.error.bind(console),_=I,T=f;Object.assign(r,h),h=null;var M,v,b,A,k,F,L,G,j,R,K,U,Y,te=r.wasmBinary,ne=!1,le=x=>x.startsWith("file://");function N(){return M.buffer!=A.buffer&&Ce(),A}function oe(){return M.buffer!=A.buffer&&Ce(),k}function X(){return M.buffer!=A.buffer&&Ce(),F}function D(){return M.buffer!=A.buffer&&Ce(),L}function z(){return M.buffer!=A.buffer&&Ce(),G}function se(){return M.buffer!=A.buffer&&Ce(),j}function me(){return M.buffer!=A.buffer&&Ce(),R}function $e(){return M.buffer!=A.buffer&&Ce(),Y}if(a){let x=function(E){try{var O=E.data,B=O.Bb;if(B==="load"){let q=[];self.onmessage=ue=>q.push(ue),self.startWorker=()=>{postMessage({Bb:"loaded"});for(let ue of q)x(ue);self.onmessage=x};for(let ue of O.Rb)r[ue]&&!r[ue].proxy||(r[ue]=(...Fe)=>{postMessage({Bb:"callHandler",Qb:ue,args:Fe})},ue=="print"&&(_=r[ue]),ue=="printErr"&&(T=r[ue]));M=O.kc,Ce(),ke(O.lc)}else if(B==="run"){Zn(O.Ab),bi(O.Ab,0,0,1,0,0),Tn(),ie(O.Ab),Be||(_i(),Be=!0);try{Hs(O.fc,O.Hb)}catch(q){if(q!="unwind")throw q}}else O.target!=="setimmediate"&&(B==="checkMailbox"?Be&&ae():B&&(T(`worker: received unknown command ${B}`),T(O)))}catch(q){throw ma(),q}};var ke,Be=!1;T=function(...E){E=E.join(" "),console.error(E)},self.alert=function(...E){postMessage({Bb:"alert",text:E.join(" "),ic:yi()})},self.onunhandledrejection=E=>{throw E.reason||E},self.onmessage=x}function Ce(){var x=M.buffer;r.HEAP8=A=new Int8Array(x),r.HEAP16=F=new Int16Array(x),r.HEAPU8=k=new Uint8Array(x),r.HEAPU16=L=new Uint16Array(x),r.HEAP32=G=new Int32Array(x),r.HEAPU32=j=new Uint32Array(x),r.HEAPF32=R=new Float32Array(x),r.HEAPF64=Y=new Float64Array(x),r.HEAP64=K=new BigInt64Array(x),r.HEAPU64=U=new BigUint64Array(x)}function Z(){a?startWorker(r):_t.Ca()}a||(M=new WebAssembly.Memory({initial:256,maximum:65536,shared:!0}),Ce());var V,fe=0,Te=null;function Ie(){if(--fe==0&&Te){var x=Te;Te=null,x()}}function Ee(x){throw T(x="Aborted("+x+")"),ne=!0,x=new WebAssembly.RuntimeError(x+". Build with -sASSERTIONS for more info."),n(x),x}function De(){return{a:{L:_e,Aa:ze,b:xn,$:Ar,A:hs,pa:ms,X:fs,Z:S,qa:ee,na:W,ga:J,ma:re,J:pe,Y:Se,V:it,oa:gt,W:Ze,va:Lt,E:gs,Q:_s,O:Ds,D:xs,u:bs,r:Fr,P:or,z:Zi,R:he,ja:$,T:we,aa:nt,M:rt,F:st,ia:ie,sa:Je,t:fn,Ba:jn,w:Qr,o:jr,l:Ln,c:Dr,n:dr,j:kl,v:El,p:Sl,f:Ll,s:Qs,m:Dl,e:$l,k:Fl,i:Ol,g:jl,d:Xs,da:Rl,ea:Nl,fa:sa,ba:ia,ca:oa,N:eo,xa:Bl,ua:Yu,h:la,C:ro,G:Gl,ta:aa,x:Vl,ra:Ul,U:Wl,q:zl,y:Hl,K:Kl,S:ca,za:ql,ya:io,ka:Ms,la:ua,_:ve,B:Ql,I:da,ha:Xl,H:lo,a:M,wa:ye}}}var xe={829644:(x,E,O,B,q)=>{if(r===void 0||!r.Eb)return 1;if((x=rn(Number(x>>>0))).startsWith("./")&&(x=x.substring(2)),!(x=r.Eb.get(x)))return 2;if(E=Number(E>>>0),O=Number(O>>>0),B=Number(B>>>0),E+O>x.byteLength)return 3;try{let ue=x.subarray(E,E+O);switch(q){case 0:oe().set(ue,B>>>0);break;case 1:r.mc?r.mc(B,ue):r.bc(B,ue);break;default:return 4}return 0}catch{return 4}},830468:(x,E,O)=>{r.Ob(x,oe().subarray(E>>>0,E+O>>>0))},830532:()=>r.nc(),830574:x=>{r.Nb(x)},830611:()=>{r.Vb()},830642:()=>{r.Wb()},830671:()=>{r.$b()},830696:x=>r.Ub(x),830729:x=>r.Yb(x),830761:(x,E,O)=>{r.Kb(Number(x),Number(E),Number(O),!0)},830824:(x,E,O)=>{r.Kb(Number(x),Number(E),Number(O))},830881:()=>typeof wasmOffsetConverter<"u",830938:x=>{r.jb("Abs",x,void 0)},830989:x=>{r.jb("Neg",x,void 0)},831040:x=>{r.jb("Floor",x,void 0)},831093:x=>{r.jb("Ceil",x,void 0)},831145:x=>{r.jb("Reciprocal",x,void 0)},831203:x=>{r.jb("Sqrt",x,void 0)},831255:x=>{r.jb("Exp",x,void 0)},831306:x=>{r.jb("Erf",x,void 0)},831357:x=>{r.jb("Sigmoid",x,void 0)},831412:(x,E,O)=>{r.jb("HardSigmoid",x,{alpha:E,beta:O})},831491:x=>{r.jb("Log",x,void 0)},831542:x=>{r.jb("Sin",x,void 0)},831593:x=>{r.jb("Cos",x,void 0)},831644:x=>{r.jb("Tan",x,void 0)},831695:x=>{r.jb("Asin",x,void 0)},831747:x=>{r.jb("Acos",x,void 0)},831799:x=>{r.jb("Atan",x,void 0)},831851:x=>{r.jb("Sinh",x,void 0)},831903:x=>{r.jb("Cosh",x,void 0)},831955:x=>{r.jb("Asinh",x,void 0)},832008:x=>{r.jb("Acosh",x,void 0)},832061:x=>{r.jb("Atanh",x,void 0)},832114:x=>{r.jb("Tanh",x,void 0)},832166:x=>{r.jb("Not",x,void 0)},832217:(x,E,O)=>{r.jb("Clip",x,{min:E,max:O})},832286:x=>{r.jb("Clip",x,void 0)},832338:(x,E)=>{r.jb("Elu",x,{alpha:E})},832396:x=>{r.jb("Gelu",x,void 0)},832448:x=>{r.jb("Relu",x,void 0)},832500:(x,E)=>{r.jb("LeakyRelu",x,{alpha:E})},832564:(x,E)=>{r.jb("ThresholdedRelu",x,{alpha:E})},832634:(x,E)=>{r.jb("Cast",x,{to:E})},832692:x=>{r.jb("Add",x,void 0)},832743:x=>{r.jb("Sub",x,void 0)},832794:x=>{r.jb("Mul",x,void 0)},832845:x=>{r.jb("Div",x,void 0)},832896:x=>{r.jb("Pow",x,void 0)},832947:x=>{r.jb("Equal",x,void 0)},833e3:x=>{r.jb("Greater",x,void 0)},833055:x=>{r.jb("GreaterOrEqual",x,void 0)},833117:x=>{r.jb("Less",x,void 0)},833169:x=>{r.jb("LessOrEqual",x,void 0)},833228:(x,E,O,B,q)=>{r.jb("ReduceMean",x,{keepDims:!!E,noopWithEmptyAxes:!!O,axes:B?Array.from(z().subarray(Number(B)>>>0,Number(q)>>>0)):[]})},833403:(x,E,O,B,q)=>{r.jb("ReduceMax",x,{keepDims:!!E,noopWithEmptyAxes:!!O,axes:B?Array.from(z().subarray(Number(B)>>>0,Number(q)>>>0)):[]})},833577:(x,E,O,B,q)=>{r.jb("ReduceMin",x,{keepDims:!!E,noopWithEmptyAxes:!!O,axes:B?Array.from(z().subarray(Number(B)>>>0,Number(q)>>>0)):[]})},833751:(x,E,O,B,q)=>{r.jb("ReduceProd",x,{keepDims:!!E,noopWithEmptyAxes:!!O,axes:B?Array.from(z().subarray(Number(B)>>>0,Number(q)>>>0)):[]})},833926:(x,E,O,B,q)=>{r.jb("ReduceSum",x,{keepDims:!!E,noopWithEmptyAxes:!!O,axes:B?Array.from(z().subarray(Number(B)>>>0,Number(q)>>>0)):[]})},834100:(x,E,O,B,q)=>{r.jb("ReduceL1",x,{keepDims:!!E,noopWithEmptyAxes:!!O,axes:B?Array.from(z().subarray(Number(B)>>>0,Number(q)>>>0)):[]})},834273:(x,E,O,B,q)=>{r.jb("ReduceL2",x,{keepDims:!!E,noopWithEmptyAxes:!!O,axes:B?Array.from(z().subarray(Number(B)>>>0,Number(q)>>>0)):[]})},834446:(x,E,O,B,q)=>{r.jb("ReduceLogSum",x,{keepDims:!!E,noopWithEmptyAxes:!!O,axes:B?Array.from(z().subarray(Number(B)>>>0,Number(q)>>>0)):[]})},834623:(x,E,O,B,q)=>{r.jb("ReduceSumSquare",x,{keepDims:!!E,noopWithEmptyAxes:!!O,axes:B?Array.from(z().subarray(Number(B)>>>0,Number(q)>>>0)):[]})},834803:(x,E,O,B,q)=>{r.jb("ReduceLogSumExp",x,{keepDims:!!E,noopWithEmptyAxes:!!O,axes:B?Array.from(z().subarray(Number(B)>>>0,Number(q)>>>0)):[]})},834983:x=>{r.jb("Where",x,void 0)},835036:(x,E,O)=>{r.jb("Transpose",x,{perm:E?Array.from(z().subarray(Number(E)>>>0,Number(O)>>>0)):[]})},835160:(x,E,O,B)=>{r.jb("DepthToSpace",x,{blocksize:E,mode:rn(O),format:B?"NHWC":"NCHW"})},835293:(x,E,O,B)=>{r.jb("DepthToSpace",x,{blocksize:E,mode:rn(O),format:B?"NHWC":"NCHW"})},835426:(x,E,O,B,q,ue,Fe,We,tt,dt,Et,Vt,Yt,zn,js)=>{r.jb("ConvTranspose",x,{format:tt?"NHWC":"NCHW",autoPad:E,dilations:[O],group:B,kernelShape:[q],pads:[ue,Fe],strides:[We],wIsConst:()=>!!N()[dt>>>0],outputPadding:Et?Array.from(z().subarray(Number(Et)>>>0,Number(Vt)>>>0)):[],outputShape:Yt?Array.from(z().subarray(Number(Yt)>>>0,Number(zn)>>>0)):[],activation:rn(js)})},835859:(x,E,O,B,q,ue,Fe,We,tt,dt,Et,Vt,Yt,zn)=>{r.jb("ConvTranspose",x,{format:We?"NHWC":"NCHW",autoPad:E,dilations:Array.from(z().subarray(Number(O)>>>0,2+(Number(O)>>>0)>>>0)),group:B,kernelShape:Array.from(z().subarray(Number(q)>>>0,2+(Number(q)>>>0)>>>0)),pads:Array.from(z().subarray(Number(ue)>>>0,4+(Number(ue)>>>0)>>>0)),strides:Array.from(z().subarray(Number(Fe)>>>0,2+(Number(Fe)>>>0)>>>0)),wIsConst:()=>!!N()[tt>>>0],outputPadding:dt?Array.from(z().subarray(Number(dt)>>>0,Number(Et)>>>0)):[],outputShape:Vt?Array.from(z().subarray(Number(Vt)>>>0,Number(Yt)>>>0)):[],activation:rn(zn)})},836520:(x,E,O,B,q,ue,Fe,We,tt,dt,Et,Vt,Yt,zn,js)=>{r.jb("ConvTranspose",x,{format:tt?"NHWC":"NCHW",autoPad:E,dilations:[O],group:B,kernelShape:[q],pads:[ue,Fe],strides:[We],wIsConst:()=>!!N()[dt>>>0],outputPadding:Et?Array.from(z().subarray(Number(Et)>>>0,Number(Vt)>>>0)):[],outputShape:Yt?Array.from(z().subarray(Number(Yt)>>>0,Number(zn)>>>0)):[],activation:rn(js)})},836953:(x,E,O,B,q,ue,Fe,We,tt,dt,Et,Vt,Yt,zn)=>{r.jb("ConvTranspose",x,{format:We?"NHWC":"NCHW",autoPad:E,dilations:Array.from(z().subarray(Number(O)>>>0,2+(Number(O)>>>0)>>>0)),group:B,kernelShape:Array.from(z().subarray(Number(q)>>>0,2+(Number(q)>>>0)>>>0)),pads:Array.from(z().subarray(Number(ue)>>>0,4+(Number(ue)>>>0)>>>0)),strides:Array.from(z().subarray(Number(Fe)>>>0,2+(Number(Fe)>>>0)>>>0)),wIsConst:()=>!!N()[tt>>>0],outputPadding:dt?Array.from(z().subarray(Number(dt)>>>0,Number(Et)>>>0)):[],outputShape:Vt?Array.from(z().subarray(Number(Vt)>>>0,Number(Yt)>>>0)):[],activation:rn(zn)})},837614:(x,E)=>{r.jb("GlobalAveragePool",x,{format:E?"NHWC":"NCHW"})},837705:(x,E,O,B,q,ue,Fe,We,tt,dt,Et,Vt,Yt,zn)=>{r.jb("AveragePool",x,{format:zn?"NHWC":"NCHW",auto_pad:E,ceil_mode:O,count_include_pad:B,storage_order:q,dilations:ue?Array.from(z().subarray(Number(ue)>>>0,Number(Fe)>>>0)):[],kernel_shape:We?Array.from(z().subarray(Number(We)>>>0,Number(tt)>>>0)):[],pads:dt?Array.from(z().subarray(Number(dt)>>>0,Number(Et)>>>0)):[],strides:Vt?Array.from(z().subarray(Number(Vt)>>>0,Number(Yt)>>>0)):[]})},838184:(x,E)=>{r.jb("GlobalAveragePool",x,{format:E?"NHWC":"NCHW"})},838275:(x,E,O,B,q,ue,Fe,We,tt,dt,Et,Vt,Yt,zn)=>{r.jb("AveragePool",x,{format:zn?"NHWC":"NCHW",auto_pad:E,ceil_mode:O,count_include_pad:B,storage_order:q,dilations:ue?Array.from(z().subarray(Number(ue)>>>0,Number(Fe)>>>0)):[],kernel_shape:We?Array.from(z().subarray(Number(We)>>>0,Number(tt)>>>0)):[],pads:dt?Array.from(z().subarray(Number(dt)>>>0,Number(Et)>>>0)):[],strides:Vt?Array.from(z().subarray(Number(Vt)>>>0,Number(Yt)>>>0)):[]})},838754:(x,E)=>{r.jb("GlobalMaxPool",x,{format:E?"NHWC":"NCHW"})},838841:(x,E,O,B,q,ue,Fe,We,tt,dt,Et,Vt,Yt,zn)=>{r.jb("MaxPool",x,{format:zn?"NHWC":"NCHW",auto_pad:E,ceil_mode:O,count_include_pad:B,storage_order:q,dilations:ue?Array.from(z().subarray(Number(ue)>>>0,Number(Fe)>>>0)):[],kernel_shape:We?Array.from(z().subarray(Number(We)>>>0,Number(tt)>>>0)):[],pads:dt?Array.from(z().subarray(Number(dt)>>>0,Number(Et)>>>0)):[],strides:Vt?Array.from(z().subarray(Number(Vt)>>>0,Number(Yt)>>>0)):[]})},839316:(x,E)=>{r.jb("GlobalMaxPool",x,{format:E?"NHWC":"NCHW"})},839403:(x,E,O,B,q,ue,Fe,We,tt,dt,Et,Vt,Yt,zn)=>{r.jb("MaxPool",x,{format:zn?"NHWC":"NCHW",auto_pad:E,ceil_mode:O,count_include_pad:B,storage_order:q,dilations:ue?Array.from(z().subarray(Number(ue)>>>0,Number(Fe)>>>0)):[],kernel_shape:We?Array.from(z().subarray(Number(We)>>>0,Number(tt)>>>0)):[],pads:dt?Array.from(z().subarray(Number(dt)>>>0,Number(Et)>>>0)):[],strides:Vt?Array.from(z().subarray(Number(Vt)>>>0,Number(Yt)>>>0)):[]})},839878:(x,E,O,B,q)=>{r.jb("Gemm",x,{alpha:E,beta:O,transA:B,transB:q})},839982:x=>{r.jb("MatMul",x,void 0)},840036:(x,E,O,B)=>{r.jb("ArgMax",x,{keepDims:!!E,selectLastIndex:!!O,axis:B})},840144:(x,E,O,B)=>{r.jb("ArgMin",x,{keepDims:!!E,selectLastIndex:!!O,axis:B})},840252:(x,E)=>{r.jb("Softmax",x,{axis:E})},840315:(x,E)=>{r.jb("Concat",x,{axis:E})},840375:(x,E,O,B,q)=>{r.jb("Split",x,{axis:E,numOutputs:O,splitSizes:B?Array.from(z().subarray(Number(B)>>>0,Number(q)>>>0)):[]})},840531:x=>{r.jb("Expand",x,void 0)},840585:(x,E)=>{r.jb("Gather",x,{axis:Number(E)})},840656:(x,E)=>{r.jb("GatherElements",x,{axis:Number(E)})},840735:(x,E)=>{r.jb("GatherND",x,{batch_dims:Number(E)})},840814:(x,E,O,B,q,ue,Fe,We,tt,dt,Et)=>{r.jb("Resize",x,{antialias:E,axes:O?Array.from(z().subarray(Number(O)>>>0,Number(B)>>>0)):[],coordinateTransformMode:rn(q),cubicCoeffA:ue,excludeOutside:Fe,extrapolationValue:We,keepAspectRatioPolicy:rn(tt),mode:rn(dt),nearestMode:rn(Et)})},841176:(x,E,O,B,q,ue,Fe)=>{r.jb("Slice",x,{starts:E?Array.from(z().subarray(Number(E)>>>0,Number(O)>>>0)):[],ends:B?Array.from(z().subarray(Number(B)>>>0,Number(q)>>>0)):[],axes:ue?Array.from(z().subarray(Number(ue)>>>0,Number(Fe)>>>0)):[]})},841440:x=>{r.jb("Tile",x,void 0)},841492:(x,E,O)=>{r.jb("InstanceNormalization",x,{epsilon:E,format:O?"NHWC":"NCHW"})},841606:(x,E,O)=>{r.jb("InstanceNormalization",x,{epsilon:E,format:O?"NHWC":"NCHW"})},841720:x=>{r.jb("Range",x,void 0)},841773:(x,E)=>{r.jb("Einsum",x,{equation:rn(E)})},841854:(x,E,O,B,q)=>{r.jb("Pad",x,{mode:E,value:O,pads:B?Array.from(z().subarray(Number(B)>>>0,Number(q)>>>0)):[]})},841997:(x,E,O,B,q,ue)=>{r.jb("BatchNormalization",x,{epsilon:E,momentum:O,spatial:!!q,trainingMode:!!B,format:ue?"NHWC":"NCHW"})},842166:(x,E,O,B,q,ue)=>{r.jb("BatchNormalization",x,{epsilon:E,momentum:O,spatial:!!q,trainingMode:!!B,format:ue?"NHWC":"NCHW"})},842335:(x,E,O)=>{r.jb("CumSum",x,{exclusive:Number(E),reverse:Number(O)})},842432:(x,E,O)=>{r.jb("DequantizeLinear",x,{axis:E,blockSize:O})},842522:(x,E,O,B,q)=>{r.jb("GridSample",x,{align_corners:E,mode:rn(O),padding_mode:rn(B),format:q?"NHWC":"NCHW"})},842692:(x,E,O,B,q)=>{r.jb("GridSample",x,{align_corners:E,mode:rn(O),padding_mode:rn(B),format:q?"NHWC":"NCHW"})},842862:(x,E)=>{r.jb("ScatterND",x,{reduction:rn(E)})},842947:(x,E,O,B,q,ue,Fe,We,tt)=>{r.jb("Attention",x,{numHeads:E,isUnidirectional:O,maskFilterValue:B,scale:q,doRotary:ue,qkvHiddenSizes:Fe?Array.from(z().subarray(Number(We)>>>0,Number(We)+Fe>>>0)):[],pastPresentShareBuffer:!!tt})},843219:x=>{r.jb("BiasAdd",x,void 0)},843274:x=>{r.jb("BiasSplitGelu",x,void 0)},843335:x=>{r.jb("FastGelu",x,void 0)},843391:(x,E,O,B,q,ue,Fe,We,tt,dt,Et,Vt,Yt,zn,js,Zl)=>{r.jb("Conv",x,{format:Vt?"NHWC":"NCHW",auto_pad:E,dilations:O?Array.from(z().subarray(Number(O)>>>0,Number(B)>>>0)):[],group:q,kernel_shape:ue?Array.from(z().subarray(Number(ue)>>>0,Number(Fe)>>>0)):[],pads:We?Array.from(z().subarray(Number(We)>>>0,Number(tt)>>>0)):[],strides:dt?Array.from(z().subarray(Number(dt)>>>0,Number(Et)>>>0)):[],w_is_const:()=>!!N()[Number(Yt)>>>0],activation:rn(zn),activation_params:js?Array.from(me().subarray(Number(js)>>>0,Number(Zl)>>>0)):[]})},843975:x=>{r.jb("Gelu",x,void 0)},844027:(x,E,O,B,q,ue,Fe,We,tt)=>{r.jb("GroupQueryAttention",x,{numHeads:E,kvNumHeads:O,scale:B,softcap:q,doRotary:ue,rotaryInterleaved:Fe,smoothSoftmax:We,localWindowSize:tt})},844244:(x,E,O,B)=>{r.jb("LayerNormalization",x,{axis:E,epsilon:O,simplified:!!B})},844355:(x,E,O,B)=>{r.jb("LayerNormalization",x,{axis:E,epsilon:O,simplified:!!B})},844466:(x,E,O,B,q,ue)=>{r.jb("MatMulNBits",x,{k:E,n:O,accuracyLevel:B,bits:q,blockSize:ue})},844593:(x,E,O,B,q,ue)=>{r.jb("MultiHeadAttention",x,{numHeads:E,isUnidirectional:O,maskFilterValue:B,scale:q,doRotary:ue})},844752:(x,E)=>{r.jb("QuickGelu",x,{alpha:E})},844816:(x,E,O,B,q)=>{r.jb("RotaryEmbedding",x,{interleaved:!!E,numHeads:O,rotaryEmbeddingDim:B,scale:q})},844955:(x,E,O)=>{r.jb("SkipLayerNormalization",x,{epsilon:E,simplified:!!O})},845057:(x,E,O)=>{r.jb("SkipLayerNormalization",x,{epsilon:E,simplified:!!O})},845159:(x,E,O,B)=>{r.jb("GatherBlockQuantized",x,{gatherAxis:E,quantizeAxis:O,blockSize:B})},845280:x=>{r.Zb(x)},845314:(x,E)=>r.ac(Number(x),Number(E),r.Fb.dc,r.Fb.errors)};function ze(x,E,O){return Tr(async()=>{await r.Xb(Number(x),Number(E),Number(O))})}function _e(){return typeof wasmOffsetConverter<"u"}class Le{name="ExitStatus";constructor(E){this.message=`Program terminated with exit(${E})`,this.status=E}}var qe=x=>{x.terminate(),x.onmessage=()=>{}},Ne=[],ot=x=>{Qe.length==0&&(Pn(),kt(Qe[0]));var E=Qe.pop();if(!E)return 6;ct.push(E),wt[x.Ab]=E,E.Ab=x.Ab;var O={Bb:"run",fc:x.ec,Hb:x.Hb,Ab:x.Ab};return E.postMessage(O,x.Mb),0},Ve=0,de=(x,E,...O)=>{for(var B=2*O.length,q=ho(),ue=vi(8*B),Fe=ue>>>3,We=0;We<O.length;We++){var tt=O[We];typeof tt=="bigint"?(K[Fe+2*We]=1n,K[Fe+2*We+1]=tt):(K[Fe+2*We]=0n,$e()[Fe+2*We+1>>>0]=tt)}return x=fa(x,0,B,ue,E),wi(q),x};function ye(x){if(a)return de(0,1,x);if(b=x,!(0<Ve)){for(var E of ct)qe(E);for(E of Qe)qe(E);Qe=[],ct=[],wt={},ne=!0}m(0,new Le(x))}function Pe(x){if(a)return de(1,0,x);ve(x)}var ve=x=>{if(b=x,a)throw Pe(x),"unwind";ye(x)},Qe=[],ct=[],zt=[],wt={},on=x=>{var E=x.Ab;delete wt[E],Qe.push(x),ct.splice(ct.indexOf(x),1),x.Ab=0,co(E)};function Tn(){zt.forEach(x=>x())}var kt=x=>new Promise(E=>{x.onmessage=q=>{var ue=(q=q.data).Bb;if(q.Gb&&q.Gb!=yi()){var Fe=wt[q.Gb];Fe?Fe.postMessage(q,q.Mb):T(`Internal error! Worker sent a message "${ue}" to target pthread ${q.Gb}, but that thread no longer exists!`)}else ue==="checkMailbox"?ae():ue==="spawnThread"?ot(q):ue==="cleanupThread"?on(wt[q.hc]):ue==="loaded"?(x.loaded=!0,E(x)):ue==="alert"?alert(`Thread ${q.ic}: ${q.text}`):q.target==="setimmediate"?x.postMessage(q):ue==="callHandler"?r[q.Qb](...q.args):ue&&T(`worker sent an unknown command ${ue}`)},x.onerror=q=>{throw T(`worker sent an error! ${q.filename}:${q.lineno}: ${q.message}`),q};var O,B=[];for(O of[])r.propertyIsEnumerable(O)&&B.push(O);x.postMessage({Bb:"load",Rb:B,kc:M,lc:v})});function Pn(){var x=new Worker((()=>{let E=URL;return import.meta.url>"file:"&&import.meta.url<"file;"?new E("ort.bundle.min.mjs",import.meta.url):new URL(import.meta.url)})(),{type:"module",workerData:"em-pthread",name:"em-pthread"});Qe.push(x)}var Zn=x=>{Ce();var E=se()[x+52>>>2>>>0];x=se()[x+56>>>2>>>0],_a(E,E-x),wi(E)},Hs=(x,E)=>{Ve=0,x=ya(x,E),0<Ve?b=x:uo(x)};class Ks{constructor(E){this.Ib=E-24}}function xn(x,E,O){var B=new Ks(x>>>=0);throw E>>>=0,O>>>=0,se()[B.Ib+16>>>2>>>0]=0,se()[B.Ib+4>>>2>>>0]=E,se()[B.Ib+8>>>2>>>0]=O,x}function Kn(x,E,O,B){return a?de(2,1,x,E,O,B):Ar(x,E,O,B)}function Ar(x,E,O,B){if(x>>>=0,O>>>=0,B>>>=0,l===void 0)return 6;var q=[];return a&&q.length===0?Kn(x,E>>>=0,O,B):(x={ec:O,Ab:x,Hb:B,Mb:q},a?(x.Bb="spawnThread",postMessage(x,q),0):ot(x))}var Ls=typeof TextDecoder<"u"?new TextDecoder:void 0,vt=(x,E=0,O=NaN)=>{var B=(E>>>=0)+O;for(O=E;x[O]&&!(O>=B);)++O;if(16<O-E&&x.buffer&&Ls)return Ls.decode(x.buffer instanceof ArrayBuffer?x.subarray(E,O):x.slice(E,O));for(B="";E<O;){var q=x[E++];if(128&q){var ue=63&x[E++];if((224&q)==192)B+=String.fromCharCode((31&q)<<6|ue);else{var Fe=63&x[E++];65536>(q=(240&q)==224?(15&q)<<12|ue<<6|Fe:(7&q)<<18|ue<<12|Fe<<6|63&x[E++])?B+=String.fromCharCode(q):(q-=65536,B+=String.fromCharCode(55296|q>>10,56320|1023&q))}}else B+=String.fromCharCode(q)}return B},rn=(x,E)=>(x>>>=0)?vt(oe(),x,E):"";function hs(x,E,O){return a?de(3,1,x,E,O):0}function ms(x,E){if(a)return de(4,1,x,E)}var _r=x=>{for(var E=0,O=0;O<x.length;++O){var B=x.charCodeAt(O);127>=B?E++:2047>=B?E+=2:55296<=B&&57343>=B?(E+=4,++O):E+=3}return E},It=(x,E,O)=>{var B=oe();if(E>>>=0,0<O){var q=E;O=E+O-1;for(var ue=0;ue<x.length;++ue){var Fe=x.charCodeAt(ue);if(55296<=Fe&&57343>=Fe&&(Fe=65536+((1023&Fe)<<10)|1023&x.charCodeAt(++ue)),127>=Fe){if(E>=O)break;B[E++>>>0]=Fe}else{if(2047>=Fe){if(E+1>=O)break;B[E++>>>0]=192|Fe>>6}else{if(65535>=Fe){if(E+2>=O)break;B[E++>>>0]=224|Fe>>12}else{if(E+3>=O)break;B[E++>>>0]=240|Fe>>18,B[E++>>>0]=128|Fe>>12&63}B[E++>>>0]=128|Fe>>6&63}B[E++>>>0]=128|63&Fe}}B[E>>>0]=0,x=E-q}else x=0;return x};function fs(x,E){if(a)return de(5,1,x,E)}function S(x,E,O){if(a)return de(6,1,x,E,O)}function ee(x,E,O){return a?de(7,1,x,E,O):0}function W(x,E){if(a)return de(8,1,x,E)}function J(x,E,O){if(a)return de(9,1,x,E,O)}function re(x,E,O,B){if(a)return de(10,1,x,E,O,B)}function pe(x,E,O,B){if(a)return de(11,1,x,E,O,B)}function Se(x,E,O,B){if(a)return de(12,1,x,E,O,B)}function it(x){if(a)return de(13,1,x)}function gt(x,E){if(a)return de(14,1,x,E)}function Ze(x,E,O){if(a)return de(15,1,x,E,O)}var At,et,Lt=()=>Ee(""),Rt=x=>{for(var E="";oe()[x>>>0];)E+=At[oe()[x++>>>0]];return E},Fn={},Cn={};function bn(x,E,O={}){return(function(B,q,ue={}){var Fe=q.name;if(!B)throw new et(`type "${Fe}" must have a positive integer typeid pointer`);if(Cn.hasOwnProperty(B)){if(ue.Sb)return;throw new et(`Cannot register type '${Fe}' twice`)}Cn[B]=q,Fn.hasOwnProperty(B)&&(q=Fn[B],delete Fn[B],q.forEach(We=>We()))})(x,E,O)}var Bn=(x,E,O)=>{switch(E){case 1:return O?B=>N()[B>>>0]:B=>oe()[B>>>0];case 2:return O?B=>X()[B>>>1>>>0]:B=>D()[B>>>1>>>0];case 4:return O?B=>z()[B>>>2>>>0]:B=>se()[B>>>2>>>0];case 8:return O?B=>K[B>>>3]:B=>U[B>>>3];default:throw new TypeError(`invalid integer width (${E}): ${x}`)}};function gs(x,E,O){O>>>=0,bn(x>>>=0,{name:E=Rt(E>>>0),fromWireType:B=>B,toWireType:function(B,q){if(typeof q!="bigint"&&typeof q!="number")throw q=q===null?"null":(B=typeof q)=="object"||B==="array"||B==="function"?q.toString():""+q,new TypeError(`Cannot convert "${q}" to ${this.name}`);return typeof q=="number"&&(q=BigInt(q)),q},Cb:tr,readValueFromPointer:Bn(E,O,E.indexOf("u")==-1),Db:null})}var tr=8;function _s(x,E,O,B){bn(x>>>=0,{name:E=Rt(E>>>0),fromWireType:function(q){return!!q},toWireType:function(q,ue){return ue?O:B},Cb:tr,readValueFromPointer:function(q){return this.fromWireType(oe()[q>>>0])},Db:null})}var nr=[],rr=[];function Dr(x){9<(x>>>=0)&&--rr[x+1]==0&&(rr[x]=void 0,nr.push(x))}var On=x=>{if(!x)throw new et("Cannot use deleted val. handle = "+x);return rr[x]},wn=x=>{switch(x){case void 0:return 2;case null:return 4;case!0:return 6;case!1:return 8;default:let E=nr.pop()||rr.length;return rr[E]=x,rr[E+1]=1,E}};function yr(x){return this.fromWireType(se()[x>>>2>>>0])}var $r={name:"emscripten::val",fromWireType:x=>{var E=On(x);return Dr(x),E},toWireType:(x,E)=>wn(E),Cb:tr,readValueFromPointer:yr,Db:null};function Ds(x){return bn(x>>>0,$r)}var ys=(x,E)=>{switch(E){case 4:return function(O){return this.fromWireType(me()[O>>>2>>>0])};case 8:return function(O){return this.fromWireType($e()[O>>>3>>>0])};default:throw new TypeError(`invalid float width (${E}): ${x}`)}};function xs(x,E,O){O>>>=0,bn(x>>>=0,{name:E=Rt(E>>>0),fromWireType:B=>B,toWireType:(B,q)=>q,Cb:tr,readValueFromPointer:ys(E,O),Db:null})}function bs(x,E,O,B,q){if(x>>>=0,O>>>=0,E=Rt(E>>>0),q===-1&&(q=4294967295),q=We=>We,B===0){var ue=32-8*O;q=We=>We<<ue>>>ue}var Fe=E.includes("unsigned")?function(We,tt){return tt>>>0}:function(We,tt){return tt};bn(x,{name:E,fromWireType:q,toWireType:Fe,Cb:tr,readValueFromPointer:Bn(E,O,B!==0),Db:null})}function Fr(x,E,O){function B(ue){var Fe=se()[ue>>>2>>>0];return ue=se()[ue+4>>>2>>>0],new q(N().buffer,ue,Fe)}var q=[Int8Array,Uint8Array,Int16Array,Uint16Array,Int32Array,Uint32Array,Float32Array,Float64Array,BigInt64Array,BigUint64Array][E];bn(x>>>=0,{name:O=Rt(O>>>0),fromWireType:B,Cb:tr,readValueFromPointer:B},{Sb:!0})}function or(x,E){bn(x>>>=0,{name:E=Rt(E>>>0),fromWireType:function(O){for(var B,q=se()[O>>>2>>>0],ue=O+4,Fe=ue,We=0;We<=q;++We){var tt=ue+We;We!=q&&oe()[tt>>>0]!=0||(Fe=rn(Fe,tt-Fe),B===void 0?B=Fe:(B+="\0",B+=Fe),Fe=tt+1)}return Pr(O),B},toWireType:function(O,B){B instanceof ArrayBuffer&&(B=new Uint8Array(B));var q=typeof B=="string";if(!(q||B instanceof Uint8Array||B instanceof Uint8ClampedArray||B instanceof Int8Array))throw new et("Cannot pass non-string to std::string");var ue=q?_r(B):B.length,Fe=xi(4+ue+1),We=Fe+4;if(se()[Fe>>>2>>>0]=ue,q)It(B,We,ue+1);else if(q)for(q=0;q<ue;++q){var tt=B.charCodeAt(q);if(255<tt)throw Pr(Fe),new et("String has UTF-16 code units that do not fit in 8 bits");oe()[We+q>>>0]=tt}else for(q=0;q<ue;++q)oe()[We+q>>>0]=B[q];return O!==null&&O.push(Pr,Fe),Fe},Cb:tr,readValueFromPointer:yr,Db(O){Pr(O)}})}var Ue=typeof TextDecoder<"u"?new TextDecoder("utf-16le"):void 0,Ke=(x,E)=>{for(var O=x>>1,B=O+E/2;!(O>=B)&&D()[O>>>0];)++O;if(32<(O<<=1)-x&&Ue)return Ue.decode(oe().slice(x,O));for(O="",B=0;!(B>=E/2);++B){var q=X()[x+2*B>>>1>>>0];if(q==0)break;O+=String.fromCharCode(q)}return O},ut=(x,E,O)=>{if(O??=2147483647,2>O)return 0;var B=E;O=(O-=2)<2*x.length?O/2:x.length;for(var q=0;q<O;++q){var ue=x.charCodeAt(q);X()[E>>>1>>>0]=ue,E+=2}return X()[E>>>1>>>0]=0,E-B},tn=x=>2*x.length,$s=(x,E)=>{for(var O=0,B="";!(O>=E/4);){var q=z()[x+4*O>>>2>>>0];if(q==0)break;++O,65536<=q?(q-=65536,B+=String.fromCharCode(55296|q>>10,56320|1023&q)):B+=String.fromCharCode(q)}return B},ws=(x,E,O)=>{if(E>>>=0,O??=2147483647,4>O)return 0;var B=E;O=B+O-4;for(var q=0;q<x.length;++q){var ue=x.charCodeAt(q);if(55296<=ue&&57343>=ue&&(ue=65536+((1023&ue)<<10)|1023&x.charCodeAt(++q)),z()[E>>>2>>>0]=ue,(E+=4)+4>O)break}return z()[E>>>2>>>0]=0,E-B},Fs=x=>{for(var E=0,O=0;O<x.length;++O){var B=x.charCodeAt(O);55296<=B&&57343>=B&&++O,E+=4}return E};function Zi(x,E,O){if(x>>>=0,E>>>=0,O=Rt(O>>>=0),E===2)var B=Ke,q=ut,ue=tn,Fe=We=>D()[We>>>1>>>0];else E===4&&(B=$s,q=ws,ue=Fs,Fe=We=>se()[We>>>2>>>0]);bn(x,{name:O,fromWireType:We=>{for(var tt,dt=se()[We>>>2>>>0],Et=We+4,Vt=0;Vt<=dt;++Vt){var Yt=We+4+Vt*E;Vt!=dt&&Fe(Yt)!=0||(Et=B(Et,Yt-Et),tt===void 0?tt=Et:(tt+="\0",tt+=Et),Et=Yt+E)}return Pr(We),tt},toWireType:(We,tt)=>{if(typeof tt!="string")throw new et(`Cannot pass non-string to C++ string type ${O}`);var dt=ue(tt),Et=xi(4+dt+E);return se()[Et>>>2>>>0]=dt/E,q(tt,Et+4,dt+E),We!==null&&We.push(Pr,Et),Et},Cb:tr,readValueFromPointer:yr,Db(We){Pr(We)}})}function he(x,E){bn(x>>>=0,{Tb:!0,name:E=Rt(E>>>0),Cb:0,fromWireType:()=>{},toWireType:()=>{}})}function $(x){bi(x>>>0,!o,1,!i,131072,!1),Tn()}var Q=x=>{if(!ne)try{if(x(),!(0<Ve))try{a?uo(b):ve(b)}catch(E){E instanceof Le||E=="unwind"||m(0,E)}}catch(E){E instanceof Le||E=="unwind"||m(0,E)}};function ie(x){x>>>=0,typeof Atomics.jc=="function"&&(Atomics.jc(z(),x>>>2,x).value.then(ae),x+=128,Atomics.store(z(),x>>>2,1))}var ae=()=>{var x=yi();x&&(ie(x),Q(po))};function we(x,E){(x>>>=0)==E>>>0?setTimeout(ae):a?postMessage({Gb:x,Bb:"checkMailbox"}):(x=wt[x])&&x.postMessage({Bb:"checkMailbox"})}var je=[];function nt(x,E,O,B,q){for(E>>>=0,B/=2,je.length=B,O=q>>>0>>>3,q=0;q<B;q++)je[q]=K[O+2*q]?K[O+2*q+1]:$e()[O+2*q+1>>>0];return(E?xe[E]:Jl[x])(...je)}var rt=()=>{Ve=0};function st(x){x>>>=0,a?postMessage({Bb:"cleanupThread",hc:x}):on(wt[x])}function Je(x){}var jt=(x,E)=>{var O=Cn[x];if(O===void 0)throw x=ha(x),O=Rt(x),Pr(x),new et(`${E} has unknown type ${O}`);return O},Dt=(x,E,O)=>{var B=[];return x=x.toWireType(B,O),B.length&&(se()[E>>>2>>>0]=wn(B)),x};function fn(x,E,O){return E>>>=0,O>>>=0,x=On(x>>>0),E=jt(E,"emval::as"),Dt(E,O,x)}function jn(x,E){return E>>>=0,x=On(x>>>0),(E=jt(E,"emval::as")).toWireType(null,x)}var an=x=>{try{x()}catch(E){Ee(E)}},gn=0,nn=null,xr=0,Or=[],Gn={},qr={},un=0,Sn=null,qn=[];function Tr(x){return(function(E){if(!ne){if(gn===0){var O=!1,B=!1;E((q=0)=>{if(!ne&&(xr=q,O=!0,B)){gn=2,an(()=>ba(nn)),typeof MainLoop<"u"&&MainLoop.Pb&&MainLoop.resume(),q=!1;try{var ue=(function(){var tt=z()[nn+8>>>2>>>0];return tt=_t[qr[tt]],--Ve,tt()})()}catch(tt){ue=tt,q=!0}var Fe=!1;if(!nn){var We=Sn;We&&(Sn=null,(q?We.reject:We.resolve)(ue),Fe=!0)}if(q&&!Fe)throw ue}}),B=!0,O||(gn=1,nn=(function(){var q=xi(65548),ue=q+12;se()[q>>>2>>>0]=ue,se()[q+4>>>2>>>0]=ue+65536,ue=Or[0];var Fe=Gn[ue];return Fe===void 0&&(Fe=un++,Gn[ue]=Fe,qr[Fe]=ue),ue=Fe,z()[q+8>>>2>>>0]=ue,q})(),typeof MainLoop<"u"&&MainLoop.Pb&&MainLoop.pause(),an(()=>mo(nn)))}else gn===2?(gn=0,an(fo),Pr(nn),nn=null,qn.forEach(Q)):Ee(`invalid state: ${gn}`);return xr}})(E=>{x().then(E)})}function Qr(x){return x>>>=0,Tr(async()=>{var E=await On(x);return wn(E)})}var Rn=[];function jr(x,E,O,B){return O>>>=0,B>>>=0,(x=Rn[x>>>0])(null,E=On(E>>>0),O,B)}var Nn={},dn=x=>{var E=Nn[x];return E===void 0?Rt(x):E};function Ln(x,E,O,B,q){return O>>>=0,B>>>=0,q>>>=0,(x=Rn[x>>>0])(E=On(E>>>0),E[O=dn(O)],B,q)}var vn=()=>typeof globalThis=="object"?globalThis:Function("return this")();function dr(x){return(x>>>=0)==0?wn(vn()):(x=dn(x),wn(vn()[x]))}var qs=x=>{var E=Rn.length;return Rn.push(x),E},vs=(x,E)=>{for(var O=Array(x),B=0;B<x;++B)O[B]=jt(se()[E+4*B>>>2>>>0],"parameter "+B);return O},ea=(x,E)=>Object.defineProperty(E,"name",{value:x});function kl(x,E,O){var B=(E=vs(x,E>>>0)).shift();x--;var q=`return function (obj, func, destructorsRef, args) {
`,ue=0,Fe=[];O===0&&Fe.push("obj");for(var We=["retType"],tt=[B],dt=0;dt<x;++dt)Fe.push("arg"+dt),We.push("argType"+dt),tt.push(E[dt]),q+=`  var arg${dt} = argType${dt}.readValueFromPointer(args${ue?"+"+ue:""});
`,ue+=E[dt].Cb;return q+=`  var rv = ${O===1?"new func":"func.call"}(${Fe.join(", ")});
`,B.Tb||(We.push("emval_returnValue"),tt.push(Dt),q+=`  return emval_returnValue(retType, destructorsRef, rv);
`),We.push(q+`};
`),x=(function(Et){var Vt=Function;if(!(Vt instanceof Function))throw new TypeError(`new_ called with constructor type ${typeof Vt} which is not a function`);var Yt=ea(Vt.name||"unknownFunctionName",function(){});return Yt.prototype=Vt.prototype,Yt=new Yt,(Et=Vt.apply(Yt,Et))instanceof Object?Et:Yt})(We)(...tt),O=`methodCaller<(${E.map(Et=>Et.name).join(", ")}) => ${B.name}>`,qs(ea(O,x))}function El(x){return x=dn(x>>>0),wn(r[x])}function Sl(x,E){return E>>>=0,x=On(x>>>0),E=On(E),wn(x[E])}function Ll(x){9<(x>>>=0)&&(rr[x+1]+=1)}function Qs(){return wn([])}function Dl(x){x=On(x>>>0);for(var E=Array(x.length),O=0;O<x.length;O++)E[O]=x[O];return wn(E)}function $l(x){return wn(dn(x>>>0))}function Fl(){return wn({})}function Ol(x){for(var E=On(x>>>=0);E.length;){var O=E.pop();E.pop()(O)}Dr(x)}function jl(x,E,O){E>>>=0,O>>>=0,x=On(x>>>0),E=On(E),O=On(O),x[E]=O}function Xs(x,E){return E>>>=0,x=(x=jt(x>>>0,"_emval_take_value")).readValueFromPointer(E),wn(x)}function Rl(x,E){x=-9007199254740992>x||9007199254740992<x?NaN:Number(x),E>>>=0,x=new Date(1e3*x),z()[E>>>2>>>0]=x.getUTCSeconds(),z()[E+4>>>2>>>0]=x.getUTCMinutes(),z()[E+8>>>2>>>0]=x.getUTCHours(),z()[E+12>>>2>>>0]=x.getUTCDate(),z()[E+16>>>2>>>0]=x.getUTCMonth(),z()[E+20>>>2>>>0]=x.getUTCFullYear()-1900,z()[E+24>>>2>>>0]=x.getUTCDay(),x=(x.getTime()-Date.UTC(x.getUTCFullYear(),0,1,0,0,0,0))/864e5|0,z()[E+28>>>2>>>0]=x}var ta=x=>x%4==0&&(x%100!=0||x%400==0),na=[0,31,60,91,121,152,182,213,244,274,305,335],ra=[0,31,59,90,120,151,181,212,243,273,304,334];function Nl(x,E){x=-9007199254740992>x||9007199254740992<x?NaN:Number(x),E>>>=0,x=new Date(1e3*x),z()[E>>>2>>>0]=x.getSeconds(),z()[E+4>>>2>>>0]=x.getMinutes(),z()[E+8>>>2>>>0]=x.getHours(),z()[E+12>>>2>>>0]=x.getDate(),z()[E+16>>>2>>>0]=x.getMonth(),z()[E+20>>>2>>>0]=x.getFullYear()-1900,z()[E+24>>>2>>>0]=x.getDay();var O=(ta(x.getFullYear())?na:ra)[x.getMonth()]+x.getDate()-1|0;z()[E+28>>>2>>>0]=O,z()[E+36>>>2>>>0]=-60*x.getTimezoneOffset(),O=new Date(x.getFullYear(),6,1).getTimezoneOffset();var B=new Date(x.getFullYear(),0,1).getTimezoneOffset();x=0|(O!=B&&x.getTimezoneOffset()==Math.min(B,O)),z()[E+32>>>2>>>0]=x}function sa(x){x>>>=0;var E=new Date(z()[x+20>>>2>>>0]+1900,z()[x+16>>>2>>>0],z()[x+12>>>2>>>0],z()[x+8>>>2>>>0],z()[x+4>>>2>>>0],z()[x>>>2>>>0],0),O=z()[x+32>>>2>>>0],B=E.getTimezoneOffset(),q=new Date(E.getFullYear(),6,1).getTimezoneOffset(),ue=new Date(E.getFullYear(),0,1).getTimezoneOffset(),Fe=Math.min(ue,q);return 0>O?z()[x+32>>>2>>>0]=+(q!=ue&&Fe==B):0<O!=(Fe==B)&&(q=Math.max(ue,q),E.setTime(E.getTime()+6e4*((0<O?Fe:q)-B))),z()[x+24>>>2>>>0]=E.getDay(),O=(ta(E.getFullYear())?na:ra)[E.getMonth()]+E.getDate()-1|0,z()[x+28>>>2>>>0]=O,z()[x>>>2>>>0]=E.getSeconds(),z()[x+4>>>2>>>0]=E.getMinutes(),z()[x+8>>>2>>>0]=E.getHours(),z()[x+12>>>2>>>0]=E.getDate(),z()[x+16>>>2>>>0]=E.getMonth(),z()[x+20>>>2>>>0]=E.getYear(),x=E.getTime(),BigInt(isNaN(x)?-1:x/1e3)}function ia(x,E,O,B,q,ue,Fe){return a?de(16,1,x,E,O,B,q,ue,Fe):-52}function oa(x,E,O,B,q,ue){if(a)return de(17,1,x,E,O,B,q,ue)}var Os={},zl=()=>performance.timeOrigin+performance.now();function eo(x,E){if(a)return de(18,1,x,E);if(Os[x]&&(clearTimeout(Os[x].id),delete Os[x]),!E)return 0;var O=setTimeout(()=>{delete Os[x],Q(()=>ga(x,performance.timeOrigin+performance.now()))},E);return Os[x]={id:O,qc:E},0}function Bl(x,E,O,B){x>>>=0,E>>>=0,O>>>=0,B>>>=0;var q=new Date().getFullYear(),ue=new Date(q,0,1).getTimezoneOffset();q=new Date(q,6,1).getTimezoneOffset();var Fe=Math.max(ue,q);se()[x>>>2>>>0]=60*Fe,z()[E>>>2>>>0]=+(ue!=q),x=(E=We=>{var tt=Math.abs(We);return`UTC${0<=We?"-":"+"}${String(Math.floor(tt/60)).padStart(2,"0")}${String(tt%60).padStart(2,"0")}`})(ue),E=E(q),q<ue?(It(x,O,17),It(E,B,17)):(It(x,B,17),It(E,O,17))}var aa=()=>Date.now();function Yu(x,E,O){return 0<=x&&3>=x?(x===0?x=Date.now():x=performance.timeOrigin+performance.now(),K[O>>>0>>>3]=BigInt(Math.round(1e6*x)),0):28}var to=[],no=(x,E)=>{to.length=0;for(var O;O=oe()[x++>>>0];){var B=O!=105;E+=(B&=O!=112)&&E%8?4:0,to.push(O==112?se()[E>>>2>>>0]:O==106?K[E>>>3]:O==105?z()[E>>>2>>>0]:$e()[E>>>3>>>0]),E+=B?8:4}return to};function la(x,E,O){return x>>>=0,E=no(E>>>0,O>>>0),xe[x](...E)}function ro(x,E,O){return x>>>=0,E=no(E>>>0,O>>>0),xe[x](...E)}var Gl=()=>{};function Vl(x,E){return T(rn(x>>>0,E>>>0))}var Ul=()=>{throw Ve+=1,"unwind"};function Wl(){return 4294901760}var Hl=()=>navigator.hardwareConcurrency;function Kl(){return Ee("Cannot use emscripten_pc_get_function without -sUSE_OFFSET_CONVERTER"),0}function ca(x){x>>>=0;var E=oe().length;if(x<=E||4294901760<x)return!1;for(var O=1;4>=O;O*=2){var B=E*(1+.2/O);B=Math.min(B,x+100663296);e:{B=(Math.min(4294901760,65536*Math.ceil(Math.max(x,B)/65536))-M.buffer.byteLength+65535)/65536|0;try{M.grow(B),Ce();var q=1;break e}catch{}q=void 0}if(q)return!0}return!1}var mi=()=>(Ee("Cannot use convertFrameToPC (needed by __builtin_return_address) without -sUSE_OFFSET_CONVERTER"),0),Ys={},so=x=>{x.forEach(E=>{mi()})};function ql(){var x=Error().stack.toString().split(`
`);return x[0]=="Error"&&x.shift(),so(x),Ys.Lb=mi(),Ys.cc=x,Ys.Lb}function io(x,E,O){if(x>>>=0,E>>>=0,Ys.Lb==x)var B=Ys.cc;else(B=Error().stack.toString().split(`
`))[0]=="Error"&&B.shift(),so(B);for(var q=3;B[q]&&mi()!=x;)++q;for(x=0;x<O&&B[x+q];++x)z()[E+4*x>>>2>>>0]=mi();return x}var fi,oo={},ao=()=>{if(!fi){var x,E={USER:"web_user",LOGNAME:"web_user",PATH:"/",PWD:"/",HOME:"/home/web_user",LANG:(typeof navigator=="object"&&navigator.languages&&navigator.languages[0]||"C").replace("-","_")+".UTF-8",_:"./this.program"};for(x in oo)oo[x]===void 0?delete E[x]:E[x]=oo[x];var O=[];for(x in E)O.push(`${x}=${E[x]}`);fi=O}return fi};function Ms(x,E){if(a)return de(19,1,x,E);x>>>=0,E>>>=0;var O=0;return ao().forEach((B,q)=>{var ue=E+O;for(q=se()[x+4*q>>>2>>>0]=ue,ue=0;ue<B.length;++ue)N()[q++>>>0]=B.charCodeAt(ue);N()[q>>>0]=0,O+=B.length+1}),0}function ua(x,E){if(a)return de(20,1,x,E);x>>>=0,E>>>=0;var O=ao();se()[x>>>2>>>0]=O.length;var B=0;return O.forEach(q=>B+=q.length+1),se()[E>>>2>>>0]=B,0}function Ql(x){return a?de(21,1,x):52}function da(x,E,O,B){return a?de(22,1,x,E,O,B):52}function Xl(x,E,O,B){return a?de(23,1,x,E,O,B):70}var Yl=[null,[],[]];function lo(x,E,O,B){if(a)return de(24,1,x,E,O,B);E>>>=0,O>>>=0,B>>>=0;for(var q=0,ue=0;ue<O;ue++){var Fe=se()[E>>>2>>>0],We=se()[E+4>>>2>>>0];E+=8;for(var tt=0;tt<We;tt++){var dt=oe()[Fe+tt>>>0],Et=Yl[x];dt===0||dt===10?((x===1?_:T)(vt(Et)),Et.length=0):Et.push(dt)}q+=We}return se()[B>>>2>>>0]=q,0}a||(function(){for(var x=r.numThreads-1;x--;)Pn();Ne.unshift(()=>{fe++,(function(E){a?E():Promise.all(Qe.map(kt)).then(E)})(()=>Ie())})})();for(var pa=Array(256),gi=0;256>gi;++gi)pa[gi]=String.fromCharCode(gi);At=pa,et=r.BindingError=class extends Error{constructor(x){super(x),this.name="BindingError"}},r.InternalError=class extends Error{constructor(x){super(x),this.name="InternalError"}},rr.push(0,1,void 0,1,null,1,!0,1,!1,1),r.count_emval_handles=()=>rr.length/2-5-nr.length;var _t,Jl=[ye,Pe,Kn,hs,ms,fs,S,ee,W,J,re,pe,Se,it,gt,Ze,ia,oa,eo,Ms,ua,Ql,da,Xl,lo];(async function(){function x(B,q){return _t=B.exports,_t=(function(){var ue=_t,Fe={};for(let[We,tt]of Object.entries(ue))Fe[We]=typeof tt=="function"?(...dt)=>{Or.push(We);try{return tt(...dt)}finally{ne||(Or.pop(),nn&&gn===1&&Or.length===0&&(gn=0,Ve+=1,an(xa),typeof Fibers<"u"&&Fibers.rc()))}}:tt;return Fe})(),_t=(function(){var ue=_t,Fe=tt=>dt=>tt(dt)>>>0,We=tt=>()=>tt()>>>0;return(ue=Object.assign({},ue)).Da=Fe(ue.Da),ue.fb=We(ue.fb),ue.hb=Fe(ue.hb),ue.tb=Fe(ue.tb),ue.ub=We(ue.ub),ue.__cxa_get_exception_ptr=Fe(ue.__cxa_get_exception_ptr),ue})(),zt.push(_t.ib),v=q,Ie(),_t}fe++;var E=De();if(r.instantiateWasm)return new Promise(B=>{r.instantiateWasm(E,(q,ue)=>{x(q,ue),B(q.exports)})});if(a)return new Promise(B=>{ke=q=>{var ue=new WebAssembly.Instance(q,De());B(x(ue,q))}});V??=r.locateFile?r.locateFile?r.locateFile("ort-wasm-simd-threaded.jsep.wasm",g):g+"ort-wasm-simd-threaded.jsep.wasm":new URL(""+new URL("ort-wasm-simd-threaded.jsep-B0T3yYHD.wasm",import.meta.url).href,import.meta.url).href;try{var O=await(async function(B){var q=V;if(!te&&typeof WebAssembly.instantiateStreaming=="function"&&!le(q))try{var ue=fetch(q,{credentials:"same-origin"});return await WebAssembly.instantiateStreaming(ue,B)}catch(Fe){T(`wasm streaming compile failed: ${Fe}`),T("falling back to ArrayBuffer instantiation")}return(async function(Fe,We){try{var tt=await(async function(dt){if(!te)try{var Et=await u(dt);return new Uint8Array(Et)}catch{}if(dt==V&&te)dt=new Uint8Array(te);else{if(!p)throw"both async and sync fetching of the wasm failed";dt=p(dt)}return dt})(Fe);return await WebAssembly.instantiate(tt,We)}catch(dt){T(`failed to asynchronously prepare wasm: ${dt}`),Ee(dt)}})(q,B)})(E);return x(O.instance,O.module)}catch(B){return n(B),Promise.reject(B)}})();var ha=x=>(ha=_t.Da)(x),_i=()=>(_i=_t.Ea)();r._OrtInit=(x,E)=>(r._OrtInit=_t.Fa)(x,E),r._OrtGetLastError=(x,E)=>(r._OrtGetLastError=_t.Ga)(x,E),r._OrtCreateSessionOptions=(x,E,O,B,q,ue,Fe,We,tt,dt)=>(r._OrtCreateSessionOptions=_t.Ha)(x,E,O,B,q,ue,Fe,We,tt,dt),r._OrtAppendExecutionProvider=(x,E,O,B,q)=>(r._OrtAppendExecutionProvider=_t.Ia)(x,E,O,B,q),r._OrtAddFreeDimensionOverride=(x,E,O)=>(r._OrtAddFreeDimensionOverride=_t.Ja)(x,E,O),r._OrtAddSessionConfigEntry=(x,E,O)=>(r._OrtAddSessionConfigEntry=_t.Ka)(x,E,O),r._OrtReleaseSessionOptions=x=>(r._OrtReleaseSessionOptions=_t.La)(x),r._OrtCreateSession=(x,E,O)=>(r._OrtCreateSession=_t.Ma)(x,E,O),r._OrtReleaseSession=x=>(r._OrtReleaseSession=_t.Na)(x),r._OrtGetInputOutputCount=(x,E,O)=>(r._OrtGetInputOutputCount=_t.Oa)(x,E,O),r._OrtGetInputOutputMetadata=(x,E,O,B)=>(r._OrtGetInputOutputMetadata=_t.Pa)(x,E,O,B),r._OrtFree=x=>(r._OrtFree=_t.Qa)(x),r._OrtCreateTensor=(x,E,O,B,q,ue)=>(r._OrtCreateTensor=_t.Ra)(x,E,O,B,q,ue),r._OrtGetTensorData=(x,E,O,B,q)=>(r._OrtGetTensorData=_t.Sa)(x,E,O,B,q),r._OrtReleaseTensor=x=>(r._OrtReleaseTensor=_t.Ta)(x),r._OrtCreateRunOptions=(x,E,O,B)=>(r._OrtCreateRunOptions=_t.Ua)(x,E,O,B),r._OrtAddRunConfigEntry=(x,E,O)=>(r._OrtAddRunConfigEntry=_t.Va)(x,E,O),r._OrtReleaseRunOptions=x=>(r._OrtReleaseRunOptions=_t.Wa)(x),r._OrtCreateBinding=x=>(r._OrtCreateBinding=_t.Xa)(x),r._OrtBindInput=(x,E,O)=>(r._OrtBindInput=_t.Ya)(x,E,O),r._OrtBindOutput=(x,E,O,B)=>(r._OrtBindOutput=_t.Za)(x,E,O,B),r._OrtClearBoundOutputs=x=>(r._OrtClearBoundOutputs=_t._a)(x),r._OrtReleaseBinding=x=>(r._OrtReleaseBinding=_t.$a)(x),r._OrtRunWithBinding=(x,E,O,B,q)=>(r._OrtRunWithBinding=_t.ab)(x,E,O,B,q),r._OrtRun=(x,E,O,B,q,ue,Fe,We)=>(r._OrtRun=_t.bb)(x,E,O,B,q,ue,Fe,We),r._OrtEndProfiling=x=>(r._OrtEndProfiling=_t.cb)(x),r._JsepOutput=(x,E,O)=>(r._JsepOutput=_t.db)(x,E,O),r._JsepGetNodeName=x=>(r._JsepGetNodeName=_t.eb)(x);var yi=()=>(yi=_t.fb)(),Pr=r._free=x=>(Pr=r._free=_t.gb)(x),xi=r._malloc=x=>(xi=r._malloc=_t.hb)(x),bi=(x,E,O,B,q,ue)=>(bi=_t.kb)(x,E,O,B,q,ue),ma=()=>(ma=_t.lb)(),fa=(x,E,O,B,q)=>(fa=_t.mb)(x,E,O,B,q),co=x=>(co=_t.nb)(x),uo=x=>(uo=_t.ob)(x),ga=(x,E)=>(ga=_t.pb)(x,E),po=()=>(po=_t.qb)(),_a=(x,E)=>(_a=_t.rb)(x,E),wi=x=>(wi=_t.sb)(x),vi=x=>(vi=_t.tb)(x),ho=()=>(ho=_t.ub)(),ya=r.dynCall_ii=(x,E)=>(ya=r.dynCall_ii=_t.vb)(x,E),mo=x=>(mo=_t.wb)(x),xa=()=>(xa=_t.xb)(),ba=x=>(ba=_t.yb)(x),fo=()=>(fo=_t.zb)();return r.stackSave=()=>ho(),r.stackRestore=x=>wi(x),r.stackAlloc=x=>vi(x),r.setValue=function(x,E,O="i8"){switch(O.endsWith("*")&&(O="*"),O){case"i1":case"i8":N()[x>>>0]=E;break;case"i16":X()[x>>>1>>>0]=E;break;case"i32":z()[x>>>2>>>0]=E;break;case"i64":K[x>>>3]=BigInt(E);break;case"float":me()[x>>>2>>>0]=E;break;case"double":$e()[x>>>3>>>0]=E;break;case"*":se()[x>>>2>>>0]=E;break;default:Ee(`invalid type for setValue: ${O}`)}},r.getValue=function(x,E="i8"){switch(E.endsWith("*")&&(E="*"),E){case"i1":case"i8":return N()[x>>>0];case"i16":return X()[x>>>1>>>0];case"i32":return z()[x>>>2>>>0];case"i64":return K[x>>>3];case"float":return me()[x>>>2>>>0];case"double":return $e()[x>>>3>>>0];case"*":return se()[x>>>2>>>0];default:Ee(`invalid type for getValue: ${E}`)}},r.UTF8ToString=rn,r.stringToUTF8=It,r.lengthBytesUTF8=_r,(function x(){if(0<fe)Te=x;else if(a)t(r),Z();else{for(;0<Ne.length;)Ne.shift()(r);0<fe?Te=x:(r.calledRun=!0,ne||(Z(),t(r)))}})(),r.PTR_SIZE=4,s}),NI=dh,bv=globalThis.self?.name?.startsWith("em-pthread"),bv&&dh()}),ph,qm,wv,Ir,zI,kc,vv,Mv,hh,Av,mh,BI,fh,GI,xg=He(()=>{yg(),ph=typeof location>"u"?void 0:location.origin,qm=import.meta.url>"file:"&&import.meta.url<"file;",wv=()=>{{if(qm){let e=URL;return new URL(new e("ort.bundle.min.mjs",import.meta.url).href,ph).href}return import.meta.url}},Ir=wv(),zI=()=>{if(Ir&&!Ir.startsWith("blob:"))return Ir.substring(0,Ir.lastIndexOf("/")+1)},kc=(e,t)=>{try{let n=t??Ir;return(n?new URL(e,n):new URL(e)).origin===ph}catch{return!1}},vv=(e,t)=>{let n=t??Ir;try{return(n?new URL(e,n):new URL(e)).href}catch{return}},Mv=(e,t)=>`${t??"./"}${e}`,hh=async e=>{let t=await(await fetch(e,{credentials:"same-origin"})).blob();return URL.createObjectURL(t)},Av=async e=>(await import(e)).default,mh=(AG(),ml(OI)).default,BI=async()=>{if(!Ir)throw new Error("Failed to load proxy worker: cannot determine the script source URL.");if(kc(Ir))return[void 0,mh()];let e=await hh(Ir);return[e,mh(e)]},fh=(TG(),ml(RI)).default,GI=async(e,t,n)=>{if(!e&&!t&&fh&&Ir&&kc(Ir))return[void 0,fh];{let r="ort-wasm-simd-threaded.jsep.mjs",s=e??vv(r,t),i=n&&s&&!kc(s,t),o=i?await hh(s):s??Mv(r,t);return[i?o:void 0,await Av(o)]}}}),gh,Ec,Ea,_h,Tv,Pv,Cv,bg,sn,Yi=He(()=>{xg(),Ec=!1,Ea=!1,_h=!1,Tv=()=>{if(typeof SharedArrayBuffer>"u")return!1;try{return typeof MessageChannel<"u"&&new MessageChannel().port1.postMessage(new SharedArrayBuffer(1)),WebAssembly.validate(new Uint8Array([0,97,115,109,1,0,0,0,1,4,1,96,0,0,3,2,1,0,5,4,1,3,1,1,10,11,1,9,0,65,0,254,16,2,0,26,11]))}catch{return!1}},Pv=()=>{try{return WebAssembly.validate(new Uint8Array([0,97,115,109,1,0,0,0,1,4,1,96,0,0,3,2,1,0,10,30,1,28,0,65,0,253,15,253,12,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,253,186,1,26,11]))}catch{return!1}},Cv=()=>{try{return WebAssembly.validate(new Uint8Array([0,97,115,109,1,0,0,0,1,5,1,96,0,1,123,3,2,1,0,10,19,1,17,0,65,1,253,15,65,2,253,15,65,3,253,15,253,147,2,11]))}catch{return!1}},bg=async e=>{if(Ec)return Promise.resolve();if(Ea)throw new Error("multiple calls to 'initializeWebAssembly()' detected.");if(_h)throw new Error("previous call to 'initializeWebAssembly()' failed.");Ea=!0;let t=e.initTimeout,n=e.numThreads;if(e.simd!==!1){if(e.simd==="relaxed"){if(!Cv())throw new Error("Relaxed WebAssembly SIMD is not supported in the current environment.")}else if(!Pv())throw new Error("WebAssembly SIMD is not supported in the current environment.")}let r=Tv();n>1&&!r&&(typeof self<"u"&&!self.crossOriginIsolated&&console.warn("env.wasm.numThreads is set to "+n+", but this will not work unless you enable crossOriginIsolated mode. See https://web.dev/cross-origin-isolation-guide/ for more info."),console.warn("WebAssembly multi-threading is not supported in the current environment. Falling back to single-threading."),e.numThreads=n=1);let s=e.wasmPaths,i=typeof s=="string"?s:void 0,o=s?.mjs,a=o?.href??o,l=s?.wasm,c=l?.href??l,d=e.wasmBinary,[u,p]=await GI(a,i,n>1),h=!1,m=[];if(t>0&&m.push(new Promise(g=>{setTimeout(()=>{h=!0,g()},t)})),m.push(new Promise((g,I)=>{let f={numThreads:n};if(d)f.wasmBinary=d;else if(c||i)f.locateFile=_=>c??i+_;else if(a&&a.indexOf("blob:")!==0)f.locateFile=_=>new URL(_,a).href;else if(u){let _=zI();_&&(f.locateFile=T=>_+T)}p(f).then(_=>{Ea=!1,Ec=!0,gh=_,g(),u&&URL.revokeObjectURL(u)},_=>{Ea=!1,_h=!0,I(_)})})),await Promise.race(m),h)throw new Error(`WebAssembly backend initializing failed due to timeout: ${t}ms`)},sn=()=>{if(Ec&&gh)return gh;throw new Error("WebAssembly is not initialized yet.")}}),is,yu,en,wg=He(()=>{Yi(),is=(e,t)=>{let n=sn(),r=n.lengthBytesUTF8(e)+1,s=n._malloc(r);return n.stringToUTF8(e,s,r),t.push(s),s},yu=(e,t,n,r)=>{if(typeof e=="object"&&e!==null){if(n.has(e))throw new Error("Circular reference in options");n.add(e)}Object.entries(e).forEach(([s,i])=>{let o=t?t+s:s;if(typeof i=="object")yu(i,o+".",n,r);else if(typeof i=="string"||typeof i=="number")r(o,i.toString());else if(typeof i=="boolean")r(o,i?"1":"0");else throw new Error(`Can't handle extra config type: ${typeof i}`)})},en=e=>{let t=sn(),n=t.stackSave();try{let r=t.PTR_SIZE,s=t.stackAlloc(2*r);t._OrtGetLastError(s,s+r);let i=Number(t.getValue(s,r===4?"i32":"i64")),o=t.getValue(s+r,"*"),a=o?t.UTF8ToString(o):"";throw new Error(`${e} ERROR_CODE: ${i}, ERROR_MESSAGE: ${a}`)}finally{t.stackRestore(n)}}}),VI,PG=He(()=>{Yi(),wg(),VI=e=>{let t=sn(),n=0,r=[],s=e||{};try{if(e?.logSeverityLevel===void 0)s.logSeverityLevel=2;else if(typeof e.logSeverityLevel!="number"||!Number.isInteger(e.logSeverityLevel)||e.logSeverityLevel<0||e.logSeverityLevel>4)throw new Error(`log serverity level is not valid: ${e.logSeverityLevel}`);if(e?.logVerbosityLevel===void 0)s.logVerbosityLevel=0;else if(typeof e.logVerbosityLevel!="number"||!Number.isInteger(e.logVerbosityLevel))throw new Error(`log verbosity level is not valid: ${e.logVerbosityLevel}`);e?.terminate===void 0&&(s.terminate=!1);let i=0;return e?.tag!==void 0&&(i=is(e.tag,r)),n=t._OrtCreateRunOptions(s.logSeverityLevel,s.logVerbosityLevel,!!s.terminate,i),n===0&&en("Can't create run options."),e?.extra!==void 0&&yu(e.extra,"",new WeakSet,(o,a)=>{let l=is(o,r),c=is(a,r);t._OrtAddRunConfigEntry(n,l,c)!==0&&en(`Can't set a run config entry: ${o} - ${a}.`)}),[n,r]}catch(i){throw n!==0&&t._OrtReleaseRunOptions(n),r.forEach(o=>t._free(o)),i}}}),Iv,kv,Ev,Sa,Sv,UI,CG=He(()=>{Yi(),wg(),Iv=e=>{switch(e){case"disabled":return 0;case"basic":return 1;case"extended":return 2;case"all":return 99;default:throw new Error(`unsupported graph optimization level: ${e}`)}},kv=e=>{switch(e){case"sequential":return 0;case"parallel":return 1;default:throw new Error(`unsupported execution mode: ${e}`)}},Ev=e=>{e.extra||(e.extra={}),e.extra.session||(e.extra.session={});let t=e.extra.session;t.use_ort_model_bytes_directly||(t.use_ort_model_bytes_directly="1"),e.executionProviders&&e.executionProviders.some(n=>(typeof n=="string"?n:n.name)==="webgpu")&&(e.enableMemPattern=!1)},Sa=(e,t,n,r)=>{let s=is(t,r),i=is(n,r);sn()._OrtAddSessionConfigEntry(e,s,i)!==0&&en(`Can't set a session config entry: ${t} - ${n}.`)},Sv=async(e,t,n)=>{for(let r of t){let s=typeof r=="string"?r:r.name,i=[];switch(s){case"webnn":if(s="WEBNN",typeof r!="string"){let d=r?.deviceType;d&&Sa(e,"deviceType",d,n)}break;case"webgpu":if(s="JS",typeof r!="string"){let d=r;if(d?.preferredLayout){if(d.preferredLayout!=="NCHW"&&d.preferredLayout!=="NHWC")throw new Error(`preferredLayout must be either 'NCHW' or 'NHWC': ${d.preferredLayout}`);Sa(e,"preferredLayout",d.preferredLayout,n)}}break;case"wasm":case"cpu":continue;default:throw new Error(`not supported execution provider: ${s}`)}let o=is(s,n),a=i.length,l=0,c=0;if(a>0){l=sn()._malloc(a*sn().PTR_SIZE),n.push(l),c=sn()._malloc(a*sn().PTR_SIZE),n.push(c);for(let d=0;d<a;d++)sn().setValue(l+d*sn().PTR_SIZE,i[d][0],"*"),sn().setValue(c+d*sn().PTR_SIZE,i[d][1],"*")}await sn()._OrtAppendExecutionProvider(e,o,l,c,a)!==0&&en(`Can't append execution provider: ${s}.`)}},UI=async e=>{let t=sn(),n=0,r=[],s=e||{};Ev(s);try{let i=Iv(s.graphOptimizationLevel??"all"),o=kv(s.executionMode??"sequential"),a=typeof s.logId=="string"?is(s.logId,r):0,l=s.logSeverityLevel??2;if(!Number.isInteger(l)||l<0||l>4)throw new Error(`log serverity level is not valid: ${l}`);let c=s.logVerbosityLevel??0;if(!Number.isInteger(c)||c<0||c>4)throw new Error(`log verbosity level is not valid: ${c}`);let d=typeof s.optimizedModelFilePath=="string"?is(s.optimizedModelFilePath,r):0;if(n=t._OrtCreateSessionOptions(i,!!s.enableCpuMemArena,!!s.enableMemPattern,o,!!s.enableProfiling,0,a,l,c,d),n===0&&en("Can't create session options."),s.executionProviders&&await Sv(n,s.executionProviders,r),s.enableGraphCapture!==void 0){if(typeof s.enableGraphCapture!="boolean")throw new Error(`enableGraphCapture must be a boolean value: ${s.enableGraphCapture}`);Sa(n,"enableGraphCapture",s.enableGraphCapture.toString(),r)}if(s.freeDimensionOverrides)for(let[u,p]of Object.entries(s.freeDimensionOverrides)){if(typeof u!="string")throw new Error(`free dimension override name must be a string: ${u}`);if(typeof p!="number"||!Number.isInteger(p)||p<0)throw new Error(`free dimension override value must be a non-negative integer: ${p}`);let h=is(u,r);t._OrtAddFreeDimensionOverride(n,h,p)!==0&&en(`Can't set a free dimension override: ${u} - ${p}.`)}return s.extra!==void 0&&yu(s.extra,"",new WeakSet,(u,p)=>{Sa(n,u,p,r)}),[n,r]}catch(i){throw n!==0&&t._OrtReleaseSessionOptions(n)!==0&&en("Can't release session options."),r.forEach(o=>t._free(o)),i}}}),Io,zs,Fi,vg,xu,Mg,Ag,Qm,Mt=He(()=>{Io=e=>{switch(e){case"int8":return 3;case"uint8":return 2;case"bool":return 9;case"int16":return 5;case"uint16":return 4;case"int32":return 6;case"uint32":return 12;case"float16":return 10;case"float32":return 1;case"float64":return 11;case"string":return 8;case"int64":return 7;case"uint64":return 13;case"int4":return 22;case"uint4":return 21;default:throw new Error(`unsupported data type: ${e}`)}},zs=e=>{switch(e){case 3:return"int8";case 2:return"uint8";case 9:return"bool";case 5:return"int16";case 4:return"uint16";case 6:return"int32";case 12:return"uint32";case 10:return"float16";case 1:return"float32";case 11:return"float64";case 8:return"string";case 7:return"int64";case 13:return"uint64";case 22:return"int4";case 21:return"uint4";default:throw new Error(`unsupported data type: ${e}`)}},Fi=(e,t)=>{let n=[-1,4,1,1,2,2,4,8,-1,1,2,8,4,8,-1,-1,-1,-1,-1,-1,-1,.5,.5][e],r=typeof t=="number"?t:t.reduce((s,i)=>s*i,1);return n>0?Math.ceil(r*n):void 0},vg=e=>{switch(e){case"float16":return typeof Float16Array<"u"&&Float16Array.from?Float16Array:Uint16Array;case"float32":return Float32Array;case"uint8":return Uint8Array;case"int8":return Int8Array;case"uint16":return Uint16Array;case"int16":return Int16Array;case"int32":return Int32Array;case"bool":return Uint8Array;case"float64":return Float64Array;case"uint32":return Uint32Array;case"int64":return BigInt64Array;case"uint64":return BigUint64Array;default:throw new Error(`unsupported type: ${e}`)}},xu=e=>{switch(e){case"verbose":return 0;case"info":return 1;case"warning":return 2;case"error":return 3;case"fatal":return 4;default:throw new Error(`unsupported logging level: ${e}`)}},Mg=e=>e==="float32"||e==="float16"||e==="int32"||e==="int64"||e==="uint32"||e==="uint8"||e==="bool"||e==="uint4"||e==="int4",Ag=e=>e==="float32"||e==="float16"||e==="int32"||e==="int64"||e==="uint32"||e==="uint64"||e==="int8"||e==="uint8"||e==="bool"||e==="uint4"||e==="int4",Qm=e=>{switch(e){case"none":return 0;case"cpu":return 1;case"cpu-pinned":return 2;case"texture":return 3;case"gpu-buffer":return 4;case"ml-tensor":return 5;default:throw new Error(`unsupported data location: ${e}`)}}}),Tg,WI=He(()=>{yg(),Tg=async e=>{if(typeof e=="string"){let t=await fetch(e);if(!t.ok)throw new Error(`failed to load external data file: ${e}`);let n=t.headers.get("Content-Length"),r=n?parseInt(n,10):0;if(r<1073741824)return new Uint8Array(await t.arrayBuffer());{if(!t.body)throw new Error(`failed to load external data file: ${e}, no response body.`);let s=t.body.getReader(),i;try{i=new ArrayBuffer(r)}catch(a){if(a instanceof RangeError){let l=Math.ceil(r/65536);i=new WebAssembly.Memory({initial:l,maximum:l}).buffer}else throw a}let o=0;for(;;){let{done:a,value:l}=await s.read();if(a)break;let c=l.byteLength;new Uint8Array(i,o,c).set(l),o+=c}return new Uint8Array(i,0,r)}}else return e instanceof Blob?new Uint8Array(await e.arrayBuffer()):e instanceof Uint8Array?e:new Uint8Array(e)}}),Lv,Dv,$v,Fv,Pg,Ov,Ut,Ws=He(()=>{Mt(),Lv=["V","I","W","E","F"],Dv=(e,t)=>{console.log(`[${Lv[e]},${new Date().toISOString()}]${t}`)},Pg=(e,t)=>{$v=e,Fv=t},Ov=(e,t)=>{let n=xu(e),r=xu($v);n>=r&&Dv(n,typeof t=="function"?t():t)},Ut=(...e)=>{Fv&&Ov(...e)}}),jv,Vo,Ae,bu,HI,KI,qI,Ft=He(()=>{jv=class{static calcMatMulShape(e,t){return e[1]!==t[0]?void 0:[e[0],t[1]]}},Vo=class{static calcShape(e,t,n=!1){let r=e.length,s=t.length;if(r===0)return t;if(s===0)return e;let i=Math.max(e.length,t.length),o=new Array(i);if(n){if(r<2||s<2)return;let a=jv.calcMatMulShape([e[r-2],e[r-1]],[t[s-2],t[s-1]]);if(a===void 0)return;[o[i-2],o[i-1]]=a}for(let a=n?3:1;a<=i;a++){let l=r-a<0?1:e[r-a],c=s-a<0?1:t[s-a];if(l!==c&&l>1&&c>1)return;let d=Math.max(l,c);if(l&&c)o[i-a]=Math.max(l,c);else{if(d>1)return;o[i-a]=0}}return o}static isValidBroadcast(e,t){let n=e.length,r=t.length;if(n>r)return!1;for(let s=1;s<=n;s++)if(e[n-s]!==1&&e[n-s]!==t[r-s])return!1;return!0}},Ae=class nu{static size(t){return nu.getSizeFromDimensionRange(t,0,t.length)}static convertShape(t,n=4){let r=t.length;if(r===0)return[];let s=new Array(r),i=r-1;for(;i>=0;){if(t[i]%n===0){s[i]=t[i]/n;break}if(n%t[i]!==0)throw new Error("cannot convert shape");s[i]=1,n/=t[i],i--}for(i--;i>=0;i--)s[i]=t[i];return s}static sizeFromDimension(t,n){if(n<0||n>t.length)throw new Error(`invalid dimension of ${n} for sizeFromDimension as Tensor has ${t.length} dimensions.`);return nu.getSizeFromDimensionRange(t,n,t.length)}static sizeToDimension(t,n){if(n<0||n>t.length)throw new Error(`invalid dimension of ${n} for sizeToDimension as Tensor has ${t.length} dimensions.`);return nu.getSizeFromDimensionRange(t,0,n)}static getSizeFromDimensionRange(t,n,r){let s=1;for(let i=n;i<r;i++){if(t[i]<0)throw new Error("cannot get valid size from specified dimension range. Most likely the range contains negative values in them.");s*=Number(t[i])}return s}static computeStrides(t){let n=t.length;if(n===0)return[];if(n===1)return[1];let r=new Array(n);r[n-1]=1,r[n-2]=t[n-1];for(let s=n-3;s>=0;--s)r[s]=r[s+1]*t[s+1];return r}static normalizeAxis(t,n){if(t<-n&&t>=n)throw new Error("unsupported axis for this operation.");return t<0?t+n:t}static normalizeAxes(t,n){return t.map(r=>this.normalizeAxis(r,n??t.length))}static sortBasedOnPerm(t,n){return n?n.map(r=>t[r]):t.slice().reverse()}static padShape(t,n){let r=t.length;return t.map((s,i)=>s+n[i]+n[i+r])}static areEqual(t,n){return t.length!==n.length?!1:t.every((r,s)=>r===n[s])}},bu=class Ka{static adjustPoolAttributes(t,n,r,s,i,o){if(!t&&r.length!==n.length-2)throw new Error("length of specified kernel shapes should be 2 less than length of input dimensions");if(t)for(let a=0;a<n.length-2;a++)a>=r.length?r.push(n[a+2]):r[a]=n[a+2];for(let a=0;a<r.length;a++)if(a<s.length){if(s[a]<0)throw new Error("strides should be greater than or equal to 1")}else s.push(1);for(let a=0;a<r.length;a++)if(a<i.length){if(i[a]<0)throw new Error("dilations should be greater than or equal to 1")}else i.push(1);for(let a=0;a<r.length*2;a++)if(a<o.length){if(o[a]<0)throw new Error("pad should be greater than or equal to 1")}else o.push(0);for(let a=0;a<r.length;a++){if(r[a]<=0)throw new Error("kernel shapes need to be greater than 0");if(o[a]>=r[a]||o[a+r.length]>=r[a])throw new Error("pads should be smaller than kernel")}}static adjustPadsBasedOnAutoPad(t,n,r,s,i,o,a){if(a){if(i.length!==2*(t.length-2))throw new Error("length of pads should be twice the length of data dimensions");if(n.length!==t.length-2)throw new Error("length of strides should be the length of data dimensions");if(s.length!==t.length-2)throw new Error("length of kernel shapes should be the length of data dimensions");for(let l=0;l<t.length-2;l++)Ka.adjustPadAndReturnShape(t[l+(o?1:2)],n[l],r[l],s[l],i,l,l+t.length-2,a)}}static computePoolOutputShape(t,n,r,s,i,o,a){if(n.length<=0)throw new Error("input shape must be of size greater than 0");let l=[n[0],n[1]];return Ka.computeShapeHelper(t,n,l,r,s,i,o,a),l}static computeConvOutputShape(t,n,r,s,i,o,a){if(t.length<=0||n.length<=0)throw new Error("invalid input tensor dims or invalid filter tensor dims");let l=[t[0],n[0]];return Ka.computeShapeHelper(!1,t,l,r,s,i,o,a),l}static computeShapeHelper(t,n,r,s,i,o,a,l){if(t)for(let c=0;c<n.length-2;c++)r.push(1);else for(let c=0;c<n.length-2;c++)r.push(Ka.adjustPadAndReturnShape(n[c+2],s[c],i[c],o[c],a,c,c+n.length-2,l))}static adjustPadAndReturnShape(t,n,r,s,i,o,a,l){let c=r*(s-1)+1;if(l&&l!=="NOTSET")switch(l){case"VALID":return i[o]=0,i[a]=0,Math.floor((t-c)/n+1);case"SAME_LOWER":case"SAME_UPPER":if(r!==1)throw new Error("Dilation not supported for SAME_UPPER or SAME_LOWER");{let d=((t+n-1)/n-1)*n+s-t;return i[o]=Math.floor(l==="SAME_LOWER"?(d+1)/2:d/2),i[a]=d-i[o],Math.floor((t+d-s)/n+1)}default:throw new Error("Unsupported AutoPad type")}else return Math.floor((t+i[o]+i[a]-c)/n+1)}},HI=class{static getShapeOfGemmResult(e,t,n,r,s){if(e.length!==2||n.length!==2)throw new Error("shape need to be of size 2");let i,o,a;t?(i=e[1],o=e[0]):(i=e[0],o=e[1]);let l=-1;if(r?(a=n[0],l=1):(a=n[1],l=0),n[l]!==o)throw new Error("dimension mismatch");if(i<=0||a<=0||o<=0)throw new Error("invalid shape specified");if(s&&!Vo.isValidBroadcast(s,[i,a]))throw new Error("gemm: invalid bias shape for broadcast");return[i,a,o]}},KI=-34028234663852886e22,qI=34028234663852886e22}),Cg,QI=He(()=>{Mt(),Cg=(e,t)=>new(vg(t))(e)}),Xm,yh,Rv,xh,Nv,bh,wh,vh,zv,XI,IG=He(()=>{Ws(),Xm=(e,t=!0)=>{if(e.byteLength%8!==0)throw new Error("Invalid Uint8Array length - must be a multiple of 8 (BigInt).");let n=e.byteLength/8,r=new BigInt64Array(e.buffer,e.byteOffset,n),s=new Int32Array(n);for(let i=0;i<n;i++){let o=r[i];if(o>2147483647n||o<-2147483648n)throw new Error(`Overflow occurred when converting BigInt to Int32 at index ${i}: ${o}`);s[i]=Number(o)}return t?new Uint8Array(s.buffer):s},yh=(e,t=!0)=>{if(e.byteLength%4!==0)throw new Error("Invalid Uint8Array length - must be a multiple of 4 (Int32).");let n=e.byteLength/4,r=new Int32Array(e.buffer,e.byteOffset,n),s=BigInt64Array.from(r,BigInt);return t?new Uint8Array(s.buffer):s},Rv=1,xh=()=>Rv++,Nv=new Map([["float32",32],["float16",16],["int32",32],["uint32",32],["int64",64],["uint64",64],["int8",8],["uint8",8],["int4",4],["uint4",4]]),bh=(e,t)=>{let n=Nv.get(e);if(!n)throw new Error("Unsupported data type.");return t.length>0?Math.ceil(t.reduce((r,s)=>r*s)*n/8):0},wh=class{constructor(e){this.shouldConvertInt64toInt32=!1,this.isInt64ToInt32Converted=!1;let{sessionId:t,context:n,tensor:r,dataType:s,shape:i,shouldConvertInt64toInt32:o=!1}=e;this.sessionId=t,this.mlContext=n,this.mlTensor=r,this.dataType=s,this.tensorShape=i,this.shouldConvertInt64toInt32=o}get tensor(){return this.mlTensor}get type(){return this.dataType}get shape(){return this.tensorShape}get byteLength(){return bh(this.dataType,this.tensorShape)}destroy(){Ut("verbose",()=>"[WebNN] TensorWrapper.destroy"),this.mlTensor.destroy()}write(e){this.mlContext.writeTensor(this.mlTensor,e)}async read(e,t){if(e){let n=await this.mlContext.readTensor(this.mlTensor),r=yh(new Uint8Array(n));if(t){(t instanceof ArrayBuffer?new Uint8Array(t):new Uint8Array(t.buffer,t.byteOffset,t.byteLength)).set(r);return}else return r.buffer}else return t?this.mlContext.readTensor(this.mlTensor,t):this.mlContext.readTensor(this.mlTensor)}canReuseTensor(e,t,n){return this.mlContext===e&&this.dataType===t&&this.tensorShape.length===n.length&&this.tensorShape.every((r,s)=>r===n[s])}setIsInt64ToInt32Converted(e){this.isInt64ToInt32Converted=e}},vh=class{constructor(e,t){this.tensorManager=e,this.wrapper=t}get tensorWrapper(){return this.wrapper}releaseTensor(){this.tensorWrapper&&(this.tensorManager.releaseTensor(this.tensorWrapper),this.wrapper=void 0)}async ensureTensor(e,t,n,r){let s=t,i=this.tensorManager.getMLContext(e),o=s==="int64"&&!i.opSupportLimits().input.dataTypes.includes("int64");if(o&&(s="int32",Ut("verbose",()=>"[WebNN] TensorIdTracker.ensureTensor: convert dataType from int64 to int32")),this.wrapper){if(this.wrapper.canReuseTensor(i,s,n))return this.wrapper.tensor;if(r){if(this.wrapper.byteLength!==bh(s,n))throw new Error("Unable to copy data to tensor with different size.");this.activeUpload=new Uint8Array(await this.wrapper.read())}this.tensorManager.releaseTensor(this.wrapper)}let a=typeof MLTensorUsage>"u"?void 0:MLTensorUsage.READ|MLTensorUsage.WRITE;return this.wrapper=await this.tensorManager.getCachedTensor(e,s,n,a,!0,!0,o),r&&this.activeUpload&&(this.wrapper.write(this.activeUpload),this.activeUpload=void 0),this.wrapper.tensor}upload(e){let t=e;if(this.wrapper)if(this.wrapper.shouldConvertInt64toInt32&&(t=Xm(e,!0),this.wrapper.setIsInt64ToInt32Converted(!0)),t.byteLength===this.wrapper.byteLength){this.wrapper.write(t);return}else Ut("verbose",()=>"Data size does not match tensor size. Releasing tensor."),this.releaseTensor();this.activeUpload?this.activeUpload.set(t):this.activeUpload=new Uint8Array(t)}async download(e){if(this.activeUpload){let t=this.wrapper?.isInt64ToInt32Converted?yh(this.activeUpload):this.activeUpload;if(e){e instanceof ArrayBuffer?new Uint8Array(e).set(t):new Uint8Array(e.buffer,e.byteOffset,e.byteLength).set(t);return}else return t.buffer}if(!this.wrapper)throw new Error("Tensor has not been created.");return e?this.wrapper.read(this.wrapper?.shouldConvertInt64toInt32,e):this.wrapper.read(this.wrapper?.shouldConvertInt64toInt32)}},zv=class{constructor(e){this.backend=e,this.tensorTrackersById=new Map,this.freeTensors=[],this.externalTensors=new Set}getMLContext(e){let t=this.backend.getMLContext(e);if(!t)throw new Error("MLContext not found for session.");return t}reserveTensorId(){let e=xh();return this.tensorTrackersById.set(e,new vh(this)),e}releaseTensorId(e){let t=this.tensorTrackersById.get(e);t&&(this.tensorTrackersById.delete(e),t.tensorWrapper&&this.releaseTensor(t.tensorWrapper))}async ensureTensor(e,t,n,r,s){Ut("verbose",()=>`[WebNN] TensorManager.ensureTensor {tensorId: ${t}, dataType: ${n}, shape: ${r}, copyOld: ${s}}`);let i=this.tensorTrackersById.get(t);if(!i)throw new Error("Tensor not found.");return i.ensureTensor(e,n,r,s)}upload(e,t){let n=this.tensorTrackersById.get(e);if(!n)throw new Error("Tensor not found.");n.upload(t)}async download(e,t){Ut("verbose",()=>`[WebNN] TensorManager.download {tensorId: ${e}, dstBuffer: ${t?.byteLength}}`);let n=this.tensorTrackersById.get(e);if(!n)throw new Error("Tensor not found.");return n.download(t)}releaseTensorsForSession(e){for(let t of this.freeTensors)t.sessionId===e&&t.destroy();this.freeTensors=this.freeTensors.filter(t=>t.sessionId!==e)}registerTensor(e,t,n,r){let s=this.getMLContext(e),i=xh(),o=new wh({sessionId:e,context:s,tensor:t,dataType:n,shape:r});return this.tensorTrackersById.set(i,new vh(this,o)),this.externalTensors.add(o),i}async getCachedTensor(e,t,n,r,s,i,o=!1){let a=this.getMLContext(e);for(let[c,d]of this.freeTensors.entries())if(d.canReuseTensor(a,t,n)){Ut("verbose",()=>`[WebNN] Reusing tensor {dataType: ${t}, shape: ${n}}`);let u=this.freeTensors.splice(c,1)[0];return u.sessionId=e,u}Ut("verbose",()=>`[WebNN] MLContext.createTensor {dataType: ${t}, shape: ${n}}`);let l=await a.createTensor({dataType:t,shape:n,dimensions:n,usage:r,writable:s,readable:i});return new wh({sessionId:e,context:a,tensor:l,dataType:t,shape:n,shouldConvertInt64toInt32:o})}releaseTensor(e){this.externalTensors.has(e)&&this.externalTensors.delete(e),this.freeTensors.push(e)}},XI=(...e)=>new zv(...e)}),Sc,Bv,YI,kG=He(()=>{Mt(),Yi(),QI(),IG(),Ws(),Sc=new Map([[1,"float32"],[10,"float16"],[6,"int32"],[12,"uint32"],[7,"int64"],[13,"uint64"],[22,"int4"],[21,"uint4"],[3,"int8"],[2,"uint8"],[9,"uint8"]]),Bv=(e,t)=>{if(e===t)return!0;if(e===void 0||t===void 0)return!1;let n=Object.keys(e).sort(),r=Object.keys(t).sort();return n.length===r.length&&n.every((s,i)=>s===r[i]&&e[s]===t[s])},YI=class{constructor(e){this.tensorManager=XI(this),this.mlContextBySessionId=new Map,this.sessionIdsByMLContext=new Map,this.mlContextCache=[],this.sessionGraphInputs=new Map,this.temporaryGraphInputs=[],this.temporarySessionTensorIds=new Map,Pg(e.logLevel,!!e.debug)}get currentSessionId(){if(this.activeSessionId===void 0)throw new Error("No active session");return this.activeSessionId}onRunStart(e){Ut("verbose",()=>`[WebNN] onRunStart {sessionId: ${e}}`),this.activeSessionId=e}onRunEnd(e){Ut("verbose",()=>`[WebNN] onRunEnd {sessionId: ${e}}`);let t=this.temporarySessionTensorIds.get(e);if(t){for(let n of t)Ut("verbose",()=>`[WebNN] releasing temporary tensor {tensorId: ${n}}`),this.tensorManager.releaseTensorId(n);this.temporarySessionTensorIds.delete(e),this.activeSessionId=void 0}}async createMLContext(e){if(e instanceof GPUDevice){let n=this.mlContextCache.findIndex(r=>r.gpuDevice===e);if(n!==-1)return this.mlContextCache[n].mlContext;{let r=await navigator.ml.createContext(e);return this.mlContextCache.push({gpuDevice:e,mlContext:r}),r}}else if(e===void 0){let n=this.mlContextCache.findIndex(r=>r.options===void 0&&r.gpuDevice===void 0);if(n!==-1)return this.mlContextCache[n].mlContext;{let r=await navigator.ml.createContext();return this.mlContextCache.push({mlContext:r}),r}}let t=this.mlContextCache.findIndex(n=>Bv(n.options,e));if(t!==-1)return this.mlContextCache[t].mlContext;{let n=await navigator.ml.createContext(e);return this.mlContextCache.push({options:e,mlContext:n}),n}}registerMLContext(e,t){this.mlContextBySessionId.set(e,t);let n=this.sessionIdsByMLContext.get(t);n||(n=new Set,this.sessionIdsByMLContext.set(t,n)),n.add(e),this.temporaryGraphInputs.length>0&&(this.sessionGraphInputs.set(e,this.temporaryGraphInputs),this.temporaryGraphInputs=[])}onReleaseSession(e){this.sessionGraphInputs.delete(e);let t=this.mlContextBySessionId.get(e);if(!t)return;this.tensorManager.releaseTensorsForSession(e),this.mlContextBySessionId.delete(e);let n=this.sessionIdsByMLContext.get(t);if(n.delete(e),n.size===0){this.sessionIdsByMLContext.delete(t);let r=this.mlContextCache.findIndex(s=>s.mlContext===t);r!==-1&&this.mlContextCache.splice(r,1)}}getMLContext(e){return this.mlContextBySessionId.get(e)}reserveTensorId(){return this.tensorManager.reserveTensorId()}releaseTensorId(e){Ut("verbose",()=>`[WebNN] releaseTensorId {tensorId: ${e}}`),this.tensorManager.releaseTensorId(e)}async ensureTensor(e,t,n,r,s){let i=Sc.get(n);if(!i)throw new Error(`Unsupported ONNX data type: ${n}`);return this.tensorManager.ensureTensor(e??this.currentSessionId,t,i,r,s)}async createTemporaryTensor(e,t,n){Ut("verbose",()=>`[WebNN] createTemporaryTensor {onnxDataType: ${t}, shape: ${n}}`);let r=Sc.get(t);if(!r)throw new Error(`Unsupported ONNX data type: ${t}`);let s=this.tensorManager.reserveTensorId();await this.tensorManager.ensureTensor(e,s,r,n,!1);let i=this.temporarySessionTensorIds.get(e);return i?i.push(s):this.temporarySessionTensorIds.set(e,[s]),s}uploadTensor(e,t){if(!sn().shouldTransferToMLTensor)throw new Error("Trying to upload to a MLTensor while shouldTransferToMLTensor is false");Ut("verbose",()=>`[WebNN] uploadTensor {tensorId: ${e}, data: ${t.byteLength}}`),this.tensorManager.upload(e,t)}async downloadTensor(e,t){return this.tensorManager.download(e,t)}createMLTensorDownloader(e,t){return async()=>{let n=await this.tensorManager.download(e);return Cg(n,t)}}registerMLTensor(e,t,n,r){let s=Sc.get(n);if(!s)throw new Error(`Unsupported ONNX data type: ${n}`);let i=this.tensorManager.registerTensor(e,t,s,r);return Ut("verbose",()=>`[WebNN] registerMLTensor {tensor: ${t}, dataType: ${s}, dimensions: ${r}} -> {tensorId: ${i}}`),i}registerMLConstant(e,t,n,r,s,i,o=!1){if(!i)throw new Error("External mounted files are not available.");let a=e;e.startsWith("./")&&(a=e.substring(2));let l=i.get(a);if(!l)throw new Error(`File with name ${a} not found in preloaded files.`);if(t+n>l.byteLength)throw new Error("Out of bounds: data offset and length exceed the external file data size.");let c=l.slice(t,t+n).buffer,d;switch(s.dataType){case"float32":d=new Float32Array(c);break;case"float16":d=typeof Float16Array<"u"&&Float16Array.from?new Float16Array(c):new Uint16Array(c);break;case"int32":d=new Int32Array(c);break;case"uint32":d=new Uint32Array(c);break;case"int64":o?(d=Xm(new Uint8Array(c),!1),s.dataType="int32"):d=new BigInt64Array(c);break;case"uint64":d=new BigUint64Array(c);break;case"int8":d=new Int8Array(c);break;case"int4":case"uint4":case"uint8":d=new Uint8Array(c);break;default:throw new Error(`Unsupported data type: ${s.dataType} in creating WebNN Constant from external data.`)}return Ut("verbose",()=>`[WebNN] registerMLConstant {dataType: ${s.dataType}, shape: ${s.shape}}} ${o?"(Note: it was int64 data type and registered to int32 as workaround)":""}`),r.constant(s,d)}registerGraphInput(e){this.temporaryGraphInputs.push(e)}isGraphInput(e,t){let n=this.sessionGraphInputs.get(e);return n?n.includes(t):!1}isInt64Supported(e){return!!this.mlContextBySessionId.get(e)?.opSupportLimits().input.dataTypes.includes("int64")}flush(){}}}),Ig=He(()=>{}),Mh,Lc,Dc,Gv,Vv,Ah,Ym,Uv,JI,EG=He(()=>{Ws(),Ig(),Mh=new Map([[64,250],[128,200],[256,200],[512,200],[2048,230],[4096,200],[8192,50],[16384,50],[32768,50],[65536,50],[131072,50],[262144,50],[524288,50],[1048576,50],[2097152,30],[4194304,20],[8388608,10],[12582912,10],[16777216,10],[26214400,15],[33554432,22],[44236800,2],[58982400,6],[67108864,6],[134217728,6],[167772160,6]]),Lc=[],Dc=e=>Math.ceil(Number(e)/16)*16,Gv=e=>{for(let t=0;t<Lc.length;t++){let n=Lc[t];if(e<=n)return n}return Math.ceil(e/16)*16},Vv=1,Ah=()=>Vv++,Ym=async(e,t,n,r)=>{let s=Dc(n),i=e.device.createBuffer({size:s,usage:GPUBufferUsage.COPY_DST|GPUBufferUsage.MAP_READ});try{let o=e.getCommandEncoder();e.endComputePass(),o.copyBufferToBuffer(t,0,i,0,s),e.flush(),await i.mapAsync(GPUMapMode.READ);let a=i.getMappedRange();if(r){let l=r();return l.set(new Uint8Array(a,0,n)),l}else return new Uint8Array(a.slice(0,n))}finally{i.destroy()}},Uv=class{constructor(e){this.backend=e,this.storageCache=new Map,this.freeBuffers=new Map,this.freeUniformBuffers=new Map,this.buffersPending=[],this.capturedPendingBuffers=new Map;for(let[t]of Mh)Lc.push(t),this.freeBuffers.set(t,[]),this.freeUniformBuffers.set(t,[]);this.sessionCount=0}upload(e,t){let n=t.buffer,r=t.byteOffset,s=t.byteLength,i=Dc(s),o=this.storageCache.get(e);if(!o)throw new Error("gpu data for uploading does not exist");if(Number(o.originalSize)!==s)throw new Error(`inconsistent data size. gpu data size=${o.originalSize}, data size=${s}`);let a=this.backend.device.createBuffer({mappedAtCreation:!0,size:i,usage:GPUBufferUsage.MAP_WRITE|GPUBufferUsage.COPY_SRC}),l=a.getMappedRange();new Uint8Array(l).set(new Uint8Array(n,r,s)),a.unmap();let c=this.backend.device.createCommandEncoder();c.copyBufferToBuffer(a,0,o.gpuData.buffer,0,i),this.backend.device.queue.submit([c.finish()]),a.destroy(),Ut("verbose",()=>`[WebGPU] GpuDataManager.upload(id=${e})`)}memcpy(e,t){let n=this.storageCache.get(e);if(!n)throw new Error("source gpu data for memcpy does not exist");let r=this.storageCache.get(t);if(!r)throw new Error("destination gpu data for memcpy does not exist");if(n.originalSize!==r.originalSize)throw new Error("inconsistent source and destination gpu data size");let s=Dc(n.originalSize),i=this.backend.getCommandEncoder();this.backend.endComputePass(),i.copyBufferToBuffer(n.gpuData.buffer,0,r.gpuData.buffer,0,s)}registerExternalBuffer(e,t,n){let r;if(n){if(r=n[0],e===n[1])return Ut("verbose",()=>`[WebGPU] GpuDataManager.registerExternalBuffer(size=${t}) => id=${r}, buffer is the same, skip.`),r;if(this.backend.capturedCommandList.has(this.backend.currentSessionId))throw new Error(`Registering a different external buffer under graph capture mode is not supported yet.
             Please use the previous external buffer!`)}else r=Ah();return this.storageCache.set(r,{gpuData:{id:r,type:0,buffer:e},originalSize:t}),Ut("verbose",()=>`[WebGPU] GpuDataManager.registerExternalBuffer(size=${t}) => id=${r}, registered.`),r}unregisterExternalBuffer(e){e!==void 0&&(this.storageCache.delete(e),Ut("verbose",()=>`[WebGPU] GpuDataManager.unregisterExternalBuffer() => id=${e}`))}create(e,t=GPUBufferUsage.STORAGE|GPUBufferUsage.COPY_SRC|GPUBufferUsage.COPY_DST){let n=Gv(e),r,s=(t&GPUBufferUsage.STORAGE)===GPUBufferUsage.STORAGE,i=(t&GPUBufferUsage.UNIFORM)===GPUBufferUsage.UNIFORM;if(s||i){let a=(s?this.freeBuffers:this.freeUniformBuffers).get(n);a?a.length>0?r=a.pop():r=this.backend.device.createBuffer({size:n,usage:t}):r=this.backend.device.createBuffer({size:n,usage:t})}else r=this.backend.device.createBuffer({size:n,usage:t});let o={id:Ah(),type:0,buffer:r};return this.storageCache.set(o.id,{gpuData:o,originalSize:Number(e)}),Ut("verbose",()=>`[WebGPU] GpuDataManager.create(size=${e}) => id=${o.id}`),o}get(e){return this.storageCache.get(e)?.gpuData}release(e){let t=typeof e=="bigint"?Number(e):e,n=this.storageCache.get(t);if(!n){if(this.storageCache.size===0)return 0;throw new Error("releasing data does not exist")}return Ut("verbose",()=>`[WebGPU] GpuDataManager.release(id=${t}), gpuDataId=${n.gpuData.id}`),this.storageCache.delete(t),this.buffersPending.push(n.gpuData.buffer),n.originalSize}async download(e,t){let n=this.storageCache.get(Number(e));if(!n)throw new Error("data does not exist");await Ym(this.backend,n.gpuData.buffer,n.originalSize,t)}refreshPendingBuffers(){if(this.buffersPending.length!==0)if(this.backend.sessionStatus==="default"){for(let e of this.buffersPending){let t=Mh.get(e.size);if((e.usage&GPUBufferUsage.STORAGE)===GPUBufferUsage.STORAGE){let n=this.freeBuffers.get(e.size)||[];t===void 0||n.length>=t?e.destroy():n.push(e)}else if((e.usage&GPUBufferUsage.UNIFORM)===GPUBufferUsage.UNIFORM){let n=this.freeUniformBuffers.get(e.size)||[];t===void 0||n.length>=t?e.destroy():n.push(e)}else e.destroy()}this.buffersPending=[]}else{let e=this.capturedPendingBuffers.get(this.backend.currentSessionId);e||(e=[],this.capturedPendingBuffers.set(this.backend.currentSessionId,e));for(let t of this.buffersPending)e.push(t);this.buffersPending=[]}}dispose(){this.freeBuffers.forEach(e=>{e.forEach(t=>{t.destroy()})}),this.freeUniformBuffers.forEach(e=>{e.forEach(t=>{t.destroy()})}),this.storageCache.forEach(e=>{e.gpuData.buffer.destroy()}),this.capturedPendingBuffers.forEach(e=>{e.forEach(t=>{t.destroy()})}),this.storageCache=new Map,this.freeBuffers=new Map,this.freeUniformBuffers=new Map,this.capturedPendingBuffers=new Map}onCreateSession(){this.sessionCount+=1}onReleaseSession(e){let t=this.capturedPendingBuffers.get(e);t&&(t.forEach(n=>{n.destroy()}),this.capturedPendingBuffers.delete(e)),this.sessionCount-=1,this.sessionCount===0&&(Ut("warning",()=>"[WebGPU] Clearing webgpu buffer cache"),this.storageCache.forEach(n=>{n.gpuData.buffer.destroy()}),this.storageCache=new Map)}},JI=(...e)=>new Uv(...e)}),Wv,Xt,En=He(()=>{Wv=class{constructor(e){Object.assign(this,e)}get cacheKey(){return this.key||(this.key=Object.getOwnPropertyNames(this).sort().map(e=>`${this[e]}`).join(";")),this.key}},Xt=e=>new Wv(e)}),Uo,$c,Jn,ar,ft,An,Jm,$o,ui,mt,La,Re,ht,ZI,kg,Hv,ek,Ot=He(()=>{Mt(),Ft(),Uo=64,$c=(e,t)=>{if(t===3)throw new Error("vec3 has same alignment as vec4, use vec4 instead");switch(Number(e)){case 10:return t>1?`vec${t}<f16>`:"f16";case 1:return t>1?`vec${t}<f32>`:"f32";case 6:return t>1?`vec${t}<i32>`:"i32";case 12:return t>1?`vec${t}<u32>`:"u32";case 7:if(t>1)throw new Error("currently not supported vecX of uint64 yet");return["vec2<u32>","i32"];case 13:if(t>1)throw new Error("currently not supported vecX of uint64 yet");return["vec2<u32>","u32"];case 9:if(t!==4)throw new Error("bool must be vec4");return["u32","vec4<bool>"];case 22:return"i32";case 21:return"u32";default:throw new Error(`Unknown data type: ${e}`)}},Jn=(e,t=1)=>{let n=$c(e,t);return typeof n=="string"?n:n[0]},ar=(e,t=1)=>{let n=$c(e,t);return typeof n=="string"?n:n[1]},ft=(...e)=>{let t=[];return e.forEach(n=>{n.length!==0&&t.push({type:12,data:n},{type:12,data:Ae.computeStrides(n)})}),t},An=e=>e%4===0?4:e%2===0?2:1,Jm=(e="f32",t,n="0")=>!t||t===1?`${e}(${n})`:`vec${t}<${e}>(${n})`,$o=(e,t,n)=>e==="f32"?n:t===1?`f32(${n})`:`vec${t}<f32>(${n})`,ui=(e,t)=>t===4?`(${e}.x + ${e}.y + ${e}.z + ${e}.w)`:t===2?`(${e}.x + ${e}.y)`:t===3?`(${e}.x + ${e}.y + ${e}.z)`:e,mt=(e,t,n,r)=>e.startsWith("uniforms.")&&n>4?typeof t=="string"?r==="f16"?`${e}[(${t}) / 8][(${t}) % 8 / 4][(${t}) % 8 % 4]`:`${e}[(${t}) / 4][(${t}) % 4]`:r==="f16"?`${e}[${Math.floor(t/8)}][${Math.floor(t%8/4)}][${t%8%4}]`:`${e}[${Math.floor(t/4)}][${t%4}]`:n>1?`${e}[${t}]`:e,La=(e,t,n,r,s)=>{let i=typeof n=="number",o=i?n:n.length,a=[...new Array(o).keys()],l=o<2?"u32":o<=4?`vec${o}<u32>`:`array<u32, ${o}>`,c=$c(t,s),d=typeof c=="string"?c:c[1],u=typeof c=="string"?c:c[0],p={indices:l,value:d,storage:u,tensor:t},h=oe=>typeof oe=="string"?oe:`${oe}u`,m={offsetToIndices:!1,indicesToOffset:!1,broadcastedIndicesToOffset:!1,set:!1,setByIndices:!1,get:!1,getByIndices:!1},g=i?"uniforms.":"",I=`${g}${e}_shape`,f=`${g}${e}_strides`,_="";for(let oe=0;oe<o-1;oe++)_+=`
    let dim${oe} = current / ${mt(f,oe,o)};
    let rest${oe} = current % ${mt(f,oe,o)};
    indices[${oe}] = dim${oe};
    current = rest${oe};
    `;_+=`indices[${o-1}] = current;`;let T=o<2?"":`
  fn o2i_${e}(offset: u32) -> ${p.indices} {
    var indices: ${p.indices};
    var current = offset;
    ${_}
    return indices;
  }`,M=oe=>(m.offsetToIndices=!0,o<2?oe:`o2i_${e}(${oe})`),v=[];if(o>=2)for(let oe=o-1;oe>=0;oe--)v.push(`${mt(f,oe,o)} * (indices[${oe}])`);let b=o<2?"":`
  fn i2o_${e}(indices: ${p.indices}) -> u32 {
    return ${v.join("+")};
  }`,A=oe=>(m.indicesToOffset=!0,o<2?oe:`i2o_${e}(${oe})`),k=(...oe)=>o===0?"0u":`${p.indices}(${oe.map(h).join(",")})`,F=(oe,X)=>o<2?`${oe}`:`${mt(oe,X,o)}`,L=(oe,X,D)=>o<2?`${oe}=${D};`:`${mt(oe,X,o)}=${D};`,G={},j=(oe,X)=>{m.broadcastedIndicesToOffset=!0;let D=`${X.name}broadcastedIndicesTo${e}Offset`;if(D in G)return`${D}(${oe})`;let z=[];for(let se=o-1;se>=0;se--){let me=X.indicesGet("outputIndices",se+X.rank-o);z.push(`${F(f,se)} * (${me} % ${F(I,se)})`)}return G[D]=`fn ${D}(outputIndices: ${X.type.indices}) -> u32 {
             return ${z.length>0?z.join("+"):"0u"};
           }`,`${D}(${oe})`},R=(oe,X)=>(()=>{if(p.storage===p.value)return`${e}[${oe}]=${X};`;if(p.storage==="vec2<u32>"&&p.value==="i32")return`${e}[${oe}]=vec2<u32>(u32(${X}), select(0u, 0xFFFFFFFFu, ${X} < 0));`;if(p.storage==="vec2<u32>"&&p.value==="u32")return`${e}[${oe}]=vec2<u32>(u32(${X}), 0u);`;if(p.storage==="u32"&&p.value==="vec4<bool>")return`${e}[${oe}]=dot(vec4<u32>(0x1, 0x100, 0x10000, 0x1000000), vec4<u32>(${X}));`;throw new Error(`not supported combination of storage type ${p.storage} and value type ${p.value} yet`)})(),K=oe=>(()=>{if(p.storage===p.value)return`${e}[${oe}]`;if(p.storage==="vec2<u32>"&&p.value==="i32")return`i32(${e}[${oe}].x)`;if(p.storage==="vec2<u32>"&&p.value==="u32")return`u32(${e}[${oe}].x)`;if(p.storage==="u32"&&p.value==="vec4<bool>")return`vec4<bool>(bool(${e}[${oe}] & 0xFFu), bool(${e}[${oe}] & 0xFF00u), bool(${e}[${oe}] & 0xFF0000u), bool(${e}[${oe}] & 0xFF000000u))`;throw new Error(`not supported combination of storage type ${p.storage} and value type ${p.value} yet`)})(),U=o<2?"":`
  fn get_${e}ByIndices(indices: ${p.indices}) -> ${d} {
    return ${K(`i2o_${e}(indices)`)};
  }`,Y=o<2?"":(()=>{let oe=a.map(D=>`d${D}: u32`).join(", "),X=a.map(D=>`d${D}`).join(", ");return`
  fn get_${e}(${oe}) -> ${d} {
    return get_${e}ByIndices(${k(X)});
  }`})(),te=(...oe)=>{if(oe.length!==o)throw new Error(`indices length must be ${o}`);let X=oe.map(h).join(",");return o===0?K("0u"):o===1?K(X[0]):(m.get=!0,m.getByIndices=!0,m.indicesToOffset=!0,`get_${e}(${X})`)},ne=oe=>o<2?K(oe):(m.getByIndices=!0,m.indicesToOffset=!0,`get_${e}ByIndices(${oe})`),le=o<2?"":`
  fn set_${e}ByIndices(indices: ${p.indices}, value: ${d}) {
    ${R(`i2o_${e}(indices)`,"value")}
  }`,N=o<2?"":(()=>{let oe=a.map(D=>`d${D}: u32`).join(", "),X=a.map(D=>`d${D}`).join(", ");return`
  fn set_${e}(${oe}, value: ${d}) {
    set_${e}ByIndices(${k(X)}, value);
  }`})();return{impl:()=>{let oe=[],X=!1;return m.offsetToIndices&&(oe.push(T),X=!0),m.indicesToOffset&&(oe.push(b),X=!0),m.broadcastedIndicesToOffset&&(Object.values(G).forEach(D=>oe.push(D)),X=!0),m.set&&(oe.push(N),X=!0),m.setByIndices&&(oe.push(le),X=!0),m.get&&(oe.push(Y),X=!0),m.getByIndices&&(oe.push(U),X=!0),!i&&X&&oe.unshift(`const ${I} = ${p.indices}(${n.join(",")});`,`const ${f} = ${p.indices}(${Ae.computeStrides(n).join(",")});`),oe.join(`
`)},type:p,offsetToIndices:M,indicesToOffset:A,broadcastedIndicesToOffset:j,indices:k,indicesGet:F,indicesSet:L,set:(...oe)=>{if(oe.length!==o+1)throw new Error(`indices length must be ${o}`);let X=oe[o];if(typeof X!="string")throw new Error("value must be string");let D=oe.slice(0,o).map(h).join(",");return o===0?R("0u",X):o===1?R(D[0],X):(m.set=!0,m.setByIndices=!0,m.indicesToOffset=!0,`set_${e}(${D}, ${X})`)},setByOffset:R,setByIndices:(oe,X)=>o<2?R(oe,X):(m.setByIndices=!0,m.indicesToOffset=!0,`set_${e}ByIndices(${oe}, ${X});`),get:te,getByOffset:K,getByIndices:ne,usage:r,name:e,strides:f,shape:I,rank:o}},Re=(e,t,n,r=1)=>La(e,t,n,"input",r),ht=(e,t,n,r=1)=>La(e,t,n,"output",r),ZI=(e,t,n)=>La(e,t,n,"atomicOutput",1),kg=(e,t,n,r=1)=>La(e,t,n,"internal",r),Hv=class{constructor(e,t){this.normalizedDispatchGroup=e,this.limits=t,this.internalVariables=[],this.variables=[],this.uniforms=[],this.variableIndex=0}guardAgainstOutOfBoundsWorkgroupSizes(e){return`if (global_idx >= ${typeof e=="number"?`${e}u`:e}) { return; }`}mainStart(e=Uo){let t=typeof e=="number"?e:e[0],n=typeof e=="number"?1:e[1],r=typeof e=="number"?1:e[2];if(t>this.limits.maxComputeWorkgroupSizeX||n>this.limits.maxComputeWorkgroupSizeY||r>this.limits.maxComputeWorkgroupSizeZ)throw new Error(`workgroup size [${t}, ${n}, ${r}] exceeds the maximum workgroup size [${this.limits.maxComputeWorkgroupSizeX}, ${this.limits.maxComputeWorkgroupSizeY}, ${this.limits.maxComputeWorkgroupSizeZ}].`);if(t*n*r>this.limits.maxComputeInvocationsPerWorkgroup)throw new Error(`workgroup size [${t}, ${n}, ${r}] exceeds the maximum workgroup invocations ${this.limits.maxComputeInvocationsPerWorkgroup}.`);let s=this.normalizedDispatchGroup[1]===1&&this.normalizedDispatchGroup[2]===1,i=s?`@builtin(global_invocation_id) global_id : vec3<u32>,
    @builtin(workgroup_id) workgroup_id : vec3<u32>,
    @builtin(local_invocation_index) local_idx : u32,
    @builtin(local_invocation_id) local_id : vec3<u32>`:`@builtin(global_invocation_id) global_id : vec3<u32>,
                                             @builtin(local_invocation_id) local_id : vec3<u32>,
    @builtin(local_invocation_index) local_idx : u32,
    @builtin(workgroup_id) workgroup_id : vec3<u32>,
    @builtin(num_workgroups) num_workgroups : vec3<u32>`,o=s?`let global_idx = global_id.x;
         let workgroup_index = workgroup_id.x;`:`let workgroup_index = workgroup_id.z * num_workgroups[0] * num_workgroups[1] +
             workgroup_id.y * num_workgroups[0] + workgroup_id.x;
         let global_idx = workgroup_index * ${t*n*r}u + local_idx;`;return`@compute @workgroup_size(${t}, ${n}, ${r})
  fn main(${i}) {
    ${o}
  `}appendVariableUniforms(e){e.rank!==0&&(e.shape.startsWith("uniforms.")&&this.uniforms.push({name:e.shape.replace("uniforms.",""),type:"u32",length:e.rank}),e.strides.startsWith("uniforms.")&&this.uniforms.push({name:e.strides.replace("uniforms.",""),type:"u32",length:e.rank}))}declareVariable(e,t){if(e.usage==="internal")throw new Error("cannot use internal variable with declareVariable(). use registerInternalVariables() instead.");this.variables.push(e),this.appendVariableUniforms(e);let n=e.usage==="input"?"read":"read_write",r=e.usage==="atomicOutput"?"atomic<i32>":e.type.storage;return`@group(0) @binding(${t}) var<storage, ${n}> ${e.name}: array<${r}>;`}declareVariables(...e){return e.map(t=>this.declareVariable(t,this.variableIndex++)).join(`
`)}registerInternalVariable(e){if(e.usage!=="internal")throw new Error("cannot use input or output variable with registerInternalVariable(). use declareVariables() instead.");this.internalVariables.push(e),this.appendVariableUniforms(e)}registerInternalVariables(...e){return e.forEach(t=>this.registerInternalVariable(t)),this}registerUniform(e,t,n=1){return this.uniforms.push({name:e,type:t,length:n}),this}registerUniforms(e){return this.uniforms=this.uniforms.concat(e),this}uniformDeclaration(){if(this.uniforms.length===0)return"";let e=[];for(let{name:t,type:n,length:r}of this.uniforms)if(r&&r>4)n==="f16"?e.push(`@align(16) ${t}:array<mat2x4<${n}>, ${Math.ceil(r/8)}>`):e.push(`${t}:array<vec4<${n}>, ${Math.ceil(r/4)}>`);else{let s=r==null||r===1?n:`vec${r}<${n}>`;e.push(`${t}:${s}`)}return`
      struct Uniforms { ${e.join(", ")} };
      @group(0) @binding(${this.variableIndex}) var<uniform> uniforms: Uniforms;`}get additionalImplementations(){return this.uniformDeclaration()+this.variables.map(e=>e.impl()).join(`
`)+this.internalVariables.map(e=>e.impl()).join(`
`)}get variablesInfo(){if(this.uniforms.length===0)return;let e=t=>[12,10,1,6][["u32","f16","f32","i32"].indexOf(t)];return this.uniforms.map(t=>[e(t.type),t.length??1])}},ek=(e,t)=>new Hv(e,t)}),Kv,Th,qv,Qv,Xv,Yv,Sr,tk,nk,hi=He(()=>{Mt(),Ft(),En(),Ot(),Kv=(e,t)=>{if(!e||e.length!==1)throw new Error("Transpose requires 1 input.");if(t.length!==0&&t.length!==e[0].dims.length)throw new Error(`perm size ${t.length} does not match input rank ${e[0].dims.length}`)},Th=(e,t)=>t.length!==0?t:[...new Array(e).keys()].reverse(),qv=(e,t)=>Ae.sortBasedOnPerm(e,Th(e.length,t)),Qv=(e,t,n,r)=>{let s=`fn perm(i: ${r.type.indices}) -> ${n.type.indices} {
    var a: ${n.type.indices};`;for(let i=0;i<t;++i)s+=`a[${e[i]}]=i[${i}];`;return s+="return a;}"},Xv=(e,t)=>{let n=[],r=[];for(let s=0;s<e.length;++s)e[s]!==1&&n.push(e[s]),e[t[s]]!==1&&r.push(t[s]);return{newShape:n,newPerm:r}},Yv=(e,t)=>{let n=0;for(let r=0;r<e.length;++r)if(t[e[r]]!==1){if(e[r]<n)return!1;n=e[r]}return!0},Sr=(e,t)=>{let n=e.dataType,r=e.dims.length,s=Th(r,t),i=qv(e.dims,s),o=e.dims,a=i,l=r<2||Yv(s,e.dims),c;if(l)return c=m=>{let g=Re("input",n,o,4),I=ht("output",n,a,4);return`
  ${m.registerUniform("output_size","u32").declareVariables(g,I)}
  ${m.mainStart()}
    ${m.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
    output[global_idx] = input[global_idx];
  }`},{name:"TransposeCopy",shaderCache:{inputDependencies:["type"]},getRunData:()=>{let m=Ae.size(i);return{outputs:[{dims:i,dataType:e.dataType}],dispatchGroup:{x:Math.ceil(m/64/4)},programUniforms:[{type:12,data:Math.ceil(m/4)}]}},getShaderSource:c};let{newShape:d,newPerm:u}=Xv(e.dims,s),p=Ae.areEqual(u,[2,3,1]),h=Ae.areEqual(u,[3,1,2]);if(d.length===2||p||h){o=p?[d[0],d[1]*d[2]]:h?[d[0]*d[1],d[2]]:d,a=[o[1],o[0]];let m=16;return c=g=>{let I=Re("a",n,o.length),f=ht("output",n,a.length);return`
  ${g.registerUniform("output_size","u32").declareVariables(I,f)}
  var<workgroup> tile : array<array<${f.type.value}, ${m+1}>, ${m}>;
  ${g.mainStart([m,m,1])}
    let stride = (uniforms.output_shape[1] - 1) / ${m} + 1;
    let workgroup_id_x = workgroup_index % stride;
    let workgroup_id_y = workgroup_index / stride;
    let input_col = workgroup_id_y * ${m}u + local_id.x;
    let input_row = workgroup_id_x * ${m}u + local_id.y;
    if (input_row < uniforms.a_shape[0] && input_col < uniforms.a_shape[1]) {
      tile[local_id.y][local_id.x] = ${I.getByIndices(`${I.type.indices}(input_row, input_col)`)};
    }
    workgroupBarrier();

    let output_col = workgroup_id_x * ${m}u + local_id.x;
    let output_row = workgroup_id_y * ${m}u + local_id.y;
    if (output_row < uniforms.output_shape[0] && output_col < uniforms.output_shape[1]) {
      ${f.setByIndices(`${f.type.indices}(output_row, output_col)`,"tile[local_id.x][local_id.y]")}
    }
  }`},{name:"TransposeShared",shaderCache:{inputDependencies:["type"]},getRunData:()=>{let g=Ae.size(i);return{outputs:[{dims:i,dataType:e.dataType}],dispatchGroup:{x:Math.ceil(a[1]/m),y:Math.ceil(a[0]/m)},programUniforms:[{type:12,data:g},...ft(o,a)]}},getShaderSource:c}}return c=m=>{let g=Re("a",n,o.length),I=ht("output",n,a.length);return`
  ${m.registerUniform("output_size","u32").declareVariables(g,I)}

  ${Qv(s,r,g,I)}

  ${m.mainStart()}
    ${m.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}

    let indices = ${I.offsetToIndices("global_idx")};
    let aIndices = perm(indices);

    ${I.setByOffset("global_idx",g.getByIndices("aIndices"))}
  }`},{name:"Transpose",shaderCache:{hint:`${t}`,inputDependencies:["rank"]},getRunData:()=>{let m=Ae.size(i);return{outputs:[{dims:i,dataType:e.dataType}],dispatchGroup:{x:Math.ceil(m/64)},programUniforms:[{type:12,data:m},...ft(o,a)]}},getShaderSource:c}},tk=(e,t)=>{Kv(e.inputs,t.perm),e.compute(Sr(e.inputs[0],t.perm))},nk=e=>Xt({perm:e.perm})}),Jv,Zv,eM,tM,nM,rM,sM,iM,oM,aM,Zr,rk,sk,ik,ok,ak,lk,ck,uk,dk,pk,SG=He(()=>{Mt(),Ft(),Ot(),Eg(),hi(),Jv={max:"select(bestValue, candidate, candidate > bestValue)",min:"select(bestValue, candidate, candidate < bestValue)",mean:"bestValue + candidate",sum:"bestValue + candidate",prod:"bestValue * candidate",sumSquare:"bestValue + candidate * candidate",logSumExp:"bestValue + exp(candidate)",l1:"bestValue + abs(candidate)",l2:"bestValue + candidate * candidate",logSum:"bestValue + candidate"},Zv={max:"select(bestValue, candidate, candidate > bestValue)",min:"select(bestValue, candidate, candidate < bestValue)",mean:"bestValue + candidate",sum:"bestValue + candidate",prod:"bestValue * candidate",sumSquare:"bestValue + candidate",logSumExp:"bestValue + candidate",l1:"bestValue + candidate",l2:"bestValue + candidate",logSum:"bestValue + candidate"},eM={max:"_A[offset]",min:"_A[offset]",mean:"0",sum:"0",prod:"1",sumSquare:"0",logSumExp:"0",l1:"0",l2:"0",logSum:"0"},tM={max:"bestValue",min:"bestValue",sum:"bestValue",prod:"bestValue",sumSquare:"bestValue",logSumExp:"log(bestValue)",l1:"bestValue",l2:"sqrt(bestValue)",logSum:"log(bestValue)"},nM=(e,t)=>{let n=[];for(let r=t-e;r<t;++r)n.push(r);return n},rM=(e,t)=>{let n=[],r=e.length;for(let i=0;i<r;i++)t.indexOf(i)===-1&&n.push(e[i]);let s=t.map(i=>e[i]);return[n,s]},sM=(e,t)=>{let n=e.length+t.length,r=[],s=0;for(let i=0;i<n;i++)t.indexOf(i)===-1?r.push(e[s++]):r.push(1);return r},iM=(e,t)=>{for(let n=0;n<e.length;++n)if(e[e.length-n-1]!==t-1-n)return!1;return!0},oM=(e,t)=>{let n=[];if(!iM(e,t)){for(let r=0;r<t;++r)e.indexOf(r)===-1&&n.push(r);e.forEach(r=>n.push(r))}return n},aM=(e,t,n,r,s,i,o)=>{let a=n[0].dims,l=Ae.size(i),c=Ae.size(o),d=Re("_A",n[0].dataType,a),u=ht("output",s,i),p=64;l===1&&(p=256);let h=`
          var<workgroup> aBestValues : array<f32, ${p}>;
       `,m=g=>`
        ${g.registerUniform("reduceSize","u32").declareVariables(d,u)}
        ${h}
        fn DIV_CEIL(a : u32, b : u32) -> u32 {
          return ((a - 1u) / b + 1u);
         }
         ${g.mainStart(p)}

          let outputIndex = global_idx / ${p};
          let offset = outputIndex * uniforms.reduceSize;

          var bestValue = f32(${eM[r]});
          let Length = uniforms.reduceSize;
          for (var k = local_idx; k < Length; k = k + ${p}) {
           let candidate = f32(${d.getByOffset("offset + k")});
           bestValue = ${Jv[r]};
          }
          aBestValues[local_idx] = bestValue;
          workgroupBarrier();

         var reduceSize = min(Length, ${p}u);
         for (var currentSize = reduceSize / 2u; reduceSize > 1u;
             currentSize = reduceSize / 2u) {
           let interval = DIV_CEIL(reduceSize, 2u);
           if (local_idx < currentSize) {
            let candidate = aBestValues[local_idx + interval];
            bestValue = ${Zv[r]};
            aBestValues[local_idx] = bestValue;
           }
           reduceSize = interval;
           workgroupBarrier();
         }

         if (local_idx == 0u) {
          ${u.setByOffset("outputIndex",`${r==="mean"?`${u.type.storage}(bestValue / f32(uniforms.reduceSize))`:`${u.type.storage}(${tM[r]})`}`)};
         }
        }`;return{name:e,shaderCache:{hint:`${t};${p}`,inputDependencies:["type"]},getShaderSource:m,getRunData:()=>({outputs:[{dims:i,dataType:s}],dispatchGroup:{x:l},programUniforms:[{type:12,data:c}]})}},Zr=(e,t,n,r)=>{let s=e.inputs.length===1?n:Zm(e.inputs,n),i=s.axes;i.length===0&&!s.noopWithEmptyAxes&&(i=e.inputs[0].dims.map((h,m)=>m));let o=Ae.normalizeAxes(i,e.inputs[0].dims.length),a=o,l=e.inputs[0],c=oM(a,e.inputs[0].dims.length);c.length>0&&(l=e.compute(Sr(e.inputs[0],c),{inputs:[0],outputs:[-1]})[0],a=nM(a.length,l.dims.length));let[d,u]=rM(l.dims,a),p=d;s.keepDims&&(p=sM(d,o)),e.compute(aM(t,s.cacheKey,[l],r,e.inputs[0].dataType,p,u),{inputs:[l]})},rk=(e,t)=>{Zr(e,"ReduceMeanShared",t,"mean")},sk=(e,t)=>{Zr(e,"ReduceL1Shared",t,"l1")},ik=(e,t)=>{Zr(e,"ReduceL2Shared",t,"l2")},ok=(e,t)=>{Zr(e,"ReduceLogSumExpShared",t,"logSumExp")},ak=(e,t)=>{Zr(e,"ReduceMaxShared",t,"max")},lk=(e,t)=>{Zr(e,"ReduceMinShared",t,"min")},ck=(e,t)=>{Zr(e,"ReduceProdShared",t,"prod")},uk=(e,t)=>{Zr(e,"ReduceSumShared",t,"sum")},dk=(e,t)=>{Zr(e,"ReduceSumSquareShared",t,"sumSquare")},pk=(e,t)=>{Zr(e,"ReduceLogSumShared",t,"logSum")}}),es,lM,wu,Zm,ts,cM,uM,dM,pM,hM,mM,fM,gM,_M,yM,ns,hk,mk,fk,gk,_k,yk,xk,bk,wk,vk,Eg=He(()=>{Mt(),Ft(),En(),Ot(),SG(),es=e=>{if(!e||e.length===0||e.length>2)throw new Error("Reduce op requires 1 or 2 inputs.");if(e.length===2&&e[1].dims.length!==1)throw new Error("Invalid axes input dims.")},lM=e=>["","",`var value = ${e.getByIndices("input_indices")};`,""],wu=(e,t,n,r,s,i,o=!1,a=!1)=>{let l=[],c=n[0].dims,d=c.length,u=Ae.normalizeAxes(s,d),p=!a&&u.length===0;c.forEach((g,I)=>{p||u.indexOf(I)>=0?o&&l.push(1):l.push(g)});let h=l.length,m=Ae.size(l);return{name:e,shaderCache:t,getShaderSource:g=>{let I=[],f=Re("_A",n[0].dataType,d),_=ht("output",i,h),T=r(f,_,u),M=T[2];for(let v=0,b=0;v<d;v++)p||u.indexOf(v)>=0?(o&&b++,M=`for(var j${v}: u32 = 0; j${v} < ${c[v]}; j${v}++) {
                  ${T[2].includes("last_index")?`let last_index = j${v};`:""}
                  ${f.indicesSet("input_indices",v,`j${v}`)}
                  ${M}
                }`):(I.push(`${f.indicesSet("input_indices",v,_.indicesGet("output_indices",b))};`),b++);return`

        ${g.registerUniform("output_size","u32").declareVariables(f,_)}

        ${g.mainStart()}
          ${g.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
          var input_indices: ${f.type.indices};
          let output_indices = ${_.offsetToIndices("global_idx")};

          ${I.join(`
`)}
          ${T[0]}       // init ops for reduce max/min
          ${T[1]}
          ${M}
          ${T[3]}
          ${T.length===4?_.setByOffset("global_idx","value"):T.slice(4).join(`
`)}
        }`},getRunData:()=>({outputs:[{dims:l,dataType:i}],dispatchGroup:{x:Math.ceil(m/64)},programUniforms:[{type:12,data:m},...ft(c,l)]})}},Zm=(e,t)=>{let n=[];return e[1].dims[0]>0&&e[1].getBigInt64Array().forEach(r=>n.push(Number(r))),Xt({axes:n,keepDims:t.keepDims,noopWithEmptyAxes:t.noopWithEmptyAxes})},ts=(e,t,n,r)=>{let s=e.inputs,i=s.length===1?n:Zm(s,n);e.compute(wu(t,{hint:i.cacheKey,inputDependencies:["rank"]},[s[0]],i.noopWithEmptyAxes&&i.axes.length===0?lM:r,i.axes,s[0].dataType,i.keepDims,i.noopWithEmptyAxes),{inputs:[0]})},cM=(e,t)=>{es(e.inputs),ts(e,"ReduceLogSum",t,(n,r)=>[`var value = ${r.type.storage}(0);`,"",`value += ${n.getByIndices("input_indices")};`,"value = log(value);"])},uM=(e,t)=>{es(e.inputs),ts(e,"ReduceL1",t,(n,r)=>[`var value = ${r.type.storage}(0);`,"",`value += abs(${n.getByIndices("input_indices")});`,""])},dM=(e,t)=>{es(e.inputs),ts(e,"ReduceL2",t,(n,r)=>[`var t = ${r.type.value}(0); var value = ${r.type.value}(0);`,"",`t = ${n.getByIndices("input_indices")}; value += (t * t);`,"value = sqrt(value);"])},pM=(e,t)=>{es(e.inputs),ts(e,"ReduceLogSumExp",t,(n,r)=>[`var value = ${r.type.storage}(0);`,"",`value += exp(${n.getByIndices("input_indices")});`,"value = log(value);"])},hM=(e,t)=>{es(e.inputs),ts(e,"ReduceMax",t,(n,r,s)=>{let i=[];for(let o=0;o<n.rank;o++)(s.indexOf(o)>=0||s.length===0)&&i.push(n.indicesSet("input_indices",o,0));return[`${i.join(`
`)}`,`var value = ${n.getByIndices("input_indices")};`,`value = max(value, ${n.getByIndices("input_indices")});`,""]})},mM=(e,t)=>{es(e.inputs),ts(e,"ReduceMean",t,(n,r,s)=>{let i=1;for(let o=0;o<n.rank;o++)(s.indexOf(o)>=0||s.length===0)&&(i*=e.inputs[0].dims[o]);return["var sum = f32(0);","",`sum += f32(${n.getByIndices("input_indices")});`,`let value = ${r.type.value}(sum / ${i});`]})},fM=(e,t)=>{es(e.inputs),ts(e,"ReduceMin",t,(n,r,s)=>{let i=[];for(let o=0;o<n.rank;o++)(s.indexOf(o)>=0||s.length===0)&&i.push(`input_indices[${o}] = 0;`);return[`${i.join(`
`)}`,`var value = ${n.getByIndices("input_indices")};`,`value = min(value, ${n.getByIndices("input_indices")});`,""]})},gM=(e,t)=>{es(e.inputs),ts(e,"ReduceProd",t,(n,r)=>[`var value = ${r.type.storage}(1);`,"",`value *= ${n.getByIndices("input_indices")};`,""])},_M=(e,t)=>{es(e.inputs),ts(e,"ReduceSum",t,(n,r)=>[`var value = ${r.type.storage}(0);`,"",`value += ${n.getByIndices("input_indices")};`,""])},yM=(e,t)=>{es(e.inputs),ts(e,"ReduceSumSquare",t,(n,r)=>[`var t = ${r.type.value}(0); var value = ${r.type.value}(0);`,"",`t = ${n.getByIndices("input_indices")}; value += t * t;`,""])},ns=(e,t,n)=>{if(t.length===0)return n;let r=1,s=1;for(let i=0;i<t.length;i++)t.indexOf(i)===-1?r*=e[i]:s*=e[i];return s<32&&r>1024},hk=(e,t)=>{ns(e.inputs[0].dims,t.axes,t.noopWithEmptyAxes)?mM(e,t):rk(e,t)},mk=(e,t)=>{ns(e.inputs[0].dims,t.axes,t.noopWithEmptyAxes)?uM(e,t):sk(e,t)},fk=(e,t)=>{ns(e.inputs[0].dims,t.axes,t.noopWithEmptyAxes)?dM(e,t):ik(e,t)},gk=(e,t)=>{ns(e.inputs[0].dims,t.axes,t.noopWithEmptyAxes)?pM(e,t):ok(e,t)},_k=(e,t)=>{ns(e.inputs[0].dims,t.axes,t.noopWithEmptyAxes)?hM(e,t):ak(e,t)},yk=(e,t)=>{ns(e.inputs[0].dims,t.axes,t.noopWithEmptyAxes)?fM(e,t):lk(e,t)},xk=(e,t)=>{ns(e.inputs[0].dims,t.axes,t.noopWithEmptyAxes)?gM(e,t):ck(e,t)},bk=(e,t)=>{ns(e.inputs[0].dims,t.axes,t.noopWithEmptyAxes)?_M(e,t):uk(e,t)},wk=(e,t)=>{ns(e.inputs[0].dims,t.axes,t.noopWithEmptyAxes)?yM(e,t):dk(e,t)},vk=(e,t)=>{ns(e.inputs[0].dims,t.axes,t.noopWithEmptyAxes)?cM(e,t):pk(e,t)}}),Ph,Mk,Ak,ef,LG=He(()=>{Mt(),En(),Eg(),Ph=e=>{if(!e||e.length===0||e.length>2)throw new Error("ArgMinMaxOp op requires 1 or 2 inputs.");if(e[0].dataType!==1)throw new Error("Invalid input type.")},Mk=(e,t)=>{Ph(e.inputs);let n=(r,s,i)=>{let o=[];for(let a=0;a<r.rank;a++)(i.indexOf(a)>=0||i.length===0)&&o.push(`input_indices[${a}] = 0;`);return[`${o.join(`
`)}`,`var value = ${r.getByIndices("input_indices")};
var best_index : i32 = 0;`,`if (${r.getByIndices("input_indices")} ${t.selectLastIndex>0?"<=":"<"} value) {
         value = ${r.getByIndices("input_indices")};
         best_index = i32(last_index);
       }`,"",s.setByOffset("global_idx","best_index")]};e.compute(wu("ArgMin",{hint:t.cacheKey,inputDependencies:["rank"]},[e.inputs[0]],n,[t.axis],7,t.keepDims),{inputs:[0]})},Ak=(e,t)=>{Ph(e.inputs);let n=(r,s,i)=>{let o=[];for(let a=0;a<r.rank;a++)(i.indexOf(a)>=0||i.length===0)&&o.push(`input_indices[${a}] = 0;`);return[`${o.join(`
`)}`,`var value = ${r.getByIndices("input_indices")};
var best_index : i32 = 0;`,`if (${r.getByIndices("input_indices")} ${t.selectLastIndex>0?">=":">"} value) {
         value = ${r.getByIndices("input_indices")};
         best_index = i32(last_index);
       }`,"",s.setByOffset("global_idx","best_index")]};e.compute(wu("argMax",{hint:t.cacheKey,inputDependencies:["rank"]},[e.inputs[0]],n,[t.axis],7,t.keepDims),{inputs:[0]})},ef=e=>Xt(e)}),xM,Fc,bM,wM,vM,gl,MM,Tk,Sg=He(()=>{Mt(),Ft(),Ig(),Ot(),xM=(e,t)=>{let n=e[0],r=e[1],s=e[2],i=e[3],o=e[4],a=e[5];if(o&&a)throw new Error("Attention cannot have both past and attention_bias");if(n.dims.length!==3)throw new Error('Input "input" must have 3 dimensions');let l=n.dims[0],c=n.dims[1],d=n.dims[2];if(s.dims.length!==1)throw new Error('Input "bias" is expected to have 1 dimensions');if(r.dims.length!==2)throw new Error('Input "weights" is expected to have 2 dimensions');if(r.dims[0]!==d)throw new Error("Input 1 dimension 0 should have same length as dimension 2 of input 0");if(s.dims[0]!==r.dims[1])throw new Error('Input "bias" dimension 0 should have same length as dimension 1 of input "weights"');let u=s.dims[0]/3,p=u,h=p;if(t.qkvHiddenSizes.length>0){if(t.qkvHiddenSizes.length!==3)throw new Error("qkv_hidden_sizes attribute should have 3 elements");for(let T of t.qkvHiddenSizes)if(T%t.numHeads!==0)throw new Error("qkv_hidden_sizes should be divisible by num_heads");u=t.qkvHiddenSizes[0],p=t.qkvHiddenSizes[1],h=t.qkvHiddenSizes[2]}let m=c;if(u!==p)throw new Error("qkv_hidden_sizes first element should be same as the second");if(s.dims[0]!==u+p+h)throw new Error('Input "bias" dimension 0 should have same length as sum of Q/K/V hidden sizes');let g=0;if(o){if(p!==h)throw new Error('Input "past" expect k_hidden_size == v_hidden_size');if(o.dims.length!==5)throw new Error('Input "past" must have 5 dimensions');if(o.dims[0]!==2)throw new Error('Input "past" first dimension must be 2');if(o.dims[1]!==l)throw new Error('Input "past" second dimension must be batch_size');if(o.dims[2]!==t.numHeads)throw new Error('Input "past" third dimension must be num_heads');if(o.dims[4]!==p/t.numHeads)throw new Error('Input "past" fifth dimension must be k_hidden_size / num_heads');t.pastPresentShareBuffer||(g=o.dims[3])}let I=m+g,f=-1,_=0;if(i)throw new Error("Mask not supported");if(o)throw new Error("past is not supported");if(a){if(a.dims.length!==4)throw new Error('Input "attention_bias" must have 4 dimensions');if(a.dims[0]!==l||a.dims[1]!==t.numHeads||a.dims[2]!==c||a.dims[3]!==I)throw new Error('Expect "attention_bias" shape (batch_size, num_heads, sequence_length, total_sequence_length)')}return{batchSize:l,sequenceLength:c,pastSequenceLength:g,kvSequenceLength:m,totalSequenceLength:I,maxSequenceLength:f,inputHiddenSize:d,hiddenSize:u,vHiddenSize:h,headSize:Math.floor(u/t.numHeads),vHeadSize:Math.floor(h/t.numHeads),numHeads:t.numHeads,isUnidirectional:!1,pastPresentShareBuffer:!1,maskFilterValue:t.maskFilterValue,maskType:_,scale:t.scale,broadcastResPosBias:!1,passPastInKv:!1,qkvFormat:1}},Fc=(e,t,n)=>t&&e?`
      let total_sequence_length_input = u32(${t.getByOffset("0")});
      let present_sequence_length = max(total_sequence_length_input, uniforms.past_sequence_length);
      let is_subsequent_prompt: bool = sequence_length > 1 && sequence_length != total_sequence_length_input;
      let is_first_prompt: bool = is_subsequent_prompt == false && sequence_length == total_sequence_length_input;
      total_sequence_length = u32(${e?.getByOffset("batchIdx")}) + 1;
      var past_sequence_length: u32 = 0;
      if (is_first_prompt == false) {
        past_sequence_length = total_sequence_length - sequence_length;
      }
       `:`
    ${n?"let past_sequence_length = uniforms.past_sequence_length":""};
    let present_sequence_length = total_sequence_length;
    `,bM=(e,t,n,r,s,i,o,a)=>{let l=An(o?1:i),c=64,d=i/l;d<c&&(c=32);let u=Math.ceil(i/l/c),p=[{type:12,data:t},{type:12,data:n},{type:12,data:r},{type:12,data:s},{type:12,data:d},{type:12,data:u}],h=Jn(e.dataType,l),m=ar(1,l),g=["type"];o&&g.push("type"),a&&g.push("type");let I=f=>{let _=ht("x",e.dataType,e.dims,l),T=[_],M=o?Re("seq_lens",o.dataType,o.dims):void 0;M&&T.push(M);let v=a?Re("total_sequence_length_input",a.dataType,a.dims):void 0;v&&T.push(v);let b=ar(e.dataType),A=[{name:"batch_size",type:"u32"},{name:"num_heads",type:"u32"},{name:"past_sequence_length",type:"u32"},{name:"sequence_length",type:"u32"},{name:"total_sequence_length",type:"u32"},{name:"elements_per_thread",type:"u32"}];return`
  var<workgroup> thread_max: array<f32, ${c}>;
  var<workgroup> thread_sum: array<f32, ${c}>;
  ${f.registerUniforms(A).declareVariables(...T)}
  ${f.mainStart([c,1,1])}
    let batchIdx = workgroup_id.z / uniforms.num_heads;
    let headIdx = workgroup_id.z % uniforms.num_heads;
    let sequence_length = uniforms.sequence_length;
    var total_sequence_length = uniforms.total_sequence_length;
    ${Fc(M,v,!1)}
    let local_offset = local_idx * uniforms.elements_per_thread;
    let offset = (global_idx / ${c}) * uniforms.total_sequence_length + local_offset;
    let seq_causal_length = ${o?"u32(past_sequence_length + workgroup_id.y + 1)":"total_sequence_length"};
    var thread_max_vector = ${m}(-3.402823e+38f);
    for (var i: u32 = 0; i < uniforms.elements_per_thread && i + local_offset < seq_causal_length; i++) {
      thread_max_vector = max(${m}(x[offset + i]), thread_max_vector);
    }
    thread_max[local_idx] = ${(()=>{switch(l){case 1:return"thread_max_vector";case 2:return"max(thread_max_vector.x, thread_max_vector.y)";case 4:return"max(max(thread_max_vector.x, thread_max_vector.y), max(thread_max_vector.z, thread_max_vector.w))";default:throw new Error(`Unsupported components: ${l}`)}})()};
    workgroupBarrier();

    var max_value =  f32(-3.402823e+38f);
    for (var i = 0u; i < ${c}; i++) {
      max_value = max(thread_max[i], max_value);
    }

    var sum_vector = ${m}(0);
    for (var i: u32 = 0; i < uniforms.elements_per_thread && i + local_offset < seq_causal_length; i++) {
      sum_vector += exp(${m}(x[offset + i]) - max_value);
    }
    thread_sum[local_idx] = ${(()=>{switch(l){case 1:return"sum_vector";case 2:return"sum_vector.x + sum_vector.y";case 4:return"sum_vector.x + sum_vector.y + sum_vector.z + sum_vector.w";default:throw new Error(`Unsupported components: ${l}`)}})()};
    workgroupBarrier();

    var sum: f32 = 0;
    for (var i = 0u; i < ${c}; i++) {
      sum += thread_sum[i];
    }

    if (sum == 0) {
      for (var i: u32 = 0; i < uniforms.elements_per_thread && i + local_offset < seq_causal_length; i++) {
        x[offset + i] = ${_.type.value}(${b}(1.0) / ${b}(seq_causal_length));
      }
    } else {
      for (var i: u32 = 0; i < uniforms.elements_per_thread && i + local_offset < seq_causal_length; i++) {
        var f32input = ${m}(x[offset + i]);
        x[offset + i] = ${_.type.value}(exp(f32input - max_value) / sum);
      }
    }
      ${o?`
        for (var total_seq_id: u32 = seq_causal_length; total_seq_id + local_offset < uniforms.total_sequence_length; total_seq_id++) {
          x[offset + total_seq_id] = ${_.type.value}(${b}(0));
        }`:""};
  }`};return{name:"AttentionProbsSoftmax",shaderCache:{hint:`${c};${h};${l}`,inputDependencies:g},getShaderSource:I,getRunData:()=>({outputs:[],dispatchGroup:{x:1,y:s,z:t*n},programUniforms:p})}},wM=(e,t,n,r,s,i,o,a,l)=>{let c=o+i.kvSequenceLength,d=[i.batchSize,i.numHeads,i.sequenceLength,c],u=e>1&&r,p=i.kvNumHeads?i.kvNumHeads:i.numHeads,h=u?[i.batchSize,p,c,i.headSize]:void 0,m=i.nReps?i.nReps:1,g=i.scale===0?1/Math.sqrt(i.headSize):i.scale,I=An(i.headSize),f=i.headSize/I,_=12,T={x:Math.ceil(c/_),y:Math.ceil(i.sequenceLength/_),z:i.batchSize*i.numHeads},M=[{type:12,data:i.sequenceLength},{type:12,data:f},{type:12,data:c},{type:12,data:i.numHeads},{type:12,data:i.headSize},{type:1,data:g},{type:12,data:o},{type:12,data:i.kvSequenceLength},{type:12,data:m}],v=u&&r&&Ae.size(r.dims)>0,b=["type","type"];v&&b.push("type"),s&&b.push("type"),a&&b.push("type"),l&&b.push("type");let A=[{dims:d,dataType:t.dataType,gpuDataType:0}];u&&A.push({dims:h,dataType:t.dataType,gpuDataType:0});let k=F=>{let L=Re("q",t.dataType,t.dims,I),G=Re("key",n.dataType,n.dims,I),j=[L,G];if(v){let le=Re("past_key",r.dataType,r.dims,I);j.push(le)}s&&j.push(Re("attention_bias",s.dataType,s.dims));let R=a?Re("seq_lens",a.dataType,a.dims):void 0;R&&j.push(R);let K=l?Re("total_sequence_length_input",l.dataType,l.dims):void 0;K&&j.push(K);let U=ht("output",t.dataType,d),Y=[U];u&&Y.push(ht("present_key",t.dataType,h,I));let te=ar(1,I),ne=[{name:"M",type:"u32"},{name:"K",type:"u32"},{name:"N",type:"u32"},{name:"num_heads",type:"u32"},{name:"head_size",type:"u32"},{name:"alpha",type:"f32"},{name:"past_sequence_length",type:"u32"},{name:"kv_sequence_length",type:"u32"},{name:"n_reps",type:"u32"}];return`
  const TILE_SIZE = ${_}u;

  var<workgroup> tileQ: array<${L.type.storage}, ${_*_}>;
  var<workgroup> tileK: array<${L.type.storage}, ${_*_}>;
  ${F.registerUniforms(ne).declareVariables(...j,...Y)}
  ${F.mainStart([_,_,1])}
    // x holds the N and y holds the M
    let headIdx = workgroup_id.z % uniforms.num_heads;
    let kvHeadIdx = ${m===1?"headIdx":"headIdx / uniforms.n_reps"};
    let kv_num_heads = ${m===1?"uniforms.num_heads":"uniforms.num_heads / uniforms.n_reps"};
    let batchIdx = workgroup_id.z / uniforms.num_heads;
    let m = workgroup_id.y * TILE_SIZE;
    let n = workgroup_id.x * TILE_SIZE;
    let sequence_length = uniforms.M;
    var total_sequence_length = uniforms.N;
    ${Fc(R,K,!0)}
    let absKvHeadIdx = batchIdx * kv_num_heads + kvHeadIdx;
    let qOffset = workgroup_id.z * uniforms.M * uniforms.K + m * uniforms.K;
    ${v&&u?"let pastKeyOffset = absKvHeadIdx * uniforms.past_sequence_length * uniforms.K;":""};
    let kOffset = absKvHeadIdx * uniforms.kv_sequence_length * uniforms.K;
    ${u?"let presentKeyOffset = absKvHeadIdx * uniforms.N * uniforms.K;":""}
    var value = ${te}(0);
    for (var w: u32 = 0u; w < uniforms.K; w += TILE_SIZE) {
      if (global_id.y < uniforms.M && w + local_id.x < uniforms.K) {
        tileQ[TILE_SIZE * local_id.y + local_id.x] = q[qOffset + local_id.y * uniforms.K + w + local_id.x];
      }
      if (n + local_id.y < uniforms.N && w + local_id.x < uniforms.K) {
        var idx = TILE_SIZE * local_id.y + local_id.x;
      ${v&&u?`
              if (n + local_id.y < past_sequence_length) {
                tileK[idx] = past_key[pastKeyOffset + (n + local_id.y) * uniforms.K + w + local_id.x];
              } else if (n + local_id.y - past_sequence_length < uniforms.kv_sequence_length) {
                tileK[idx] = key[kOffset + (n + local_id.y - past_sequence_length) * uniforms.K + w + local_id.x];
              }`:`
          if (n + local_id.y < uniforms.kv_sequence_length) {
            tileK[idx] = key[kOffset + (n + local_id.y) * uniforms.K + w + local_id.x];
          }`}
      ${u?`if (n + local_id.y < present_sequence_length) {
        present_key[presentKeyOffset + (n + local_id.y) * uniforms.K + w + local_id.x] = tileK[idx];
      }`:""}
      }
      workgroupBarrier();

      for (var k: u32 = 0u; k < TILE_SIZE && w+k < uniforms.K; k++) {
          value += ${te}(tileQ[TILE_SIZE * local_id.y + k] * tileK[TILE_SIZE * local_id.x + k]);
      }

      workgroupBarrier();
    }

    if (global_id.y < uniforms.M && global_id.x < total_sequence_length) {
      let headOffset = workgroup_id.z * uniforms.M * uniforms.N;
      let outputIdx = headOffset + global_id.y * uniforms.N + global_id.x;
      var sum: f32 = ${(()=>{switch(I){case 1:return"value";case 2:return"value.x + value.y";case 4:return"value.x + value.y + value.z + value.w";default:throw new Error(`Unsupported components: ${I}`)}})()};
        output[outputIdx] = ${U.type.value} (sum * uniforms.alpha) + ${s?"attention_bias[outputIdx]":"0.0"};
    }
  }`};return{name:"AttentionProbs",shaderCache:{hint:`${I};${s!==void 0};${r!==void 0};${e}`,inputDependencies:b},getRunData:()=>({outputs:A,dispatchGroup:T,programUniforms:M}),getShaderSource:k}},vM=(e,t,n,r,s,i,o=void 0,a=void 0)=>{let l=i+s.kvSequenceLength,c=s.nReps?s.nReps:1,d=s.vHiddenSize*c,u=e>1&&r,p=s.kvNumHeads?s.kvNumHeads:s.numHeads,h=u?[s.batchSize,p,l,s.headSize]:void 0,m=[s.batchSize,s.sequenceLength,d],g=12,I={x:Math.ceil(s.vHeadSize/g),y:Math.ceil(s.sequenceLength/g),z:s.batchSize*s.numHeads},f=[{type:12,data:s.sequenceLength},{type:12,data:l},{type:12,data:s.vHeadSize},{type:12,data:s.numHeads},{type:12,data:s.headSize},{type:12,data:d},{type:12,data:i},{type:12,data:s.kvSequenceLength},{type:12,data:c}],_=u&&r&&Ae.size(r.dims)>0,T=["type","type"];_&&T.push("type"),o&&T.push("type"),a&&T.push("type");let M=[{dims:m,dataType:t.dataType,gpuDataType:0}];u&&M.push({dims:h,dataType:t.dataType,gpuDataType:0});let v=b=>{let A=Re("probs",t.dataType,t.dims),k=Re("v",n.dataType,n.dims),F=[A,k];_&&F.push(Re("past_value",r.dataType,r.dims));let L=o?Re("seq_lens",o.dataType,o.dims):void 0;o&&F.push(L);let G=a?Re("total_sequence_length_input",a.dataType,a.dims):void 0;a&&F.push(G);let j=[ht("output",t.dataType,m)];u&&j.push(ht("present_value",t.dataType,h));let R=[{name:"M",type:"u32"},{name:"K",type:"u32"},{name:"N",type:"u32"},{name:"num_heads",type:"u32"},{name:"head_size",type:"u32"},{name:"v_hidden_size",type:"u32"},{name:"past_sequence_length",type:"u32"},{name:"kv_sequence_length",type:"u32"},{name:"n_reps",type:"u32"}];return`
  const TILE_SIZE = ${g}u;
  var<workgroup> tileQ: array<${A.type.value}, ${g*g}>;
  var<workgroup> tileV: array<${A.type.value}, ${g*g}>;
  ${b.registerUniforms(R).declareVariables(...F,...j)}
  ${b.mainStart([g,g,1])}
   let headIdx = workgroup_id.z % uniforms.num_heads;
   let batchIdx = workgroup_id.z / uniforms.num_heads;
   let kvHeadIdx = ${c===1?"headIdx":"headIdx / uniforms.n_reps"};
   let kv_num_heads = ${c===1?"uniforms.num_heads":"uniforms.num_heads / uniforms.n_reps"};
   let m = global_id.y;
   let n = global_id.x;
   let sequence_length = uniforms.M;
   var total_sequence_length = uniforms.K;
   ${Fc(L,G,!0)}
   let offsetA = workgroup_id.z * uniforms.M * uniforms.K + m * uniforms.K;
   let absKvHeadIdx = batchIdx * kv_num_heads + kvHeadIdx; // kvHeadIdx is relative to the batch
   ${_&&u?"let pastValueOffset = absKvHeadIdx * uniforms.N * uniforms.past_sequence_length + n;":""};
   let vOffset = absKvHeadIdx * uniforms.N * uniforms.kv_sequence_length + n;
   ${u?"let presentValueOffset = absKvHeadIdx * uniforms.N * uniforms.K + n;":""}
   var value = ${A.type.storage}(0);
   for (var w: u32 = 0u; w < uniforms.K; w += TILE_SIZE) {
      if (m < uniforms.M && w + local_id.x < uniforms.K) {
        tileQ[TILE_SIZE * local_id.y + local_id.x] = probs[offsetA + w + local_id.x];
      }
      if (n < uniforms.N && w + local_id.y < uniforms.K) {
        var idx = TILE_SIZE * local_id.y + local_id.x;
        ${_&&u?`
        if (w + local_id.y < past_sequence_length) {
          tileV[idx] = past_value[pastValueOffset + (w + local_id.y) * uniforms.N];
        } else if (w + local_id.y - past_sequence_length < uniforms.kv_sequence_length) {
          tileV[idx] = v[vOffset + (w + local_id.y - past_sequence_length) * uniforms.N];
        }
      `:`
            if (w + local_id.y < uniforms.kv_sequence_length) {
              tileV[idx] = v[vOffset + (w + local_id.y) * uniforms.N];
            }`}
        ${u?`
            if (w + local_id.y < present_sequence_length) {
          present_value[presentValueOffset + (w + local_id.y) * uniforms.N] = tileV[idx];
        }`:""}
      }
     workgroupBarrier();
     for (var k: u32 = 0u; k < TILE_SIZE && w+k < total_sequence_length; k++) {
       value += tileQ[TILE_SIZE * local_id.y + k] * tileV[TILE_SIZE * k + local_id.x];
     }
     workgroupBarrier();
   }

   // we need to transpose output from BNSH_v to BSND_v
   if (m < uniforms.M && n < uniforms.N) {
     let outputIdx = batchIdx * uniforms.M * uniforms.v_hidden_size + m * uniforms.v_hidden_size
       + headIdx * uniforms.N + n;
     output[outputIdx] = value;
   }
  }`};return{name:"AttentionScore",shaderCache:{hint:`${r!==void 0};${e}`,inputDependencies:T},getRunData:()=>({outputs:M,dispatchGroup:I,programUniforms:f}),getShaderSource:v}},gl=(e,t,n,r,s,i,o,a,l,c,d=void 0,u=void 0)=>{let p=Math.min(e.outputCount,1+(o?1:0)+(a?1:0)),h=p>1?c.pastSequenceLength:0,m=h+c.kvSequenceLength,g=l&&Ae.size(l.dims)>0?l:void 0,I=[t,n];p>1&&o&&Ae.size(o.dims)>0&&I.push(o),g&&I.push(g),d&&I.push(d),u&&I.push(u);let f=e.compute(wM(p,t,n,o,g,c,h,d,u),{inputs:I,outputs:p>1?[-1,1]:[-1]})[0];e.compute(bM(f,c.batchSize,c.numHeads,h,c.sequenceLength,m,d,u),{inputs:d&&u?[f,d,u]:[f],outputs:[]});let _=[f,r];p>1&&a&&Ae.size(a.dims)>0&&_.push(a),d&&_.push(d),u&&_.push(u),e.compute(vM(p,f,r,a,c,h,d,u),{inputs:_,outputs:p>1?[0,2]:[0]})},MM=(e,t)=>{let n=[t.batchSize,t.numHeads,t.sequenceLength,t.headSize],r=t.sequenceLength,s=t.inputHiddenSize,i=t.headSize,o=12,a={x:Math.ceil(t.headSize/o),y:Math.ceil(t.sequenceLength/o),z:t.batchSize*t.numHeads},l=[e.inputs[0],e.inputs[1],e.inputs[2]],c=[{type:12,data:r},{type:12,data:s},{type:12,data:i},{type:12,data:t.numHeads},{type:12,data:t.headSize},{type:12,data:t.hiddenSize},{type:12,data:t.hiddenSize+t.hiddenSize+t.vHiddenSize}],d=u=>{let p=ht("output_q",l[0].dataType,n),h=ht("output_k",l[0].dataType,n),m=ht("output_v",l[0].dataType,n),g=Re("input",l[0].dataType,l[0].dims),I=Re("weight",l[1].dataType,l[1].dims),f=Re("bias",l[2].dataType,l[2].dims),_=g.type.storage,T=[{name:"M",type:"u32"},{name:"K",type:"u32"},{name:"N",type:"u32"},{name:"num_heads",type:"u32"},{name:"head_size",type:"u32"},{name:"hidden_size",type:"u32"},{name:"ldb",type:"u32"}];return`
  const TILE_SIZE = ${o}u;
  var<workgroup> tileInput: array<${_}, ${o*o}>;
  var<workgroup> tileWeightQ: array<${_}, ${o*o}>;
  var<workgroup> tileWeightK: array<${_}, ${o*o}>;
  var<workgroup> tileWeightV: array<${_}, ${o*o}>;
  ${u.registerUniforms(T).declareVariables(g,I,f,p,h,m)}
  ${u.mainStart([o,o,1])}
    let batchIndex = workgroup_id.z / uniforms.num_heads;
    let headNumber = workgroup_id.z % uniforms.num_heads;
    let m = global_id.y;
    let n = global_id.x;

    let inputOffset = batchIndex * (uniforms.M * uniforms.K) + m * uniforms.K;
    let biasOffsetQ = headNumber * uniforms.head_size;
    let biasOffsetK = uniforms.hidden_size + biasOffsetQ;
    let biasOffsetV = uniforms.hidden_size + biasOffsetK;

    var valueQ = ${_}(0);
    var valueK = ${_}(0);
    var valueV = ${_}(0);
    for (var w: u32 = 0u; w < uniforms.K; w += TILE_SIZE) {
      if (m < uniforms.M && w + local_id.x < uniforms.K) {
        tileInput[TILE_SIZE * local_id.y + local_id.x] = input[inputOffset + w + local_id.x];
      }
      if (n < uniforms.N && w + local_id.y < uniforms.K) {
        let offset = n + (w + local_id.y) * uniforms.ldb;
        tileWeightQ[TILE_SIZE * local_id.y + local_id.x] = weight[biasOffsetQ + offset];
        tileWeightK[TILE_SIZE * local_id.y + local_id.x] = weight[biasOffsetK + offset];
        tileWeightV[TILE_SIZE * local_id.y + local_id.x] = weight[biasOffsetV + offset];
      }
      workgroupBarrier();
      for (var k: u32 = 0u; k<TILE_SIZE && w+k < uniforms.K; k++) {
        let inputTileOffset = TILE_SIZE * local_id.y + k;
        let weightTileOffset = TILE_SIZE * k + local_id.x;
        valueQ += tileInput[inputTileOffset] * tileWeightQ[weightTileOffset];
        valueK += tileInput[inputTileOffset] * tileWeightK[weightTileOffset];
        valueV += tileInput[inputTileOffset] * tileWeightV[weightTileOffset];
      }

      workgroupBarrier();
    }

    let headOffset = (m * uniforms.N + n) % uniforms.head_size;
    valueQ += bias[headOffset + biasOffsetQ];
    valueK += bias[headOffset + biasOffsetK];
    valueV += bias[headOffset + biasOffsetV];

    let offset = workgroup_id.z * uniforms.M * uniforms.N;
    if (m < uniforms.M && n < uniforms.N) {
      let outputIdx = offset + m * uniforms.N + n;
      output_q[outputIdx] = valueQ;
      output_k[outputIdx] = valueK;
      output_v[outputIdx] = valueV;
    }
  }`};return e.compute({name:"AttentionPrepare",shaderCache:{inputDependencies:["type","type","type"]},getRunData:()=>({outputs:[{dims:n,dataType:e.inputs[0].dataType,gpuDataType:0},{dims:n,dataType:e.inputs[0].dataType,gpuDataType:0},{dims:n,dataType:e.inputs[0].dataType,gpuDataType:0}],dispatchGroup:a,programUniforms:c}),getShaderSource:d},{inputs:l,outputs:[-1,-1,-1]})},Tk=(e,t)=>{let n=xM(e.inputs,t),[r,s,i]=MM(e,n);return gl(e,r,s,i,e.inputs[4],void 0,void 0,void 0,e.inputs[5],n)}}),AM,TM,PM,Pk,DG=He(()=>{ds(),Mt(),Ft(),En(),Ot(),AM=(e,t)=>{if(!e||e.length!==5)throw new Error("BatchNormalization requires 5 inputs");let n=(r,s,i)=>{let o=s.length;if(o!==r.length)throw new Error(`${i}: num dimensions != ${o}`);s.forEach((a,l)=>{if(a!==r[l])throw new Error(`${i}: dim[${l}] do not match`)})};if(e[0].dims.length>1){let r=t.format==="NHWC"?t.spatial?e[0].dims.slice(-1):e[0].dims.slice(-1).concat(e[0].dims.slice(1,e[0].dims.length-1)):e[0].dims.slice(1,t.spatial?2:void 0);n(e[1].dims,r,"Invalid input scale"),n(e[2].dims,r,"Invalid input B"),n(e[3].dims,r,"Invalid input mean"),n(e[4].dims,r,"Invalid input var")}else n(e[1].dims,[1],"Invalid input scale"),n(e[2].dims,[1],"Invalid input B"),n(e[3].dims,[1],"Invalid input mean"),n(e[4].dims,[1],"Invalid input var")},TM=(e,t)=>{let{epsilon:n,spatial:r,format:s}=t,i=e[0].dims,o=r?An(i[i.length-1]):1,a=s==="NHWC"&&i.length>1?o:1,l=Ae.size(i)/o,c=r,d=c?i.length:i,u=Re("x",e[0].dataType,e[0].dims,o),p=Re("scale",e[1].dataType,e[1].dims,a),h=Re("bias",e[2].dataType,e[2].dims,a),m=Re("inputMean",e[3].dataType,e[3].dims,a),g=Re("inputVar",e[4].dataType,e[4].dims,a),I=ht("y",e[0].dataType,d,o),f=()=>{let T="";if(r)T=`let cOffset = ${i.length===1?"0u":s==="NHWC"?`outputIndices[${i.length-1}] / ${o}`:"outputIndices[1]"};`;else if(s==="NCHW")T=`
            ${I.indicesSet("outputIndices","0","0")}
            let cOffset = ${I.indicesToOffset("outputIndices")};`;else{T=`var cIndices = ${p.type.indices}(0);
                       cIndices[0] = outputIndices[${i.length-1}];`;for(let M=1;M<p.rank;M++)T+=`cIndices[${M}] = outputIndices[${M}];`;T+=`let cOffset = ${p.indicesToOffset("cIndices")};`}return T},_=T=>`
  const epsilon = ${n};
  ${T.registerUniform("outputSize","u32").declareVariables(u,p,h,m,g,I)}
  ${T.mainStart()}
  ${T.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.outputSize")}
    var outputIndices = ${I.offsetToIndices(`global_idx * ${o}`)};
    ${f()}
    let scale = ${p.getByOffset("cOffset")};
    let bias = ${h.getByOffset("cOffset")};
    let inputMean = ${m.getByOffset("cOffset")};
    let inputVar = ${g.getByOffset("cOffset")};
    let x = ${u.getByOffset("global_idx")};
    let value = (x - inputMean) * inverseSqrt(inputVar + epsilon) * scale + bias;
    ${I.setByOffset("global_idx","value")}
  }`;return{name:"BatchNormalization",shaderCache:{hint:`${t.epsilon}_${t.format}_${r}_${o}`,inputDependencies:c?["rank","type","type","type","type"]:void 0},getShaderSource:_,getRunData:()=>({outputs:[{dims:e[0].dims,dataType:e[0].dataType}],dispatchGroup:{x:Math.ceil(l/64)},programUniforms:c?[{type:12,data:l},...ft(i)]:[{type:12,data:l}]})}},PM=e=>Xt(e),Pk=(e,t)=>{let{inputs:n,outputCount:r}=e,s=PM({...t,outputCount:r});if(ln.webgpu.validateInputContent&&AM(n,s),t.trainingMode)throw new Error("BatchNormalization trainingMode is not supported yet.");e.compute(TM(n,s))}}),CM,IM,Ck,$G=He(()=>{Ft(),Ot(),CM=e=>{if(e[0].dims.length!==3)throw new Error("input should have 3 dimensions");if(![320,640,1280].includes(e[0].dims[2]))throw new Error("number of channels should be 320, 640 or 1280");if(e[1].dims.length!==1)throw new Error("bias is expected to have 1 dimensions");if(e[0].dims[2]!==e[1].dims[0])throw new Error("last dimension of input and bias are not the same")},IM=e=>{let t=e[0].dims,n=e[0].dims[2],r=Ae.size(t)/4,s=e[0].dataType,i=Re("input",s,t,4),o=Re("bias",s,[n],4),a=Re("residual",s,t,4),l=ht("output",s,t,4);return{name:"BiasAdd",getRunData:()=>({outputs:[{dims:t,dataType:e[0].dataType}],dispatchGroup:{x:Math.ceil(r/64)}}),getShaderSource:c=>`
  const channels = ${n}u / 4;
  ${c.declareVariables(i,o,a,l)}

  ${c.mainStart()}
    ${c.guardAgainstOutOfBoundsWorkgroupSizes(r)}
    let value = ${i.getByOffset("global_idx")}
      + ${o.getByOffset("global_idx % channels")} + ${a.getByOffset("global_idx")};
    ${l.setByOffset("global_idx","value")}
  }`}},Ck=e=>{CM(e.inputs),e.compute(IM(e.inputs))}}),kM,Kt,Ik,kk,Ek,Sk,Lk,Dk,$k,Fk,Ok,EM,jk,Rk,Nk,zk,qa,Bk,ru,Gk,Vk,Uk,Wk,Hk,Kk,qk,Qk,Xk,Yk,Jk,Zk,eE,tE,nE,rE,Ch,sE,tf,nf,iE,oE,aE,SM,LM,lE,Lg=He(()=>{Mt(),Ft(),En(),Ot(),kM=(e,t,n,r,s,i,o)=>{let a=Math.ceil(t/4),l="";typeof s=="string"?l=`${s}(a)`:l=s("a");let c=Re("inputData",n,[a],4),d=ht("outputData",r,[a],4),u=[{name:"vec_size",type:"u32"}];return o&&u.push(...o),`
      ${e.registerUniforms(u).declareVariables(c,d)}

  ${i??""}

  ${e.mainStart()}
    ${e.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.vec_size")}

    let a = ${c.getByOffset("global_idx")};
    ${d.setByOffset("global_idx",l)}
  }`},Kt=(e,t,n,r,s,i=e.dataType,o,a)=>{let l=[{type:12,data:Math.ceil(Ae.size(e.dims)/4)}];return o&&l.push(...o),{name:t,shaderCache:{hint:s,inputDependencies:["type"]},getShaderSource:c=>kM(c,Ae.size(e.dims),e.dataType,i,n,r,a),getRunData:c=>({outputs:[{dims:e.dims,dataType:i}],dispatchGroup:{x:Math.ceil(Ae.size(c[0].dims)/64/4)},programUniforms:l})}},Ik=e=>{e.compute(Kt(e.inputs[0],"Abs","abs"))},kk=e=>{e.compute(Kt(e.inputs[0],"Acos","acos"))},Ek=e=>{e.compute(Kt(e.inputs[0],"Acosh","acosh"))},Sk=e=>{e.compute(Kt(e.inputs[0],"Asin","asin"))},Lk=e=>{e.compute(Kt(e.inputs[0],"Asinh","asinh"))},Dk=e=>{e.compute(Kt(e.inputs[0],"Atan","atan"))},$k=e=>{e.compute(Kt(e.inputs[0],"Atanh","atanh"))},Fk=e=>Xt(e),Ok=(e,t)=>{let n;switch(t.to){case 10:n="vec4<f16>";break;case 1:n="vec4<f32>";break;case 12:n="vec4<u32>";break;case 6:n="vec4<i32>";break;case 9:n="vec4<bool>";break;default:throw new RangeError(`not supported type (specified in attribute 'to' from 'Cast' operator): ${t.to}`)}e.compute(Kt(e.inputs[0],"Cast",n,void 0,t.cacheKey,t.to))},EM=e=>{let t,n,r=e.length>=2&&e[1].data!==0,s=e.length>=3&&e[2].data!==0;switch(e[0].dataType){case 1:t=r?e[1].getFloat32Array()[0]:-34028234663852886e22,n=s?e[2].getFloat32Array()[0]:34028234663852886e22;break;case 10:t=r?e[1].getUint16Array()[0]:64511,n=s?e[2].getUint16Array()[0]:31743;break;default:throw new Error("Unsupport data type")}return Xt({min:t,max:n})},jk=(e,t)=>{let n=t||EM(e.inputs),r=ar(e.inputs[0].dataType);e.compute(Kt(e.inputs[0],"Clip",s=>`clamp(${s}, vec4<${r}>(uniforms.min), vec4<${r}>(uniforms.max))`,void 0,n.cacheKey,void 0,[{type:e.inputs[0].dataType,data:n.min},{type:e.inputs[0].dataType,data:n.max}],[{name:"min",type:r},{name:"max",type:r}]),{inputs:[0]})},Rk=e=>{e.compute(Kt(e.inputs[0],"Ceil","ceil"))},Nk=e=>{e.compute(Kt(e.inputs[0],"Cos","cos"))},zk=e=>{e.compute(Kt(e.inputs[0],"Cosh","cosh"))},qa=e=>Xt(e),Bk=(e,t)=>{let n=ar(e.inputs[0].dataType);e.compute(Kt(e.inputs[0],"Elu",r=>`elu_vf32(${r})`,`
  const elu_alpha_ = ${n}(${t.alpha});

  fn elu_f32(a: ${n}) -> ${n} {
  return select((exp(a) - 1.0) * elu_alpha_, a, a >= 0.0);
  }

  fn elu_vf32(v: vec4<${n}>) -> vec4<${n}> {
  return vec4(elu_f32(v.x), elu_f32(v.y), elu_f32(v.z), elu_f32(v.w));
  }`,t.cacheKey))},ru=(e="f32")=>`
const r0: ${e} = 0.3275911;
const r1: ${e} = 0.254829592;
const r2: ${e} = -0.284496736;
const r3: ${e} = 1.421413741;
const r4: ${e} = -1.453152027;
const r5: ${e} = 1.061405429;

fn erf_vf32(v: vec4<${e}>) -> vec4<${e}> {
  let absv = abs(v);
  let x = 1.0 / (1.0 + r0 * absv);
  return sign(v) * (1.0 - ((((r5 * x + r4) * x + r3) * x + r2) * x + r1) * x * exp(-absv * absv));
}`,Gk=e=>{let t=ar(e.inputs[0].dataType);e.compute(Kt(e.inputs[0],"Erf",n=>`erf_vf32(${n})`,ru(t)))},Vk=e=>{e.compute(Kt(e.inputs[0],"Exp","exp"))},Uk=e=>{e.compute(Kt(e.inputs[0],"Floor","floor"))},Wk=e=>{let t=ar(e.inputs[0].dataType);e.compute(Kt(e.inputs[0],"Gelu",n=>`0.5 * ${n} * (1.0 + erf_vf32(${n} * 0.7071067811865475))`,ru(t)))},Hk=(e,t)=>{let n=ar(e.inputs[0].dataType);e.compute(Kt(e.inputs[0],"LeakyRelu",r=>`select(leaky_relu_alpha_ * ${r}, ${r}, ${r} >= vec4<${n}>(0.0))`,`const leaky_relu_alpha_ = ${n}(${t.alpha});`,t.cacheKey))},Kk=e=>{e.compute(Kt(e.inputs[0],"Not",t=>`!${t}`))},qk=e=>{e.compute(Kt(e.inputs[0],"Neg",t=>`-${t}`))},Qk=e=>{e.compute(Kt(e.inputs[0],"Reciprocal",t=>`1.0/${t}`))},Xk=e=>{let t=ar(e.inputs[0].dataType);e.compute(Kt(e.inputs[0],"Relu",n=>`select(vec4<${t}>(0.0), ${n}, ${n} > vec4<${t}>(0.0))`))},Yk=e=>{e.compute(Kt(e.inputs[0],"Sigmoid",t=>`(1.0 / (1.0 + exp(-${t})))`))},Jk=e=>Xt(e),Zk=(e,t)=>{let n=ar(e.inputs[0].dataType);e.compute(Kt(e.inputs[0],"HardSigmoid",r=>`max(vec4<${n}>(0.0), min(vec4<${n}>(1.0), ${t.alpha} * ${r} + vec4<${n}>(${t.beta})))`,void 0,t.cacheKey))},eE=e=>{e.compute(Kt(e.inputs[0],"Sin","sin"))},tE=e=>{e.compute(Kt(e.inputs[0],"Sinh","sinh"))},nE=e=>{e.compute(Kt(e.inputs[0],"Sqrt","sqrt"))},rE=e=>{e.compute(Kt(e.inputs[0],"Tan","tan"))},Ch=e=>`sign(${e}) * (1 - exp(-2 * abs(${e}))) / (1 + exp(-2 * abs(${e})))`,sE=e=>{e.compute(Kt(e.inputs[0],"Tanh",Ch))},tf=(e="f32")=>`
const fast_gelu_a: ${e} = 0.5;
const fast_gelu_b: ${e} = 0.7978845608028654;
const fast_gelu_c: ${e} = 0.035677408136300125;

fn tanh_v(v: vec4<${e}>) -> vec4<${e}> {
  return ${Ch("v")};
}
`,nf=e=>`(fast_gelu_a + fast_gelu_a * tanh_v(${e} * (fast_gelu_c * ${e} * ${e} + fast_gelu_b))) * ${e}`,iE=e=>{let t=ar(e.inputs[0].dataType);e.compute(Kt(e.inputs[0],"FastGelu",nf,tf(t),void 0,e.inputs[0].dataType))},oE=(e,t)=>{let n=ar(e.inputs[0].dataType);return e.compute(Kt(e.inputs[0],"ThresholdedRelu",r=>`select(vec4<${n}>(0.0), ${r}, ${r} > thresholded_relu_alpha_)`,`const thresholded_relu_alpha_ = vec4<${n}>(${t.alpha});`,t.cacheKey)),0},aE=e=>{e.compute(Kt(e.inputs[0],"Log","log"))},SM=(e,t)=>`
const alpha = vec4<${e}>(${t});
const one = ${e}(1.0);
const zero = ${e}(0.0);

fn quick_gelu_impl(x: vec4<${e}>) -> vec4<${e}> {
  let v = x *alpha;
  var x1 : vec4<${e}>;
  for (var i = 0; i < 4; i = i + 1) {
    if (v[i] >= zero) {
      x1[i] = one / (one + exp(-v[i]));
    } else {
      x1[i] = one - one / (one + exp(v[i]));
    }
  }
  return x * x1;
}
`,LM=e=>`quick_gelu_impl(${e})`,lE=(e,t)=>{let n=ar(e.inputs[0].dataType);e.compute(Kt(e.inputs[0],"QuickGelu",LM,SM(n,t.alpha),t.cacheKey,e.inputs[0].dataType))}}),DM,$M,cE,FG=He(()=>{Ft(),Ot(),Lg(),DM=e=>{if(e[0].dims.length!==3)throw new Error("input should have 3 dimensions");if(![2560,5120,10240].includes(e[0].dims[2]))throw new Error("hidden state should be 2560, 5120 or 10240");if(e[1].dims.length!==1)throw new Error("bias is expected to have 1 dimensions");if(e[0].dims[2]!==e[1].dims[0])throw new Error("last dimension of input and bias are not the same")},$M=e=>{let t=e[0].dims.slice();t[2]=t[2]/2;let n=Re("input",e[0].dataType,e[0].dims,4),r=Re("bias",e[0].dataType,[e[0].dims[2]],4),s=ht("output",e[0].dataType,t,4),i=Ae.size(t)/4,o=Jn(e[0].dataType);return{name:"BiasSplitGelu",getRunData:()=>({outputs:[{dims:t,dataType:e[0].dataType}],dispatchGroup:{x:Math.ceil(i/64)}}),getShaderSource:a=>`
  const M_SQRT2 = sqrt(2.0);
  const halfChannels = ${e[0].dims[2]/4/2}u;

  ${a.declareVariables(n,r,s)}

  ${ru(o)}

  ${a.mainStart()}
    ${a.guardAgainstOutOfBoundsWorkgroupSizes(i)}
    let biasIdx = global_idx % halfChannels;
    let batchIndex = global_idx / halfChannels;
    let inputOffset = biasIdx + batchIndex * halfChannels * 2;
    let valueLeft = input[inputOffset] + bias[biasIdx];
    let valueRight = input[inputOffset + halfChannels] + bias[biasIdx + halfChannels];
    let geluRight = valueRight * 0.5 * (erf_vf32(valueRight / M_SQRT2) + 1);

    ${s.setByOffset("global_idx","valueLeft * geluRight")}
  }`}},cE=e=>{DM(e.inputs),e.compute($M(e.inputs))}}),FM,OM,rs,uE,dE,pE,hE,mE,fE,gE,_E,yE,xE,OG=He(()=>{Mt(),Ft(),Ot(),FM=(e,t,n,r,s,i,o,a,l,c,d,u)=>{let p,h;typeof a=="string"?p=h=(_,T)=>`${a}((${_}),(${T}))`:typeof a=="function"?p=h=a:(p=a.scalar,h=a.vector);let m=ht("outputData",d,r.length,4),g=Re("aData",l,t.length,4),I=Re("bData",c,n.length,4),f;if(s)if(i){let _=Ae.size(t)===1,T=Ae.size(n)===1,M=t.length>0&&t[t.length-1]%4===0,v=n.length>0&&n[n.length-1]%4===0;_||T?f=m.setByOffset("global_idx",h(_?`${g.type.value}(${g.getByOffset("0")}.x)`:g.getByOffset("global_idx"),T?`${I.type.value}(${I.getByOffset("0")}.x)`:I.getByOffset("global_idx"))):f=`
            let outputIndices = ${m.offsetToIndices("global_idx * 4u")};
            let offsetA = ${g.broadcastedIndicesToOffset("outputIndices",m)};
            let offsetB = ${I.broadcastedIndicesToOffset("outputIndices",m)};
            ${m.setByOffset("global_idx",h(o||M?g.getByOffset("offsetA / 4u"):`${g.type.value}(${g.getByOffset("offsetA / 4u")}[offsetA % 4u])`,o||v?I.getByOffset("offsetB / 4u"):`${I.type.value}(${I.getByOffset("offsetB / 4u")}[offsetB % 4u])`))}
          `}else f=m.setByOffset("global_idx",h(g.getByOffset("global_idx"),I.getByOffset("global_idx")));else{if(!i)throw new Error("no necessary to use scalar implementation for element-wise binary op implementation.");let _=(T,M,v="")=>{let b=`aData[indexA${M}][componentA${M}]`,A=`bData[indexB${M}][componentB${M}]`;return`
            let outputIndices${M} = ${m.offsetToIndices(`global_idx * 4u + ${M}u`)};
            let offsetA${M} = ${g.broadcastedIndicesToOffset(`outputIndices${M}`,m)};
            let offsetB${M} = ${I.broadcastedIndicesToOffset(`outputIndices${M}`,m)};
            let indexA${M} = offsetA${M} / 4u;
            let indexB${M} = offsetB${M} / 4u;
            let componentA${M} = offsetA${M} % 4u;
            let componentB${M} = offsetB${M} % 4u;
            ${T}[${M}] = ${v}(${p(b,A)});
          `};d===9?f=`
            var data = vec4<u32>(0);
            ${_("data",0,"u32")}
            ${_("data",1,"u32")}
            ${_("data",2,"u32")}
            ${_("data",3,"u32")}
            outputData[global_idx] = dot(vec4<u32>(0x1, 0x100, 0x10000, 0x1000000), vec4<u32>(data));`:f=`
            ${_("outputData[global_idx]",0)}
            ${_("outputData[global_idx]",1)}
            ${_("outputData[global_idx]",2)}
            ${_("outputData[global_idx]",3)}
          `}return`
        ${e.registerUniform("vec_size","u32").declareVariables(g,I,m)}

        ${u??""}

        ${e.mainStart()}
        ${e.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.vec_size")}
        ${f}
      }`},OM=(e,t,n,r,s,i,o=n.dataType)=>{let a=n.dims.map(g=>Number(g)??1),l=r.dims.map(g=>Number(g)??1),c=!Ae.areEqual(a,l),d=a,u=Ae.size(a),p=!1,h=!1,m=[c];if(c){let g=Vo.calcShape(a,l,!1);if(!g)throw new Error("Can't perform binary op on the given tensors");d=g.slice(),u=Ae.size(d);let I=Ae.size(a)===1,f=Ae.size(l)===1,_=a.length>0&&a[a.length-1]%4===0,T=l.length>0&&l[l.length-1]%4===0;m.push(I),m.push(f),m.push(_),m.push(T);let M=1;for(let v=1;v<d.length;v++){let b=a[a.length-v],A=l[l.length-v];if(b===A)M*=b;else break}M%4===0?(h=!0,p=!0):(I||f||_||T)&&(p=!0)}else p=!0;return m.push(p),{name:e,shaderCache:{hint:t+m.map(g=>g.toString()).join("_"),inputDependencies:["rank","rank"]},getShaderSource:g=>FM(g,a,l,d,p,c,h,s,n.dataType,r.dataType,o,i),getRunData:()=>({outputs:[{dims:d,dataType:o}],dispatchGroup:{x:Math.ceil(u/64/4)},programUniforms:[{type:12,data:Math.ceil(Ae.size(d)/4)},...ft(a,l,d)]})}},rs=(e,t,n,r,s,i)=>{e.compute(OM(t,s??"",e.inputs[0],e.inputs[1],n,r,i))},uE=e=>{rs(e,"Add",(t,n)=>`${t}+${n}`)},dE=e=>{rs(e,"Div",(t,n)=>`${t}/${n}`)},pE=e=>{rs(e,"Equal",{scalar:(t,n)=>`u32(${t}==${n})`,vector:(t,n)=>`vec4<u32>(${t}==${n})`},void 0,void 0,9)},hE=e=>{rs(e,"Mul",(t,n)=>`${t}*${n}`)},mE=e=>{let t=Re("input",e.inputs[0].dataType,e.inputs[0].dims).type.value;rs(e,"Pow",{scalar:(n,r)=>`pow_custom(${n},${r})`,vector:(n,r)=>`pow_vector_custom(${n},${r})`},`
    fn pow_custom(a : ${t}, b : ${t}) -> ${t} {
      if (b == ${t}(0.0)) {
        return ${t}(1.0);
      } else if (a < ${t}(0.0) && f32(b) != floor(f32(b))) {
        return ${t}(pow(f32(a), f32(b))); // NaN
      }
      return select(sign(a), ${t}(1.0), round(f32(abs(b) % ${t}(2.0))) != 1.0) * ${t}(${t==="i32"?"round":""}(pow(f32(abs(a)), f32(b))));
    }
    fn pow_vector_custom(a : vec4<${t}>, b : vec4<${t}>) -> vec4<${t}> {
      // TODO: implement vectorized pow
      return vec4<${t}>(pow_custom(a.x, b.x), pow_custom(a.y, b.y), pow_custom(a.z, b.z), pow_custom(a.w, b.w));
    }
      `)},fE=e=>{rs(e,"Sub",(t,n)=>`${t}-${n}`)},gE=e=>{rs(e,"Greater",{scalar:(t,n)=>`u32(${t}>${n})`,vector:(t,n)=>`vec4<u32>(${t}>${n})`},void 0,void 0,9)},_E=e=>{rs(e,"Less",{scalar:(t,n)=>`u32(${t}<${n})`,vector:(t,n)=>`vec4<u32>(${t}<${n})`},void 0,void 0,9)},yE=e=>{rs(e,"GreaterOrEqual",{scalar:(t,n)=>`u32(${t}>=${n})`,vector:(t,n)=>`vec4<u32>(${t}>=${n})`},void 0,void 0,9)},xE=e=>{rs(e,"LessOrEqual",{scalar:(t,n)=>`u32(${t}<=${n})`,vector:(t,n)=>`vec4<u32>(${t}<=${n})`},void 0,void 0,9)}}),jM,RM,NM,zM,bE,wE,jG=He(()=>{Mt(),Ft(),En(),Ot(),jM=(e,t)=>{if(!e||e.length<1)throw new Error("too few inputs");let n=0,r=e[n],s=r.dataType,i=r.dims.length;e.forEach((o,a)=>{if(a!==n){if(o.dataType!==s)throw new Error("input tensors should be one type");if(o.dims.length!==i)throw new Error("input tensors should have the same shape");o.dims.forEach((l,c)=>{if(c!==t&&l!==r.dims[c])throw new Error("non concat dimensions must match")})}})},RM=(e,t)=>`
  fn calculateInputIndex(index: u32) -> u32 {
    let sizeInConcatAxis = array<u32, ${e}u>(${t});
    for (var i: u32 = 0u; i < ${e}; i += 1u ) {
      if (index < sizeInConcatAxis[i]) {
        return i;
      }
    }
    return ${e}u;
  }`,NM=(e,t)=>{let n=e.length,r=[];for(let s=0;s<n;++s){let i=t.setByOffset("global_idx",e[s].getByIndices("indices"));n===1?r.push(i):s===0?r.push(`if (inputIndex == ${s}u) { ${i} }`):s===n-1?r.push(`else { ${i} }`):r.push(`else if (inputIndex == ${s}) { ${i} }`)}return r.join(`
`)},zM=(e,t,n,r)=>{let s=Ae.size(n),i=new Array(e.length),o=new Array(e.length),a=0,l=[],c=[],d=[{type:12,data:s}];for(let g=0;g<e.length;++g)a+=e[g].dims[t],i[g]=a,c.push(e[g].dims.length),o[g]=Re(`input${g}`,r,c[g]),l.push("rank"),d.push({type:12,data:i[g]});for(let g=0;g<e.length;++g)d.push(...ft(e[g].dims));d.push(...ft(n));let u=ht("output",r,n.length),p=u.indicesGet("indices",t),h=Array.from(Array(i.length).keys()).map(g=>`uniforms.sizeInConcatAxis${g}`).join(","),m=g=>`

  ${(()=>{g.registerUniform("outputSize","u32");for(let I=0;I<e.length;I++)g.registerUniform(`sizeInConcatAxis${I}`,"u32");return g.declareVariables(...o,u)})()}

  ${RM(i.length,h)}

  ${g.mainStart()}
    ${g.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.outputSize")}

    var indices = ${u.offsetToIndices("global_idx")};

    let inputIndex = calculateInputIndex(${p});
    if (inputIndex != 0u) {
      let sizeInConcatAxis = array<u32, ${i.length}u>(${h});
      ${p} -= sizeInConcatAxis[inputIndex - 1u];
    }

    ${NM(o,u)}
  }`;return{name:"Concat",shaderCache:{hint:`${t}`,inputDependencies:l},getRunData:()=>({outputs:[{dims:n,dataType:r}],dispatchGroup:{x:Math.ceil(s/64)},programUniforms:d}),getShaderSource:m}},bE=(e,t)=>{let n=e.inputs,r=n[0].dims,s=Ae.normalizeAxis(t.axis,r.length);jM(n,s);let i=r.slice();i[s]=n.reduce((a,l)=>a+(l.dims.length>s?l.dims[s]:0),0);let o=n.filter(a=>Ae.size(a.dims)>0);e.compute(zM(o,s,i,n[0].dataType),{inputs:o})},wE=e=>Xt({axis:e.axis})}),Wi,Hi,Ki,Dg,Ji=He(()=>{Mt(),Ft(),Wi=(e,t,n="f32")=>{switch(e.activation){case"Relu":return`value = max(value, ${t}(0.0));`;case"Sigmoid":return`value = (${t}(1.0) / (${t}(1.0) + exp(-value)));`;case"Clip":return`value = clamp(value, ${t}(${n}(uniforms.clip_min)), ${t}(${n}(uniforms.clip_max)));`;case"HardSigmoid":return`value = max(${t}(0.0), min(${t}(1.0), ${n}(uniforms.alpha) * value + ${n}(uniforms.beta)));`;case"LeakyRelu":return`value = select(${n}(uniforms.alpha) * value, value, value >= ${t}(0.0));`;case"Tanh":return`let e2x = exp(-2.0 * abs(value));
              value = sign(value) * (1.0 - e2x) / (1.0 + e2x);
        `;case"":return"";default:throw new Error(`Unsupported activation ${e.activation}`)}},Hi=(e,t)=>{e.activation==="Clip"?t.push({type:1,data:e.clipMax},{type:1,data:e.clipMin}):e.activation==="HardSigmoid"?t.push({type:1,data:e.alpha},{type:1,data:e.beta}):e.activation==="LeakyRelu"&&t.push({type:1,data:e.alpha})},Ki=(e,t)=>{e.activation==="Clip"?t.push({name:"clip_max",type:"f32"},{name:"clip_min",type:"f32"}):e.activation==="HardSigmoid"?t.push({name:"alpha",type:"f32"},{name:"beta",type:"f32"}):e.activation==="LeakyRelu"&&t.push({name:"alpha",type:"f32"})},Dg=e=>{let t=e?.activation||"";if(t==="HardSigmoid"){let[n,r]=e?.activation_params||[.2,.5];return{activation:t,alpha:n,beta:r}}else if(t==="Clip"){let[n,r]=e?.activation_params||[KI,qI];return{activation:t,clipMax:r,clipMin:n}}else if(t==="LeakyRelu"){let[n]=e?.activation_params||[.01];return{activation:t,alpha:n}}return{activation:t}}}),er,vE,$g=He(()=>{er=(e,t)=>{switch(e){case 1:return t;case 2:return`vec2<${t}>`;case 3:return`vec3<${t}>`;case 4:return`vec4<${t}>`;default:throw new Error(`${e}-component is not supported.`)}},vE=e=>`
      ${e?"value = value + getBiasByOutputCoords(coords);":""}
      `}),ME,RG=He(()=>{ME=e=>`
fn getIndexFromCoords4D(coords : vec4<i32>, shape : vec4<i32>) -> i32 {
  return dot(coords, vec4<i32>(
      shape.y * shape.z * shape.w, shape.z * shape.w, shape.w, 1));
}
fn getOutputIndexFromCoords(coords : vec4<i32>) -> i32 {
  return dot(coords, vec4<i32>(
    i32(${e}.x), i32(${e}.y), i32(${e}.z), 1));
}
`}),ol,Fg,Og=He(()=>{Mt(),Ft(),Ot(),Ji(),ol=(e,t,n,r,s)=>{let i=r-n;return`
      ${Array.from({length:n}).map((o,a)=>`
      if (${mt(t.shape,a,t.rank)} != 1) {
        ${t.indicesSet(e,a,mt(s,a+i,r))}
      } else {
        ${t.indicesSet(e,a,0)}
      }`).join("")}
`},Fg=(e,t,n,r,s=!1,i)=>{let o=e[0].dims,a=e[1].dims,l=o[o.length-2],c=a[a.length-1],d=o[o.length-1],u=An(c),p=An(d),h=An(l),m=Ae.size(n)/u/h,g=e.length>2,I=r?r.slice(0,-2):n.slice(0,-2),f=[Ae.size(I),l,c],_=[{type:12,data:m},{type:12,data:l},{type:12,data:c},{type:12,data:d}];Hi(t,_),_.push(...ft(I,o,a)),g&&_.push(...ft(e[2].dims)),_.push(...ft(f));let T=M=>{let v=kg("batch_dims",e[0].dataType,I.length),b=Re("a",e[0].dataType,o.length,p),A=Re("b",e[1].dataType,a.length,u),k=ht("output",e[0].dataType,f.length,u),F=Jn(k.type.tensor),L=Wi(t,k.type.value,F),G=[b,A],j="";if(g){let U=s?u:1;G.push(Re("bias",e[2].dataType,e[2].dims.length,U)),j=`${s?`value += bias[col / ${U}];`:`value += ${k.type.value}(bias[row + i]);`}`}let R=[{name:"output_size",type:"u32"},{name:"M",type:"u32"},{name:"N",type:"u32"},{name:"K",type:"u32"}];Ki(t,R);let K=()=>{let U=`var a_data: ${b.type.value};`;for(let Y=0;Y<p;Y++)U+=`
              let b_data${Y} = b[(b_offset + (k + ${Y}) * uniforms.N + col) / ${u}];`;for(let Y=0;Y<h;Y++){U+=`a_data = a[(a_offset + (row + ${Y}) * uniforms.K + k) / ${p}];`;for(let te=0;te<p;te++)U+=`
            values[${Y}] = fma(${A.type.value}(a_data${p===1?"":`[${te}]`}), b_data${te}, values[${Y}]);
`}return U};return`
  ${M.registerUniforms(R).registerInternalVariables(v).declareVariables(...G,k)}
  ${M.mainStart()}
    ${M.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
    let col = (global_idx % (uniforms.N / ${u})) * ${u};
    var index1 = global_idx / (uniforms.N / ${u});
    let stride1 = uniforms.M / ${h};
    let row = (index1 % stride1) * ${h};
    let batch = index1 / stride1;

    ${n.length===2?"":`let batch_indices = ${v.offsetToIndices("batch")};`}

    var a_indices: ${b.type.indices};
    ${ol("a_indices",b,b.rank-2,v.rank,"batch_indices")}
    ${b.indicesSet("a_indices",b.rank-2,0)}
    ${b.indicesSet("a_indices",b.rank-1,0)}
    let a_offset = ${b.indicesToOffset("a_indices")};

    var b_indices: ${A.type.indices};
    ${ol("b_indices",A,A.rank-2,v.rank,"batch_indices")}
    ${A.indicesSet("b_indices",A.rank-2,0)}
    ${A.indicesSet("b_indices",A.rank-1,0)}
    let b_offset = ${A.indicesToOffset("b_indices")};
    var values: array<${k.type.value}, ${h}>;
    for (var k: u32 = 0u; k < uniforms.K; k = k + ${p}) {
      ${K()}
    }
    for (var i = 0u; i < ${h}u; i++) {
      var value = values[i];
      ${j}
      ${L}
      let cur_indices = ${k.type.indices}(batch, row + i, col);
      let offset = ${k.indicesToOffset("cur_indices")};
      ${k.setByOffset(`offset / ${u}`,"value")};
    }
  }
  `};return{name:"MatMulNaive",shaderCache:{hint:`${t.activation};${u};${p};${h};${s}`,inputDependencies:g?["rank","rank","rank"]:["rank","rank"]},getRunData:()=>({outputs:[{dims:i?i(n):n,dataType:e[0].dataType}],dispatchGroup:{x:Math.ceil(m/64)},programUniforms:_}),getShaderSource:T}}}),BM,GM,rf,Ih,VM,sf,UM,vu,jg=He(()=>{Mt(),Ft(),Ot(),Ji(),Og(),$g(),BM=(e,t)=>e?`
        mm_Asub[inputRow][inputCol] = mm_readA(batch,
          kStart + inputRow,
          globalRowStart / innerElementSize + inputCol${t?", batchIndices":""});
        `:`
        mm_Asub[inputRow][inputCol] = mm_readA(batch,
          globalRow + innerRow,
          kStart / innerElementSize + inputCol${t?", batchIndices":""});
        `,GM=(e,t)=>e?`
        let ACached0 = mm_Asub[k * innerElementSize][localRow];
        let ACached1 = mm_Asub[k * innerElementSize + 1][localRow];
        let ACached2 = mm_Asub[k * innerElementSize + 2][localRow];
        ${t===3?"":"let ACached3 = mm_Asub[k * innerElementSize + 3][localRow];"}
        for (var i = 0; i < rowPerThread; i = i + 1) {
          acc[i] = BCached0 * ACached0[i] + acc[i];
          acc[i] = BCached1 * ACached1[i] + acc[i];
          acc[i] = BCached2 * ACached2[i] + acc[i];
          ${t===3?"":"acc[i] = BCached3 * ACached3[i] + acc[i];"}
        }`:`
        for (var i = 0; i < rowPerThread; i = i + 1) {
          let ACached = mm_Asub[tileRow + i][k];
          acc[i] = BCached0 * ACached.x + acc[i];
          acc[i] = BCached1 * ACached.y + acc[i];
          acc[i] = BCached2 * ACached.z + acc[i];
          ${t===3?"":"acc[i] = BCached3 * ACached.w + acc[i];"}
        }`,rf=(e,t,n="f32",r,s=!1,i=32,o=!1,a=32)=>{let l=t[1]*e[1],c=t[0]*e[0],d=s?l:i,u=s?i:l,p=d/t[0],h=i/t[1];if(!((s&&p===4&&e[1]===4||!s&&(p===3||p===4))&&d%t[0]===0&&i%t[1]===0&&e[0]===4))throw new Error(`If transposeA ${s} is true, innerElementSize ${p} and workPerThread[1] ${e[1]} must be 4.
      Otherwise, innerElementSize ${p} must be 3 or 4.
  tileAWidth ${d} must be divisible by workgroupSize[0]${t[0]}. tileInner ${i} must be divisible by workgroupSize[1] ${t[1]}. colPerThread ${e[0]} must be 4.`);return`
var<workgroup> mm_Asub: array<array<vec${p}<${n}>, ${d/p}>, ${u}>;
var<workgroup> mm_Bsub: array<array<vec4<${n}>, ${c/e[0]}>, ${i}>;

const rowPerThread = ${e[1]};
const colPerThread = ${e[0]};
const innerElementSize = ${p};
const tileInner = ${i};

@compute @workgroup_size(${t[0]}, ${t[1]}, ${t[2]})
fn main(@builtin(local_invocation_id) localId : vec3<u32>,
        @builtin(global_invocation_id) globalId : vec3<u32>,
        @builtin(workgroup_id) workgroupId : vec3<u32>) {
  let localRow = i32(localId.y);
  let tileRow = localRow * rowPerThread;
  let tileCol = i32(localId.x);

  let globalRow =i32(globalId.y) * rowPerThread;
  let globalCol = i32(globalId.x);
  let batch = ${o?"0":"i32(globalId.z)"};
  ${r?`let batchIndices = ${r.offsetToIndices("u32(batch)")};`:""}
  let globalRowStart = i32(workgroupId.y) * ${l};

  let num_tiles = ${o?`${Math.ceil(a/i)}`:"(uniforms.dim_inner - 1) / tileInner + 1"};
  var kStart = ${o?`i32(globalId.z) * ${a}`:"0"};

  var acc: array<vec4<${n}>, rowPerThread>;

  // Loop over shared dimension.
  let tileRowB = localRow * ${h};
  for (var t = 0; t < num_tiles; t = t + 1) {
      // Load one tile of A into local memory.
      for (var innerRow = 0; innerRow < rowPerThread; innerRow = innerRow + 1) {
          let inputRow = tileRow + innerRow;
          let inputCol = tileCol;
          ${BM(s,r)}
      }

      // Load one tile of B into local memory.
      for (var innerRow = 0; innerRow < ${h}; innerRow = innerRow + 1) {
          let inputRow = tileRowB + innerRow;
          let inputCol = tileCol;
          mm_Bsub[inputRow][inputCol] = mm_readB(batch, kStart + inputRow, globalCol${r?", batchIndices":""});
      }
      kStart = kStart + tileInner;
      workgroupBarrier();

      // Compute acc values for a single thread.
      for (var k = 0; k < tileInner / innerElementSize; k = k + 1) {
          let BCached0 = mm_Bsub[k * innerElementSize][tileCol];
          let BCached1 = mm_Bsub[k * innerElementSize + 1][tileCol];
          let BCached2 = mm_Bsub[k * innerElementSize + 2][tileCol];
          ${p===3?"":"let BCached3 = mm_Bsub[k * innerElementSize + 3][tileCol];"}

          ${GM(s,p)}
      }

      workgroupBarrier();
  }

  for (var innerRow = 0; innerRow < rowPerThread; innerRow = innerRow + 1) {
      mm_write(batch, globalRow + innerRow, globalCol, acc[innerRow]);
  }
}`},Ih=(e,t)=>e?`
            mm_Asub[inputRow][inputCol] = mm_readA(batch,
              kStart + inputRow,
              globalRowStart + inputCol${t?", batchIndices":""});
            `:`
            mm_Asub[inputRow][inputCol] = mm_readA(batch,
              globalRowStart + inputRow,
              kStart + inputCol${t?", batchIndices":""});
            `,VM=e=>e?"let ACached = mm_Asub[k][tileRow + innerRow];":"let ACached = mm_Asub[tileRow + innerRow][k];",sf=(e,t,n="f32",r,s=!1,i=32,o=!1,a=32,l=!1)=>{let c=e[1]*t[1],d=e[0]*t[0],u=s?c:i,p=s?i:c;if(!(p%t[1]===0&&u%t[0]===0&&i%t[1]===0))throw new Error(`tileAHight ${p} must be divisible by workgroupSize[1]${t[1]}, tileAWidth ${u} must be divisible by workgroupSize[0]${t[0]}, tileInner ${i} must be divisible by workgroupSize[1]${t[1]}`);let h=p/t[1],m=u/t[0],g=i/t[1],I=l?`
    let localRow = i32(localId.y);
    let localCol = i32(localId.x);
    let globalRowStart = i32(workgroupId.y) * ${c};
    let globalColStart = i32(workgroupId.x) * ${d};

    // Loop over shared dimension.
    for (var t = 0; t < num_tiles; t = t + 1) {
      // Load one tile of A into local memory.
      for (var inputRow = localRow; inputRow < ${p}; inputRow = inputRow + ${t[1]}) {
        for (var inputCol = localCol; inputCol < ${u}; inputCol = inputCol + ${t[0]}) {
          ${Ih(s,r)}
        }
      }
      // Load one tile of B into local memory.
      for (var inputRow = localRow; inputRow < ${i}; inputRow = inputRow + ${t[1]}) {
            for (var inputCol = localCol; inputCol < ${d}; inputCol = inputCol + ${t[0]}) {
          mm_Bsub[inputRow][inputCol] = mm_readB(batch,
            kStart + inputRow,
            globalColStart + inputCol${r?", batchIndices":""});
        }
      }
      kStart = kStart + tileInner;
      workgroupBarrier();

      // Compute acc values for a single thread.
      var BCached : array<${n}, colPerThread>;
      for (var k = 0; k < tileInner; k = k + 1) {
        for (var inner = 0; inner < colPerThread; inner = inner + 1) {
          BCached[inner] = mm_Bsub[k][localCol + inner * ${t[0]}];
        }
        for (var innerRow = 0; innerRow < rowPerThread; innerRow = innerRow + 1) {
          let ACached = ${s?`mm_Asub[k][localRow + innerRow * ${t[1]}];`:`mm_Asub[localRow + innerRow * ${t[1]}][k];`}
          for (var innerCol = 0; innerCol < colPerThread; innerCol = innerCol + 1) {
            acc[innerRow][innerCol] = acc[innerRow][innerCol] +
                ACached * BCached[innerCol];
          }
        }
      }
      workgroupBarrier();
    }
    for (var innerRow = 0; innerRow < rowPerThread; innerRow = innerRow + 1) {
      let gRow = globalRowStart + localRow + innerRow * ${t[1]};
      for (var innerCol = 0; innerCol < colPerThread; innerCol = innerCol + 1) {
        let gCol = globalColStart + localCol + innerCol * ${t[0]};
        mm_write(batch, gRow, gCol, acc[innerRow][innerCol]);
      }
    }
    `:`
let tileRow = i32(localId.y) * rowPerThread;
let tileCol = i32(localId.x) * colPerThread;

let globalRow = i32(globalId.y) * rowPerThread;
let globalCol = i32(globalId.x) * colPerThread;
let globalRowStart = i32(workgroupId.y) * ${c};

let tileRowA = i32(localId.y) * ${h};
let tileColA = i32(localId.x) * ${m};
let tileRowB = i32(localId.y) * ${g};
// Loop over shared dimension.
for (var t = 0; t < num_tiles; t = t + 1) {
  // Load one tile of A into local memory.
  for (var innerRow = 0; innerRow < ${h}; innerRow = innerRow + 1) {
    for (var innerCol = 0; innerCol < ${m}; innerCol = innerCol + 1) {
      let inputRow = tileRowA + innerRow;
      let inputCol = tileColA + innerCol;
      ${Ih(s,r)}
    }
  }

  // Load one tile of B into local memory.
  for (var innerRow = 0; innerRow < ${g}; innerRow = innerRow + 1) {
    for (var innerCol = 0; innerCol < colPerThread; innerCol = innerCol + 1) {
      let inputRow = tileRowB + innerRow;
      let inputCol = tileCol + innerCol;
      mm_Bsub[inputRow][inputCol] = mm_readB(batch,
        kStart + inputRow,
        globalCol + innerCol${r?", batchIndices":""});
    }
  }
  kStart = kStart + tileInner;
  workgroupBarrier();

  // Compute acc values for a single thread.
  var BCached : array<${n}, colPerThread>;
  for (var k = 0; k < tileInner; k = k + 1) {
    for (var inner = 0; inner < colPerThread; inner = inner + 1) {
      BCached[inner] = mm_Bsub[k][tileCol + inner];
    }

    for (var innerRow = 0; innerRow < rowPerThread; innerRow = innerRow + 1) {
      ${VM(s)}
      for (var innerCol = 0; innerCol < colPerThread; innerCol = innerCol + 1) {
        acc[innerRow][innerCol] = acc[innerRow][innerCol] + ACached * BCached[innerCol];
      }
    }
  }

  workgroupBarrier();
}

for (var innerRow = 0; innerRow < rowPerThread; innerRow = innerRow + 1) {
  for (var innerCol = 0; innerCol < colPerThread; innerCol = innerCol + 1) {
    mm_write(batch, globalRow + innerRow, globalCol + innerCol,
        acc[innerRow][innerCol]);
  }
}
`;return`
  var<workgroup> mm_Asub : array<array<${n}, ${u}>, ${p}>;
  var<workgroup> mm_Bsub : array<array<${n}, ${d}>, ${i}>;
  const rowPerThread = ${e[1]};
  const colPerThread = ${e[0]};
  const tileInner = ${i};

@compute @workgroup_size(${t[0]}, ${t[1]}, ${t[2]})
fn main(@builtin(local_invocation_id) localId : vec3<u32>,
        @builtin(global_invocation_id) globalId : vec3<u32>,
        @builtin(workgroup_id) workgroupId : vec3<u32>) {
    let batch = ${o?"0":"i32(globalId.z)"};
    ${r?`let batchIndices = ${r.offsetToIndices("u32(batch)")};`:""}
    let num_tiles = ${o?`${Math.ceil(a/i)}`:"(uniforms.dim_inner - 1) / tileInner + 1"};
    var kStart = ${o?`i32(globalId.z) * ${a}`:"0"};

    var acc : array<array<${n}, colPerThread>, rowPerThread>;
    ${I}
  }
`},UM=(e,t,n,r,s=!1)=>{let[i,o,a,l]=r,c=Jn(r[0].type.tensor);return`
    fn mm_readA(batch: i32, row: i32, colIn: i32, batchIndices: ${i.type.indices}) -> ${er(e,c)} {
      var value = ${er(e,c)}(0.0);
      let col = colIn * ${e};
      if(row < uniforms.dim_a_outer && col < uniforms.dim_inner)
      {
        var aIndices: ${o.type.indices};
        ${ol("aIndices",o,o.rank-2,i.rank,"batchIndices")}
        ${o.indicesSet("aIndices",o.rank-2,"u32(row)")}
        ${o.indicesSet("aIndices",o.rank-1,"u32(colIn)")}
        value = ${o.getByIndices("aIndices")};
      }
      return value;
    }

    fn mm_readB(batch: i32, row: i32, colIn: i32, batchIndices: ${i.type.indices}) -> ${er(e,c)} {
      var value = ${er(e,c)}(0.0);
      let col = colIn * ${e};
      if(row < uniforms.dim_inner && col < uniforms.dim_b_outer)
      {
        var bIndices: ${a.type.indices};
        ${ol("bIndices",a,a.rank-2,i.rank,"batchIndices")}
        ${a.indicesSet("bIndices",a.rank-2,"u32(row)")}
        ${a.indicesSet("bIndices",a.rank-1,"u32(colIn)")}
        value = ${a.getByIndices("bIndices")};
      }
      return value;
    }

    fn mm_write(batch: i32, row: i32, colIn: i32, valueIn: ${er(e,c)}) {
      let col = colIn * ${e};
      if (row < uniforms.dim_a_outer && col < uniforms.dim_b_outer) {
        var value = valueIn;
        let coords = vec3<i32>(batch, row, colIn);
        ${t?`value = value + ${s?"bias[colIn]":`${er(e,c)}(bias[row])`};`:""}
        ${n}
        ${l.setByIndices("vec3<u32>(coords)","value")}
      }
    }
    `},vu=(e,t,n,r,s=!1,i)=>{let o=e[0].dims,a=e[1].dims,l=o.slice(0,-2),c=a.slice(0,-2),d=r?r.slice(0,-2):n.slice(0,-2),u=Ae.size(d),p=o[o.length-2],h=o[o.length-1],m=a[a.length-1],g=h%4===0&&m%4===0,I=p<=8?[4,1,1]:[4,4,1],f=[8,8,1],_=[Math.ceil(m/f[0]/I[0]),Math.ceil(p/f[1]/I[1]),Math.ceil(u/f[2]/I[2])],T=g?4:1,M=[...l,p,h/T],v=M.length,b=[...c,h,m/T],A=b.length,k=[u,p,m/T],F=[{type:6,data:p},{type:6,data:m},{type:6,data:h}];Hi(t,F),F.push(...ft(d,M,b));let L=["rank","rank"],G=e.length>2;G&&(F.push(...ft(e[2].dims)),L.push("rank")),F.push(...ft(k));let j=R=>{let K=d.length,U=kg("batchDims",e[0].dataType,K,1),Y=Jn(e[0].dataType),te=Re("a",e[0].dataType,v,T),ne=Re("b",e[1].dataType,A,T),le=ht("result",e[0].dataType,k.length,T),N=[te,ne];if(G){let se=s?T:1;N.push(Re("bias",e[2].dataType,e[2].dims.length,se))}let oe=[{name:"dim_a_outer",type:"i32"},{name:"dim_b_outer",type:"i32"},{name:"dim_inner",type:"i32"}];Ki(t,oe);let X=Jn(le.type.tensor),D=Wi(t,le.type.value,X),z=UM(T,G,D,[U,te,ne,le],s);return`
  ${R.registerUniforms(oe).registerInternalVariables(U).declareVariables(...N,le)}
  ${z}
  ${g?rf(I,f,Y,U):sf(I,f,Y,U)}
                   `};return{name:"MatMul",shaderCache:{hint:`${I};${t.activation};${g};${s}`,inputDependencies:L},getRunData:()=>({outputs:[{dims:i?i(n):n,dataType:e[0].dataType}],dispatchGroup:{x:_[0],y:_[1],z:_[2]},programUniforms:F}),getShaderSource:j}}}),WM,AE,NG=He(()=>{Mt(),Ws(),Ot(),Ji(),$g(),RG(),jg(),WM=(e,t,n,r,s=!1,i,o=4,a=4,l=4,c="f32")=>{let d=F=>{switch(F){case 1:return"resData = x[xIndex];";case 3:return`resData = vec3<${c}>(x[xIndex], x[xIndex + 1], x[xIndex + 2]);`;case 4:return"resData = x[xIndex / 4];";default:throw new Error(`innerElementSize ${F} is not supported.`)}},u=F=>{switch(F){case 1:return"return w[row * i32(uniforms.w_shape[3]) + colIn];";case 4:return"return w[row * i32(uniforms.w_shape[3]) / 4 + colIn];";default:throw new Error(`innerElementSize ${F} is not supported.`)}},p=e?`
    let coord = vec4<i32>(batch, xRow, xCol, xCh);
    `:`
    let coord = vec4<i32>(batch, xCh, xRow, xCol);
    `,h=e?`
    let coords = vec4<i32>(
      batch,
      row / outWidth,
      row % outWidth,
      col);
    `:`
    let coords = vec4<i32>(
      batch,
      row,
      col / outWidth,
      col % outWidth);
    `,m=e?"i32(uniforms.x_shape[1])":"i32(uniforms.x_shape[2])",g=e?"i32(uniforms.x_shape[2])":"i32(uniforms.x_shape[3])",I=e?"row":"col",f=e?"col":"row",_=`
    let inChannels = i32(uniforms.w_shape[2]);
    let outWidth = ${e?"i32(uniforms.result_shape[2])":"i32(uniforms.result_shape[3])"};
    let outRow = ${I} / outWidth;
    let outCol = ${I} % outWidth;

    let WRow = ${f} / (i32(uniforms.w_shape[1]) * inChannels);
    let WCol = ${f} / inChannels % i32(uniforms.w_shape[1]);
    let xRow = outRow * uniforms.stride[0] + uniforms.dilation[0] * WRow - uniforms.pad[0];
    let xCol = outCol * uniforms.stride[1] + uniforms.dilation[1] * WCol - uniforms.pad[1];
    let xCh = ${f} % inChannels;
    var resData = ${er(o,c)}(0.0);
    // The bounds checking is always needed since we use it to pad zero for
    // the 'same' padding type.
    if (xRow >= 0 && xRow < ${m} && xCol >= 0 && xCol < ${g}) {
      ${p}
      let xIndex = getIndexFromCoords4D(coord, vec4<i32>(uniforms.x_shape));
      ${d(o)}
    }
    return resData;`,T=e?t&&r?`
    let col = colIn * ${o};
    ${_}`:`
    let col = colIn * ${o};
    if (row < uniforms.dim_a_outer && col < uniforms.dim_inner) {
      ${_}
    }
    return ${er(o,c)}(0.0);`:r&&n?`
    let col = colIn * ${o};
    ${_}`:`
    let col = colIn * ${o};
    if (row < uniforms.dim_inner && col < uniforms.dim_b_outer) {
      ${_}
    }
    return ${er(o,c)}(0.0);`,M=e?r&&n?u(a):`
    let col = colIn * ${a};
    if (row < uniforms.dim_inner && col < uniforms.dim_b_outer) {
      ${u(a)}
    }
    return ${er(a,c)}(0.0);`:`
    let col = colIn * ${a};
    if (row < uniforms.dim_inner && col < uniforms.dim_a_outer) {
      ${u(a)}
    }
    return ${er(a,c)}(0.0);`,v=er(l,c),b=er(e?o:a,c),A=er(e?a:o,c),k=Wi(i,v,c);return`
    fn mm_readA(batch: i32, row : i32, colIn : i32) -> ${b} {
      ${e?T:M}
    }

    fn mm_readB(batch: i32, row : i32, colIn : i32) -> ${A} {
      ${e?M:T}
    }

    fn mm_write(batch: i32, row : i32, colIn : i32, valueIn : ${v}) {
      let col = colIn * ${l};
      if (row < uniforms.dim_a_outer && col < uniforms.dim_b_outer)
      {
      var value = valueIn;
      let outWidth = ${e?"i32(uniforms.result_shape[2])":"i32(uniforms.result_shape[3])"};
      ${h}
      ${vE(s)}
      ${k}
      setOutputAtCoords(coords[0], coords[1], coords[2], coords[3], value);
      }
    }`},AE=(e,t,n,r,s,i,o,a,l)=>{let c=t.format==="NHWC",d=c?e[0].dims[3]:e[0].dims[1],u=n[0],p=c?n[2]:n[3],h=c?n[1]:n[2],m=c?n[3]:n[1],g=c&&(d%4===0||d%3===0)&&m%4===0,I=c?m:p*h,f=c?p*h:m,_=[8,8,1],T=r<=8?[4,1,1]:[4,4,1],M=[Math.ceil(I/_[0]/T[0]),Math.ceil(f/_[1]/T[1]),Math.ceil(u/_[2]/T[2])];Ut("verbose",()=>`[conv2d_mm_webgpu] dispatch = ${M}`);let v=g?c&&d%4!==0?3:4:1,b=_[1]*T[1],A=_[0]*T[0],k=Math.max(_[0]*v,_[1]),F=r%b===0,L=s%A===0,G=i%k===0,j=g?[v,4,4]:[1,1,1],R=[{type:6,data:r},{type:6,data:s},{type:6,data:i},{type:6,data:[t.pads[0],t.pads[1]]},{type:6,data:t.strides},{type:6,data:t.dilations}];Hi(t,R),R.push(...ft(e[0].dims,e[1].dims));let K=["rank","rank"];o&&(R.push(...ft(e[2].dims)),K.push("rank")),R.push(...ft(n));let U=Y=>{let te=[{name:"dim_a_outer",type:"i32"},{name:"dim_b_outer",type:"i32"},{name:"dim_inner",type:"i32"},{name:"pad",type:"i32",length:2},{name:"stride",type:"i32",length:2},{name:"dilation",type:"i32",length:2}];Ki(t,te);let ne=g?4:1,le=Jn(e[0].dataType),N=`
      fn setOutputAtIndex(flatIndex : i32, value : ${g?`vec4<${le}>`:le}) {
        result[flatIndex] = ${g?`vec4<${le}>`:le}(value);
      }
      fn setOutputAtCoords(d0 : i32, d1 : i32, d2 : i32, d3 : i32, value : ${g?`vec4<${le}>`:le}) {
        let flatIndex = getOutputIndexFromCoords(vec4<i32>(d0, d1, d2, d3));
        setOutputAtIndex(flatIndex ${g?"/ 4":""}, value);
      }`,oe=Re("x",e[0].dataType,e[0].dims.length,v===3?1:v),X=Re("w",e[1].dataType,e[1].dims.length,ne),D=[oe,X],z=ht("result",e[0].dataType,n.length,ne);if(o){let se=Re("bias",e[2].dataType,e[2].dims.length,ne);D.push(se),N+=`
        fn getBiasByOutputCoords(coords : vec4<i32>) -> ${g?`vec4<${le}>`:le} {
          return bias[coords.${c?"w":"y"}${g?"/ 4":""}];
        }`}return`
        ${ME("uniforms.result_strides")}
        //struct Uniforms { xShape : vec4<i32>, wShape : vec4<i32>, outShape : vec4<i32>,
        //  outShapeStrides: vec3<i32>, filterDims : vec2<i32>, pad : vec2<i32>, stride : vec2<i32>,
        //  dilation : vec2<i32>, dimAOuter : i32, dimBOuter : i32, dimInner : i32 };
        ${Y.registerUniforms(te).declareVariables(...D,z)}
        ${N}
        ${WM(c,F,L,G,o,t,j[0],j[1],j[2],le)}
        ${g?rf(T,_,le,void 0,!c,k):sf(T,_,le,void 0,!c,k,!1,void 0,a)}`};return{name:"Conv2DMatMul",shaderCache:{hint:`${t.cacheKey};${v};${g};${F};${L};${G};${b};${A};${k}`,inputDependencies:K},getRunData:()=>({outputs:[{dims:l?l(n):n,dataType:e[0].dataType}],dispatchGroup:{x:M[0],y:M[1],z:M[2]},programUniforms:R}),getShaderSource:U}}}),HM,kh,Da,KM,Eh,qM,TE,PE,zG=He(()=>{Mt(),Ws(),Ft(),Ot(),Ji(),$g(),HM=e=>{let t=1;for(let n=0;n<e.length;n++)t*=e[n];return t},kh=e=>typeof e=="number"?[e,e,e]:e,Da=(e,t)=>t<=1?e:e+(e-1)*(t-1),KM=(e,t,n,r=1)=>{let s=Da(t,r);return Math.floor((e[0]*(n-1)-n+s)/2)},Eh=(e,t,n,r,s)=>{s==null&&(s=KM(e,t[0],r[0]));let i=[0,0,0,n];for(let o=0;o<3;o++)e[o]+2*s>=t[o]&&(i[o]=Math.trunc((e[o]-t[o]+2*s)/r[o]+1));return i},qM=(e,t,n,r,s,i,o,a,l,c)=>{let d,u,p,h;if(e==="VALID"&&(e=0),typeof e=="number"){d={top:e,bottom:e,left:e,right:e,front:e,back:e};let m=Eh([t,n,r,1],[a,l,c],1,[s,i,o],e);u=m[0],p=m[1],h=m[2]}else if(Array.isArray(e)){if(!e.every((g,I,f)=>g===f[0]))throw Error(`Unsupported padding parameter: ${e}`);d={top:e[0],bottom:e[1],left:e[2],right:e[3],front:e[4],back:e[5]};let m=Eh([t,n,r,1],[a,l,c],1,[s,i,o],e[0]);u=m[0],p=m[1],h=m[2]}else if(e==="SAME_UPPER"){u=Math.ceil(t/s),p=Math.ceil(n/i),h=Math.ceil(r/o);let m=(u-1)*s+a-t,g=(p-1)*i+l-n,I=(h-1)*o+c-r,f=Math.floor(m/2),_=m-f,T=Math.floor(g/2),M=g-T,v=Math.floor(I/2),b=I-v;d={top:T,bottom:M,left:v,right:b,front:f,back:_}}else throw Error(`Unknown padding parameter: ${e}`);return{padInfo:d,outDepth:u,outHeight:p,outWidth:h}},TE=(e,t,n,r,s,i=!1,o="channelsLast")=>{let a,l,c,d,u;if(o==="channelsLast")[a,l,c,d,u]=e;else if(o==="channelsFirst")[a,u,l,c,d]=e;else throw new Error(`Unknown dataFormat ${o}`);let[p,,h,m,g]=t,[I,f,_]=kh(n),[T,M,v]=kh(r),b=Da(h,T),A=Da(m,M),k=Da(g,v),{padInfo:F,outDepth:L,outHeight:G,outWidth:j}=qM(s,l,c,d,I,f,_,b,A,k),R=i?p*u:p,K=[0,0,0,0,0];return o==="channelsFirst"?K=[a,R,L,G,j]:o==="channelsLast"&&(K=[a,L,G,j,R]),{batchSize:a,dataFormat:o,inDepth:l,inHeight:c,inWidth:d,inChannels:u,outDepth:L,outHeight:G,outWidth:j,outChannels:R,padInfo:F,strideDepth:I,strideHeight:f,strideWidth:_,filterDepth:h,filterHeight:m,filterWidth:g,effectiveFilterDepth:b,effectiveFilterHeight:A,effectiveFilterWidth:k,dilationDepth:T,dilationHeight:M,dilationWidth:v,inShape:e,outShape:K,filterShape:t}},PE=(e,t,n,r,s,i)=>{let o=i==="channelsLast";o?e[0].dims[3]:e[0].dims[1];let a=[64,1,1],l={x:n.map((I,f)=>f)},c=[Math.ceil(HM(l.x.map(I=>n[I]))/a[0]),1,1];Ut("verbose",()=>`[conv3d_naive_webgpu] dispatch = ${c}`);let d=1,u=Ae.size(n),p=[{type:12,data:u},{type:12,data:r},{type:12,data:s},{type:12,data:t.strides},{type:12,data:t.dilations}];Hi(t,p),p.push(...ft(e[0].dims,e[1].dims));let h=["rank","rank"],m=e.length===3;m&&(p.push(...ft(e[2].dims)),h.push("rank")),p.push(...ft(n));let g=I=>{let f=[{name:"output_size",type:"u32"},{name:"filter_dims",type:"u32",length:r.length},{name:"pads",type:"u32",length:s.length},{name:"strides",type:"u32",length:t.strides.length},{name:"dilations",type:"u32",length:t.dilations.length}];Ki(t,f);let _=1,T=Jn(e[0].dataType),M=Re("x",e[0].dataType,e[0].dims.length,d),v=Re("W",e[1].dataType,e[1].dims.length,_),b=[M,v],A=ht("result",e[0].dataType,n.length,_),k="";if(m){let G=Re("bias",e[2].dataType,e[2].dims.length,_);b.push(G),k+=`
        fn getBiasByOutputCoords(coords : array<u32, 5>) -> ${T} {
          return bias[${o?mt("coords",4,5):mt("coords",1,5)}];
        }`}let F=er(d,T),L=Wi(t,F,T);return`
            ${k}
            fn getX(d0 : u32, d1 : u32, d2 : u32, d3 : u32, d4 : u32) -> f32 {
              let aIndices = array<u32, 5>(d0, d1, d2, d3, d4);
              return ${M.getByIndices("aIndices")};
            }
            fn getW(d0 : u32, d1 : u32, d2 : u32, d3 : u32, d4 : u32) -> f32 {
              let aIndices = array<u32, 5>(d0, d1, d2, d3, d4);
              return ${v.getByIndices("aIndices")};
            }
          ${I.registerUniforms(f).declareVariables(...b,A)}
          ${I.mainStart()}
          ${I.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
              let coords = ${A.offsetToIndices("global_idx")};
              let batch = ${mt("coords",0,M.rank)};
              let d2 = ${o?mt("coords",M.rank-1,M.rank):mt("coords",1,M.rank)};
              let xFRCCorner = vec3<u32>(${o?mt("coords",1,M.rank):mt("coords",2,M.rank)},
              ${o?mt("coords",2,M.rank):mt("coords",3,M.rank)},
              ${o?mt("coords",3,M.rank):mt("coords",4,M.rank)}) * uniforms.strides - uniforms.pads;
              let xFCorner = xFRCCorner.x;
              let xRCorner = xFRCCorner.y;
              let xCCorner = xFRCCorner.z;
              let xShapeY = ${o?mt("uniforms.x_shape",1,M.rank):mt("uniforms.x_shape",2,M.rank)};
              let xShapeZ = ${o?mt("uniforms.x_shape",2,M.rank):mt("uniforms.x_shape",3,M.rank)};
              let xShapeW = ${o?mt("uniforms.x_shape",3,M.rank):mt("uniforms.x_shape",4,M.rank)};
              let xShapeU = ${o?mt("uniforms.x_shape",4,M.rank):mt("uniforms.x_shape",1,M.rank)};
              let inputDepthNearestVec4 = (xShapeU / 4) * 4;
              let inputDepthVec4Remainder = xShapeU % 4;

              var value = 0.0;
              for (var wF = 0u; wF < uniforms.filter_dims[0]; wF++) {
                let xF = xFCorner + wF * uniforms.dilations[0];
                if (xF < 0 || xF >= xShapeY) {
                  continue;
                }

                for (var wR = 0u; wR < uniforms.filter_dims[1]; wR++) {
                  let xR = xRCorner + wR * uniforms.dilations[1];
                  if (xR < 0 || xR >= xShapeZ) {
                    continue;
                  }

                  for (var wC = 0u; wC < uniforms.filter_dims[2]; wC++) {
                    let xC = xCCorner + wC * uniforms.dilations[2];
                    if (xC < 0 || xC >= xShapeW) {
                      continue;
                    }

                    for (var d1 = 0u; d1 < inputDepthNearestVec4; d1 += 4) {
                      ${o?`let xValues = vec4<f32>(
                               getX(batch, xF, xR, xC, d1),
                               getX(batch, xF, xR, xC, d1 + 1),
                               getX(batch, xF, xR, xC, d1 + 2),
                               getX(batch, xF, xR, xC, d1 + 3));
                            `:`let xValues = vec4<f32>(
                               getX(batch, d1, xF, xR, xC),
                               getX(batch, d1 + 1, xF, xR, xC),
                               getX(batch, d1 + 2, xF, xR, xC),
                               getX(batch, d1 + 3, xF, xR, xC));
                            `}
                            let wValues = vec4<f32>(
                              getW(d2, d1, wF, wR, wC),
                              getW(d2, d1 + 1, wF, wR, wC),
                              getW(d2, d1 + 2, wF, wR, wC),
                              getW(d2, d1 + 3, wF, wR, wC));
                      value += dot(xValues, wValues);
                    }
                    if (inputDepthVec4Remainder == 1) {
                        ${o?`value += getX(batch, xF, xR, xC, inputDepthNearestVec4)
                          * getW(d2, inputDepthNearestVec4, wF, wR, wC);`:`value += getX(batch, inputDepthNearestVec4, xF, xR, xC)
                          * getW(d2, inputDepthNearestVec4, wF, wR, wC);`}
                    } else if (inputDepthVec4Remainder == 2) {
                      ${o?`let xValues = vec2<f32>(
                        getX(batch, xF, xR, xC, inputDepthNearestVec4),
                        getX(batch, xF, xR, xC, inputDepthNearestVec4 + 1));
                      `:`let xValues = vec2<f32>(
                        getX(batch, inputDepthNearestVec4, xF, xR, xC),
                        getX(batch, inputDepthNearestVec4 + 1, xF, xR, xC));
                    `}
                    let wValues = vec2<f32>(
                      getW(d2, inputDepthNearestVec4, wF, wR, wC),
                      getW(d2, inputDepthNearestVec4 + 1, wF, wR, wC));
                      value += dot(xValues, wValues);
                    } else if (inputDepthVec4Remainder == 3) {
                      ${o?`let xValues = vec3<f32>(
                        getX(batch, xF, xR, xC, inputDepthNearestVec4),
                        getX(batch, xF, xR, xC, inputDepthNearestVec4 + 1),
                        getX(batch, xF, xR, xC, inputDepthNearestVec4 + 2));
                      `:`let xValues = vec3<f32>(
                        getX(batch, inputDepthNearestVec4, xF, xR, xC),
                        getX(batch, inputDepthNearestVec4 + 1, xF, xR, xC),
                        getX(batch, inputDepthNearestVec4 + 2, xF, xR, xC));
                    `}
                    let wValues = vec3<f32>(
                      getW(d2, inputDepthNearestVec4, wF, wR, wC),
                      getW(d2, inputDepthNearestVec4 + 1, wF, wR, wC),
                      getW(d2, inputDepthNearestVec4 + 2, wF, wR, wC));
                      value += dot(xValues, wValues);
                    }
                  }
                }
              }
              ${m?"value = value + getBiasByOutputCoords(coords)":""};
              ${L}
              result[global_idx] = f32(value);
          }`};return{name:"Conv3DNaive",shaderCache:{hint:`${t.cacheKey};${o};${d};${m}`,inputDependencies:h},getRunData:()=>({outputs:[{dims:n,dataType:e[0].dataType}],dispatchGroup:{x:c[0],y:c[1],z:c[2]},programUniforms:p}),getShaderSource:g}}}),CE,IE,BG=He(()=>{Mt(),Ft(),Ot(),Ji(),CE=(e,t,n,r)=>{let s=e.length>2,i=s?"value += b[output_channel];":"",o=e[0].dims,a=e[1].dims,l=t.format==="NHWC",c=l?n[3]:n[1],d=c/t.group,u=l&&d>=4?An(c):1,p=Ae.size(n)/u,h=[{type:12,data:p},{type:12,data:t.dilations},{type:12,data:[t.strides[0],t.strides[1]]},{type:12,data:[t.pads[0],t.pads[1]]},{type:12,data:d}];Hi(t,h),h.push(...ft(o,[a[0],a[1],a[2],a[3]/u]));let m=s?["rank","rank","rank"]:["rank","rank"];h.push(...ft([n[0],n[1],n[2],n[3]/u]));let g=I=>{let f=ht("output",e[0].dataType,n.length,u),_=Jn(f.type.tensor),T=Wi(t,f.type.value,_),M=Re("x",e[0].dataType,o.length),v=Re("w",e[1].dataType,a.length,u),b=[M,v];s&&b.push(Re("b",e[2].dataType,e[2].dims,u));let A=[{name:"output_size",type:"u32"},{name:"dilations",type:"u32",length:t.dilations.length},{name:"strides",type:"u32",length:2},{name:"pads",type:"u32",length:2},{name:"output_channels_per_group",type:"u32"}];Ki(t,A);let k=l?`
      for (var wHeight: u32 = 0u; wHeight < uniforms.w_shape[0]; wHeight++) {
        let xHeight = xRCCorner.x + wHeight * uniforms.dilations[0];

        if (xHeight < 0u || xHeight >= uniforms.x_shape[1]) {
          continue;
        }

        for (var wWidth: u32 = 0u; wWidth < uniforms.w_shape[1]; wWidth++) {
          let xWidth = xRCCorner.y + wWidth * uniforms.dilations[1];
          if (xWidth < 0u || xWidth >= uniforms.x_shape[2]) {
            continue;
          }

          for (var wInChannel: u32 = 0u; wInChannel < uniforms.w_shape[2]; wInChannel++) {
            let input_channel = in_channel_offset + wInChannel;
            let xVal = ${M.get("batch","xHeight","xWidth","input_channel")};
            let wVal = ${v.get("wHeight","wWidth","wInChannel","output_channel")};
            value += xVal * wVal;
          }
        }
      }
      `:`
      for (var wInChannel: u32 = 0u; wInChannel < uniforms.w_shape[1]; wInChannel++) {
        let input_channel = in_channel_offset + wInChannel;
        for (var wHeight: u32 = 0u; wHeight < uniforms.w_shape[2]; wHeight++) {
          let xHeight = xRCCorner.x + wHeight * uniforms.dilations[0];

          if (xHeight < 0u || xHeight >= uniforms.x_shape[2]) {
            continue;
          }

          for (var wWidth: u32 = 0u; wWidth < uniforms.w_shape[3]; wWidth++) {
            let xWidth = xRCCorner.y + wWidth * uniforms.dilations[1];
            if (xWidth < 0u || xWidth >= uniforms.x_shape[3]) {
              continue;
            }

            let xVal = ${M.get("batch","input_channel","xHeight","xWidth")};
            let wVal = ${v.get("output_channel","wInChannel","wHeight","wWidth")};
            value += xVal * wVal;
          }
        }
      }
      `;return`
  ${I.registerUniforms(A).declareVariables(...b,f)}

  ${I.mainStart()}
    ${I.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}

    let outputIndices = ${f.offsetToIndices("global_idx")};
    let batch: u32 = outputIndices[0];
    let output_channel: u32 = outputIndices[${l?3:1}];
    let xRCCorner: vec2<u32> = vec2<u32>(outputIndices[${l?1:2}], outputIndices[${l?2:3}]) * uniforms.strides - uniforms.pads;
    let group_id: u32 = output_channel * ${u} / uniforms.output_channels_per_group;
    var in_channel_offset = group_id * uniforms.w_shape[${l?2:1}];

    var value: ${f.type.value} = ${f.type.value}(0);
    ${k}
    ${i}
    ${T}
    ${f.setByOffset("global_idx","value")}
  }`};return{name:"GroupedConv",shaderCache:{hint:`${t.cacheKey}_${u}`,inputDependencies:m},getRunData:()=>({outputs:[{dims:r?r(n):n,dataType:e[0].dataType}],dispatchGroup:{x:Math.ceil(p/64)},programUniforms:h}),getShaderSource:g}},IE=(e,t,n,r)=>{let s=e.length>2,i=An(n[3]),o=An(n[2]),a=Ae.size(n)/i/o,l=[e[0].dims[0],e[0].dims[1],e[0].dims[2],e[0].dims[3]/i],c=[e[1].dims[0],e[1].dims[1],e[1].dims[2],e[1].dims[3]/i],d=[n[0],n[1],n[2],n[3]/i],u=[{type:12,data:a},{type:6,data:[t.strides[0],t.strides[1]]},{type:6,data:[t.pads[0],t.pads[1]]}];Hi(t,u),u.push(...ft(l,c,d));let p=(o-1)*t.strides[1]+c[1],h=m=>{let g=ht("output",e[0].dataType,d.length,i),I=Jn(g.type.tensor),f=Wi(t,g.type.value,I),_=Re("x",e[0].dataType,l.length,i),T=Re("w",e[1].dataType,c.length,i),M=[_,T];s&&M.push(Re("b",e[2].dataType,e[2].dims,i));let v=s?"value += b[output_channel];":"",b=[{name:"output_size",type:"u32"},{name:"strides",type:"i32",length:2},{name:"pads",type:"i32",length:2}];return Ki(t,b),`
  ${m.registerUniforms(b).declareVariables(...M,g)}
  ${m.mainStart()}
    ${m.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
    let width0 = uniforms.output_shape[3];
    let output_channel = global_idx % width0;
    var index1 = global_idx / width0;
    let width1 = uniforms.output_shape[2] / ${o}u;
    let col = (index1 % width1) * ${o}u;
    index1 = index1 / width1;
    let row = index1 % uniforms.output_shape[1];
    let batch = index1 / uniforms.output_shape[1];

    let x_corner = vec2<i32>(i32(row), i32(col)) * uniforms.strides - uniforms.pads;

    var x_vals: array<${_.type.value}, ${p}>;
    var values: array<${g.type.value}, ${o}>;
    let input_channel = output_channel;
    // Use constant instead of uniform can give better performance for w's height/width.
    for (var w_height: u32 = 0u; w_height < ${c[0]}; w_height++) {
      let x_height = x_corner.x + i32(w_height);
      if (x_height >= 0 && u32(x_height) < uniforms.x_shape[1]) {
        for (var i = 0; i < ${p}; i++) {
          let x_width = x_corner.y + i;
          if (x_width >= 0 && u32(x_width) < uniforms.x_shape[2]) {
            x_vals[i] = ${_.get("batch","u32(x_height)","u32(x_width)","input_channel")};
          } else {
            x_vals[i] = ${_.type.value}(0);
          }
        }
        for (var w_width: u32 = 0u; w_width < ${c[1]}; w_width++) {
          let w_val = ${T.get("w_height","w_width","0","output_channel")};
          for (var i = 0u; i < ${o}u; i++) {
            values[i] = fma(x_vals[i * u32(uniforms.strides[1]) + w_width], w_val, values[i]);
          }
        }
      }
    }

    for (var i = 0u; i < ${o}u; i++) {
      var value = values[i];
      ${v}
      ${f}
      ${g.set("batch","row","col + i","output_channel","value")};
    }
  }`};return{name:"GroupedConv-Vectorize",shaderCache:{hint:`${t.cacheKey};${i};${o};${p};${c[0]};${c[1]}`,inputDependencies:s?["rank","rank","type"]:["rank","rank"]},getRunData:()=>({outputs:[{dims:r?r(n):n,dataType:e[0].dataType}],dispatchGroup:{x:Math.ceil(a/64)},programUniforms:u}),getShaderSource:h}}}),QM,Oc,XM,jc,of,Sh,YM,JM,af,GG=He(()=>{Ft(),NG(),zG(),jg(),BG(),Ji(),Og(),hi(),QM=(e,t,n,r,s,i)=>{let o=e[0],a=e.slice(i?1:2,i?3:4),l=a.length,c=t[0],d=t.slice(2).map((p,h)=>p+(p-1)*(n[h]-1)),u=a.map((p,h)=>p+r[h]+r[h+l]).map((p,h)=>Math.floor((p-d[h]+s[h])/s[h]));return u.splice(0,0,o),u.splice(i?3:1,0,c),u},Oc=[2,3,1,0],XM=(e,t)=>{if(!e||e.length!==2&&e.length!==3)throw new Error("Conv requires 2 or 3 inputs");if(e[0].dims.length>5)throw new Error("greater than 5D is not supported");if(e[0].dims.length!==e[1].dims.length)throw new Error("filter does not have same dimension as input");let n=e[0].dims[t.format==="NHWC"?e[0].dims.length-1:1],r=e[1].dims[1]*t.group;if(n!==r)throw new Error("FILTER_IN_CHANNEL should be equal to DATA_CHANNEL");if(e.length===3&&(e[2].dims.length!==1||e[1].dims[0]!==e[2].dims[0]))throw new Error("invalid bias");let s=e[0].dims.length-2;if(t.dilations.length!==s)throw new Error(`dilations should be ${s}D`);if(t.strides.length!==s)throw new Error(`strides should be ${s}D`);if(t.pads.length!==s*2)throw new Error(`pads should be ${s*2}D`);if(t.kernelShape.length!==0&&t.kernelShape.length!==e[1].dims.length-2)throw new Error("invalid kernel shape")},jc=(e,t)=>{let n=e.kernelShape.slice();n.length<t[1].dims.length-2&&n.push(...Array(t[1].dims.length-2-n.length).fill(0));for(let i=2;i<t[1].dims.length;++i)n[i-2]===0&&(n[i-2]=t[1].dims[i]);let r=e.pads.slice();bu.adjustPadsBasedOnAutoPad(t[0].dims,e.strides,e.dilations,n,r,e.format==="NHWC",e.autoPad);let s=Object.assign({},e);return Object.assign(s,{kernelShape:n,pads:r}),s},of=e=>{let t=Dg(e),n=e.format,r=["NOTSET","VALID","SAME_UPPER","SAME_LOWER"][e.auto_pad],s=e.dilations,i=e.group,o=e.kernel_shape,a=e.pads,l=e.strides,c=e.w_is_const();return{autoPad:r,format:n,dilations:s,group:i,kernelShape:o,pads:a,strides:l,wIsConst:c,...t,cacheKey:`${e.format};${t.activation};`}},Sh=(e,t,n,r)=>{let s=n.format==="NHWC",i=QM(t[0].dims,t[1].dims,n.dilations,n.pads,n.strides,s);if(n.group!==1){let b=[t[0]];if(s){let A=e.kernelCustomData.wT??e.compute(Sr(t[1],Oc),{inputs:[1],outputs:[n.wIsConst?-2:-1]})[0];n.wIsConst&&!e.kernelCustomData.wT&&(e.kernelCustomData.wT=A),b.push(A)}else b.push(t[1]);t.length===3&&b.push(t[2]),!e.adapterInfo.isArchitecture("ampere")&&s&&t[1].dims[0]===n.group&&t[1].dims[1]===1&&n.dilations[0]===1&&n.dilations[1]===1?e.compute(IE(b,n,i,r),{inputs:b}):e.compute(CE(b,n,i,r),{inputs:b});return}let o=t.length===3,a=t[0].dims[s?1:2],l=t[0].dims[s?2:3],c=t[0].dims[s?3:1],d=t[1].dims[2],u=t[1].dims[3],p=i[s?1:2],h=i[s?2:3],m=i[s?3:1],g=s&&d===a&&u===l&&n.pads[0]===0&&n.pads[1]===0;if(g||d===1&&u===1&&n.dilations[0]===1&&n.dilations[1]===1&&n.strides[0]===1&&n.strides[1]===1&&n.pads[0]===0&&n.pads[1]===0){let b=i[0],A,k,F,L=[];if(s){let R=e.kernelCustomData.wT??e.compute(Sr(t[1],Oc),{inputs:[1],outputs:[n.wIsConst?-2:-1]})[0];if(n.wIsConst&&!e.kernelCustomData.wT&&(e.kernelCustomData.wT=R),g){let K=a*l*c;A=t[0].reshape([1,b,K]),k=R.reshape([1,K,m]),F=[1,b,m]}else A=t[0].reshape([b,a*l,c]),k=R.reshape([1,c,m]),F=[b,p*h,m];L.push(A),L.push(k)}else A=t[0].reshape([b,c,a*l]),k=t[1].reshape([1,m,c]),F=[b,m,p*h],L.push(k),L.push(A);o&&L.push(t[2]);let G=F[2],j=L[0].dims[L[0].dims.length-1];G<8&&j<8?e.compute(Fg(L,n,i,F,s,r),{inputs:L}):e.compute(vu(L,n,i,F,s,r),{inputs:L});return}let I=!0,f=e.kernelCustomData.wT??e.compute(Sr(t[1],Oc),{inputs:[1],outputs:[n.wIsConst?-2:-1]})[0];n.wIsConst&&!e.kernelCustomData.wT&&(e.kernelCustomData.wT=f);let _=[t[0],f];o&&_.push(t[2]);let T=s?p*h:m,M=s?m:p*h,v=d*u*c;e.compute(AE(_,n,i,T,M,v,o,I,r),{inputs:_})},YM=(e,t)=>{let n=t.format==="NHWC",r=[e.inputs[0].reshape(n?[e.inputs[0].dims[0],1,e.inputs[0].dims[1],e.inputs[0].dims[2]]:[e.inputs[0].dims[0],e.inputs[0].dims[1],1,e.inputs[0].dims[2]]),e.inputs[1].reshape([e.inputs[1].dims[0],e.inputs[1].dims[1],1,e.inputs[1].dims[2]])];e.inputs.length===3&&r.push(e.inputs[2]);let s=[0,t.pads[0],0,t.pads[1]],i=[1].concat(t.strides),o=[1].concat(t.dilations),a=[1].concat(t.kernelShape),l=jc({...t,pads:s,strides:i,dilations:o,kernelShape:a},r);Sh(e,r,l,c=>n?[c[0],c[2],c[3]]:[c[0],c[1],c[3]])},JM=(e,t,n)=>{let r=n.format==="NHWC"?"channelsLast":"channelsFirst",s=jc(n,t),i=n.autoPad==="NOTSET"?n.pads:n.autoPad,o=TE(t[0].dims,t[1].dims,n.strides,n.dilations,i,!1,r);e.compute(PE(t,s,o.outShape,[o.filterDepth,o.filterHeight,o.filterWidth],[o.padInfo.front,o.padInfo.top,o.padInfo.left],r))},af=(e,t)=>{if(XM(e.inputs,t),e.inputs[0].dims.length===3)YM(e,t);else if(e.inputs[0].dims.length===5)JM(e,e.inputs,t);else{let n=jc(t,e.inputs);Sh(e,e.inputs,n)}}}),kE,VG=He(()=>{Mt(),Ws(),Ft(),Ot(),kE=(e,t,n)=>{let r=e.length>2,s=t.outputShape,i=t.format==="NHWC",o=t.group,a=e[1].dims,l=a[2]/o,c=a[3],d=i?An(l):1,u=i&&c===1&&l>=4,p=u?Math.floor(l/4)*4:Math.floor(l/d)*d,h=l-p,m=i?An(c):1,g=i?c===1?d:m:1,I=Ae.size(s)/m,f=[Math.ceil(I/64),1,1];Ut("verbose",()=>`[conv2d_backprop_webgpu] dispatch = ${f}`);let _=["rank","rank"],T=[t.strides[0],t.strides[1]],M=[t.kernelShape[i?1:2],t.kernelShape[i?2:3]],v=[t.dilations[0],t.dilations[1]],b=[M[0]+(t.dilations[0]<=1?0:(t.kernelShape[i?1:2]-1)*(t.dilations[0]-1)),M[1]+(t.dilations[1]<=1?0:(t.kernelShape[i?2:3]-1)*(t.dilations[1]-1))],A=[b[0]-1-Math.floor((t.pads[0]+t.pads[2])/2),b[1]-1-Math.floor((t.pads[1]+t.pads[3])/2)],k=[{type:12,data:I},{type:12,data:T},{type:12,data:M},{type:12,data:v},{type:12,data:b},{type:6,data:A},{type:12,data:p},{type:12,data:l},{type:12,data:c},...ft(e[0].dims,e[1].dims)];r&&(k.push(...ft(e[2].dims)),_.push("rank")),k.push(...ft(s));let F=L=>{let G=[{name:"output_size",type:"u32"},{name:"strides",type:"u32",length:T.length},{name:"filter_dims",type:"u32",length:M.length},{name:"dilations",type:"u32",length:M.length},{name:"effective_filter_dims",type:"u32",length:b.length},{name:"pads",type:"i32",length:A.length},{name:"input_channels_per_group_int",type:"u32"},{name:"input_channels_per_group",type:"u32"},{name:"output_channels_per_group",type:"u32"}],j=Jn(e[0].dataType),R=i?1:2,K=i?2:3,U=i?3:1,Y=Re("W",e[1].dataType,e[1].dims.length,g),te=Re("Dy",e[0].dataType,e[0].dims.length,d),ne=[te,Y];r&&ne.push(Re("bias",e[2].dataType,[s[U]].length,m));let le=ht("result",e[0].dataType,s.length,m),N=()=>{let D="";if(u)d===4?D+=`
        let xValue = ${te.getByOffset("x_offset")};
        let wValue = ${Y.getByOffset("w_offset")};
        dotProd = dotProd + dot(xValue, wValue);
        x_offset += 1u;
        w_offset += 1u;`:d===2?D+=`
          dotProd = dotProd + dot(vec4<${j}>(${te.getByOffset("x_offset")}, ${te.getByOffset("x_offset + 1u")}), vec4<${j}>(${Y.getByOffset("w_offset")}, ${Y.getByOffset("w_offset + 1u")}));
          x_offset += 2u;
          w_offset += 2u;`:d===1&&(D+=`
          dotProd = dotProd + dot(vec4<${j}>(${te.getByOffset("x_offset")}, ${te.getByOffset("x_offset + 1u")}, ${te.getByOffset("x_offset + 2u")}, ${te.getByOffset("x_offset + 3u")}), vec4<${j}>(${Y.getByOffset("w_offset")}, ${Y.getByOffset("w_offset + 1u")}, ${Y.getByOffset("w_offset + 2u")}, ${Y.getByOffset("w_offset + 3u")}));
          x_offset += 4u;
          w_offset += 4u;`);else if(D+=`
                  let xValue = ${i?te.getByOffset(`${te.indicesToOffset(`${te.type.indices}(batch, idyR, idyC, inputChannel)`)} / ${d}`):te.get("batch","inputChannel","idyR","idyC")};
        `,d===1)D+=`
          let w_offset = ${Y.indicesToOffset(`${Y.type.indices}(u32(wRPerm), u32(wCPerm), inputChannel, wOutChannel)`)};
          let wValue = ${Y.getByOffset(`w_offset / ${g}`)};
          dotProd = dotProd + xValue * wValue;`;else for(let z=0;z<d;z++)D+=`
            let wValue${z} = ${Y.getByOffset(`${Y.indicesToOffset(`${Y.type.indices}(u32(wRPerm), u32(wCPerm), inputChannel + ${z}, wOutChannel)`)} / ${g}`)};
            dotProd = dotProd + xValue[${z}] * wValue${z};`;return D},oe=()=>{if(h===0)return"";if(!u)throw new Error(`packInputAs4 ${u} is not true.`);let D="";if(d===1){D+="dotProd = dotProd";for(let z=0;z<h;z++)D+=`
            + ${te.getByOffset(`x_offset + ${z}`)} * ${Y.getByOffset(`w_offset + ${z}`)}`;D+=";"}else if(d===2){if(h!==2)throw new Error(`Invalid inputChannelsRemainder ${h}.`);D+=`
          let xValue = ${te.getByOffset("x_offset")};
          let wValue = ${Y.getByOffset("w_offset")};
          dotProd = dotProd + dot(xValue, wValue);`}return D},X=`
            let outputIndices = ${le.offsetToIndices(`global_idx * ${m}`)};
            let batch = ${le.indicesGet("outputIndices",0)};
            let d1 = ${le.indicesGet("outputIndices",U)};
            let r = ${le.indicesGet("outputIndices",R)};
            let c = ${le.indicesGet("outputIndices",K)};
            let dyCorner = vec2<i32>(i32(r), i32(c)) - uniforms.pads;
            let dyRCorner = dyCorner.x;
            let dyCCorner = dyCorner.y;
            let groupId = d1 / uniforms.output_channels_per_group;
            let wOutChannel = d1 - groupId * uniforms.output_channels_per_group;
            // Convolve dy(?, ?, d2) with w(:, :, d1, d2) to compute dx(xR, xC, d1).
            // ? = to be determined. : = across all values in that axis.
            var dotProd = ${le.type.value}(0.0);
            var wR: u32 = 0;
            if (uniforms.dilations.x == 1) {
              // Minimum wR >= 0 that satisfies (dyRCorner + wR) % (uniforms.strides.x) == 0
              wR = u32(((dyRCorner + i32(uniforms.strides.x) - 1) / i32(uniforms.strides.x)) * i32(uniforms.strides.x) - dyRCorner);
            }
            for (; wR < uniforms.effective_filter_dims.x; wR = wR + 1) {
              if (wR % uniforms.dilations.x != 0) {
                continue;
              }
              let dyR = (${j}(dyRCorner) + ${j}(wR)) / ${j}(uniforms.strides[0]);
              let wRPerm = uniforms.filter_dims.x - 1 - wR / uniforms.dilations.x;
              if (dyR < 0.0 || dyR >= ${j}(uniforms.Dy_shape[${R}]) || fract(dyR) > 0.0 ||
                  wRPerm < 0) {
                continue;
              }
              let idyR: u32 = u32(dyR);
              var wC: u32 = 0;
              if (uniforms.dilations.y == 1) {
                // Minimum wC >= 0 that satisfies (dyCCorner + wC) % (uniforms.strides.y) == 0
                wC = u32(((dyCCorner + i32(uniforms.strides.y) - 1) / i32(uniforms.strides.y)) * i32(uniforms.strides.y) - dyCCorner);
              }
              for (; wC < uniforms.effective_filter_dims.y; wC = wC + 1) {
                if (wC % uniforms.dilations.y != 0) {
                  continue;
                }
                let dyC = (${j}(dyCCorner) + ${j}(wC)) / ${j}(uniforms.strides.y);
                let wCPerm = uniforms.filter_dims.y - 1 - wC / uniforms.dilations.y;
                if (dyC < 0.0 || dyC >= ${j}(uniforms.Dy_shape[${K}]) ||
                    fract(dyC) > 0.0 || wCPerm < 0) {
                  continue;
                }
                let idyC: u32 = u32(dyC);
                var inputChannel = groupId * uniforms.input_channels_per_group;
                ${u?`
                var x_offset = ${te.indicesToOffset(`${te.type.indices}(batch, idyR, idyC, inputChannel)`)} / ${d};
                var w_offset = ${Y.indicesToOffset(`${Y.type.indices}(wRPerm, wCPerm, inputChannel, wOutChannel)`)} / ${g};
                  `:""}
                for (var d2: u32 = 0; d2 < uniforms.input_channels_per_group_int; d2 = d2 + ${u?4:d}) {
                  ${N()}
                  inputChannel = inputChannel + ${u?4:d};
                }
                ${oe()}
                wC = wC + uniforms.strides.y - 1;
              }
              wR = wR + uniforms.strides[0] - 1;
            }
            let value = dotProd${r?` + bias[d1 / ${m}]`:""};
            ${le.setByOffset("global_idx","value")};
          `;return`
    ${L.registerUniforms(G).declareVariables(...ne,le)}
      ${L.mainStart()}
      ${L.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")};
    ${X}}`};return{name:"ConvTranspose2D",shaderCache:{hint:`${t.cacheKey};${d}${g}${m}${u}${h}`,inputDependencies:_},getRunData:()=>({dispatchGroup:{x:f[0],y:f[1],z:f[2]},outputs:[{dims:n?n(s):s,dataType:e[0].dataType}],programUniforms:k}),getShaderSource:F}}}),ZM,eA,tA,Lh,EE,nA,Dh,rA,SE,UG=He(()=>{VG(),Ji(),hi(),ZM=(e,t,n,r,s,i)=>(e-1)*t+n+(r-1)*s+1-i,eA=(e,t,n,r,s)=>{let i=Math.floor(e/2);t==="SAME_UPPER"?(n[r]=i,n[s]=e-i):t==="SAME_LOWER"&&(n[r]=e-i,n[s]=i)},tA=(e,t,n,r,s,i,o,a,l,c)=>{let d=e.length-2,u=c.length===0;l.length<d&&l.push(...Array(d-l.length).fill(0));let p=e[0],h=t[a?3:1]*s;for(let m=0,g=e.length-d-(a?1:0);m<d;++m,++g){let I=e[g],f=u?I*o[m]:c[m],_=ZM(I,o[m],i[m],t[g],n[m],f);eA(_,r,i,m,m+d),u&&c.push(o[m]*(I-1)+l[m]+(t[g]-1)*n[m]+1-i[m]-i[m+d])}c.splice(0,0,p),c.splice(a?3:1,0,h)},Lh=(e,t)=>{let n=e.kernelShape.slice();if(e.kernelShape.length===0||e.kernelShape.reduce((u,p)=>u*p,1)===0){n.length=0;for(let u=2;u<t[1].dims.length;++u)n.push(t[1].dims[u])}let r=e.format==="NHWC";n.splice(0,0,t[1].dims[0]),n.splice(r?3:1,0,t[1].dims[1]);let s=e.pads.slice(),i=e.outputShape.slice(),o=e.outputPadding.slice(),a=t[0].dims,l=e.dilations.slice();if(l.reduce((u,p)=>u+p,0)===0){let u=t[0].dims.length-2;l=new Array(u).fill(1)}let c=e.strides.slice();if(c.reduce((u,p)=>u+p,0)===0){let u=t[0].dims.length-2;c=new Array(u).fill(1)}tA(a,n,l,e.autoPad,e.group,s,c,r,o,i);let d=Object.assign({},e);return Object.assign(d,{kernelShape:n,pads:s,outputPadding:o,outputShape:i,dilations:l,strides:c}),d},EE=e=>{let t=Dg(e),n=e.format,r=["NOTSET","VALID","SAME_UPPER","SAME_LOWER"][typeof e.autoPad>"u"?0:e.autoPad],s=e.dilations,i=e.group,o=e.kernelShape,a=e.pads,l=e.strides,c=e.wIsConst(),d=e.outputPadding,u=e.outputShape;return{autoPad:r,format:n,dilations:s,group:i,kernelShape:o,outputPadding:d,outputShape:u,pads:a,strides:l,wIsConst:c,...t,cacheKey:`${e.format};${t.activation};`}},nA=(e,t)=>{if(!e||e.length!==2&&e.length!==3)throw new Error("Conv requires 2 or 3 inputs");if(e[0].dims.length!==4&&e[0].dims.length!==3)throw new Error("currently only support 2-dimensional conv");if(e[0].dims.length!==e[1].dims.length)throw new Error("filter does not have same dimension as input");let n=e[0].dims[t.format==="NHWC"?e[0].dims.length-1:1],r=e[1].dims[0];if(n!==r)throw new Error("FILTER_IN_CHANNEL should be equal to DATA_CHANNEL");let s=e[1].dims[1]*t.group;if(e.length===3&&(e[2].dims.length!==1||e[2].dims[0]!==s))throw new Error("invalid bias");let i=e[0].dims.length-2;if(t.dilations.reduce((o,a)=>o+a,0)>0&&t.dilations.length!==i)throw new Error(`dilations should be ${i}D`);if(t.strides.reduce((o,a)=>o+a,0)>0&&t.strides.length!==i)throw new Error(`strides should be ${i}D`);if(t.pads.reduce((o,a)=>o+a,0)>0&&t.pads.length!==i*2)throw new Error(`pads should be ${i*2}D`);if(t.outputPadding.length!==i&&t.outputPadding.length!==0)throw new Error(`output_padding should be ${i}D`);if(t.kernelShape.reduce((o,a)=>o+a,0)>0&&t.kernelShape.length!==0&&t.kernelShape.length!==e[1].dims.length-2)throw new Error("invalid kernel shape");if(t.outputShape.length!==0&&t.outputShape.length!==e[0].dims.length-2)throw new Error("invalid output shape")},Dh=(e,t,n,r)=>{let s=e.kernelCustomData.wT??e.compute(Sr(t[1],[2,3,0,1]),{inputs:[1],outputs:[n.wIsConst?-2:-1]})[0];n.wIsConst&&!e.kernelCustomData.wT&&(e.kernelCustomData.wT=s);let i=[t[0],s];t.length===3&&i.push(t[2]),e.compute(kE(i,n,r),{inputs:i})},rA=(e,t)=>{let n=t.format==="NHWC",r=[e.inputs[0].reshape(n?[e.inputs[0].dims[0],1,e.inputs[0].dims[1],e.inputs[0].dims[2]]:[e.inputs[0].dims[0],e.inputs[0].dims[1],1,e.inputs[0].dims[2]]),e.inputs[1].reshape([e.inputs[1].dims[0],e.inputs[1].dims[1],1,e.inputs[1].dims[2]])];e.inputs.length===3&&r.push(e.inputs[2]);let s=t.kernelShape;(s.length===0||s[0]===0)&&(s=[e.inputs[1].dims[2]]);let i=t.dilations;(i.length===0||i[0]===0)&&(i=[1]);let o=t.strides;(o.length===0||o[0]===0)&&(o=[1]);let a=t.pads;a.length===0&&(a=[0,0]),a=[0,a[0],0,a[1]],o=[1].concat(o),i=[1].concat(i),s=[1].concat(s);let l=t.outputPadding;l=[0].concat(l);let c=Lh({...t,pads:a,strides:o,dilations:i,kernelShape:s,outputPadding:l},r);Dh(e,r,c,d=>n?[d[0],d[2],d[3]]:[d[0],d[1],d[3]])},SE=(e,t)=>{if(nA(e.inputs,t),e.inputs[0].dims.length===3)rA(e,t);else{let n=Lh(t,e.inputs);Dh(e,e.inputs,n)}}}),sA,LE,DE,WG=He(()=>{Mt(),Ft(),En(),Ot(),sA=(e,t,n,r)=>{let s=Ae.size(t),i=t.length,o=Re("input",e,i),a=ht("output",e,i),l=n.dataType===6?n.getInt32Array()[0]:Number(n.getBigInt64Array()[0]),c=Ae.normalizeAxis(l,i),d=u=>{let p=` i32(${o.indicesGet("inputIndices","uniforms.axis")}) `,h=mt("uniforms.input_shape","uniforms.axis",i),m=r.reverse?p+(r.exclusive?" + 1":""):"0",g=r.reverse?h:p+(r.exclusive?"":" + 1");return`
                ${u.registerUniform("outputSize","u32").registerUniform("axis","u32").declareVariables(o,a)}
                ${u.mainStart()}
                  ${u.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.outputSize")}
                  var inputIndices = ${a.offsetToIndices("global_idx")};
                  var sum = ${a.type.value}(0);
                  let first : i32 = ${m};
                  let last : i32 = ${g};
                  for (var i : i32 = first; i < last; i++) {
                    ${o.indicesSet("inputIndices","uniforms.axis","u32(i)")};
                    sum = sum + ${o.getByIndices("inputIndices")};
                  }
                  ${a.setByOffset("global_idx","sum")};
                }`};return{name:"CumSum",shaderCache:{hint:r.cacheKey,inputDependencies:["rank"]},getRunData:()=>({outputs:[{dims:t,dataType:e}],dispatchGroup:{x:Math.ceil(s/64)},programUniforms:[{type:12,data:s},{type:12,data:c},...ft(t,t)]}),getShaderSource:d}},LE=(e,t)=>{let n=e.inputs[0].dims,r=e.inputs[0].dataType,s=e.inputs[1];e.compute(sA(r,n,s,t),{inputs:[0]})},DE=e=>{let t=e.exclusive===1,n=e.reverse===1;return Xt({exclusive:t,reverse:n})}}),iA,oA,aA,$E,FE,HG=He(()=>{Mt(),Ft(),En(),Ot(),iA=e=>{if(!e||e.length!==1)throw new Error("DepthToSpace requires 1 input.");if(e[0].dims.length!==4)throw new Error("DepthToSpace requires 4D input.")},oA=(e,t,n,r)=>{let s=[];s.push(`fn perm(i: ${r.type.indices}) -> ${n.type.indices} {
    var a: ${n.type.indices};`);for(let i=0;i<t;++i)s.push(n.indicesSet("a",e[i],`i[${i}]`));return s.push("return a;}"),s.join(`
`)},aA=(e,t)=>{let n,r,s,i,o,a,l=t.format==="NHWC",c=t.blocksize,d=t.mode==="DCR";l?([n,r,s,i]=e.dims,o=d?[n,r,s,c,c,i/c**2]:[n,r,s,i/c**2,c,c],a=d?[0,1,3,2,4,5]:[0,1,4,2,5,3]):([n,r,s,i]=[e.dims[0],e.dims[2],e.dims[3],e.dims[1]],o=d?[n,c,c,i/c**2,r,s]:[n,i/c**2,c,c,r,s],a=d?[0,3,4,1,5,2]:[0,1,4,2,5,3]);let u=e.reshape(o),p=u.dims.length,h=e.dataType,m=Re("a",h,p),g=ht("output",h,p),I=f=>`
  ${f.registerUniform("output_size","u32").declareVariables(m,g)}

  ${oA(a,p,m,g)}

  ${f.mainStart()}
    ${f.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}

    let indices = ${g.offsetToIndices("global_idx")};
    let aIndices = perm(indices);

    ${g.setByOffset("global_idx",m.getByIndices("aIndices"))}
  }`;return{name:"DepthToSpace",shaderCache:{hint:`${e.dims};${t.blocksize};${t.mode}`,inputDependencies:["rank"]},getRunData:f=>{let _=l?[n,r*c,s*c,i/c**2]:[n,i/c**2,r*c,s*c],T=Ae.size(_),M=u.dims,v=Ae.sortBasedOnPerm(M,a);return{outputs:[{dims:_,dataType:f[0].dataType}],dispatchGroup:{x:Math.ceil(T/64)},programUniforms:[{type:12,data:T},...ft(M,v)]}},getShaderSource:I}},$E=(e,t)=>{iA(e.inputs),e.compute(aA(e.inputs[0],t))},FE=e=>Xt({blocksize:e.blocksize,mode:e.mode,format:e.format})}),Rc,$a,$h,lA,cA,uA,dA,Fh,pA,OE,jE,KG=He(()=>{Mt(),Ft(),En(),Ot(),Rc="[a-zA-Z]|\\.\\.\\.",$a="("+Rc+")+",$h="^"+$a+"$",lA="("+$a+",)*"+$a,cA="^"+lA+"$",uA=class{constructor(e=-1){this.symbolToIndices=new Map,this.inputIndex=e}addSymbol(e,t){let n=this.symbolToIndices.get(e);n===void 0?n=[t]:n.push(t),this.symbolToIndices.set(e,n)}},dA=class{constructor(e,t){this.equation=t,this.hasEllipsis=!1,this.symbolToInfo=new Map,this.lhs=new Array,this.outputDims=[];let[n,r]=t.includes("->")?t.split("->",2):[t,""];if(!n.match(RegExp(cA)))throw new Error("Invalid LHS term");if(n.split(",").forEach((s,i)=>{let o=e[i].dims.slice();if(!s.match(RegExp($h)))throw new Error("Invalid LHS term");let a=this.processTerm(s,!0,o,i);this.lhs.push(a)}),r==="")r+=[...this.symbolToInfo.entries()].filter(([s,i])=>i.count===1||s==="...").map(([s])=>s).join("");else if(!r.match(RegExp($a)))throw new Error("Invalid RHS");r.match(RegExp(Rc,"g"))?.forEach(s=>{if(s==="...")this.outputDims=this.outputDims.concat(this.ellipsisDims);else{let i=this.symbolToInfo.get(s);if(i===void 0)throw new Error("Invalid RHS symbol");this.outputDims.push(i.dimValue)}}),this.rhs=this.processTerm(r,!1,this.outputDims)}addSymbol(e,t,n){let r=this.symbolToInfo.get(e);if(r!==void 0){if(r.dimValue!==t&&r.count!==1)throw new Error("Dimension mismatch");r.count++,r.inputIndices.push(n)}else r={count:1,dimValue:t,inputIndices:[n]};this.symbolToInfo.set(e,r)}processTerm(e,t,n,r=-1){let s=n.length,i=!1,o=[],a=0;if(!e.match(RegExp($h))&&!t&&e!=="")throw new Error("Invalid LHS term");let l=e.match(RegExp(Rc,"g")),c=new uA(r);return l?.forEach((d,u)=>{if(d==="..."){if(i)throw new Error("Only one ellipsis is allowed per input term");i=!0;let p=s-l.length+1;if(p<0)throw new Error("Ellipsis out of bounds");if(o=n.slice(a,a+p),this.hasEllipsis){if(this.ellipsisDims.length!==o.length||this.ellipsisDims.toString()!==o.toString())throw new Error("Ellipsis dimensions mismatch")}else if(t)this.hasEllipsis=!0,this.ellipsisDims=o;else throw new Error("Ellipsis must be specified in the LHS");for(let h=0;h<o.length;h++){let m=String.fromCharCode(48+h);c.addSymbol(m,u+h),this.addSymbol(m,n[a++],r)}}else c.addSymbol(d,u+(this.hasEllipsis?this.ellipsisDims.length-1:0)),this.addSymbol(d,n[a++],r)}),c}},Fh=e=>e+"_max",pA=(e,t,n,r)=>{let s=e.map(c=>c.length).map((c,d)=>Re(`input${d}`,t,c)),i=Ae.size(r),o=ht("output",t,r.length),a=[...n.symbolToInfo.keys()].filter(c=>!n.rhs.symbolToIndices.has(c)),l=c=>{let d=[],u="var prod = 1.0;",p="var sum = 0.0;",h="sum += prod;",m=[],g=[],I=[],f=[],_=n.symbolToInfo.size===n.rhs.symbolToIndices.size;n.symbolToInfo.forEach((M,v)=>{if(n.rhs.symbolToIndices.has(v)){let b=n.rhs.symbolToIndices.get(v)?.[0];b!==void 0&&n.lhs.forEach((A,k)=>{if(M.inputIndices.includes(k)){let F=A.symbolToIndices.get(v);if(F===void 0)throw new Error("Invalid symbol error");F.forEach(L=>{d.push(`${s[k].indicesSet(`input${k}Indices`,L,o.indicesGet("outputIndices",b))}`)})}})}else n.lhs.forEach((b,A)=>{if(M.inputIndices.includes(A)){let k=b.symbolToIndices.get(v);if(k===void 0)throw new Error("Invalid symbol error");k.forEach(F=>{m.push(`${s[A].indicesSet(`input${A}Indices`,F,`${v}`)}`)}),f.push(`prod *= ${s[A].getByIndices(`input${A}Indices`)};`)}}),g.push(`for(var ${v}: u32 = 0; ${v} < uniforms.${Fh(v)}; ${v}++) {`),I.push("}")});let T=_?[...d,`let sum = ${s.map((M,v)=>M.getByIndices(`input${v}Indices`)).join(" * ")};`]:[...d,p,...g,...m,u,...f,h,...I];return`
            ${c.registerUniforms(a.map(M=>({name:`${Fh(M)}`,type:"u32"}))).registerUniform("outputSize","u32").declareVariables(...s,o)}

            ${c.mainStart()}
            ${c.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.outputSize")}
            var outputIndices = ${o.offsetToIndices("global_idx")};
            ${s.map((M,v)=>`var input${v}Indices: ${s[v].type.indices};`).join(`
`)}
            ${T.join(`
`)};
            ${o.setByOffset("global_idx","sum")};
          }`};return{name:"Einsum",shaderCache:{hint:n.equation,inputDependencies:e.map(()=>"rank")},getRunData:()=>{let c=a.filter(u=>n.symbolToInfo.has(u)).map(u=>({type:12,data:n.symbolToInfo.get(u)?.dimValue||0}));c.push({type:12,data:i});let d=e.map((u,p)=>[...ft(u)]).reduce((u,p)=>u.concat(p),c);return d.push(...ft(r)),{outputs:[{dims:r,dataType:t}],dispatchGroup:{x:Math.ceil(i/64)},programUniforms:d}},getShaderSource:l}},OE=(e,t)=>{let n=new dA(e.inputs,t.equation),r=n.outputDims,s=e.inputs.map((i,o)=>i.dims);e.compute(pA(s,e.inputs[0].dataType,n,r))},jE=e=>{let t=e.equation.replace(/\s+/g,"");return Xt({equation:t})}}),hA,Oh,mA,fA,RE,qG=He(()=>{Mt(),Ft(),Ot(),hA=e=>{if(!e||e.length!==2)throw new Error("Expand requires 2 input.");let t=e[0].dims,n=Array.from(e[1].getBigInt64Array(),Number),r=n.length<t.length?0:n.length-t.length,s=t.length<n.length?0:t.length-n.length;for(;r<n.length&&s<t.length;++r,++s)if(n[r]!==t[s]&&n[r]!==1&&t[s]!==1)throw new Error("Expand requires shape to be broadcastable to input")},Oh=(e,t)=>{let n=e.length-t.length,r=[];for(let s=0;s<n;++s)r.push(e[s]);for(let s=0;s<t.length;++s)r.push(t[s]===1?e[s+n]:t[s]);return r},mA=(e,t)=>e.length>t.length?Oh(e,t):Oh(t,e),fA=e=>{let t=e[0].dims,n=Array.from(e[1].getBigInt64Array(),Number),r=mA(t,n),s=e[0].dataType,i=s===9||Ae.size(t)===1,o=s===9||t.length>0&&t[t.length-1]%4===0?4:1,a=i||r.length>0&&r[r.length-1]%4===0?4:1,l=Math.ceil(Ae.size(r)/a),c=u=>{let p=Re("input",s,t.length,o),h=ht("output",s,r.length,a),m;if(s===9){let g=(I,f,_="")=>`
          let outputIndices${f} = ${h.offsetToIndices(`outputOffset + ${f}u`)};
          let offset${f} = ${p.broadcastedIndicesToOffset(`outputIndices${f}`,h)};
          let index${f} = offset${f} / 4u;
          let component${f} = offset${f} % 4u;
          ${I}[${f}] = ${_}(${p.getByOffset(`index${f}`)}[component${f}]);
        `;m=`
        let outputOffset = global_idx * ${a};
        var data = vec4<u32>(0);
        ${g("data",0,"u32")}
        ${g("data",1,"u32")}
        ${g("data",2,"u32")}
        ${g("data",3,"u32")}
        ${h.setByOffset("global_idx","data")}
      }`}else m=`
        let outputIndices = ${h.offsetToIndices(`global_idx * ${a}`)};
        let inputOffset = ${p.broadcastedIndicesToOffset("outputIndices",h)};
        let data = ${h.type.value}(${p.getByOffset(`inputOffset / ${o}`)});
        ${h.setByOffset("global_idx","data")}
      }`;return`
    ${u.registerUniform("vec_size","u32").declareVariables(p,h)}
    ${u.mainStart()}
    ${u.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.vec_size")}
    ${m}`},d=[{type:12,data:l},...ft(t,r)];return{name:"Expand",shaderCache:{hint:`${r.length};${o}${a}`,inputDependencies:["rank"]},getShaderSource:c,getRunData:()=>({outputs:[{dims:r,dataType:e[0].dataType}],dispatchGroup:{x:Math.ceil(l/64)},programUniforms:d})}},RE=e=>{hA(e.inputs),e.compute(fA(e.inputs),{inputs:[0]})}}),gA,NE,QG=He(()=>{Mt(),Ft(),Ot(),Lg(),gA=e=>{let t=e[0].dataType,n=Ae.size(e[0].dims),r=Ae.size(e[1].dims),s=r%4===0,i=o=>{let a=Re("x",t,[1],4),l=Re("bias",t,[1],4),c=ht("y",t,[1],4),d=[{name:"output_vec_size",type:"u32"},{name:"bias_size",type:"u32"}],u=h=>`
      let bias${h}_offset: u32 = (global_idx * 4 + ${h}) % uniforms.bias_size;
      let bias${h} = ${l.getByOffset(`bias${h}_offset / 4`)}[bias${h}_offset % 4];`,p=s?`
      let bias = ${l.getByOffset("global_idx % (uniforms.bias_size / 4)")};`:`${u(0)}${u(1)}${u(2)}${u(3)}
      let bias = ${a.type.value}(bias0, bias1, bias2, bias3);`;return`${o.registerUniforms(d).declareVariables(a,l,c)}

    ${tf(ar(t))}

    ${o.mainStart(Uo)}
      ${o.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_vec_size")}

      let x = ${a.getByOffset("global_idx")};
      ${p}
      let x_in = x + bias;
      ${c.setByOffset("global_idx",nf("x_in"))}
    }`};return{name:"FastGeluWithBias",shaderCache:{hint:`${s}`,inputDependencies:["type","type"]},getShaderSource:i,getRunData:o=>({outputs:[{dims:o[0].dims,dataType:o[0].dataType}],programUniforms:[{type:12,data:Math.ceil(n/4)},{type:12,data:r}],dispatchGroup:{x:Math.ceil(n/Uo/4)}})}},NE=e=>{e.inputs.length<2||Ae.size(e.inputs[1].dims)===0?iE(e):e.compute(gA(e.inputs))}}),_A,yA,zE,BE,XG=He(()=>{Mt(),Ft(),En(),Ot(),_A=e=>{if(!e||e.length!==2)throw new Error("Gather requires 2 inputs.")},yA=(e,t)=>{let n=e[0].dims,r=e[1].dims,s=n.length,i=Ae.normalizeAxis(t.axis,s),o=n.slice(0);o.splice(i,1,...r);let a=n[i],l=e[0].dataType===9?4:1,c=Math.ceil(Ae.size(o)/l),d=[{type:12,data:c},{type:6,data:a},{type:12,data:i},...ft(e[0].dims,e[1].dims,o)],u=p=>{let h=Re("data",e[0].dataType,e[0].dims.length,l),m=Re("inputIndices",e[1].dataType,e[1].dims.length),g=ht("output",e[0].dataType,o.length,l),I=_=>{let T=r.length,M=`var indicesIndices${_}  = ${m.type.indices}(0);`;for(let v=0;v<T;v++)M+=`${T>1?`indicesIndices${_}[${v}]`:`indicesIndices${_}`} = ${o.length>1?`outputIndices${_}[uniforms.axis + ${v}]`:`outputIndices${_}`};`;M+=`
          var idx${_} = ${m.getByIndices(`indicesIndices${_}`)};
          if (idx${_} < 0) {
            idx${_} = idx${_} + uniforms.axisDimLimit;
          }
          var dataIndices${_} : ${h.type.indices};
        `;for(let v=0,b=0;v<s;v++)v===i?(M+=`${s>1?`dataIndices${_}[${v}]`:`dataIndices${_}`} = u32(idx${_});`,b+=T):(M+=`${s>1?`dataIndices${_}[${v}]`:`dataIndices${_}`} = ${o.length>1?`outputIndices${_}[${b}]`:`outputIndices${_}`};`,b++);return M},f;if(e[0].dataType===9){let _=(T,M,v="")=>`
          let outputIndices${M} = ${g.offsetToIndices(`outputOffset + ${M}u`)};
          ${I(M)};
          let offset${M} = ${h.indicesToOffset(`dataIndices${M}`)};
          let index${M} = offset${M} / 4u;
          let component${M} = offset${M} % 4u;
          ${T}[${M}] = ${v}(${h.getByOffset(`index${M}`)}[component${M}]);
        `;f=`
        let outputOffset = global_idx * ${l};
        var value = vec4<u32>(0);
        ${_("value",0,"u32")}
        ${_("value",1,"u32")}
        ${_("value",2,"u32")}
        ${_("value",3,"u32")}
        ${g.setByOffset("global_idx","value")}
      `}else f=`
      let outputIndices = ${g.offsetToIndices("global_idx")};
      ${I("")};
      let value = ${h.getByIndices("dataIndices")};
      ${g.setByOffset("global_idx","value")};
      `;return`
      ${p.registerUniform("outputSize","u32").registerUniform("axisDimLimit","i32").registerUniform("axis","u32").declareVariables(h,m,g)}
      ${p.mainStart()}
        ${p.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.outputSize")}
        ${f}
      }`};return{name:"Gather",shaderCache:{hint:t.cacheKey,inputDependencies:["rank","rank"]},getRunData:()=>({outputs:[{dims:o,dataType:e[0].dataType}],dispatchGroup:{x:Math.ceil(c/64)},programUniforms:d}),getShaderSource:u}},zE=e=>Xt({axis:e.axis}),BE=(e,t)=>{let n=e.inputs;_A(n),e.compute(yA(e.inputs,t))}}),xA,GE,VE,YG=He(()=>{Mt(),Ft(),Ot(),xA=(e,t,n,r,s,i,o,a,l)=>{let c=[{type:12,data:i},{type:12,data:r},{type:12,data:s},{type:12,data:n},{type:12,data:o},{type:12,data:a},{type:12,data:l}],d=[i];c.push(...ft(t.dims,d));let u=p=>{let h=Re("indices_data",t.dataType,t.dims.length),m=ht("input_slice_offsets_data",12,1,1),g=[h,m],I=[{name:"output_size",type:"u32"},{name:"batch_dims",type:"u32"},{name:"input_dims",type:"u32",length:s.length},{name:"sizes_from_slice_dims_data",type:"u32",length:n.length},{name:"num_slices_per_batch",type:"u32"},{name:"input_batch_stride",type:"u32"},{name:"num_slice_dims",type:"u32"}];return`
  ${p.registerUniforms(I).declareVariables(...g)}
  ${p.mainStart()}
    ${p.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
    let batch_idx = global_idx / uniforms.num_slices_per_batch;
    let base_offset = batch_idx * uniforms.input_batch_stride;

    let slice_indices_base_offset = global_idx * uniforms.num_slice_dims;
    var relative_slice_offset = 0;
    for (var dim_idx = 0u; dim_idx < uniforms.num_slice_dims; dim_idx ++) {
      var index = i32(indices_data[dim_idx + slice_indices_base_offset].x);
      let input_dim_idx = uniforms.batch_dims + dim_idx;
      if (index < 0) {
        ${s.length===1?"index += i32(uniforms.input_dims);":"index += i32(uniforms.input_dims[input_dim_idx]);"}
      }
      ${n.length===1?"relative_slice_offset += index * i32(uniforms.sizes_from_slice_dims_data);":"relative_slice_offset += index * i32(uniforms.sizes_from_slice_dims_data[dim_idx]);"}
    }

    input_slice_offsets_data[global_idx] =  base_offset + u32(relative_slice_offset);
  }`};return e.compute({name:"computeSliceOffsets",shaderCache:{hint:`${s.length}_${n.length}`,inputDependencies:["rank"]},getRunData:()=>({outputs:[{dims:d,dataType:e.inputs[1].dataType}],dispatchGroup:{x:Math.ceil(i/64)},programUniforms:c}),getShaderSource:u},{inputs:[t],outputs:[-1]})[0]},GE=(e,t)=>{let n=e.inputs,r=n[0].dims,s=n[0].dataType,i=n[1].dims,o=i[i.length-1],a=Ae.sizeToDimension(i,i.length-1),l=Ae.sizeFromDimension(r,t.batchDims+o),c=Ae.sizeToDimension(r,t.batchDims),d=Ae.sizeFromDimension(r,t.batchDims),u=a/c,p=new Array(o),h=l;for(let M=0;M<o;++M)p[o-1-M]=h,h*=r[t.batchDims+o-1-M];let m=xA(e,n[1],p,t.batchDims,r,a,u,d,o),g=t.batchDims+o;if(g>r.length)throw new Error("last dimension of indices must not be larger than rank of input tensor");let I=i.slice(0,-1).concat(r.slice(g)),f=Ae.size(I),_=[{type:12,data:f},{type:12,data:l},...ft(n[0].dims,m.dims,I)],T=M=>{let v=Re("data",n[0].dataType,n[0].dims.length),b=Re("slice_offsets",12,m.dims.length),A=ht("output",n[0].dataType,I.length);return`
          ${M.registerUniform("output_size","u32").registerUniform("slice_size","u32").declareVariables(v,b,A)}
            ${M.mainStart()}
            ${M.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
          let slice_offset = slice_offsets[global_idx / uniforms.slice_size];
          output[global_idx] = data[u32(slice_offset) + global_idx % uniforms.slice_size];
        }`};e.compute({name:"GatherND",shaderCache:{hint:t.cacheKey,inputDependencies:["rank","rank"]},getRunData:()=>({outputs:[{dims:I,dataType:s}],dispatchGroup:{x:Math.ceil(f/64)},programUniforms:_}),getShaderSource:T},{inputs:[n[0],m]})},VE=e=>({batchDims:e.batch_dims,cacheKey:""})}),bA,wA,UE,WE,JG=He(()=>{Mt(),Ft(),En(),Ot(),bA=(e,t)=>{if(e.length<3||e.length>4)throw new Error("GatherBlockQuantized requires 3 or 4 inputs.");let n=Ae.normalizeAxis(t.quantizeAxis,e[0].dims.length),r=t.blockSize,s=e[0],i=e[2],o=e.length===4?e[3]:void 0;if(i.dims.length!==s.dims.length||!s.dims.map((a,l)=>l===n?Math.ceil(a/r)===i.dims[l]:a===i.dims[l]).reduce((a,l)=>a&&l,!0))throw new Error("Scales must have the same rank as the input tensor and the dims should match except on gatherAxis.");if(o){if(o.dataType!==s.dataType)throw new Error("Zero point must have the same data type as the input tensor.");if(o.dims.length!==i.dims.length||!o.dims.map((a,l)=>a===i.dims[l]).reduce((a,l)=>a&&l,!0))throw new Error("Zero point must have the same rank as the input tensor and the dims should match except on quantizeAxis.")}},wA=(e,t)=>{let n=e[0].dims,r=e[1].dims,s=n.length,i=Ae.normalizeAxis(t.gatherAxis,s),o=Ae.normalizeAxis(t.quantizeAxis,s),a=n.slice(0);a.splice(i,1,...r);let l=Ae.size(a),c=e[2].dataType,d=e[0].dataType===22,u=[{type:12,data:l},{type:12,data:o},{type:12,data:i},{type:12,data:t.blockSize},...ft(...e.map((h,m)=>h.dims),a)],p=h=>{let m=Re("data",e[0].dataType,e[0].dims.length),g=Re("inputIndices",e[1].dataType,e[1].dims.length),I=Re("scales",e[2].dataType,e[2].dims.length),f=e.length>3?Re("zeroPoint",e[3].dataType,e[3].dims.length):void 0,_=ht("output",c,a.length),T=[m,g,I];f&&T.push(f);let M=[{name:"output_size",type:"u32"},{name:"quantize_axis",type:"u32"},{name:"gather_axis",type:"u32"},{name:"block_size",type:"u32"}];return`
        ${h.registerUniforms(M).declareVariables(...T,_)}
        ${h.mainStart()}
        let output_indices = ${_.offsetToIndices("global_idx")};
        var indices_indices = ${g.type.indices}(0);
        ${r.length>1?`
          for (var i: u32 = 0; i < ${r.length}; i++) {
            let index = ${_.indicesGet("output_indices","uniforms.gather_axis + i")};
            ${g.indicesSet("indices_indices","i","index")};
          }`:`indices_indices = ${_.indicesGet("output_indices","uniforms.gather_axis")};`};
        var data_indices = ${m.type.indices}(0);
        for (var i: u32 = 0; i < uniforms.gather_axis; i++) {
          let index = ${_.indicesGet("output_indices","i")};
          ${m.indicesSet("data_indices","i","index")};
        }
        var index_from_indices = ${g.getByIndices("indices_indices")};
        if (index_from_indices < 0) {
          index_from_indices += ${n[i]};
        }
        ${m.indicesSet("data_indices","uniforms.gather_axis","u32(index_from_indices)")};
        for (var i = uniforms.gather_axis + 1; i < ${a.length}; i++) {
          let index = ${_.indicesGet("output_indices",`i + ${r.length} - 1`)};
          ${m.indicesSet("data_indices","i","index")};
        }
        let data_offset = ${m.indicesToOffset("data_indices")};
        let data_index = data_offset % 8;
        // Convert 4-bit packed data to 8-bit packed data.
        let packed_4bit_quantized_data = ${m.getByOffset("data_offset / 8")};
        let packed_8bit_quantized_data = (packed_4bit_quantized_data >> (4 * (data_index % 2))) & 0x0f0f0f0f;
        let quantized_data_vec = ${d?"unpack4xI8":"unpack4xU8"}(u32(packed_8bit_quantized_data));
        let quantized_data = quantized_data_vec[data_index / 2];
        var scale_indices = data_indices;
        let quantize_axis_index = ${I.indicesGet("data_indices","uniforms.quantize_axis")} / uniforms.block_size;
        ${I.indicesSet("scale_indices","uniforms.quantize_axis","quantize_axis_index")};
        var scale = ${I.getByIndices("scale_indices")};
        ${f?`
              let zero_point_indices = scale_indices;
              let zero_point_offset = ${f.indicesToOffset("zero_point_indices")};
              let zero_point_index = zero_point_offset % 8;
              let packed_4bit_zero_points = ${f.getByOffset("zero_point_offset / 8")};
              let packed_8bit_zero_points = (packed_4bit_zero_points >> (4 * (zero_point_index % 2))) & 0x0f0f0f0f;
              let zero_point_vec = ${d?"unpack4xI8":"unpack4xU8"}(u32(packed_8bit_zero_points));
              let zero_point = zero_point_vec[zero_point_index / 2];`:"var zero_point = 0"};
        let dequantized_data = ${ar(c)}(quantized_data - zero_point) * scale;
        ${_.setByOffset("global_idx","dequantized_data")};
    }`};return{name:"GatherBlockQuantized",shaderCache:{hint:`${t.cacheKey};${e.filter((h,m)=>m!==1).map(h=>h.dims.join("_")).join(";")}`,inputDependencies:Array.from({length:e.length},(h,m)=>"rank")},getRunData:()=>({outputs:[{dims:a,dataType:c}],dispatchGroup:{x:Math.ceil(l/64)},programUniforms:u}),getShaderSource:p}},UE=(e,t)=>{let n=e.inputs;bA(n,t),e.compute(wA(e.inputs,t))},WE=e=>Xt({blockSize:e.blockSize,gatherAxis:e.gatherAxis,quantizeAxis:e.quantizeAxis})}),vA,MA,HE,KE,ZG=He(()=>{Mt(),Ft(),En(),Ot(),vA=e=>{if(!e||e.length!==2)throw new Error("GatherElements requires 2 inputs.");if(e[0].dims.length<1)throw new Error("GatherElements requires that the data input be rank >= 1.");if(e[0].dims.length!==e[1].dims.length)throw new Error(`GatherElements requires that the data input and
                     indices input tensors be of same rank.`)},MA=(e,t)=>{let n=e[0].dims,r=e[0].dataType,s=n.length,i=e[1].dims,o=e[1].dataType,a=Ae.normalizeAxis(t.axis,s),l=n[a],c=i.slice(0),d=Ae.size(c),u=Re("input",r,s),p=Re("indicesInput",o,i.length),h=ht("output",r,c.length),m=[{type:12,data:d},{type:6,data:l},{type:12,data:a}];return m.push(...ft(n,i,c)),{name:"GatherElements",shaderCache:{inputDependencies:["rank","rank"]},getRunData:()=>({outputs:[{dims:c,dataType:e[0].dataType}],dispatchGroup:{x:Math.ceil(d/64)},programUniforms:m}),getShaderSource:g=>`
      ${g.registerUniform("outputSize","u32").registerUniform("axisDimLimit","i32").registerUniform("axis","u32").declareVariables(u,p,h)}
      ${g.mainStart()}
      ${g.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.outputSize")}

      let outputIndices = ${h.offsetToIndices("global_idx")};

      var idx = ${p.getByOffset("global_idx")};
      if (idx < 0) {
        idx = idx + uniforms.axisDimLimit;
      }
      var inputIndices = ${u.type.indices}(outputIndices);
      ${u.indicesSet("inputIndices","uniforms.axis","u32(idx)")};
      let value = ${u.getByIndices("inputIndices")};

      ${h.setByOffset("global_idx","value")};
  }`}},HE=e=>Xt({axis:e.axis}),KE=(e,t)=>{let n=e.inputs;vA(n),e.compute(MA(e.inputs,t))}}),AA,TA,qE,QE,eV=He(()=>{Mt(),Ft(),Ot(),AA=e=>{if(!e)throw new Error("Input is missing");if(e.length<2||e.length>3)throw new Error("Invaid input number.");if(e.length===3&&e[2].dims.length>2)throw new Error("Invalid input shape of C");if(e[0].dataType!==e[1].dataType||e.length===3&&e[0].dataType!==e[2].dataType)throw new Error("Input types are mismatched")},TA=(e,t)=>{let n=e[0].dims.slice(),r=e[1].dims.slice(),[s,i,o]=HI.getShapeOfGemmResult(n,t.transA,r,t.transB,e.length===3?e[2].dims:void 0),a=[s,i];if(!a)throw new Error("Can't use gemm on the given tensors");let l=16,c=Math.ceil(i/l),d=Math.ceil(s/l),u=!0,p=Ae.size(a),h=[{type:12,data:u?c:p},{type:12,data:s},{type:12,data:i},{type:12,data:o},{type:1,data:t.alpha},{type:1,data:t.beta}],m=["type","type"];e.length===3&&(h.push(...ft(e[2].dims)),m.push("rank")),h.push(...ft(a));let g=f=>{let _="";t.transA&&t.transB?_="value += a[k * uniforms.M + m] * b[n * uniforms.K + k];":t.transA&&!t.transB?_="value += a[k * uniforms.M + m] * b[k * uniforms.N + n];":!t.transA&&t.transB?_="value += a[m * uniforms.K + k] * b[n * uniforms.K + k];":!t.transA&&!t.transB&&(_="value += a[m * uniforms.K + k] * b[k * uniforms.N + n];");let T=t.alpha===1?"":"value *= uniforms.alpha;",M=Re("a",e[0].dataType,e[0].dims),v=Re("b",e[1].dataType,e[1].dims),b=M.type.value,A=null,k=[M,v];e.length===3&&(A=Re("c",e[2].dataType,e[2].dims.length),k.push(A));let F=ht("output",e[0].dataType,a.length);k.push(F);let L=[{name:"output_size",type:"u32"},{name:"M",type:"u32"},{name:"N",type:"u32"},{name:"K",type:"u32"},{name:"alpha",type:"f32"},{name:"beta",type:"f32"}];return`
  ${f.registerUniforms(L).declareVariables(...k)}

  ${f.mainStart()}
    ${f.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}

    let m = global_idx / uniforms.N;
    let n = global_idx % uniforms.N;

    var value = ${b}(0);
    for (var k: u32 = 0u; k < uniforms.K; k++) {
      ${_}
    }

    ${T}
    ${A!=null?`let cOffset = ${A.broadcastedIndicesToOffset("vec2(m, n)",F)}; value += ${b}(uniforms.beta) * ${A.getByOffset("cOffset")};`:""}
    output[global_idx] = value;
  }`},I=f=>{let _=Re("a",e[0].dataType,e[0].dims),T=Re("b",e[1].dataType,e[1].dims),M=null,v=[_,T];e.length===3&&(M=Re("c",e[2].dataType,e[2].dims.length),v.push(M));let b=ht("output",e[0].dataType,a.length);v.push(b);let A=[{name:"num_tile_n",type:"u32"},{name:"M",type:"u32"},{name:"N",type:"u32"},{name:"K",type:"u32"},{name:"alpha",type:"f32"},{name:"beta",type:"f32"}],k="",F="";t.transA&&t.transB?(F=`
      var col = tile_row_start + local_id.x;
      var row = k_start + local_id.y;
      if (col < uniforms.M && row < uniforms.K) {
        tile_a[local_id.y][local_id.x] = a[row * uniforms.M + col];
      } else {
        tile_a[local_id.y][local_id.x] = ${_.type.value}(0);
      }

      col = k_start + local_id.x;
      row = tile_col_start + local_id.y;
      if (col < uniforms.K && row < uniforms.N) {
        tile_b[local_id.y][local_id.x] = b[row * uniforms.K + col];
      } else {
        tile_b[local_id.y][local_id.x] = ${T.type.value}(0);
      }
      `,k="value += tile_a[k][local_id.y] * tile_b[local_id.x][k];"):t.transA&&!t.transB?(F=`
      var col = tile_row_start + local_id.x;
      var row = k_start + local_id.y;
      if (col < uniforms.M && row < uniforms.K) {
        tile_a[local_id.y][local_id.x] = a[row * uniforms.M + col];
      } else {
        tile_a[local_id.y][local_id.x] = ${_.type.value}(0);
      }

      col = tile_col_start + local_id.x;
      row = k_start + local_id.y;
      if (col < uniforms.N && row < uniforms.K) {
        tile_b[local_id.y][local_id.x] = b[row * uniforms.N + col];
      } else {
        tile_b[local_id.y][local_id.x] = ${T.type.value}(0);
      }
      `,k="value += tile_a[k][local_id.y] * tile_b[k][local_id.x];"):!t.transA&&t.transB?(F=`
      var col = k_start + local_id.x;
      var row = tile_row_start + local_id.y;
      if (col < uniforms.K && row < uniforms.M) {
        tile_a[local_id.y][local_id.x] = a[row * uniforms.K + col];
      } else {
        tile_a[local_id.y][local_id.x] = ${_.type.value}(0);
      }

      col = k_start + local_id.x;
      row = tile_col_start + local_id.y;
      if (col < uniforms.K && row < uniforms.N) {
        tile_b[local_id.y][local_id.x] = b[row * uniforms.K + col];
      } else {
        tile_b[local_id.y][local_id.x] = ${T.type.value}(0);
      }
      `,k="value += tile_a[local_id.y][k] * tile_b[local_id.x][k];"):!t.transA&&!t.transB&&(F=`
      var col = k_start + local_id.x;
      var row = tile_row_start + local_id.y;
      if (col < uniforms.K && row < uniforms.M) {
        tile_a[local_id.y][local_id.x] = a[row * uniforms.K + col];
      } else {
        tile_a[local_id.y][local_id.x] = ${_.type.value}(0);
      }

      col = tile_col_start + local_id.x;
      row = k_start + local_id.y;
      if (col < uniforms.N && row < uniforms.K) {
        tile_b[local_id.y][local_id.x] = b[row * uniforms.N + col];
      } else {
        tile_b[local_id.y][local_id.x] = ${T.type.value}(0);
      }
      `,k="value += tile_a[local_id.y][k] * tile_b[k][local_id.x];");let L=t.alpha===1?"":"value *= uniforms.alpha;";return`
  ${f.registerUniforms(A).declareVariables(...v)}
  var<workgroup> tile_a: array<array<${_.type.storage}, ${l}>, ${l}>;
  var<workgroup> tile_b: array<array<${T.type.storage}, ${l}>, ${l}>;
  ${f.mainStart([l,l,1])}
    let tile_col_start = (workgroup_index % uniforms.num_tile_n) * ${l};
    let tile_row_start = (workgroup_index / uniforms.num_tile_n) * ${l};
    let num_tiles = (uniforms.K - 1) / ${l} + 1;
    var k_start = 0u;
    var value = ${b.type.value}(0);
    for (var t: u32 = 0u; t < num_tiles; t++) {
      ${F}
      k_start = k_start + ${l};
      workgroupBarrier();

      for (var k: u32 = 0u; k < ${l}; k++) {
        ${k}
      }
      workgroupBarrier();
    }

    ${L}
    let m = tile_row_start + local_id.y;
    let n = tile_col_start + local_id.x;
    ${M!=null?`let cOffset = ${M.broadcastedIndicesToOffset("vec2(m, n)",b)}; value += ${b.type.value}(uniforms.beta) * ${M.getByOffset("cOffset")};`:""}
    if (m < uniforms.M && n < uniforms.N) {
      output[m * uniforms.N + n] = value;
    }
  }`};return u?{name:"GemmShared",shaderCache:{hint:`${t.cacheKey}`,inputDependencies:m},getRunData:()=>({outputs:[{dims:a,dataType:e[0].dataType}],dispatchGroup:{x:c*d},programUniforms:h}),getShaderSource:I}:{name:"Gemm",shaderCache:{hint:`${t.cacheKey}`,inputDependencies:m},getRunData:()=>({outputs:[{dims:a,dataType:e[0].dataType}],dispatchGroup:{x:Math.ceil(p/64)},programUniforms:h}),getShaderSource:g}},qE=e=>{let t=e.transA,n=e.transB,r=e.alpha,s=e.beta;return{transA:t,transB:n,alpha:r,beta:s,cacheKey:`${e.transA};${e.transB};${e.alpha===1}`}},QE=(e,t)=>{AA(e.inputs),e.compute(TA(e.inputs,t))}}),As,Rs,Ii,ki,PA,CA,IA,kA,EA,SA,LA,DA,XE,YE,tV=He(()=>{Mt(),Ft(),En(),Ot(),[As,Rs,Ii,ki]=[0,1,2,3],PA=e=>{if(e[0].dims.length!==4)throw new Error("only 4-D tensor is supported.");if(e[0].dims.length!==e[1].dims.length)throw new Error("input dimensions must be equal to grid dimensions");if(e[0].dims.length-2!==e[1].dims[e[1].dims.length-1])throw new Error(`last dimension of grid must be equal to ${e[0].dims.length-2}`);if(e[0].dims[0]!==e[1].dims[0])throw new Error("grid batch size must match input batch size")},CA=`
  fn gs_get_cubic_coeffs(x: f32) -> vec4<f32> {
    let cubic_alpha = -0.75f;
    let x_abs = abs(x);
    var coeffs: vec4<f32>;
    coeffs[0] = (((cubic_alpha * (x_abs + 1) - 5 * cubic_alpha) * (x_abs + 1) + 8 * cubic_alpha) * (x_abs + 1) - 4 * cubic_alpha);
    coeffs[1] = (((cubic_alpha + 2) * x_abs - (cubic_alpha + 3)) * x_abs * x_abs + 1);
    coeffs[2] = (((cubic_alpha + 2) * (1 - x_abs) - (cubic_alpha + 3)) * (1 - x_abs) * (1 - x_abs) + 1);
    coeffs[3] = (((cubic_alpha * (2 - x_abs) - 5 * cubic_alpha) * (2 - x_abs) + 8 * cubic_alpha) * (2 - x_abs) - 4 * cubic_alpha);
    return coeffs;
  }
`,IA=e=>`
  fn gs_bicubic_interpolate(p: mat4x4<${e}>, x: f32, y: f32) -> ${e} {
    var v: vec4<f32>;
    var coeffs = gs_get_cubic_coeffs(x);
    for (var i = 0; i < 4; i++) {
      v[i] = coeffs[0] * p[i][0] + coeffs[1] * p[i][1] + coeffs[2] * p[i][2] + coeffs[3] * p[i][3];
    }
    coeffs = gs_get_cubic_coeffs(y);
    let pixel = ${e}(coeffs[0] * v[0] + coeffs[1] * v[1] + coeffs[2] * v[2] + coeffs[3] * v[3]);
    return pixel;
  }
`,kA=e=>`
  fn gs_denormalize(n: f32, length: i32) -> f32 {
    ${e.alignCorners===0?`
    // alignCorners: false => [-1, 1] to [-0.5, length - 0.5]
    return ((n + 1.0) * f32(length) - 1.0) / 2.0;
    `:`
    // alignCorners: true => [-1, 1] to [0, length - 1]
    return (n + 1.0) / 2.0 * (f32(length - 1));
    `}
  }
`,EA=e=>`
  ${e.paddingMode==="reflection"?`
      fn gs_reflect(x: i32, x_min: f32, x_max: f32) -> u32 {
        var dx = 0.0;
        var fx = f32(x);
        let range = x_max - x_min;
        if (fx < x_min) {
          dx = x_min - fx;
          let n = u32(dx / range);
          let r = dx - f32(n) * range;
          if (n % 2 == 0) {
            fx = x_min + r;
          } else {
            fx = x_max - r;
          }
        } else if (fx > x_max) {
          dx = fx - x_max;
          let n = u32(dx / range);
          let r = dx - f32(n) * range;
          if (n % 2 == 0) {
            fx = x_max - r;
          } else {
            fx = x_min + r;
          }
        }
        return u32(fx);
      }`:""}
`,SA=(e,t,n)=>`
  fn pixel_at_grid(r: i32, c: i32, H: i32, W: i32, batch: u32, channel: u32, border: vec4<f32>) -> ${t} {
     var pixel = ${t}(0);
     var indices = vec4<u32>(0);
     indices[${As}] = batch;
     indices[${Rs}] = channel;`+(()=>{switch(n.paddingMode){case"zeros":return`
          if (r >= 0 && r < H && c >=0 && c < W) {
            indices[${Ii}] = u32(r);
            indices[${ki}] = u32(c);
          } else {
            return ${t}(0);
          }
        `;case"border":return`
          indices[${Ii}] = u32(clamp(r, 0, H - 1));
          indices[${ki}] = u32(clamp(c, 0, W - 1));
        `;case"reflection":return`
          indices[${Ii}] = gs_reflect(r, border[1], border[3]);
          indices[${ki}] = gs_reflect(c, border[0], border[2]);
        `;default:throw new Error(`padding mode ${n.paddingMode} is not supported`)}})()+`
    return ${e.getByIndices("indices")};
  }
`,LA=(e,t,n)=>(()=>{switch(n.mode){case"nearest":return`
          let result = pixel_at_grid(i32(round(y)), i32(round(x)), H_in, W_in, indices[${As}], indices[${Rs}], border);
        `;case"bilinear":return`
          let x1 = i32(floor(x));
          let y1 = i32(floor(y));
          let x2 = x1 + 1;
          let y2 = y1 + 1;

          let p11 = pixel_at_grid(y1, x1, H_in, W_in, indices[${As}], indices[${Rs}], border);
          let p12 = pixel_at_grid(y1, x2, H_in, W_in, indices[${As}], indices[${Rs}], border);
          let p21 = pixel_at_grid(y2, x1, H_in, W_in, indices[${As}], indices[${Rs}], border);
          let p22 = pixel_at_grid(y2, x2, H_in, W_in, indices[${As}], indices[${Rs}], border);

          let dx2 = ${t}(f32(x2) - x);
          let dx1 = ${t}(x - f32(x1));
          let dy2 = ${t}(f32(y2) - y);
          let dy1 = ${t}(y - f32(y1));
          let result = dy2 * (dx2 * p11 + dx1 * p12) + dy1 * (dx2 * p21 + dx1 * p22);
        `;case"bicubic":return`
          let x0 = i32(floor(x)) - 1;
          let y0 = i32(floor(y)) - 1;
          var p: mat4x4<${t}>;
          for (var h = 0; h < 4; h++) {
            for (var w = 0; w < 4; w++) {
              p[h][w] = pixel_at_grid(h + y0, w + x0, H_in, W_in, indices[${As}], indices[${Rs}], border);
            }
          }

          let dx = x - f32(x0 + 1);
          let dy = y - f32(y0 + 1);
          let result = gs_bicubic_interpolate(p, dx, dy);
        `;default:throw new Error(`mode ${n.mode} is not supported`)}})()+`${e.setByOffset("global_idx","result")}`,DA=(e,t)=>{let n=Re("x",e[0].dataType,e[0].dims.length),r=[e[1].dims[0],e[1].dims[1],e[1].dims[2]],s=Re("grid",e[1].dataType,r.length,2),i=[e[0].dims[0],e[0].dims[1],e[1].dims[1],e[1].dims[2]];t.format==="NHWC"&&(i=[e[0].dims[0],e[1].dims[1],e[1].dims[2],e[0].dims[3]],[As,Rs,Ii,ki]=[0,3,1,2]);let o=ht("output",e[0].dataType,i.length),a=n.type.value,l=Ae.size(i),c=[{type:12,data:l},...ft(e[0].dims,r,i)],d=u=>`
  ${u.registerUniform("output_size","u32").declareVariables(n,s,o)}
  ${CA}
  ${IA(a)}
  ${kA(t)}
  ${EA(t)}
  ${SA(n,a,t)}

  ${u.mainStart()}
    ${u.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
      let H_in = i32(uniforms.x_shape[${Ii}]);
      let W_in = i32(uniforms.x_shape[${ki}]);

      ${t.alignCorners===0?`
      let x_min = -0.5;
      let x_max = f32(W_in) - 0.5;
      let y_min = -0.5;
      let y_max = f32(H_in) - 0.5;
      `:`
      let x_min = 0.0;
      let x_max = f32(W_in) - 1.0;
      let y_min = 0.0;
      let y_max = f32(H_in) - 1.0;
      `};
      let border = vec4<f32>(x_min, y_min, x_max, y_max);

      let indices = ${o.offsetToIndices("global_idx")};
      var grid_indices = vec3<u32>(indices[${As}], indices[${Ii}], indices[${ki}]);
      let nxy = ${s.getByIndices("grid_indices")};
      var x = gs_denormalize(f32(nxy[0]), W_in);
      var y = gs_denormalize(f32(nxy[1]), H_in);

      ${LA(o,a,t)}
  }`;return{name:"GridSample",shaderCache:{hint:`${t.cacheKey}`,inputDependencies:["type","type"]},getRunData:u=>{let p=Ae.size(i);return{outputs:[{dims:i,dataType:u[0].dataType}],dispatchGroup:{x:Math.ceil(p/64)},programUniforms:c}},getShaderSource:d}},XE=(e,t)=>{PA(e.inputs),e.compute(DA(e.inputs,t))},YE=e=>Xt({alignCorners:e.align_corners,mode:e.mode,paddingMode:e.padding_mode,format:e.format})}),hr,$A,JE,jh,FA,Qa,ZE,eS=He(()=>{Mt(),Ft(),En(),Ig(),Sg(),Ot(),hi(),hr=(e,t)=>e.length>t&&e[t].dims.length>0?e[t]:void 0,$A=(e,t)=>{let n=e[0],r=hr(e,1),s=hr(e,2),i=hr(e,3),o=hr(e,4),a=hr(e,5),l=hr(e,6),c=hr(e,7);if(n.dims.length!==3&&n.dims.length!==5)throw new Error("Input query is expected to have 3 or 5 dimensions");let d=n.dims[0],u=n.dims[1],p=n.dims.length===3?n.dims[2]:t.numHeads*n.dims[4],h=u,m=0,g=0,I=Math.floor(p/t.numHeads);if(l&&c&&Ae.size(l.dims)&&Ae.size(c.dims)){if(l.dims.length!==4)throw new Error('Input "past_key" is expected to have 4 dimensions');if(l.dims[0]!==d||l.dims[1]!==t.numHeads||l.dims[3]!==I)throw new Error('Input "past_key" shape (batch_size, num_heads, past_sequence_length, head_size)');if(c.dims[0]!==d||c.dims[1]!==t.numHeads||c.dims[3]!==I)throw new Error('Input "past_value" shape (batch_size, num_heads, past_sequence_length, head_size)');if(l.dims[2]!==c.dims[2])throw new Error('Input "past_key" and "past_value" shall have same dim 2 (past_sequence_length)');if(c.dims.length!==4)throw new Error('Input "past_value" is expected to have 4 dimensions');m=l.dims[2],g=l.dims[2]}else if(l&&Ae.size(l.dims)||c&&Ae.size(c.dims))throw new Error('Input "past_key" and "past_value" shall be both present or both absent');let f;if(r&&Ae.size(r.dims)>0){if(n.dims.length!==3)throw new Error('Input "query" is expected to have 3 dimensions when key is given');if(r.dims.length<3||r.dims.length>5)throw new Error('Input "key" is expected to have 3, 4, or 5 dimensions');if(n.dims[0]!==r.dims[0])throw new Error('Input "query" and "key" shall have same dim 0 (batch size)');if(r.dims.length===3){if(r.dims[2]!==n.dims[2])throw new Error('Input "query" and "key" shall have same dim 2 (hidden_size)');f=2,h=r.dims[1]}else if(r.dims.length===5){if(r.dims[2]!==t.numHeads||r.dims[3]!==2||r.dims[4]!==I)throw new Error('Expect "key" shape (batch_size, kv_sequence_length, num_heads, 2, head_size) for packed kv');if(s)throw new Error('Expect "value" be none when "key" has packed kv format.');f=5,h=r.dims[1]}else{if(r.dims[1]!==t.numHeads||r.dims[3]!==I)throw new Error('Expect "key" shape (batch_size, num_heads, kv_sequence_length, head_size) for past_key');f=0,h=r.dims[2]}}else{if(n.dims.length!==5)throw new Error('Input "query" is expected to have 5 dimensions when key is empty');if(n.dims[2]!==t.numHeads||n.dims[3]!==3)throw new Error('Expect "query" shape (batch_size, kv_sequence_length, num_heads, 3, head_size) for packed kv');f=3}if(i&&Ae.size(i.dims)>0){if(i.dims.length!==1)throw new Error('Input "bias" is expected to have 1 dimension');if(r&&r.dims.length===5&&r.dims[3]===2)throw new Error("bias is not allowed for packed kv.")}let _=m+h,T=0;if(o&&Ae.size(o.dims)>0){T=8;let A=o.dims;throw A.length===1?A[0]===d?T=1:A[0]===3*d+2&&(T=3):A.length===2&&A[0]===d&&A[1]===_&&(T=5),T===8?new Error('Input "key_padding_mask" shape shall be (batch_size) or (batch_size, total_sequence_length)'):new Error("Mask not supported")}let M=!1,v=p;if(s&&Ae.size(s.dims)>0){if(s.dims.length!==3&&s.dims.length!==4)throw new Error('Input "value" is expected to have 3 or 4 dimensions');if(n.dims[0]!==s.dims[0])throw new Error('Input "query" and "value" shall have same dim 0 (batch_size)');if(s.dims.length===3){if(h!==s.dims[1])throw new Error('Input "key" and "value" shall have the same dim 1 (kv_sequence_length)');v=s.dims[2]}else{if(h!==s.dims[2])throw new Error('Input "key" and "value" shall have the same dim 2 (kv_sequence_length)');v=s.dims[1]*s.dims[3],M=!0}}let b=!1;if(o&&Ae.size(o.dims)>0)throw new Error("Key padding mask is not supported");if(a&&Ae.size(a.dims)>0){if(a.dims.length!==4)throw new Error('Input "attention_bias" is expected to have 4 dimensions');if(a.dims[0]!==d||a.dims[1]!==t.numHeads||a.dims[2]!==u||a.dims[3]!==_)throw new Error('Expect "attention_bias" shape (batch_size, num_heads, sequence_length, total_sequence_length)')}return{batchSize:d,sequenceLength:u,pastSequenceLength:m,kvSequenceLength:h,totalSequenceLength:_,maxSequenceLength:g,inputHiddenSize:0,hiddenSize:p,vHiddenSize:v,headSize:I,vHeadSize:Math.floor(v/t.numHeads),numHeads:t.numHeads,isUnidirectional:!1,pastPresentShareBuffer:!1,maskFilterValue:t.maskFilterValue,maskType:T,scale:t.scale,broadcastResPosBias:b,passPastInKv:M,qkvFormat:f}},JE=e=>Xt({...e}),jh=Xt({perm:[0,2,1,3]}),FA=(e,t,n,r,s,i,o)=>{let a=[r,s,i],l=Ae.size(a),c=[{type:12,data:l},{type:12,data:o},{type:12,data:i}],d=u=>{let p=ht("qkv_with_bias",t.dataType,a),h=Re("qkv",t.dataType,a),m=Re("bias",n.dataType,a),g=[{name:"output_size",type:"u32"},{name:"bias_offset",type:"u32"},{name:"hidden_size",type:"u32"}];return`
  ${u.registerUniforms(g).declareVariables(h,m,p)}
  ${u.mainStart()}
    ${u.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
    let bias_offset_idx = (global_idx % uniforms.hidden_size) + uniforms.bias_offset;

    qkv_with_bias[global_idx] = qkv[global_idx] + bias[bias_offset_idx];
  }`};return e.compute({name:"MultiHeadAttentionAddBias",shaderCache:{inputDependencies:["type","type"]},getRunData:()=>({outputs:[{dims:a,dataType:t.dataType,gpuDataType:0}],dispatchGroup:{x:Math.ceil(l/64)},programUniforms:c}),getShaderSource:d},{inputs:[t,n],outputs:[-1]})[0]},Qa=(e,t,n,r,s,i,o,a)=>{let l=i;if(o&&Ae.size(o.dims)>0){if(r===1)throw new Error("AddBiasReshape is not implemented. Please export your model with packed QKV or KV");return l=FA(e,i,o,t,r,n*s,a),l=l.reshape([t,r,n,s]),n===1||r===1?l:e.compute(Sr(l,jh.perm),{inputs:[l],outputs:[-1]})[0]}else return i.dims.length===3&&(l=i.reshape([t,r,n,s])),n===1||r===1?l:e.compute(Sr(l,jh.perm),{inputs:[l],outputs:[-1]})[0]},ZE=(e,t)=>{let n=$A(e.inputs,t),r=e.inputs[0],s=hr(e.inputs,1),i=hr(e.inputs,2),o=hr(e.inputs,3),a=hr(e.inputs,4),l=hr(e.inputs,5),c=hr(e.inputs,6),d=hr(e.inputs,7);if(r.dims.length===5)throw new Error("Packed QKV is not implemented");if(s?.dims.length===5)throw new Error("Packed KV is not implemented");let u=s&&i&&s.dims.length===4&&i.dims.length===4,p=Qa(e,n.batchSize,n.numHeads,n.sequenceLength,n.headSize,r,o,0);if(u)return gl(e,p,s,i,a,void 0,c,d,l,n);if(!s||!i)throw new Error("key and value must be provided");let h=Qa(e,n.batchSize,n.numHeads,n.kvSequenceLength,n.headSize,s,o,n.hiddenSize),m=Qa(e,n.batchSize,n.numHeads,n.kvSequenceLength,n.vHeadSize,i,o,2*n.hiddenSize);gl(e,p,h,m,a,void 0,c,d,l,n)}}),OA,jA,RA,NA,lf,tS,nS,rS=He(()=>{Mt(),Ft(),En(),Ot(),OA=e=>{if(!e||e.length<1)throw new Error("too few inputs")},jA=(e,t)=>{let n=[],r=t.numOutputs;return e[1].dims[0]>0&&(e[1].getBigInt64Array().forEach(s=>n.push(Number(s))),r=n.length),Xt({numOutputs:r,axis:t.axis,splitSizes:n})},RA=e=>`
fn calculateOutputIndex(index: u32) -> u32 {
    for (var i: u32 = 0u; i < ${e}u; i += 1u ) {
    if (index < ${mt("uniforms.size_in_split_axis","i",e)}) {
        return i;
    }
    }
    return ${e}u;
}`,NA=e=>{let t=e.length,n=[];for(let r=0;r<t;++r){let s=e[r].setByIndices("indices","input[global_idx]");t===1?n.push(s):r===0?n.push(`if (output_number == ${r}u) { ${s} }`):r===t-1?n.push(`else { ${s} }`):n.push(`else if (output_number == ${r}) { ${s} }`)}return`
      fn writeBufferData(output_number: u32, indices: ${e[0].type.indices}, global_idx: u32) {
        ${n.join(`
`)}
      }`},lf=(e,t)=>{let n=e[0].dims,r=Ae.size(n),s=e[0].dataType,i=Ae.normalizeAxis(t.axis,n.length),o=new Array(t.numOutputs),a=Re("input",s,n.length),l=new Array(t.numOutputs),c=[],d=[],u=0,p=[{type:12,data:r}];for(let m=0;m<t.numOutputs;m++){u+=t.splitSizes[m],l[m]=u;let g=n.slice();g[i]=t.splitSizes[m],d.push(g),o[m]=ht(`output${m}`,s,g.length),c.push({dims:d[m],dataType:e[0].dataType})}p.push({type:12,data:l},...ft(n,...d));let h=m=>`
  ${m.registerUniform("input_size","u32").registerUniform("size_in_split_axis","u32",l.length).declareVariables(a,...o)}
  ${RA(l.length)}
  ${NA(o)}

  ${m.mainStart()}
    ${m.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.input_size")}

    var indices = ${a.offsetToIndices("global_idx")};
    var index = ${a.indicesGet("indices",i)};
    let output_number = calculateOutputIndex(index);
    if (output_number != 0) {
      index -= ${mt("uniforms.size_in_split_axis","output_number - 1u",l.length)};
      ${a.indicesSet("indices",i,"index")};
    }
    writeBufferData(output_number, indices, global_idx);
  }`;return{name:"Split",shaderCache:{hint:t.cacheKey,inputDependencies:["rank"]},getShaderSource:h,getRunData:()=>({outputs:c,dispatchGroup:{x:Math.ceil(r/64)},programUniforms:p})}},tS=(e,t)=>{OA(e.inputs);let n=e.inputs.length===1?t:jA(e.inputs,t);e.compute(lf(e.inputs,n),{inputs:[0]})},nS=e=>{let t=e.axis,n=e.splitSizes,r=e.numOutputs<0?n.length:e.numOutputs;if(r!==n.length)throw new Error("numOutputs and splitSizes lengh must be equal");return Xt({axis:t,numOutputs:r,splitSizes:n})}}),zA,Mu,sS,iS=He(()=>{Mt(),Ft(),En(),Ot(),zA=(e,t)=>{let[n,r,s,i]=e,{numHeads:o,rotaryEmbeddingDim:a}=t;if(n.dims.length!==3&&n.dims.length!==4)throw new Error(`Input 'x' is expected to have 3 or 4 dimensions, got ${n.dims.length}`);if(!Ae.areEqual(r.dims,[])&&!Ae.areEqual(r.dims,[1])&&r.dims.length!==2)throw new Error(`Input 'position_ids' is expected to have 0, 1, or 2 dimensions, got ${r.dims.length}`);if(s.dims.length!==2)throw new Error(`Input 'cos_cache' is expected to have 2 dimensions, got ${s.dims.length}`);if(i.dims.length!==2)throw new Error(`Input 'sin_cache' is expected to have 2 dimensions, got ${i.dims.length}`);if(!Ae.areEqual(s.dims,i.dims))throw new Error("Inputs 'cos_cache' and 'sin_cache' are expected to have the same shape");if(a>0&&o===0)throw new Error("num_heads must be provided if rotary_embedding_dim is specified");let l=n.dims[0],c=n.dims[n.dims.length-2],d=s.dims[0],u=Ae.sizeFromDimension(n.dims,1)/c,p=a===0?s.dims[1]*2:u/o;if(a>p)throw new Error("rotary_embedding_dim must be less than or equal to head_size");if(r.dims.length===2){if(l!==r.dims[0])throw new Error(`Input 'position_ids' dimension 0 should be of size batch_size, got ${r.dims[0]}`);if(c!==r.dims[1])throw new Error(`Input 'position_ids' dimension 1 should be of size sequence_length, got ${r.dims[1]}`)}if(p/2!==s.dims[1]&&a/2!==s.dims[1])throw new Error(`Input 'cos_cache' dimension 1 should be same as head_size / 2 or rotary_embedding_dim / 2, got ${s.dims[1]}`);if(c>d)throw new Error("Updating cos_cache and sin_cache in RotaryEmbedding is not currently supported")},Mu=(e,t)=>{let{interleaved:n,numHeads:r,rotaryEmbeddingDim:s,scale:i}=t,o=e[0].dims[0],a=Ae.sizeFromDimension(e[0].dims,1),l=e[0].dims[e[0].dims.length-2],c=a/l,d=e[2].dims[1],u=s===0?d*2:c/r,p=new Array(o,l,c/u,u-d),h=Ae.computeStrides(p),m=[{type:1,data:i},{type:12,data:p},{type:12,data:h},...e[0].dims.length===3?new Array({type:12,data:[a,c,u,1]}):[],...e[0].dims.length===4?new Array({type:12,data:[a,u,l*u,1]}):[],...ft(e[0].dims,e[1].dims,e[2].dims,e[3].dims,e[0].dims)],g=I=>{let f=Re("input",e[0].dataType,e[0].dims.length),_=Re("position_ids",e[1].dataType,e[1].dims.length),T=Re("cos_cache",e[2].dataType,e[2].dims.length),M=Re("sin_cache",e[3].dataType,e[3].dims.length),v=ht("output",e[0].dataType,e[0].dims.length);return I.registerUniforms([{name:"scale",type:"f32"},{name:"global_shape",type:"u32",length:p.length},{name:"global_strides",type:"u32",length:h.length},{name:"input_output_strides",type:"u32",length:h.length}]),`
        ${I.declareVariables(f,_,T,M,v)}

        ${I.mainStart(Uo)}
          let half_rotary_emb_dim = uniforms.${T.name}_shape[1];
          let bsnh = global_idx / uniforms.global_strides % uniforms.global_shape;
          let size = uniforms.global_shape[0] * uniforms.global_strides[0];
          ${I.guardAgainstOutOfBoundsWorkgroupSizes("size")}

          if (bsnh[3] < half_rotary_emb_dim) {
            let position_ids_idx =
                ${_.broadcastedIndicesToOffset("bsnh.xy",ht("",_.type.tensor,2))};
            let position_id =
                u32(${_.getByOffset("position_ids_idx")}) + select(0, bsnh[1], position_ids_idx == 0);
            let i = dot(bsnh, uniforms.input_output_strides) + select(0, bsnh[3], ${n});
            let j = i + select(half_rotary_emb_dim, 1, ${n});
            let re = ${f.getByOffset("i")} * ${T.get("position_id","bsnh[3]")} -
                ${f.getByOffset("j")} * ${M.get("position_id","bsnh[3]")};
            ${v.setByOffset("i","re")}
            let im = ${f.getByOffset("i")} * ${M.get("position_id","bsnh[3]")} +
                ${f.getByOffset("j")} * ${T.get("position_id","bsnh[3]")};
            ${v.setByOffset("j","im")}
          } else {
            let k = dot(bsnh, uniforms.input_output_strides) + half_rotary_emb_dim;
            ${v.setByOffset("k",f.getByOffset("k"))}
          }
        }`};return{name:"RotaryEmbedding",shaderCache:{hint:Xt({interleaved:n}).cacheKey,inputDependencies:["rank","rank","rank","rank"]},getShaderSource:g,getRunData:()=>({outputs:[{dims:e[0].dims,dataType:e[0].dataType}],dispatchGroup:{x:Math.ceil(Ae.size(p)/Uo)},programUniforms:m})}},sS=(e,t)=>{zA(e.inputs,t),e.compute(Mu(e.inputs,t))}}),BA,GA,Rh,VA,oS,nV=He(()=>{En(),Mt(),Sg(),eS(),rS(),hi(),iS(),Ot(),BA=(e,t)=>{if(t.doRotary&&e.length<=7)throw new Error("cos_cache and sin_cache inputs are required if do_rotary is specified");let n=e[0],r=e[1],s=e[2],i=e[3],o=e[4];if(t.doRotary!==0&&e.length<=7)throw new Error("cos_cast and sin_cache are expected if do_rotary attribute is non-zero");if(t.localWindowSize!==-1)throw new Error("Local attention is not supported");if(t.softcap!==0)throw new Error("Softcap is not supported");if(t.rotaryInterleaved!==0)throw new Error("Rotary interleaved is not supported");if(t.smoothSoftmax)throw new Error("Smooth softmax is not supported");if(n.dims.length!==3&&n.dims.length!==5)throw new Error("Input query is expected to have 3 or 5 dimensions");let a=!1,l=n.dims[0],c=n.dims[1],d=n.dims.length===3?a?n.dims[2]/3:n.dims[2]:t.numHeads*n.dims[4],u=c,p=0,h=!r||r.dims.length===0,m=Math.floor(h?d/(t.numHeads+2*t.kvNumHeads):d/t.numHeads);h&&(d=m*t.numHeads);let g=i&&i.dims.length!==0,I=o&&o.dims.length!==0;if(g&&i.dims.length===4&&i.dims[0]===l&&i.dims[1]!==t.kvNumHeads&&i.dims[2]===t.kvNumHeads&&i.dims[3]===m)throw new Error("BSNH pastKey/pastValue is not supported");if(g&&I){if(i.dims.length!==4)throw new Error('Input "past_key" is expected to have 4 dimensions');if(o.dims.length!==4)throw new Error('Input "past_value" is expected to have 4 dimensions');p=i.dims[2]}else if(g||I)throw new Error('Input "past_key" and "past_value" shall be both present or both absent');let f=1;if(r&&r.dims.length>0){if(n.dims.length!==3)throw new Error('Input "query" is expected to have 3 dimensions when key is given');if(r.dims.length<3||r.dims.length>5)throw new Error('Input "key" is expected to have 3, 4, or 5 dimensions');if(n.dims[0]!==r.dims[0])throw new Error('Input "query" and "key" shall have same dim 0 (batch size)');if(r.dims.length===3){if(n.dims[2]%r.dims[2]!==0)throw new Error('Dimension 2 of "query" should be a multiple of "key"');u=r.dims[1]}else if(r.dims.length===5){if(r.dims[2]!==t.numHeads||r.dims[3]!==2||r.dims[4]!==m)throw new Error('Expect "key" shape (batch_size, kv_sequence_length, num_heads, 2, head_size) for packed kv');if(s)throw new Error('Expect "value" be none when "key" has packed kv format.');u=r.dims[1]}else{if(r.dims[1]!==t.numHeads||r.dims[3]!==m)throw new Error('Expect "key" shape (batch_size, num_heads, kv_sequence_length, head_size) for past_key');u=r.dims[2]}}else{if(n.dims.length!==3&&n.dims.length!==5)throw new Error('Input "query" is expected to have 3 or 5 dimensions when key is empty');if(n.dims.length===5&&(n.dims[2]!==t.numHeads||n.dims[3]!==3))throw new Error('Expect "query" shape (batch_size, kv_sequence_length, num_heads, 3, head_size) for packed kv');f=3}let _=0,T=!1,M=t.kvNumHeads?m*t.kvNumHeads:d;if(s&&s.dims.length>0){if(s.dims.length!==3&&s.dims.length!==4)throw new Error('Input "value" is expected to have 3 or 4 dimensions');if(n.dims[0]!==s.dims[0])throw new Error('Input "query" and "value" shall have same dim 0 (batch_size)');if(s.dims.length===3){if(u!==s.dims[1])throw new Error('Input "key" and "value" shall have the same dim 1 (kv_sequence_length)');M=s.dims[2]}else{if(u!==s.dims[2])throw new Error('Input "past_key" and "past_value" shall have the same dim 2 (kv_sequence_length)');M=s.dims[1]*s.dims[3],T=!0}}let v=e.length>4?e[5]:void 0;if(v&&v.dims.length!==1&&v.dims[0]!==l)throw new Error('Input "seqlens" is expected to have 1 dimension and the same dim 0 as batch_size');return{batchSize:l,sequenceLength:c,pastSequenceLength:p,kvSequenceLength:u,totalSequenceLength:-1,maxSequenceLength:-1,inputHiddenSize:0,hiddenSize:d,vHiddenSize:M,headSize:m,vHeadSize:Math.floor(M/t.kvNumHeads),numHeads:t.numHeads,kvNumHeads:t.kvNumHeads,nReps:t.numHeads/t.kvNumHeads,pastPresentShareBuffer:!1,maskType:_,scale:t.scale,broadcastResPosBias:!1,passPastInKv:T,qkvFormat:f}},GA=Xt({perm:[0,2,1,3]}),Rh=(e,t,n)=>{let r=t,s=n.kvNumHeads;return t.dims.length===3&&n.kvSequenceLength!==0&&(r=t.reshape([n.batchSize,n.kvSequenceLength,s,n.headSize]),r=e.compute(Sr(r,GA.perm),{inputs:[r],outputs:[-1]})[0]),r},VA=(e,t,n,r)=>{let s=7,i=["type","type"],o=[e*t],a=e*t,l=[{type:12,data:a},{type:12,data:t},{type:12,data:e}],c=d=>{let u=Re("seq_lens",n.dataType,n.dims),p=Re("total_seq_lens",r.dataType,r.dims),h=ht("pos_ids",s,o),m=[{name:"output_size",type:"u32"},{name:"sequence_length",type:"u32"},{name:"batch_size",type:"u32"}];return`
  ${d.registerUniforms(m).declareVariables(u,p,h)}
  ${d.mainStart()}
    ${d.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
    let total_sequence_length = u32(${p.getByOffset("0")});
    let is_subsequent_prompt = uniforms.sequence_length > 1 && uniforms.sequence_length != total_sequence_length;
    let is_first_prompt = !is_subsequent_prompt && uniforms.sequence_length == total_sequence_length;
    let batch_idx = global_idx / uniforms.sequence_length;
    let sequence_idx = i32(global_idx % uniforms.sequence_length);
    var pos_id: i32 = 0;
    let seqlen = ${u.getByOffset("batch_idx")};
    let total_seqlen = seqlen + 1;
    if (is_first_prompt) {
      if (sequence_idx < total_seqlen) {
        pos_id = sequence_idx;
      } else {
        pos_id = 1;
      }
      ${h.setByOffset("global_idx","pos_id")}
    } else if (is_subsequent_prompt) {
      let past_seqlen = total_seqlen - i32(uniforms.sequence_length);
      if (past_seqlen + sequence_idx < total_seqlen) {
        pos_id = past_seqlen + sequence_idx;
      } else {
        pos_id = 1;
      }
      ${h.setByOffset("global_idx","pos_id")}
    } else if (global_idx < uniforms.batch_size) {
      ${h.setByOffset("global_idx","seqlen")}
    };
  }
  `};return{name:"GeneratePositionIds",shaderCache:{hint:`${e};${t}`,inputDependencies:i},getRunData:()=>({outputs:[{dims:o,dataType:s}],dispatchGroup:{x:Math.ceil(a/64)},programUniforms:l}),getShaderSource:c}},oS=(e,t)=>{let n=BA(e.inputs,t);if(e.inputs[0].dims.length===5)throw new Error("Packed QKV is not implemented");if(e.inputs[1]?.dims.length===5)throw new Error("Packed KV is not implemented");let r=e.inputs[0],s=e.inputs[1]&&e.inputs[1].dims.length>0?e.inputs[1]:void 0,i=e.inputs[2]&&e.inputs[2].dims.length>0?e.inputs[2]:void 0,o=e.inputs[3]&&e.inputs[3].dims.length!==0?e.inputs[3]:void 0,a=e.inputs[4]&&e.inputs[4].dims.length!==0?e.inputs[4]:void 0,l=e.inputs.length>4?e.inputs[5]:void 0,c=e.inputs.length>5?e.inputs[6]:void 0,d=n.kvNumHeads?n.kvNumHeads:n.numHeads,u=Xt({axis:2,numOutputs:3,splitSizes:[n.numHeads*n.headSize,d*n.headSize,d*n.headSize]}),[p,h,m]=!s&&!i?e.compute(lf([r],u),{inputs:[r],outputs:[-1,-1,-1]}):[r,s,i],g,I;if(t.doRotary){let M=e.compute(VA(n.batchSize,n.sequenceLength,l,c),{inputs:[l,c],outputs:[-1]})[0],v=e.inputs[7],b=e.inputs[8],A=Xt({interleaved:t.rotaryInterleaved!==0,numHeads:n.numHeads,rotaryEmbeddingDim:0,scale:t.scale}),k=[p,M,v,b],F=[-1];g=e.compute(Mu(k,A),{inputs:k,outputs:F})[0],k.splice(0,1,h);let L=Xt({interleaved:t.rotaryInterleaved!==0,numHeads:n.kvNumHeads,rotaryEmbeddingDim:0,scale:t.scale});I=e.compute(Mu(k,L),{inputs:k,outputs:F})[0]}let f=Qa(e,n.batchSize,n.numHeads,n.sequenceLength,n.headSize,t.doRotary?g:p,void 0,0),_=Rh(e,t.doRotary?I:h,n),T=Rh(e,m,n);gl(e,f,_,T,void 0,void 0,o,a,void 0,n,l,c)}}),Nh,UA,WA,aS,rV=He(()=>{Mt(),Ft(),hi(),Ot(),Nh=(e,t,n,r,s,i,o,a)=>{let l=An(i),c=l===1?"f32":`vec${l}f`,d=l===1?"vec2f":`mat2x${l}f`,u=s*o,p=64;u===1&&(p=256);let h=[s,o,i/l],m=[s,o,2],g=["rank","type","type"],I=[];I.push(...ft(h,m));let f=_=>{let T=Re("x",t.dataType,3,l),M=Re("scale",n.dataType,n.dims),v=Re("bias",r.dataType,r.dims),b=ht("output",1,3,2),A=[T,M,v,b];return`
  var<workgroup> workgroup_shared : array<${d}, ${p}>;
  const workgroup_size = ${p}u;
  ${_.declareVariables(...A)}
  ${_.mainStart(p)}
    let batch = workgroup_index / uniforms.x_shape[1];
    let channel = workgroup_index % uniforms.x_shape[1];
    let hight = uniforms.x_shape[2];
    // initialize workgroup memory
    var sum = ${c}(0);
    var squared_sum = ${c}(0);
    for (var h = local_idx; h < hight; h += workgroup_size) {
      let value = ${c}(${T.get("batch","channel","h")});
      sum += value;
      squared_sum += value * value;
    }
    workgroup_shared[local_idx] = ${d}(sum, squared_sum);
    workgroupBarrier();

    for (var currSize = workgroup_size >> 1;  currSize > 0; currSize = currSize >> 1) {
      if (local_idx < currSize) {
        workgroup_shared[local_idx] = workgroup_shared[local_idx] + workgroup_shared[local_idx + currSize];
      }
      workgroupBarrier();
    }
    if (local_idx == 0) {
      let sum_final = ${ui("workgroup_shared[0][0]",l)} / f32(hight * ${l});
      let squared_sum_final = ${ui("workgroup_shared[0][1]",l)} / f32(hight * ${l});

      let inv_std_dev = inverseSqrt(squared_sum_final - sum_final * sum_final + f32(${a}));
      let channel_scale = inv_std_dev * f32(scale[channel]);
      let channel_shift = f32(bias[channel]) - sum_final * channel_scale;
      output[workgroup_index] = vec2f(channel_scale, channel_shift);
    }
  }`};return e.compute({name:"InstanceNormComputeChannelScaleShift",shaderCache:{hint:`${l};${a};${p}`,inputDependencies:g},getRunData:()=>({outputs:[{dims:m,dataType:1}],dispatchGroup:{x:u},programUniforms:I}),getShaderSource:f},{inputs:[t,n,r],outputs:[-1]})[0]},UA=(e,t,n)=>{let r=t[0].dims,s=r,i=2,o=r[0],a=r[1],l=Ae.sizeFromDimension(r,i),c=An(l),d=Ae.size(s)/c,u=Nh(e,t[0],t[1],t[2],o,l,a,n.epsilon),p=[o,a,l/c],h=[o,a],m=["type","none"],g=I=>{let f=Re("x",t[0].dataType,p.length,c),_=Re("scale_shift",1,h.length,2),T=ht("output",t[0].dataType,p.length,c),M=[f,_,T];return`
  ${I.registerUniform("output_size","u32").declareVariables(...M)}
  ${I.mainStart()}
  ${I.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
      let outputIndices = ${T.offsetToIndices("global_idx")};
      let batch = outputIndices[0];
      let channel = outputIndices[1];
      let scale_shift = ${_.getByIndices("vec2<u32>(batch, channel)")};
      let value = ${f.getByOffset("global_idx")} * ${T.type.value}(scale_shift.x) + ${T.type.value}(scale_shift.y);
      ${T.setByOffset("global_idx","value")};
  }`};e.compute({name:"InstanceNormalization",shaderCache:{hint:`${c}`,inputDependencies:m},getRunData:()=>({outputs:[{dims:s,dataType:t[0].dataType}],dispatchGroup:{x:Math.ceil(d/64)},programUniforms:[{type:12,data:d},...ft(p,h,p)]}),getShaderSource:g},{inputs:[t[0],u]})},WA=(e,t,n)=>{let r=t[0].dims,s=r,i=r[0],o=r[r.length-1],a=Ae.sizeFromDimension(r,1)/o,l=An(o),c=Ae.size(s)/l,d=[{type:12,data:a},{type:12,data:Math.floor(o/l)}],u=["type","type"],p=!1,h=[0,r.length-1];for(let f=0;f<r.length-2;f++)p=p||r[f+1]!==1,h.push(f+1);p=p&&r[r.length-1]!==1;let m=p?e.compute(Sr(e.inputs[0],h),{inputs:[e.inputs[0]],outputs:[-1]})[0]:e.inputs[0].reshape(Array.from({length:r.length},(f,_)=>r[h[_]])),g=Nh(e,m,t[1],t[2],i,a,o,n.epsilon),I=f=>{let _=Jn(t[0].dataType),T=l===1?"vec2f":`mat${l}x2f`,M=A=>{let k=A===0?"x":"y",F=l===1?"f32":`vec${l}f`;switch(l){case 1:return`${_}(${F}(scale.${k}))`;case 2:return`vec2<${_}>(${F}(scale[0].${k}, scale[1].${k}))`;case 4:return`vec4<${_}>(${F}(scale[0].${k}, scale[1].${k}, scale[2].${k}, scale[3].${k}))`;default:throw new Error(`Not supported compoents ${l}`)}},v=Re("input",t[0].dataType,t[0].dims,l),b=ht("output",t[0].dataType,s,l);return`
  @group(0) @binding(0) var<storage, read> input : array<${v.type.storage}>;
  @group(0) @binding(1) var<storage, read> scale_input : array<${T}>;
  @group(0) @binding(2) var<storage, read_write> output : array<${b.type.storage}>;
  struct Uniforms {H: u32, C : u32};
  @group(0) @binding(3) var<uniform> uniforms: Uniforms;

  ${f.mainStart()}
    let current_image_number = global_idx / (uniforms.C * uniforms.H);
    let current_channel_number = global_idx % uniforms.C;

    let scale_offset = current_image_number * uniforms.C + current_channel_number;
    let scale = scale_input[scale_offset];
    output[global_idx] = fma(input[global_idx], ${M(0)}, ${M(1)});
  }`};e.compute({name:"InstanceNormalizationNHWC",shaderCache:{hint:`${l}`,inputDependencies:u},getRunData:()=>({outputs:[{dims:s,dataType:t[0].dataType}],dispatchGroup:{x:Math.ceil(c/64)},programUniforms:d}),getShaderSource:I},{inputs:[t[0],g]})},aS=(e,t)=>{t.format==="NHWC"?WA(e,e.inputs,t):UA(e,e.inputs,t)}}),HA,KA,lS,sV=He(()=>{Mt(),Ft(),Ot(),HA=e=>{if(!e||e.length<2)throw new Error("layerNorm requires at least 2 inputs.")},KA=(e,t,n)=>{let r=t.simplified,s=e[0].dims,i=e[1],o=!r&&e[2],a=s,l=Ae.normalizeAxis(t.axis,s.length),c=Ae.sizeToDimension(s,l),d=Ae.sizeFromDimension(s,l),u=Ae.size(i.dims),p=o?Ae.size(o.dims):0;if(u!==d||o&&p!==d)throw new Error(`Size of X.shape()[axis:] == ${d}.
       Size of scale and bias (if provided) must match this.
       Got scale size of ${u} and bias size of ${p}`);let h=[];for(let v=0;v<s.length;++v)v<l?h.push(s[v]):h.push(1);let m=An(d),g=["type","type"],I=[{type:12,data:c},{type:1,data:d},{type:12,data:Math.floor(d/m)},{type:1,data:t.epsilon}];o&&g.push("type");let f=n>1,_=n>2,T=v=>{let b=Jn(e[0].dataType),A=[Re("x",e[0].dataType,e[0].dims,m),Re("scale",i.dataType,i.dims,m)];o&&A.push(Re("bias",o.dataType,o.dims,m)),A.push(ht("output",e[0].dataType,a,m)),f&&A.push(ht("mean_data_output",1,h)),_&&A.push(ht("inv_std_output",1,h));let k=[{name:"norm_count",type:"u32"},{name:"norm_size",type:"f32"},{name:"norm_size_vectorized",type:"u32"},{name:"epsilon",type:"f32"}];return`
  ${v.registerUniforms(k).declareVariables(...A)}
  ${v.mainStart()}
    ${v.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.norm_count")}
    let offset = global_idx * uniforms.norm_size_vectorized;
    var mean_vector = ${Jm("f32",m)};
    var mean_square_vector = ${Jm("f32",m)};

    for (var h: u32 = 0u; h < uniforms.norm_size_vectorized; h++) {
      let value = ${$o(b,m,"x[h + offset]")};
      mean_vector += value;
      mean_square_vector += value * value;
    }
    let mean = ${ui("mean_vector",m)} / uniforms.norm_size;
    let inv_std_dev = inverseSqrt(${ui("mean_square_vector",m)} / uniforms.norm_size ${r?"":"- mean * mean"} + uniforms.epsilon);

    for (var j: u32 = 0; j < uniforms.norm_size_vectorized; j++) {
      let f32input = ${$o(b,m,"x[j + offset]")};
      let f32scale = ${$o(b,m,"scale[j]")};
      output[j + offset] = ${A[0].type.value}((f32input ${r?"":"- mean"}) * inv_std_dev * f32scale
        ${o?`+ ${$o(b,m,"bias[j]")}`:""}
      );
    }

    ${f?"mean_data_output[global_idx] = mean":""};
    ${_?"inv_std_output[global_idx] = inv_std_dev":""};
  }`},M=[{dims:a,dataType:e[0].dataType}];return f&&M.push({dims:h,dataType:1}),_&&M.push({dims:h,dataType:1}),{name:"LayerNormalization",shaderCache:{hint:`${m};${n};${r}`,inputDependencies:g},getRunData:()=>({outputs:M,dispatchGroup:{x:Math.ceil(c/64)},programUniforms:I}),getShaderSource:T}},lS=(e,t)=>{HA(e.inputs),e.compute(KA(e.inputs,t,e.outputCount))}}),qA,cS,iV=He(()=>{Ft(),Og(),jg(),qA=e=>{if(!e||e.length!==2)throw new Error("MatMul requires 2 inputs.");if(e[0].dims[e[0].dims.length-1]!==e[1].dims[e[1].dims.length-2])throw new Error("shared dimension does not match.")},cS=e=>{qA(e.inputs);let t=Vo.calcShape(e.inputs[0].dims,e.inputs[1].dims,!0);if(!t)throw new Error("Can't use matmul on the given tensors");let n=t[t.length-1],r=e.inputs[0].dims[e.inputs[0].dims.length-1];if(n<8&&r<8)e.compute(Fg(e.inputs,{activation:""},t));else{let s=t[t.length-2],i=Ae.size(e.inputs[0].dims.slice(0,-2)),o=Ae.size(e.inputs[1].dims.slice(0,-2));if(i!==1&&s===1&&o===1){let a=e.inputs[0].reshape([1,i,r]),l=e.inputs[1].reshape([1,r,n]),c=[1,i,n],d=[a,l];e.compute(vu(d,{activation:""},t,c),{inputs:d})}else e.compute(vu(e.inputs,{activation:""},t))}}}),QA,XA,YA,uS,dS,oV=He(()=>{Mt(),Ft(),En(),Ot(),QA=(e,t)=>{if(e.length<3||e.length>4)throw new Error("MatMulNBits requires 3 or 4 inputs");let n=e[0],r=n.dims.length;if(n.dims[r-1]!==t.k)throw new Error("The last dim of input shape does not match the k value");let s=Math.floor((t.k+t.blockSize-1)/t.blockSize),i=t.blockSize/8*t.bits,o=e[1];if(!Ae.areEqual(o.dims,[t.n,s,i]))throw new Error("The second inputs must be 3D tensor with shape N X nBlocksPerCol X blobSize");let a=e[2].dims;if(Ae.size(a)!==t.n*s)throw new Error("scales input size error.");if(e.length===4){let l=e[3].dims,c=t.bits>4?t.n*s:t.n*Math.floor((s+1)/2);if(Ae.size(l)!==c)throw new Error("zeroPoints input size error.")}},XA=(e,t)=>{let n=e[0].dims,r=n.length,s=n[r-2],i=t.k,o=t.n,a=n.slice(0,r-2),l=Ae.size(a),c=e[1].dims[2]/4,d=e[0].dataType,u=An(t.k),p=An(c),h=An(o),m=a.concat([s,o]),g=s>1&&o/h%2===0?2:1,I=Ae.size(m)/h/g,f=64,_=[],T=[l,s,i/u],M=Ae.convertShape(e[1].dims).slice();M.splice(-1,1,c/p),_.push(...ft(T)),_.push(...ft(M)),_.push(...ft(e[2].dims)),e.length===4&&_.push(...ft(Ae.convertShape(e[3].dims)));let v=[l,s,o/h];_.push(...ft(v));let b=A=>{let k=T.length,F=Re("a",e[0].dataType,k,u),L=Re("b",12,M.length,p),G=Re("scales",e[2].dataType,e[2].dims.length),j=[F,L,G],R=e.length===4?Re("zero_points",12,e[3].dims.length):void 0;R&&j.push(R);let K=v.length,U=ht("output",e[0].dataType,K,h),Y=Jn(e[0].dataType),te=(()=>{switch(u){case 1:return`array<${Y}, 8>`;case 2:return`mat4x2<${Y}>`;case 4:return`mat2x4<${Y}>`;default:throw new Error(`${u}-component is not supported.`)}})(),ne=()=>{let oe=`
          // reuse a data
            var input_offset = ${F.indicesToOffset(`${F.type.indices}(batch, row, word_offset)`)};
            var a_data: ${te};
            for (var j: u32 = 0; j < ${8/u}; j++) {
              a_data[j] = ${F.getByOffset("input_offset")};
              input_offset++;
            }
          `;for(let X=0;X<h*g;X++)oe+=`
            b_value = ${p===1?`b${X}_data`:`b${X}_data[i]`};
            b_value_lower = unpack4xU8(b_value & b_mask);
            b_value_upper = unpack4xU8((b_value >> 4) & b_mask);
            b_quantized_values = ${te}(${Array.from({length:4},(D,z)=>`${Y}(b_value_lower[${z}]), ${Y}(b_value_upper[${z}])`).join(", ")});
            b_dequantized_values = ${u===1?`${te}(${Array.from({length:8},(D,z)=>`(b_quantized_values[${z}] - ${R?`zero_point${X}`:"zero_point"}) * scale${X}`).join(", ")});`:`(b_quantized_values - ${te}(${Array(8).fill(`${R?`zero_point${X}`:"zero_point"}`).join(",")})) * scale${X};`};
            workgroup_shared[local_id.x * ${g} + ${Math.floor(X/h)}]${h>1?`[${X%h}]`:""} += ${Array.from({length:8/u},(D,z)=>`${u===1?`a_data[${z}] * b_dequantized_values[${z}]`:`dot(a_data[${z}], b_dequantized_values[${z}])`}`).join(" + ")};
          `;return oe},le=()=>{let oe=`
            var col_index = col * ${h};
            ${R?`
            let zero_point_bytes_per_col = (nBlocksPerCol + 1) / 2;
            var zero_point_byte_count: u32;
            var zero_point_word_index: u32;
            var zero_point_byte_offset: u32;
            let zero_point_nibble_offset: u32 = block & 0x1u;
            var zero_point_bits_offset: u32;
            var zero_point_word: u32;`:`
            // The default zero point is 8 for unsigned 4-bit quantization.
            let zero_point = ${Y}(8);`}
            `;for(let X=0;X<h*g;X++)oe+=`
            let scale${X} = ${G.getByOffset("col_index * nBlocksPerCol + block")};
            ${R?`
            zero_point_byte_count = col_index * zero_point_bytes_per_col + (block >> 0x1u);
            zero_point_word_index = zero_point_byte_count >> 0x2u;
            zero_point_byte_offset = zero_point_byte_count & 0x3u;
            zero_point_bits_offset = (zero_point_byte_offset << 3) + (zero_point_nibble_offset << 2);
            zero_point_word = ${R.getByOffset("zero_point_word_index")} >> zero_point_bits_offset;
            let zero_point${X} = ${Y}((zero_point_word) & 0xFu);`:""}
            col_index += 1;`;return oe},N=()=>{let oe=`col_index = col * ${h};`;for(let X=0;X<h*g;X++)oe+=`
            let b${X}_data = ${L.getByIndices(`${L.type.indices}(col_index, block, word)`)};
            col_index += 1;`;return oe+=`
            var b_value: u32;
            let b_mask: u32 = 0x0F0F0F0Fu;
            var b_value_lower: vec4<u32>;
            var b_value_upper: vec4<u32>;
            var b_quantized_values: ${te};
            var b_dequantized_values: ${te};`,oe};return`
        var<workgroup> workgroup_shared: array<${U.type.value}, ${g*f}>;
        ${A.declareVariables(...j,U)}
        ${A.mainStart([f,1,1])}
          let output_indices = ${U.offsetToIndices(`(global_idx / ${f}) * ${g}`)};
          let col = output_indices[2];
          let row = output_indices[1];
          let batch = output_indices[0];
          let nBlocksPerCol = uniforms.b_shape[1];

          for (var block = local_id.x; block < nBlocksPerCol; block += ${f}) {
            //process one block
            var word_offset: u32 = block * ${t.blockSize/u};
            ${le()}
            for (var word: u32 = 0; word < ${c}; word += ${p}) {
              ${N()}
              for (var i: u32 = 0; i < ${p}; i++) {
                ${ne()}
                word_offset += ${8/u};
              }
            }
          }
          workgroupBarrier();

          if (local_id.x < ${g}) {
            var output_value: ${U.type.value} = ${U.type.value}(0);
            var workgroup_shared_offset: u32 = local_id.x;
            for (var b: u32 = 0u; b < ${f}u; b++) {
              output_value += workgroup_shared[workgroup_shared_offset];
              workgroup_shared_offset += ${g};
            }
            ${U.setByIndices(`${U.type.indices}(batch, row, col + local_id.x)`,"output_value")};
          }
        }`};return{name:"MatMulNBits",shaderCache:{hint:`${t.blockSize};${t.bits};${u};${p};${h};${g};${f}`,inputDependencies:Array(e.length).fill("rank")},getRunData:()=>({outputs:[{dims:m,dataType:d}],dispatchGroup:{x:I},programUniforms:_}),getShaderSource:b}},YA=(e,t)=>{let n=e[0].dims,r=n.length,s=n[r-2],i=t.k,o=t.n,a=n.slice(0,r-2),l=Ae.size(a),c=e[1].dims[2]/4,d=e[0].dataType,u=An(t.k),p=An(c),h=a.concat([s,o]),m=128,g=o%8===0?8:o%4===0?4:1,I=m/g,f=I*p*8,_=f/u,T=f/t.blockSize,M=Ae.size(h)/g,v=[],b=[l,s,i/u],A=Ae.convertShape(e[1].dims).slice();A.splice(-1,1,c/p),v.push(...ft(b)),v.push(...ft(A)),v.push(...ft(e[2].dims)),e.length===4&&v.push(...ft(Ae.convertShape(e[3].dims)));let k=[l,s,o];v.push(...ft(k));let F=L=>{let G=b.length,j=Re("a",e[0].dataType,G,u),R=Re("b",12,A.length,p),K=Re("scales",e[2].dataType,e[2].dims.length),U=[j,R,K],Y=e.length===4?Re("zero_points",12,e[3].dims.length):void 0;Y&&U.push(Y);let te=k.length,ne=ht("output",e[0].dataType,te),le=Jn(e[0].dataType),N=()=>{switch(u){case 1:return`
          let a_data0 = vec4<${le}>(sub_a[word_offset], sub_a[word_offset + 1], sub_a[word_offset + 2], sub_a[word_offset + 3]);
          let a_data1 = vec4<${le}>(sub_a[word_offset + 4], sub_a[word_offset + 5], sub_a[word_offset + 6], sub_a[word_offset + 7]);`;case 2:return`
          let a_data0 = vec4<${le}>(sub_a[word_offset], sub_a[word_offset + 1]);
          let a_data1 = vec4<${le}>(sub_a[word_offset + 2], sub_a[word_offset + 3]);`;case 4:return`
          let a_data0 = sub_a[word_offset];
          let a_data1 = sub_a[word_offset + 1];`;default:throw new Error(`${u}-component is not supported.`)}};return`
        var<workgroup> sub_a: array<${j.type.value}, ${_}>;
        var<workgroup> inter_results: array<array<${ne.type.value}, ${I}>, ${g}>;
        ${L.declareVariables(...U,ne)}
        ${L.mainStart([I,g,1])}
          let output_indices = ${ne.offsetToIndices(`workgroup_index * ${g}`)};
          let col = output_indices[2];
          let row = output_indices[1];
          let batch = output_indices[0];
          let n_blocks_per_col = uniforms.b_shape[1];
          let num_tiles =  (n_blocks_per_col - 1) / ${T} + 1;

          // Loop over shared dimension.
          for (var tile: u32 = 0; tile < num_tiles; tile += 1) {
            let a_col_start = tile * ${_};
            // load one tile A data into shared memory.
            for (var a_offset = local_idx; a_offset < ${_}; a_offset += ${m})
            {
              let a_col = a_col_start + a_offset;
              if (a_col < uniforms.a_shape[2])
              {
                sub_a[a_offset] = ${j.getByIndices(`${j.type.indices}(batch, row, a_col)`)};
              } else {
                sub_a[a_offset] = ${j.type.value}(0);
              }
            }
            workgroupBarrier();

            // each thread process one block
            let b_row = col + local_id.y;
            let block = tile * ${T} + local_id.x;
            ${Y?`
            let zero_point_bytes_per_col = (n_blocks_per_col + 1) / 2;
            let zero_point_byte_count = b_row * zero_point_bytes_per_col + (block >> 0x1u);
            let zero_point_word_index = zero_point_byte_count >> 0x2u;
            let zero_point_byte_offset = zero_point_byte_count & 0x3u;
            let zero_point_nibble_offset: u32 = block & 0x1u;
            let zero_point_bits_offset = (zero_point_byte_offset << 3) + (zero_point_nibble_offset << 2);
            let zero_point_word = ${Y.getByOffset("zero_point_word_index")} >> zero_point_bits_offset;
            let zero_point = ${le}((zero_point_word) & 0xFu);`:`
            // The default zero point is 8 for unsigned 4-bit quantization.
            let zero_point = ${le}(8);`}
            let scale = ${K.getByOffset("b_row * n_blocks_per_col + block")};
            let b_data = ${R.getByIndices(`${R.type.indices}(b_row, block, 0)`)};
            var word_offset = local_id.x * ${t.blockSize/u};
            for (var i: u32 = 0; i < ${p}; i++) {
              ${N()}
              let b_value = ${p===1?"b_data":"b_data[i]"};
              let b_value_lower = unpack4xU8(b_value & 0x0F0F0F0Fu);
              let b_value_upper = unpack4xU8((b_value >> 4) & 0x0F0F0F0Fu);
              let b_quantized_values = mat2x4<${le}>(${Array.from({length:4},(oe,X)=>`${le}(b_value_lower[${X}]), ${le}(b_value_upper[${X}])`).join(", ")});
              let b_dequantized_values = (b_quantized_values - mat2x4<${le}>(${Array(8).fill("zero_point").join(",")})) * scale;
              inter_results[local_id.y][local_id.x] += ${Array.from({length:2},(oe,X)=>`${`dot(a_data${X}, b_dequantized_values[${X}])`}`).join(" + ")};
              word_offset += ${8/u};
            }
            workgroupBarrier();
          }

          if (local_idx < ${g}) {
            var output_value: ${ne.type.value} = ${ne.type.value}(0);
            for (var b = 0u; b < ${I}; b++) {
              output_value += inter_results[local_idx][b];
            }
            if (col + local_idx < uniforms.output_shape[2])
            {
              ${ne.setByIndices(`${ne.type.indices}(batch, row, col + local_idx)`,"output_value")}
            }
          }
        }`};return{name:"BlockwiseMatMulNBits32",shaderCache:{hint:`${t.blockSize};${u};${p};${I};${g}`,inputDependencies:Array(e.length).fill("rank")},getRunData:()=>({outputs:[{dims:h,dataType:d}],dispatchGroup:{x:M},programUniforms:v}),getShaderSource:F}},uS=(e,t)=>{QA(e.inputs,t),t.blockSize===32&&e.adapterInfo.isVendor("intel")&&e.adapterInfo.isArchitecture("gen-12lp")?e.compute(YA(e.inputs,t)):e.compute(XA(e.inputs,t))},dS=e=>Xt(e)}),JA,ZA,eT,tT,nT,rT,sT,iT,pS,aV=He(()=>{Mt(),Ft(),Ot(),JA=e=>{if(!e||e.length<1)throw new Error("Too few inputs");if(e[0].dataType!==1&&e[0].dataType!==10)throw new Error("Input type must be float or float16.");if(e.length>=2){let t=e[0].dims.length*2===e[1].dims[0];if(e.length===4&&(t=e[3].dims[0]*2===e[1].dims[0]),!t)throw new Error("The pads should be a 1D tensor of shape [2 * input_rank] or [2 * num_axes].")}},ZA=(e,t,n)=>{let r="";for(let s=t-1;s>=0;--s)r+=`
            k = i32(${e.indicesGet("indices",s)}) - ${mt("uniforms.pads",s,n)};
            if (k < 0) {
              break;
            }
            if (k >= i32(${mt("uniforms.x_shape",s,t)})) {
              break;
            }
            offset += k * i32(${mt("uniforms.x_strides",s,t)});
        `;return`
          value = ${e.type.value}(uniforms.constant_value);
          for (var i = 0; i < 1; i++) {
            var offset = 0;
            var k = 0;
            ${r}
            value = x[offset];
          }
      `},eT=(e,t,n)=>{let r="";for(let s=t-1;s>=0;--s)r+=`
                k = i32(${e.indicesGet("indices",s)}) - ${mt("uniforms.pads",s,n)};
                if (k < 0) {
                  k = -k;
                }
                {
                  let _2n_1 = 2 * (i32(${mt("uniforms.x_shape",s,t)}) - 1);
                  k = k % _2n_1;
                  if(k >= i32(${mt("uniforms.x_shape",s,t)})) {
                    k = _2n_1 - k;
                  }
                }
                offset += k * i32(${mt("uniforms.x_strides",s,t)});
            `;return`
              var offset = 0;
              var k = 0;
              ${r}
              value = x[offset];
          `},tT=(e,t,n)=>{let r="";for(let s=t-1;s>=0;--s)r+=`
                k = i32(${e.indicesGet("indices",s)}) - ${mt("uniforms.pads",s,n)};
                if (k < 0) {
                  k = 0;
                }
                if (k >= i32(${mt("uniforms.x_shape",s,t)})) {
                  k = i32(${mt("uniforms.x_shape",s,t)}) - 1;
                }
                offset += k * i32(${mt("uniforms.x_strides",s,t)});
            `;return`
              var offset = 0;
              var k = 0;
              ${r}
              value = x[offset];
          `},nT=(e,t,n)=>{let r="";for(let s=t-1;s>=0;--s)r+=`
                k = i32(${e.indicesGet("indices",s)}) - ${mt("uniforms.pads",s,n)};
                if (k < 0)  {
                  k += i32(${mt("uniforms.x_shape",s,t)}]);
                }
                if (k >= i32(${mt("uniforms.x_shape",s,t)})) {
                  k -= i32(${mt("uniforms.x_shape",s,t)});
                }
                offset += k * i32(${mt("uniforms.x_strides",s,t)});
            `;return`
              var offset = 0;
              var k = 0;
              ${r}
              value = x[offset];
          `},rT=(e,t,n)=>{switch(n.mode){case 0:return ZA(e,t,n.pads.length);case 1:return eT(e,t,n.pads.length);case 2:return tT(e,t,n.pads.length);case 3:return nT(e,t,n.pads.length);default:throw new Error("Invalid mode")}},sT=(e,t)=>{let n=Ae.padShape(e[0].dims.slice(),t.pads),r=e[0].dims,s=Ae.size(n),i=[{type:12,data:s},{type:6,data:t.pads}],o=e.length>=3&&e[2].data;t.mode===0&&i.push({type:o?e[2].dataType:1,data:t.value}),i.push(...ft(e[0].dims,n));let a=["rank"],l=c=>{let d=ht("output",e[0].dataType,n.length),u=Re("x",e[0].dataType,r.length),p=u.type.value,h=rT(d,r.length,t),m=[{name:"output_size",type:"u32"},{name:"pads",type:"i32",length:t.pads.length}];return t.mode===0&&m.push({name:"constant_value",type:o?p:"f32"}),`
            ${c.registerUniforms(m).declareVariables(u,d)}
            ${c.mainStart()}
            ${c.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}

            let indices = ${d.offsetToIndices("global_idx")};

            var value = ${p}(0);
            ${h}
            output[global_idx] = value;
        }`};return{name:"Pad",shaderCache:{hint:`${t.mode}${o}`,inputDependencies:a},getRunData:()=>({outputs:[{dims:n,dataType:e[0].dataType}],dispatchGroup:{x:Math.ceil(Ae.size(n)/64)},programUniforms:i}),getShaderSource:l}},iT=(e,t)=>{if(e.length>1){let n=e[1].getBigInt64Array(),r=e.length>=3&&e[2].data?e[2].dataType===10?e[2].getUint16Array()[0]:e[2].getFloat32Array()[0]:0,s=e[0].dims.length,i=new Int32Array(2*s).fill(0);if(e.length>=4){let a=e[3].getBigInt64Array();for(let l=0;l<a.length;l++)i[Number(a[l])]=Number(n[l]),i[Number(a[l])+s]=Number(n[l+a.length])}else n.forEach((a,l)=>i[Number(l)]=Number(a));let o=[];return i.forEach(a=>o.push(a)),{mode:t.mode,value:r,pads:o}}else return t},pS=(e,t)=>{JA(e.inputs);let n=iT(e.inputs,t);e.compute(sT(e.inputs,n),{inputs:[0]})}}),Fa,zh,Bh,Gh,Vh,oT,aT,Uh,Wh,hS,mS,Hh,fS,gS,Kh,_S,yS,xS,bS,lV=He(()=>{ds(),Mt(),Ft(),Ot(),Fa=e=>{if(ln.webgpu.validateInputContent&&(!e||e.length!==1))throw new Error("Pool ops requires 1 input.")},zh=(e,t,n)=>{let r=t.format==="NHWC",s=e.dims.slice();r&&s.splice(1,0,s.pop());let i=Object.hasOwnProperty.call(t,"dilations"),o=t.kernelShape.slice(),a=t.strides.slice(),l=i?t.dilations.slice():[],c=t.pads.slice();bu.adjustPoolAttributes(n,s,o,a,l,c);let d=bu.computePoolOutputShape(n,s,a,l,o,c,t.autoPad),u=Object.assign({},t);i?Object.assign(u,{kernelShape:o,strides:a,pads:c,dilations:l,cacheKey:t.cacheKey}):Object.assign(u,{kernelShape:o,strides:a,pads:c,cacheKey:t.cacheKey});let p=d.slice();return p.push(p.splice(1,1)[0]),[u,r?p:d]},Bh=(e,t)=>{let n=t.format==="NHWC",r=Ae.size(e),s=Ae.size(t.kernelShape),i=[{type:12,data:r},{type:12,data:s}],o=[{name:"outputSize",type:"u32"},{name:"kernelSize",type:"u32"}];if(t.kernelShape.length<=2){let a=t.kernelShape[t.kernelShape.length-1],l=t.strides[t.strides.length-1],c=t.pads[t.pads.length/2-1],d=t.pads[t.pads.length-1],u=!!(c+d);i.push({type:12,data:a},{type:12,data:l},{type:12,data:c},{type:12,data:d}),o.push({name:"kw",type:"u32"},{name:"sw",type:"u32"},{name:"pwStart",type:"u32"},{name:"pwEnd",type:"u32"});let p=!1;if(t.kernelShape.length===2){let h=t.kernelShape[t.kernelShape.length-2],m=t.strides[t.strides.length-2],g=t.pads[t.pads.length/2-2],I=t.pads[t.pads.length-2];p=!!(g+I),i.push({type:12,data:h},{type:12,data:m},{type:12,data:g},{type:12,data:I}),o.push({name:"kh",type:"u32"},{name:"sh",type:"u32"},{name:"phStart",type:"u32"},{name:"phEnd",type:"u32"})}return[i,o,!0,u,p]}else{if(n)throw new Error("Pooling with kernelShape.length > 2 is not supported for NHWC format.");let a=Ae.computeStrides(t.kernelShape);i.push({type:12,data:a},{type:12,data:t.pads},{type:12,data:t.strides}),o.push({name:"kernelStrides",type:"u32",length:a.length},{name:"pads",type:"u32",length:t.pads.length},{name:"strides",type:"u32",length:t.strides.length});let l=t.pads.reduce((c,d)=>c+d);return[i,o,!!l,!1,!1]}},Gh=(e,t,n,r,s,i,o,a,l,c,d,u)=>{let p=s.format==="NHWC",h=t.type.value,m=ht("output",t.type.tensor,r);if(s.kernelShape.length<=2){let g="",I="",f="",_=n-(p?2:1);if(d?g=`
                for (var i: u32 = 0u; i < uniforms.kw; i++) {
                  xIndices[${_}] = indices[${_}] * uniforms.sw - uniforms.pwStart + i;
                  if (xIndices[${_}] < 0 || xIndices[${_}]
                      >= uniforms.x_shape[${_}]) {
                    pad++;
                    continue;
                  }
                  let x_val = x[${t.indicesToOffset("xIndices")}];
                  ${i}
                }`:g=`
                for (var i: u32 = 0u; i < uniforms.kw; i++) {
                  xIndices[${_}] = indices[${_}] * uniforms.sw - uniforms.pwStart + i;
                  let x_val = x[${t.indicesToOffset("xIndices")}];
                  ${i}
                }`,s.kernelShape.length===2){let T=n-(p?3:2);u?I=`
                for (var j: u32 = 0u; j < uniforms.kh; j++) {
                  xIndices[${T}] = indices[${T}] * uniforms.sh - uniforms.phStart + j;
                  if (xIndices[${T}] < 0 || xIndices[${T}] >= uniforms.x_shape[${T}]) {
                    pad += i32(uniforms.kw);
                    continue;
                  }
              `:I=`
                for (var j: u32 = 0u; j < uniforms.kh; j++) {
                  xIndices[${T}] = indices[${T}] * uniforms.sh - uniforms.phStart + j;
                `,f=`
              }
            `}return`
            ${e.registerUniforms(l).declareVariables(t,m)}

            ${e.mainStart()}
              ${e.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.outputSize")}

              let indices = ${m.offsetToIndices("global_idx")};
              var xIndices = ${m.offsetToIndices("global_idx")};

              var value = ${h}(${a});
              var pad = 0;
              ${I}
              ${g}
              ${f}
              ${o}

              output[global_idx] = value;
            }`}else{if(p)throw new Error("Pooling with kernelShape.length > 2 is not supported for NHWC format.");let g=s.kernelShape.length,I=s.pads.length,f="";return c?f=`
                if (xIndices[j] >= uniforms.x_shape[j]) {
                  pad++;
                  isPad = true;
                  break;
                }
              }
              if (!isPad) {
                let x_val = x[${t.indicesToOffset("xIndices")}];
                ${i}
              }`:f=`
              }
              let x_val = x[${t.indicesToOffset("xIndices")}];
              ${i}
            `,`
            ${e.registerUniforms(l).declareVariables(t,m)}

            ${e.mainStart()}
              ${e.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.outputSize")}
              let indices = ${m.offsetToIndices("global_idx")};
              var xIndices = ${m.offsetToIndices("global_idx")};

              var offsets: array<u32, ${g}>;

              var value = ${h}(${a});
              var pad = 0;
              var isPad = false;

              for (var i: u32 = 0u; i < uniforms.kernelSize; i++) {
                var offset = i;
                for (var j = 0u; j < ${g-1}u; j++) {
                  offsets[j] = offset / ${mt("uniforms.kernelStrides","j",g)};
                  offset -= offsets[j] * ${mt("uniforms.kernelStrides","j",g)};
                }
                offsets[${g-1}] = offset;

                isPad = false;
                for (var j = ${n-g}u; j < ${n}u; j++) {
                  xIndices[j] = indices[j] * ${mt("uniforms.strides",`j - ${n-g}u`,g)}
                    + offsets[j - ${n-g}u] - ${mt("uniforms.pads","j - 2u",I)};
                  ${f}
              }
              ${o}

              output[global_idx] = value;
            }`}},Vh=e=>`${e.format};${e.ceilMode};${e.autoPad};${e.kernelShape.length}`,oT=e=>`${Vh(e)};${e.countIncludePad}`,aT=e=>`${Vh(e)};${e.storageOrder};${e.dilations}`,Uh=e=>({format:e.format,autoPad:["NOTSET","VALID","SAME_UPPER","SAME_LOWER"][e.auto_pad],ceilMode:e.ceil_mode,kernelShape:e.kernel_shape,strides:e.strides,pads:e.pads}),Wh=(e,t,n,r)=>{let[s,i]=zh(t,r,n),o=Re("x",t.dataType,t.dims.length),a=o.type.value,l="value += x_val;",c="";s.countIncludePad?c+=`value /= ${a}(uniforms.kernelSize);`:c+=`value /= ${a}(i32(uniforms.kernelSize) - pad);`;let[d,u,p,h,m]=Bh(i,s);d.push(...ft(t.dims,i));let g=["rank"];return{name:e,shaderCache:{hint:`${r.cacheKey};${p};${h};${m}`,inputDependencies:g},getRunData:()=>({outputs:[{dims:i,dataType:t.dataType}],dispatchGroup:{x:Math.ceil(Ae.size(i)/64)},programUniforms:d}),getShaderSource:I=>Gh(I,o,t.dims.length,i.length,s,l,c,0,u,p,h,m)}},hS=e=>{let t=e.count_include_pad!==0,n=Uh(e);if(n.ceilMode!==0)throw new Error("using ceil() in shape computation is not yet supported for AveragePool");let r={countIncludePad:t,...n,cacheKey:""};return{...r,cacheKey:oT(r)}},mS=(e,t)=>{Fa(e.inputs),e.compute(Wh("AveragePool",e.inputs[0],!1,t))},Hh={autoPad:"",ceilMode:0,countIncludePad:!1,kernelShape:[],strides:[],pads:[],storageOrder:0,dilations:[]},fS=e=>{let t=e.format;return{format:t,...Hh,cacheKey:t}},gS=(e,t)=>{Fa(e.inputs),e.compute(Wh("GlobalAveragePool",e.inputs[0],!0,t))},Kh=(e,t,n,r)=>{let[s,i]=zh(t,r,n),o=`
      value = max(x_val, value);
    `,a="",l=Re("x",t.dataType,t.dims.length),c=["rank"],[d,u,p,h,m]=Bh(i,s);return d.push(...ft(t.dims,i)),{name:e,shaderCache:{hint:`${r.cacheKey};${p};${h};${m}`,inputDependencies:c},getRunData:()=>({outputs:[{dims:i,dataType:t.dataType}],dispatchGroup:{x:Math.ceil(Ae.size(i)/64)},programUniforms:d}),getShaderSource:g=>Gh(g,l,t.dims.length,i.length,s,o,a,t.dataType===10?-65504:-1e5,u,p,h,m)}},_S=(e,t)=>{Fa(e.inputs),e.compute(Kh("MaxPool",e.inputs[0],!1,t))},yS=e=>{let t=e.storage_order,n=e.dilations,r=Uh(e);if(t!==0)throw new Error("column major storage order is not yet supported for MaxPool");if(r.ceilMode!==0)throw new Error("using ceil() in shape computation is not yet supported for MaxPool");let s={storageOrder:t,dilations:n,...r,cacheKey:""};return{...s,cacheKey:aT(s)}},xS=e=>{let t=e.format;return{format:t,...Hh,cacheKey:t}},bS=(e,t)=>{Fa(e.inputs),e.compute(Kh("GlobalMaxPool",e.inputs[0],!0,t))}}),lT,cT,wS,vS,cV=He(()=>{Mt(),Ft(),En(),Ot(),lT=(e,t)=>{if(e.length<2||e.length>3)throw new Error("DequantizeLinear requires 2 or 3 inputs.");if(e.length===3&&e[1].dims===e[2].dims)throw new Error("x-scale and x-zero-point must have the same shape.");if(e.length===3&&e[0].dataType!==e[2].dataType)throw new Error("x and x-zero-point must have the same data type.");if(e[0].dataType===6&&e.length>2)throw new Error("In the case of dequantizing int32 there is no zero point.");if(e[1].dims.length!==0&&e[1].dims.length!==1&&e[1].dims.length!==e[0].dims.length)throw new Error("scale input must be a scalar, a 1D tensor, or have the same rank as the input tensor.");if(e.length>2){if(e[0].dataType!==e[2].dataType)throw new Error("x and x-zero-point must have the same data type.");if(e[1].dims.length!==e[2].dims.length)throw new Error("scale and zero-point inputs must have the same rank.");if(!e[1].dims.map((n,r)=>n===e[2].dims[r]).reduce((n,r)=>n&&r,!0))throw new Error("scale and zero-point inputs must have the same shape.")}if(t.blockSize>0){if(e[1].dims.length===0||e[1].dims.length===1&&e[1].dims[0]===1)throw new Error("blockSize must be set only for block quantization.");if(!e[1].dims.map((s,i)=>i===t.axis||s===e[0].dims[i]).reduce((s,i)=>s&&i,!0))throw new Error("For block qunatization, scale input shape to match the input shape except for the axis");if(e[1].dims.length!==e[0].dims.length)throw new Error("For block qunatization the scale input rank must be the same as the x rank.");let n=e[0].dims[t.axis],r=e[1].dims[t.axis];if(t.blockSize<Math.ceil(n/r)||t.blockSize>Math.ceil(n/(r-1)-1))throw new Error("blockSize must be with in the range [ceil(dI / Si), ceil(dI / (Si - 1) - 1)].")}},cT=(e,t)=>{let n=Ae.normalizeAxis(t.axis,e[0].dims.length),r=e[0].dataType,s=r===3,i=e[0].dims,o=e[1].dataType,a=Ae.size(i),l=r===3||r===2,c=l?[Math.ceil(Ae.size(e[0].dims)/4)]:e[0].dims,d=e[1].dims,u=e.length>2?e[2]:void 0,p=u?l?[Math.ceil(Ae.size(u.dims)/4)]:u.dims:void 0,h=d.length===0||d.length===1&&d[0]===1,m=h===!1&&d.length===1,g=An(a),I=h&&(!l||g===4),f=I?g:1,_=I&&!l?g:1,T=Re("input",l?12:r,c.length,_),M=Re("scale",o,d.length),v=u?Re("zero_point",l?12:r,p.length):void 0,b=ht("output",o,i.length,f),A=[T,M];v&&A.push(v);let k=[c,d];u&&k.push(p);let F=[{type:12,data:a/f},{type:12,data:n},{type:12,data:t.blockSize},...ft(...k,i)],L=G=>{let j=[{name:"output_size",type:"u32"},{name:"axis",type:"u32"},{name:"block_size",type:"u32"}];return`
      ${G.registerUniforms(j).declareVariables(...A,b)}
      ${G.mainStart()}
          ${G.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
          let output_indices = ${b.offsetToIndices("global_idx")};

          // Set input x
          ${l?`
            let input = ${T.getByOffset("global_idx / 4")};
            let x_vec = ${s?"unpack4xI8(input)":"unpack4xU8(input)"};
            let x_value = ${f===1?"x_vec[global_idx % 4]":"x_vec"};`:`let x_value = ${T.getByOffset("global_idx")};`};

          // Set scale input
          ${h?`let scale_value= ${M.getByOffset("0")}`:m?`
            let scale_index = ${b.indicesGet("output_indices","uniforms.axis")};
            let scale_value= ${M.getByOffset("scale_index")};`:`
            var scale_indices: ${M.type.indices} = output_indices;
            let index = ${M.indicesGet("scale_indices","uniforms.axis")} / uniforms.block_size;
            ${M.indicesSet("scale_indices","uniforms.axis","index")};
            let scale_value= ${M.getByIndices("scale_indices")};`};

          // Set zero-point input
          ${v?h?l?`
                let zero_point_input = ${v.getByOffset("0")};
                let zero_point_vec =  ${s?"unpack4xI8(zero_point_input)":"unpack4xU8(zero_point_input)"};
                let zero_point_value= zero_point_vec[0]`:`let zero_point_value = ${v.getByOffset("0")}`:m?l?`
                let zero_point_index = ${b.indicesGet("output_indices","uniforms.axis")};
                let zero_point_input = ${v.getByOffset("zero_point_index / 4")};
                let zero_point_vec =  ${s?"unpack4xI8(zero_point_input)":"unpack4xU8(zero_point_input)"};
                let zero_point_value = zero_point_vec[zero_point_index % 4]`:`
                let zero_point_index = ${b.indicesGet("output_indices","uniforms.axis")};
                let zero_point_value = ${v.getByOffset("zero_point_index")};`:l?`
                let zero_point_offset = ${M.indicesToOffset("scale_indices")};
                let zero_point_input = ${v.getByOffset("zero_point_offset / 4")};
                let zero_point_vec = ${s?"unpack4xI8(zero_point_input)":"unpack4xU8(zero_point_input)"};
                let zero_point_value = zero_point_vec[zero_point_offset % 4];`:`let zero_point_value = ${v.getByIndices("scale_indices")};`:`let zero_point_value = ${l?s?"i32":"u32":T.type.value}(0);`};
      // Compute and write output
      ${b.setByOffset("global_idx",`${b.type.value}(x_value - zero_point_value) * scale_value`)};
      }`};return{name:"DequantizeLinear",shaderCache:{hint:t.cacheKey,inputDependencies:v?["rank","rank","rank"]:["rank","rank"]},getShaderSource:L,getRunData:()=>({outputs:[{dims:i,dataType:o}],dispatchGroup:{x:Math.ceil(a/f/64),y:1,z:1},programUniforms:F})}},wS=(e,t)=>{lT(e.inputs,t),e.compute(cT(e.inputs,t))},vS=e=>Xt({axis:e.axis,blockSize:e.blockSize})}),uT,dT,MS,uV=He(()=>{ds(),Mt(),Ot(),uT=(e,t,n)=>{let r=e===t,s=e<t&&n<0,i=e>t&&n>0;if(r||s||i)throw new Error("Range these inputs' contents are invalid.")},dT=(e,t,n,r)=>{let s=Math.abs(Math.ceil((t-e)/n)),i=[s],o=s,a=[{type:12,data:o},{type:r,data:e},{type:r,data:n},...ft(i)],l=c=>{let d=ht("output",r,i.length),u=d.type.value,p=[{name:"outputSize",type:"u32"},{name:"start",type:u},{name:"delta",type:u}];return`
        ${c.registerUniforms(p).declareVariables(d)}
        ${c.mainStart()}
        ${c.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.outputSize")}
        output[global_idx] = uniforms.start + ${u}(global_idx) * uniforms.delta;
      }`};return{name:"Range",shaderCache:{hint:`${r}`},getShaderSource:l,getRunData:()=>({outputs:[{dims:i,dataType:r}],dispatchGroup:{x:Math.ceil(o/64)},programUniforms:a})}},MS=e=>{let t=0,n=0,r=0;e.inputs[0].dataType===6?(t=e.inputs[0].getInt32Array()[0],n=e.inputs[1].getInt32Array()[0],r=e.inputs[2].getInt32Array()[0]):e.inputs[0].dataType===1&&(t=e.inputs[0].getFloat32Array()[0],n=e.inputs[1].getFloat32Array()[0],r=e.inputs[2].getFloat32Array()[0]),ln.webgpu.validateInputContent&&uT(t,n,r),e.compute(dT(t,n,r,e.inputs[0].dataType),{inputs:[]})}}),pT,qh,Qh,hT,AS,TS,dV=He(()=>{Mt(),Ft(),En(),Ot(),pT=(e,t,n,r)=>{if(e!=="none"&&r!=="i32"&&r!=="u32"&&r!=="f32")throw new Error(`Input ${r} is not supported with reduction ${e}.`);let s=`{
                var oldValue = 0;
                loop {
                  let newValueF32 =`,i=`;
                  let newValue = bitcast<i32>(newValueF32);
                  let res = atomicCompareExchangeWeak(&${t}, oldValue, newValue);
                  if res.exchanged {
                    break;
                  }
                  oldValue = res.old_value;
                }
              }`;switch(e){case"none":return`${t}=${n};`;case"add":return r==="i32"||r==="u32"?`atomicAdd(&${t}, bitcast<${r}>(${n}));`:`
              ${s}bitcast<${r}>(oldValue) + (${n})${i}`;case"max":return r==="i32"||r==="u32"?`atomicMax(&${t}, bitcast<${r}>(${n}));`:`
                ${s}max(bitcast<f32>(oldValue), (${n}))${i}`;case"min":return r==="i32"||r==="u32"?`atomicMin(&${t}, bitcast<${r}>(${n}));`:`${s}min(bitcast<${r}>(oldValue), (${n}))${i}`;case"mul":return`${s}(bitcast<${r}>(oldValue) * (${n}))${i}`;default:throw new Error(`Reduction ${e} is not supported.`)}},qh=(e,t)=>`${e===1?`
    let element_count_dim = uniforms.output_strides;
    let dim_value = uniforms.output_shape;`:`
    let element_count_dim = uniforms.output_strides[${t?"i - indices_start":"i"}];
    let dim_value = uniforms.output_shape[${t?"i - indices_start":"i"} + uniforms.last_index_dimension];`}
    
    if (index >= 0) {
      if (index >= i32(dim_value)) {
        index = i32(dim_value - 1);
      }
    } else {
      if (index < -i32(dim_value)) {
        index = 0;
      } else {
        index += i32(dim_value);
      }
    }
    data_offset += u32((u32(index) * element_count_dim));`,Qh=(e,t,n)=>`for (var i = 0u; i < uniforms.num_updates_elements; i++) {
        let value = updates[uniforms.num_updates_elements * ${n?"global_idx":"idx"} + i];
        ${pT(e.reduction,"output[data_offset + i]","value",t)}
      }`,hT=(e,t)=>{let n=e[0].dims,r=e[1].dims,s=n,i=1,o=Math.ceil(Ae.size(r)/i),a=r[r.length-1],l=Ae.sizeFromDimension(n,a),c=Ae.sizeFromDimension(r,0)/a,d=[{type:12,data:o},{type:12,data:a},{type:12,data:l},...ft(e[1].dims,e[2].dims,s)],u=p=>{let h=Re("indices",e[1].dataType,e[1].dims.length),m=Re("updates",e[2].dataType,e[2].dims.length,i),g=t.reduction!=="none"&&t.reduction!==""?ZI("output",e[0].dataType,s.length):ht("output",e[0].dataType,s.length,i);return`
      ${p.registerUniform("output_size","u32").registerUniform("last_index_dimension","u32").registerUniform("num_updates_elements","u32").declareVariables(h,m,g)}
      ${p.mainStart()}
        ${p.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
  var hasDuplicates = false;
  if (${t.reduction==="none"}) {
    for (var i = 0; i < ${c}; i = i + 1) {
      for (var j = i + 1; j < ${c}; j = j + 1) {
        var index_i = i32(indices[i].x);
        var index_j = i32(indices[j].x);
        if (index_i == index_j) {
          hasDuplicates = true;
          break;
        }
      }
      if (hasDuplicates) {
        break;
      }
    }
  }

  if (${t.reduction==="none"} && hasDuplicates) {
    if (global_idx != 0u) {
      return;
    }
    // Process each index-update pair individually when duplicates exist
    for (var idx = 0u; idx < ${c}u; idx++) {
      var data_offset = 0u;
      for (var i = 0u; i < uniforms.last_index_dimension; i++) {
        var index = i32(indices[idx * uniforms.last_index_dimension + i].x);
        ${qh(n.length,!1)}
      }
      ${Qh(t,g.type.value,!1)}
    }
    return;
  }

  var data_offset = 0u;
  var indices_start = uniforms.last_index_dimension * global_idx;
  var indices_end = indices_start + uniforms.last_index_dimension;
  for (var i = indices_start; i < indices_end; i++) {
    var index = i32(indices[i].x);
    ${qh(n.length,!0)}
  }
  ${Qh(t,g.type.value,!0)}
  }`};return{name:"ScatterND",shaderCache:{hint:`${t.cacheKey}_${t.reduction}`,inputDependencies:["rank","rank"]},getRunData:()=>({outputs:[{dims:s,dataType:e[0].dataType}],dispatchGroup:{x:Math.ceil(o/64)},programUniforms:d}),getShaderSource:u}},AS=e=>Xt({reduction:e.reduction}),TS=(e,t)=>{e.compute(hT(e.inputs,t),{inputs:[e.inputs[1],e.inputs[2]],outputs:[]})}}),mT,fT,gT,Xh,_T,yT,xT,bT,wT,vT,MT,AT,Yh,TT,PT,CT,IT,kT,PS,CS,pV=He(()=>{Mt(),Ft(),En(),Ot(),mT=(e,t)=>{if(e.every(n=>n>0||(()=>{throw new Error("Resize requires scales input values to be positive")})),e.length>0){if(t.mode==="linear"){if(!(e.length===2||e.length===3||e.length===4&&e[0]===1&&e[1]===1||e.length===4&&e[0]===1&&e[3]===1||e.length===5&&e[0]===1&&e[1]===1))throw new Error(`For linear mode, Resize requires scales to be 2D, 3D, 4D with either two outermost or one innermost and
            one outermost scale values equal to 1, or 5D with two outermost scale values equal to 1`)}else if(t.mode==="cubic"&&!(e.length===2||e.length===4&&e[0]===1&&e[1]===1||e.length===4&&e[0]===1&&e[3]===1))throw new Error("Resize requires scales input size to be 2 or 4 for cubic mode")}},fT=(e,t,n)=>{t.every(s=>s>=0&&s<n||(()=>{throw new Error("Resize requires axes input values to be positive and less than rank")}));let r=new Array(n).fill(1);return t.forEach((s,i)=>r[s]=e[i]),r},gT=(e,t,n,r,s,i)=>{let[o,a,l]=n>10?[1,2,3]:[-1,e.length>1?1:-1,-1],c=e[0].dims.length;if(o>0&&e.length>o&&e[o].dims.length>0)e[o].getFloat32Array().forEach(d=>i.push(d));else if(t.coordinateTransformMode==="tf_crop_and_resize")throw new Error("Resize requires RoI input to be specified when coordinateTransformMode is tfCropAndResize");if(a>0&&e.length>a&&e[a].dims.length===1&&e[a].dims[0]>0){if(e[a].getFloat32Array().forEach(d=>r.push(d)),r.length!==0&&r.length!==c&&n>=18&&r.length!==t.axes.length)throw new Error("Resize requires scales input size to be same as input rank or axes size for opset 18 and up");mT(r,t),t.axes.length>0&&fT(r,t.axes,c).forEach((d,u)=>r[u]=d)}if(l>0&&e.length>l&&e[l].dims.length===1&&e[l].dims[0]>0&&(e[l].getBigInt64Array().forEach(d=>s.push(Number(d))),s.length!==0&&s.length!==c&&n>=18&&s.length!==t.axes.length))throw new Error("Resize requires sizes input size to be same as input rank or axes size for opset 18 and up");if(t.axes.length>0){if(r.length!==0&&r.length!==t.axes.length)throw new Error('Resize requires "scales" input size to be of axes rank when axes attributes is specified');if(s.length!==0&&s.length!==t.axes.length)throw new Error('Resize requires "sizes" input size to be of rank axes rank when axes attributes is specified')}if(typeof r<"u"&&typeof s<"u"&&r.length>0&&s.length>c)throw new Error("Resize requires only of scales or sizes to be specified")},Xh=(e,t,n,r)=>`
  // The whole part and the fractional part are calculated separately due to inaccuracy of floating
  // point division. As an example, f32(21) / f32(7) may evaluate to 2.99... instead of 3, causing an
  // offset-by-one error later in floor().
  let big = (${e}) * (${t});
  let whole = ${r}(big / (${n}));
  let fract = ${r}(big % (${n})) / ${r}(${n});
  return whole + fract;
`,_T=(e,t)=>`fn getOriginalCoordinateFromResizedCoordinate(xResized: u32, xScale: f32, lengthResized: u32,
     lengthOriginal: u32, roiStart: f32, roiEnd: f32) -> ${t} { `+(()=>{switch(e){case"asymmetric":return`
          if (xScale < 1.0 || floor(xScale) != xScale) {
            return ${t}(xResized) / ${t}(xScale);
          } else {
            ${Xh("xResized","lengthOriginal","lengthResized",t)}
          }
        `;case"pytorch_half_pixel":return`if (lengthResized > 1) {
                    return (${t}(xResized) + 0.5) / ${t}(xScale) - 0.5;
                  } else {
                    return 0.0;
                  }`;case"tf_half_pixel_for_nn":return`return (${t}(xResized) + 0.5) / ${t}(xScale);`;case"align_corners":return`if (lengthResized == 1) {
                    return 0.0;
                  } else {
                    ${Xh("xResized","lengthOriginal - 1","lengthResized - 1",t)}
                  }`;case"tf_crop_and_resize":return`if (lengthResized > 1) {
                    return ${t}(roiStart) * ${t}(lengthOriginal - 1) +
                        (${t}(xResized) * ${t}(roiEnd - roiStart) * ${t}(lengthOriginal - 1)) /
                        ${t}(lengthResized - 1);
                  } else {
                    return 0.5 * ${t}(roiStart + roiEnd) * ${t}(lengthOriginal - 1);
                  }`;case"half_pixel_symmetric":return`const outputWidth = ${t}xScale * ${t}(lengthResized);
                  const adjustment = ${t}(lengthResized) / outputWidth;
                  const center = ${t}(lengthOriginal) / 2;
                  const offset = center * (1 - adjustment);
                  return offset + ((${t}(xResized) + 0.5) / ${t}(xScale)) - 0.5;`;case"half_pixel":return`return ((${t}(xResized) + 0.5) / ${t}(xScale)) - 0.5;`;default:throw new Error(`Coordinate transform mode ${e} is not supported`)}})()+"}",yT=(e,t,n)=>`fn getNearestPixelFromOriginal(xOriginal: ${n}, isDownSample: bool) -> ${n} {`+(()=>{switch(e){case"round_prefer_ceil":return"if (fract(xOriginal) == 0.5) {             return ceil(xOriginal);           } else {             return round(xOriginal);           }";case"floor":return"return floor(xOriginal);";case"ceil":return"return ceil(xOriginal);";case"round_prefer_floor":return"if (fract(xOriginal) == 0.5) {                     return floor(xOriginal);                   } else {                     return round(xOriginal);                   }";default:if(t<11)return"if (isDownSample)                     {                       return ceil(xOriginal);                     } else {                       return xOriginal;                     }";throw new Error(`Nearest mode ${e} is not supported`)}})()+"}",xT=(e,t,n)=>{let r=new Array(n).fill(0).concat(new Array(n).fill(1)),s=e.length===0?r:e.slice();return t.length>0?(t.forEach((i,o)=>{r[i]=s[o],r[o+n]=s[t.length+o]}),r):s},bT=(e,t,n,r)=>{let s=[];if(n.length>0)if(r.length>0){if(e.forEach(i=>s.push(i)),Math.max(...r)>e.length)throw new Error("axes is out of bound");r.forEach((i,o)=>s[i]=n[o])}else n.forEach(i=>s.push(i));else{if(t.length===0)throw new Error("Resize requires either scales or sizes.");s=e.map((i,o)=>Math.round(i*t[o]))}return s},wT=(e,t,n)=>{let r=(()=>{switch(n.keepAspectRatioPolicy){case"not_larger":return n.axes.length>0?Math.min(...n.axes.map(i=>t[i]),Number.MAX_VALUE):Math.min(...t,Number.MAX_VALUE);case"not_smaller":return n.axes.length>0?Math.max(...n.axes.map(i=>t[i]),Number.MIN_VALUE):Math.max(...t,Number.MIN_VALUE);default:throw new Error(`Keep aspect ratio policy ${n.keepAspectRatioPolicy} is not supported`)}})();t.fill(1,0,t.length);let s=e.slice();return n.axes.length>0?(n.axes.forEach(i=>t[i]=r),n.axes.forEach(i=>s[i]=Math.round(e[i]*t[i]))):(t.fill(r,0,t.length),s.forEach((i,o)=>s[o]=Math.round(i*t[o]))),s},vT=(e,t,n,r,s)=>`
    fn calculateOriginalIndicesFromOutputIndices(output_indices: ${e.type.indices}) -> array<${e.type.value}, ${n.length}> {
      var original_indices: array<${e.type.value}, ${n.length}>;
      for (var i:u32 = 0; i < ${n.length}; i++) {
        var output_index = ${e.indicesGet("output_indices","i")};
        var scale = ${mt("uniforms.scales","i",r)};
        var roi_low = ${mt("uniforms.roi","i",s)};
        var roi_hi = ${mt("uniforms.roi",`i + ${t.length}`,s)};
        if (scale == 1.0) {
          original_indices[i] = ${e.type.value}(output_index);
        } else {
          var input_shape_i = ${mt("uniforms.input_shape","i",t.length)};
          var output_shape_i = ${mt("uniforms.output_shape","i",n.length)};
          original_indices[i] = getOriginalCoordinateFromResizedCoordinate(output_index, scale, output_shape_i,
                                                                           input_shape_i, roi_low, roi_hi);
        }
      }
      return original_indices;
    }`,MT=(e,t,n,r,s,i,o)=>`
    fn calculateInputIndicesFromOutputIndices(output_indices: ${t.type.indices}) -> ${e.type.indices} {
      var input_indices: ${e.type.indices};
      for (var i:u32 = 0; i < ${r.length}; i++) {
        var output_index = ${t.indicesGet("output_indices","i")};
        var input_index: u32;
        var scale = ${mt("uniforms.scales","i",s)};
        if (scale == 1.0) {
          input_index = output_index;
        } else {
          var roi_low = ${mt("uniforms.roi","i",i)};
          var roi_hi = ${mt("uniforms.roi",`i + ${n.length}`,i)};
          var input_shape_i = ${mt("uniforms.input_shape","i",n.length)};
          var output_shape_i = ${mt("uniforms.output_shape","i",r.length)};
          var original_idx = getOriginalCoordinateFromResizedCoordinate(output_index, scale, output_shape_i,
                                                                        input_shape_i, roi_low, roi_hi);
          if (!${o} || (original_idx >= 0 && original_idx < ${t.type.value}(input_shape_i))) {
            if (original_idx < 0) {
              input_index = 0;
            } else if (original_idx > ${t.type.value}(input_shape_i - 1)) {
              input_index = input_shape_i - 1;
            } else {
              input_index = u32(getNearestPixelFromOriginal(original_idx, scale < 1));
            }
          } else {
            input_index = u32(original_idx);
          }
        }
        ${e.indicesSet("input_indices","i","input_index")}
      }
      return input_indices;
    }`,AT=(e,t)=>`
    fn checkInputIndices(input_indices: ${e.type.indices}) -> bool {
      for (var i:u32 = 0; i < ${t.length}; i++) {
        var input_index = ${e.indicesGet("input_indices","i")};
        if (input_index < 0 || input_index >= ${mt("uniforms.input_shape","i",t.length)}) {
          return false;
        }
      }
      return true;
    }`,Yh=(e,t,n,r)=>e.rank>r?`
    ${e.indicesSet("input_indices",t,"channel")};
    ${e.indicesSet("input_indices",n,"batch")};
`:"",TT=(e,t,n,r,s)=>{let[i,o,a,l]=n.length===2?[-1,0,1,-1]:[0,2,3,1],c=e.type.value;return`
    fn getInputValue(batch: u32, channel: u32, row: u32, col: u32) -> ${c} {
      var input_indices: ${e.type.indices};
      ${e.indicesSet("input_indices",o,`max(0, min(row, ${n[o]} - 1))`)};
      ${e.indicesSet("input_indices",a,`max(0, min(col, ${n[a]} - 1))`)};
      ${Yh(e,l,i,2)}
      return ${e.getByIndices("input_indices")};
    }

    fn bilinearInterpolation(output_indices: ${t.type.indices}) -> ${c} {
      var originalIndices = calculateOriginalIndicesFromOutputIndices(output_indices);
      var row:${c} = originalIndices[${o}];
      var col:${c} = originalIndices[${a}];
      ${r?`if (row < 0 || row > (${n[o]} - 1) || col < 0 || col > (${n[a]} - 1)) {
        return ${s};
      }`:""};
      row = max(0, min(row, ${n[o]} - 1));
      col = max(0, min(col, ${n[a]} - 1));
      var row1: u32 = u32(row);
      var col1: u32 = u32(col);
      var row2: u32 = u32(row + 1);
      var col2: u32 = u32(col + 1);
      var channel: u32 = ${n.length>2?`u32(originalIndices[${l}])`:"0"};
      var batch: u32 =  ${n.length>2?`u32(originalIndices[${i}])`:"0"};
      var x11: ${c} = getInputValue(batch, channel, row1, col1);
      var x12: ${c} = getInputValue(batch, channel, row1, col2);
      var x21: ${c} = getInputValue(batch, channel, row2, col1);
      var x22: ${c} = getInputValue(batch, channel, row2, col2);
      var dx1: ${c} = abs(row - ${c}(row1));
      var dx2: ${c} = abs(${c}(row2) - row);
      var dy1: ${c} = abs(col - ${c}(col1));
      var dy2: ${c} = abs(${c}(col2) - col);
      if (row1 == row2) {
        dx1 = 0.5;
        dx2 = 0.5;
      }
      if (col1 == col2) {
        dy1 = 0.5;
        dy2 = 0.5;
      }
      return (x11 * dx2 * dy2 + x12 * dx2 * dy1 + x21 * dx1 * dy2 + x22 * dx1 * dy1);
    }`},PT=(e,t,n,r,s,i,o,a,l,c)=>{let d=n.length===2,[u,p]=d?[0,1]:[2,3],h=e.type.value,m=g=>{let I=g===u?"row":"col";return`
      fn ${I}CubicInterpolation(input_indices: ${e.type.indices}, output_indices: ${t.type.indices}) -> ${h} {
        var output_index = ${t.indicesGet("output_indices",g)};
        var originalIdx: ${h} = getOriginalCoordinateFromResizedCoordinate(output_index, ${s[g]},
        ${r[g]}, ${n[g]}, ${i[g]}, ${i[g]} + ${n.length});
        var fractOriginalIdx: ${h} = originalIdx - floor(originalIdx);
        var coefs = getCubicInterpolationCoefs(fractOriginalIdx);

        if (${a} && (originalIdx < 0 || originalIdx > (${n[g]} - 1))) {
          return ${l};
        }
        var data: array<${h}, 4> = array<${h}, 4>(0.0, 0.0, 0.0, 0.0);
        for (var i: i32 = -1; i < 3; i++) {
          var ${I}: ${h} = originalIdx + ${h}(i);
          if (${I} < 0 || ${I} >= ${n[g]}) {
            ${c?`coefs[i + 1] = 0.0;
                        continue;`:a?`return ${l};`:`${I} = max(0, min(${I}, ${n[g]} - 1));`};
          }
        var input_indices_copy: ${e.type.indices} = input_indices;
          ${e.indicesSet("input_indices_copy",g,`u32(${I})`)};
          data[i + 1] = ${g===u?e.getByIndices("input_indices_copy"):"rowCubicInterpolation(input_indices_copy, output_indices)"};
        }
        return cubicInterpolation1D(data, coefs);
      }`};return`
    ${m(u)};
    ${m(p)};
  fn getCubicInterpolationCoefs(s: ${h}) -> array<${h}, 4> {
    var absS = abs(s);
    var coeffs: array<${h}, 4> = array<${h}, 4>(0.0, 0.0, 0.0, 0.0);
    var oneMinusAbsS: ${h} = 1.0 - absS;
    var twoMinusAbsS: ${h} = 2.0 - absS;
    var onePlusAbsS: ${h} = 1.0 + absS;
    coeffs[0] = ((${o} * onePlusAbsS - 5 * ${o}) * onePlusAbsS + 8 * ${o}) * onePlusAbsS - 4 * ${o};
    coeffs[1] = ((${o} + 2) * absS - (${o} + 3)) * absS * absS + 1;
    coeffs[2] = ((${o} + 2) * oneMinusAbsS - (${o} + 3)) * oneMinusAbsS * oneMinusAbsS + 1;
    coeffs[3] = ((${o} * twoMinusAbsS - 5 * ${o}) * twoMinusAbsS + 8 * ${o}) * twoMinusAbsS - 4 * ${o};
    return coeffs;
  }

  fn cubicInterpolation1D(x: array<${h}, 4>, coefs: array<${h}, 4>) -> ${h} {
    var coefsSum: ${h} = coefs[0] + coefs[1] + coefs[2] + coefs[3];
    return (x[0] * coefs[0] + x[1] * coefs[1]+ x[2] * coefs[2]+ x[3] * coefs[3]) / coefsSum;
  }

  fn bicubicInterpolation(output_indices: ${t.type.indices}) -> ${h} {
    var input_indices: ${e.type.indices} = output_indices;
    return colCubicInterpolation(input_indices, output_indices);
  }
    `},CT=(e,t,n,r,s)=>{let[i,o,a,l,c]=n.length===3?[-1,0,1,2,-1]:[0,2,3,4,1],d=e.type.value;return`
    fn getInputValue(batch: u32, channel: u32, depth:u32, height: u32, width: u32) -> ${d} {
      var input_indices: ${e.type.indices};
      ${e.indicesSet("input_indices",o,`max(0, min(depth, ${n[o]} - 1))`)};
      ${e.indicesSet("input_indices",a,`max(0, min(height, ${n[a]} - 1))`)};
      ${e.indicesSet("input_indices",l,`max(0, min(width, ${n[l]} - 1))`)};
      ${Yh(e,c,i,3)}
      return ${e.getByIndices("input_indices")};
    }

    fn trilinearInterpolation(output_indices: ${t.type.indices}) -> ${d} {
      var originalIndices = calculateOriginalIndicesFromOutputIndices(output_indices);
      var depth:${d} = originalIndices[${o}];
      var height:${d} = originalIndices[${a}];
      var width:${d} = originalIndices[${l}];
      ${r?`if (depth < 0 || depth > (${n[o]} - 1) || height < 0 || height > (${n[a]} - 1) || width < 0 || (width > ${n[l]} - 1)) {
      return ${s};
        }`:""};

    depth = max(0, min(depth, ${n[o]} - 1));
      height = max(0, min(height, ${n[a]} - 1));
      width = max(0, min(width, ${n[l]} - 1));
      var depth1: u32 = u32(depth);
      var height1: u32 = u32(height);
      var width1: u32 = u32(width);
      var depth2: u32 = u32(depth + 1);
      var height2: u32 = u32(height + 1);
      var width2: u32 = u32(width + 1);
      var channel: u32 = ${n.length>3?`u32(originalIndices[${c}])`:"0"};
      var batch: u32 =  ${n.length>3?`u32(originalIndices[${i}])`:"0"};

      var x111: ${d} = getInputValue(batch, channel, depth1, height1, width1);
      var x112: ${d} = getInputValue(batch, channel, depth1, height1, width2);
      var x121: ${d} = getInputValue(batch, channel, depth1, height2, width1);
      var x122: ${d} = getInputValue(batch, channel, depth1, height2, width2);
      var x211: ${d} = getInputValue(batch, channel, depth2, height1, width1);
      var x212: ${d} = getInputValue(batch, channel, depth2, height1, width2);
      var x221: ${d} = getInputValue(batch, channel, depth2, height2, width1);
      var x222: ${d} = getInputValue(batch, channel, depth2, height2, width2);
      var dx1: ${d} = abs(depth - ${d}(depth1));
      var dx2: ${d} = abs(${d}(depth2) - depth);
      var dy1: ${d} = abs(height - ${d}(height1));
      var dy2: ${d} = abs(${d}(height2) - height);
      var dz1: ${d} = abs(width - ${d}(width1));
      var dz2: ${d} = abs(${d}(width2) - width);
      if (depth1 == depth2) {
        dx1 = 0.5;
        dx2 = 0.5;
      }
      if (height1 == height2) {
        dy1 = 0.5;
        dy2 = 0.5;
      }
      if (width1 == width2) {
        dz1 = 0.5;
        dz2 = 0.5;
      }
      return (x111 * dx2 * dy2 * dz2 + x112 * dx2 * dy2 * dz1 + x121 * dx2 * dy1 *dz2 + x122 * dx2 * dy1 * dz1 +
              x211 * dx1 * dy2 * dz2 + x212 * dx1 * dy2 * dz1 + x221 * dx1 * dy1 *dz2 + x222 * dx1 * dy1 * dz1);
    }`},IT=(e,t,n,r,s,i)=>{let o=e.dims,a=xT(i,t.axes,o.length),l=bT(o,r,s,t.axes),c=r.slice();r.length===0&&(c=o.map((_,T)=>_===0?1:l[T]/_),t.keepAspectRatioPolicy!=="stretch"&&(l=wT(o,c,t)));let d=ht("output",e.dataType,l.length),u=Re("input",e.dataType,o.length),p=Ae.size(l),h=o.length===l.length&&o.every((_,T)=>_===l[T]),m=t.coordinateTransformMode==="tf_crop_and_resize",g=t.extrapolationValue,I=u.type.value,f=_=>`
      ${h?"":`
      ${_T(t.coordinateTransformMode,I)};
      ${(()=>{switch(t.mode){case"nearest":return`
              ${AT(u,o)};
              ${yT(t.nearestMode,n,I)};
              ${MT(u,d,o,l,c.length,a.length,m)};
              `;case"linear":return`
              ${vT(d,o,l,c.length,a.length)};
              ${(()=>{if(o.length===2||o.length===4)return`${TT(u,d,o,m,g)}`;if(o.length===3||o.length===5)return`${CT(u,d,o,m,g)}`;throw Error("Linear mode only supports input dims 2, 3, 4 and 5 are supported in linear mode.")})()};
            `;case"cubic":return`
            ${(()=>{if(o.length===2||o.length===4)return`${PT(u,d,o,l,c,a,t.cubicCoeffA,m,t.extrapolationValue,t.excludeOutside)}`;throw Error("Cubic mode only supports input dims 2 and 4 are supported in linear mode.")})()};
            `;default:throw Error("Invalid resize mode")}})()};
      `}
      ${_.registerUniform("output_size","u32").registerUniform("scales","f32",c.length).registerUniform("roi","f32",a.length).declareVariables(u,d)}
      ${_.mainStart()}
        ${_.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
        ${h?"output[global_idx] = input[global_idx];":`
        let output_indices = ${d.offsetToIndices("global_idx")};
        var input_indices: ${u.type.indices};
        ${(()=>{switch(t.mode){case"nearest":return`input_indices = calculateInputIndicesFromOutputIndices(output_indices);
                if (checkInputIndices(input_indices)) {
                  output[global_idx] = ${u.getByIndices("input_indices")};
                } else {
                  output[global_idx] = ${t.extrapolationValue};
                }`;case"linear":return`output[global_idx] = ${o.length===2||o.length===4?"bilinearInterpolation":"trilinearInterpolation"}(output_indices);`;case"cubic":return"output[global_idx] = bicubicInterpolation(output_indices);";default:throw Error(`Unsupported resize mode: ${t.mode}`)}})()};
`}
      }`;return{name:"Resize",shaderCache:{hint:`${t.cacheKey}|${n}|${c.length>0?t.mode==="cubic"?c:c.length:""}|${s.length>0?s:""}|${a.length>0?a:""}|${h}|${t.mode==="nearest"?o.length:o}`,inputDependencies:["rank"]},getShaderSource:f,getRunData:()=>({outputs:[{dims:l,dataType:e.dataType}],dispatchGroup:{x:Math.ceil(p/64)},programUniforms:[{type:12,data:p},{type:1,data:c},{type:1,data:a},...ft(o,l)]})}},kT=e=>{let t=e.customDataBuffer;return new Uint32Array(t,t.byteOffset,1)[0]},PS=(e,t)=>{let n=[],r=[],s=[],i=kT(e);if(t.antialias!==0)throw Error("Only default value (0) for Antialias attribute is supported");gT(e.inputs,t,i,n,r,s),e.compute(IT(e.inputs[0],t,i,n,r,s),{inputs:[0]})},CS=e=>{let t=e.antialias,n=e.axes,r=e.coordinateTransformMode,s=e.cubicCoeffA,i=e.excludeOutside!==0,o=e.extrapolationValue,a=e.keepAspectRatioPolicy,l=e.mode,c=e.nearestMode===""?"simple":e.nearestMode;return Xt({antialias:t,axes:n,coordinateTransformMode:r,cubicCoeffA:s,excludeOutside:i,extrapolationValue:o,keepAspectRatioPolicy:a,mode:l,nearestMode:c})}}),ET,ST,IS,hV=He(()=>{Mt(),Ft(),Ot(),ET=e=>{if(!e||e.length<3)throw new Error("layerNorm requires at least 3 inputs.");let t=e[0],n=e[1],r=e[2];if(t.dataType!==n.dataType||t.dataType!==r.dataType)throw new Error("All inputs must have the same data type");if(t.dims.length!==3&&t.dims.length!==2)throw new Error("Input must be 2D or 3D");if(n.dims.length!==3&&n.dims.length!==2)throw new Error("Skip must be 2D or 3D");let s=t.dims[t.dims.length-1],i=t.dims[t.dims.length-2];if(n.dims[n.dims.length-1]!==s)throw new Error("Skip must have the same hidden size as input");if(n.dims[n.dims.length-2]!==i)throw new Error("Skip must have the same sequence length as input");if(r.dims.length!==1)throw new Error("Gamma must be 1D");if(r.dims[r.dims.length-1]!==s)throw new Error("Gamma must have the same hidden size as input");if(e.length>3){let o=e[3];if(o.dims.length!==1)throw new Error("Beta must be 1D");if(o.dims[o.dims.length-1]!==s)throw new Error("Beta must have the same hidden size as input")}if(e.length>4){let o=e[4];if(o.dims.length!==1)throw new Error("Bias must be 1D");if(o.dims[o.dims.length-1]!==s)throw new Error("Bias must have the same hidden size as input")}},ST=(e,t,n,r)=>{let s=t.simplified,i=e[0].dims,o=Ae.size(i),a=i,l=o,c=i.slice(-1)[0],d=r?i.slice(0,-1).concat(1):[],u=!s&&e.length>3,p=e.length>4,h=r&&n>1,m=r&&n>2,g=n>3,I=64,f=An(c),_=[{type:12,data:l},{type:12,data:f},{type:12,data:c},{type:1,data:t.epsilon}],T=v=>{let b=[{name:"output_size",type:"u32"},{name:"components",type:"u32"},{name:"hidden_size",type:"u32"},{name:"epsilon",type:"f32"}],A=[Re("x",e[0].dataType,e[0].dims,f),Re("skip",e[1].dataType,e[1].dims,f),Re("gamma",e[2].dataType,e[2].dims,f)];u&&A.push(Re("beta",e[3].dataType,e[3].dims,f)),p&&A.push(Re("bias",e[4].dataType,e[4].dims,f)),A.push(ht("output",e[0].dataType,a,f)),h&&A.push(ht("mean_output",1,d)),m&&A.push(ht("inv_std_output",1,d)),g&&A.push(ht("input_skip_bias_sum",e[0].dataType,a,f));let k=Jn(e[0].dataType),F=Jn(1,f);return`

      ${v.registerUniforms(b).declareVariables(...A)}
      var<workgroup> sum_shared : array<${F}, ${I}>;
      var<workgroup> sum_squared_shared : array<${F}, ${I}>;

      ${v.mainStart([I,1,1])}
        let ix = local_id.x;
        let iy = global_id.x / ${I};

        let hidden_size_vectorized: u32 = uniforms.hidden_size / uniforms.components;
        var stride = hidden_size_vectorized / ${I};
        let offset = ix * stride + iy * hidden_size_vectorized;
        let offset1d = stride * ix;
        if (ix == ${I-1}) {
          stride = hidden_size_vectorized - stride * ix;
        }
        for (var i: u32 = 0; i < stride; i++) {
          let skip_value = skip[offset + i];
          let bias_value = ${p?"bias[offset1d + i]":k+"(0.0)"};
          let input_value = x[offset + i];
          let value = input_value + skip_value + bias_value;
          ${g?"input_skip_bias_sum[offset + i] = value;":""}
          output[offset + i] = value;
          let f32_value = ${$o(k,f,"value")};
          sum_shared[ix] += f32_value;
          sum_squared_shared[ix] += f32_value * f32_value;
        }
        workgroupBarrier();

        var reduce_size : u32 = ${I};
        for (var curr_size = reduce_size >> 1;  curr_size > 0; curr_size = reduce_size >> 1) {
          reduce_size = curr_size + (reduce_size & 1);
          if (ix < curr_size) {
            sum_shared[ix] += sum_shared[ix + reduce_size];
            sum_squared_shared[ix] += sum_squared_shared[ix + reduce_size];
          }
          workgroupBarrier();
        }

        let sum = sum_shared[0];
        let square_sum = sum_squared_shared[0];
        let mean = ${ui("sum",f)} / f32(uniforms.hidden_size);
        let inv_std_dev = inverseSqrt(${ui("square_sum",f)} / f32(uniforms.hidden_size) ${s?"":"- mean * mean"} + uniforms.epsilon);
        ${h?"mean_output[global_idx] = mean;":""}
        ${m?"inv_std_output[global_idx] = inv_std_dev;":""}

        for (var i: u32 = 0; i < stride; i++) {
          output[offset + i] = (output[offset + i] ${s?"":`- ${k}(mean)`}) *
            ${k}(inv_std_dev) * gamma[offset1d + i]
            ${u?"+ beta[offset1d + i]":""};
        }
      }`},M=[{dims:a,dataType:e[0].dataType}];return n>1&&M.push({dims:d,dataType:1}),n>2&&M.push({dims:d,dataType:1}),n>3&&M.push({dims:i,dataType:e[0].dataType}),{name:"SkipLayerNormalization",shaderCache:{hint:`${f};${h};${m};${g}`,inputDependencies:e.map((v,b)=>"type")},getShaderSource:T,getRunData:()=>({outputs:M,dispatchGroup:{x:Math.ceil(l/c)},programUniforms:_})}},IS=(e,t)=>{ET(e.inputs);let n=[0];e.outputCount>1&&n.push(-3),e.outputCount>2&&n.push(-3),e.outputCount>3&&n.push(3),e.compute(ST(e.inputs,t,e.outputCount,!1),{outputs:n})}}),LT,Oa,DT,Jh,$T,FT,kS,ES,mV=He(()=>{Mt(),Ft(),En(),Ot(),LT=(e,t)=>{if(!e||e.length<1)throw new Error("too few inputs");if(t.axes.length!==0){if(t.axes.length!==t.starts.length||t.axes.length!==t.ends.length)throw new Error("axes, starts and ends must have the same length")}else if(t.starts.length!==t.ends.length)throw new Error("starts and ends must have the same length");e.slice(1).forEach((n,r)=>{if(e[r+1].dataType!==6&&e[r+1].dataType!==7)throw new Error(`Input ${r} must be an array of int32 or int64`)})},Oa=(e,t)=>{let n=[];if(e.length>t)if(e[t].dataType===7)e[t].getBigInt64Array().forEach(r=>n.push(Number(r)));else if(e[t].dataType===6)e[t].getInt32Array().forEach(r=>n.push(Number(r)));else throw new Error(`Input ${t} must be an array of int32 or int64`);return n},DT=(e,t)=>{if(e.length>1){let n=Oa(e,1),r=Oa(e,2),s=Oa(e,3);return s.length===0&&(s=[...Array(e[0].dims.length).keys()]),Xt({starts:n,ends:r,axes:s})}else return t},Jh=(e,t,n,r,s)=>{let i=e;return e<0&&(i+=n[r[t]]),s[t]<0?Math.max(0,Math.min(i,n[r[t]]-1)):Math.max(0,Math.min(i,n[r[t]]))},$T=(e,t,n)=>`fn calculateInputIndices(output_indices: ${t.type.indices}) -> ${e.type.indices} {
          var input_indices: ${e.type.indices};
          var carry = 0u;
          for (var i = ${n.length}; i >= 0; i--) {
            let input_shape_i = ${mt("uniforms.input_shape","i",n.length)};
            let steps_i = ${mt("uniforms.steps","i",n.length)};
            let signs_i = ${mt("uniforms.signs","i",n.length)};
            let starts_i = ${mt("uniforms.starts","i",n.length)};
            var output_index = ${t.indicesGet("output_indices","i")};
            var input_index = output_index * steps_i + starts_i + carry;
            carry = input_index / input_shape_i;
            input_index = input_index % input_shape_i;
            if (signs_i < 0) {
              input_index = input_shape_i - input_index - 1u + starts_i;
            }
            ${e.indicesSet("input_indices","i","input_index")};
          }
          return input_indices;
      }`,FT=(e,t)=>{let n=e[0].dims,r=Ae.size(n),s=t.axes.length>0?Ae.normalizeAxes(t.axes,n.length):[...Array(n.length).keys()],i=Oa(e,4);i.forEach(f=>f!==0||(()=>{throw new Error("step cannot be 0")})),i.length===0&&(i=Array(s.length).fill(1));let o=t.starts.map((f,_)=>Jh(f,_,n,s,i)),a=t.ends.map((f,_)=>Jh(f,_,n,s,i));if(s.length!==o.length||s.length!==a.length)throw new Error("start, ends and axes should have the same number of elements");if(s.length!==n.length)for(let f=0;f<n.length;++f)s.includes(f)||(o.splice(f,0,0),a.splice(f,0,n[f]),i.splice(f,0,1));let l=i.map(f=>Math.sign(f));i.forEach((f,_,T)=>{if(f<0){let M=(a[_]-o[_])/f,v=o[_],b=v+M*i[_];o[_]=b,a[_]=v,T[_]=-f}});let c=n.slice(0);s.forEach((f,_)=>{c[f]=Math.ceil((a[f]-o[f])/i[f])});let d={dims:c,dataType:e[0].dataType},u=ht("output",e[0].dataType,c.length),p=Re("input",e[0].dataType,e[0].dims.length),h=Ae.size(c),m=[{name:"outputSize",type:"u32"},{name:"starts",type:"u32",length:o.length},{name:"signs",type:"i32",length:l.length},{name:"steps",type:"u32",length:i.length}],g=[{type:12,data:h},{type:12,data:o},{type:6,data:l},{type:12,data:i},...ft(e[0].dims,c)],I=f=>`
      ${f.registerUniforms(m).declareVariables(p,u)}
        ${$T(p,u,n)}
        ${f.mainStart()}
          ${f.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.outputSize")}
          let output_indices = ${u.offsetToIndices("global_idx")};
          let input_indices = calculateInputIndices(output_indices);
          ${u.setByOffset("global_idx",p.getByIndices("input_indices"))}
      }`;return{name:"Slice",shaderCache:{hint:`${l.length}_${o.length}_${i.length}`,inputDependencies:["rank"]},getShaderSource:I,getRunData:()=>({outputs:[d],dispatchGroup:{x:Math.ceil(r/64)},programUniforms:g})}},kS=(e,t)=>{LT(e.inputs,t);let n=DT(e.inputs,t);e.compute(FT(e.inputs,n),{inputs:[0]})},ES=e=>{let t=e.starts,n=e.ends,r=e.axes;return Xt({starts:t,ends:n,axes:r})}}),OT,jT,SS,LS,fV=He(()=>{Mt(),Ft(),En(),hi(),Ot(),OT=e=>{if(!e||e.length!==1)throw new Error("Softmax op requires 1 input.")},jT=(e,t)=>{let n=e.inputs[0],r=n.dims,s=Ae.size(r),i=r.length,o=Ae.normalizeAxis(t.axis,i),a=o<r.length-1,l,c=[];a?(c=Array.from({length:i},(A,k)=>k),c[o]=i-1,c[i-1]=o,l=e.compute(Sr(n,c),{inputs:[n],outputs:[-1]})[0]):l=n;let d=l.dims,u=d[i-1],p=s/u,h=An(u),m=u/h,g=64;p===1&&(g=256);let I=(A,k)=>k===4?`max(max(${A}.x, ${A}.y), max(${A}.z, ${A}.w))`:k===2?`max(${A}.x, ${A}.y)`:k===3?`max(max(${A}.x, ${A}.y), ${A}.z)`:A,f=Re("x",l.dataType,l.dims,h),_=ht("result",l.dataType,l.dims,h),T=f.type.value,M=Jn(l.dataType)==="f32"?`var threadMax = ${T}(-3.402823e+38f);`:`var threadMax = ${T}(-65504.0h);`,v=A=>`
      var<workgroup> rowMaxShared : ${T};
      var<workgroup> rowSumShared : ${T};
      var<workgroup> threadShared : array<${T}, ${g}>;

      fn getValue(row: i32, col: i32, row_stride: i32) -> ${T} {
        let index = row * row_stride + col;
        return x[index];
      }

      fn setValue(row: i32, col: i32, row_stride: i32, value: ${T}) {
        let index = row * row_stride + col;
        result[index] = value;
      }
      ${A.registerUniform("packedCols","i32").declareVariables(f,_)}
      ${A.mainStart(g)}
        let gindex = i32(global_idx);
        let lindex = i32(local_idx);
        const wg = ${g};
        let row = gindex / wg;
        let cols = uniforms.packedCols;
        let row_stride : i32 = uniforms.packedCols;

        // find the rows max
        ${M}
        for (var col = lindex; col < cols; col += wg) {
          let value = getValue(row, col, row_stride);
          threadMax = max(threadMax, value);
        }
        if (lindex < cols) {
          threadShared[lindex] = threadMax;
        }
        workgroupBarrier();

        var reduceSize = min(cols, wg);
        for (var currSize = reduceSize >> 1;  currSize > 0; currSize = reduceSize >> 1) {
          reduceSize = currSize + (reduceSize & 1);
          if (lindex < currSize) {
            threadShared[lindex] = max(threadShared[lindex], threadShared[lindex + reduceSize]);
          }
          workgroupBarrier();
        }
        if (lindex == 0) {
          rowMaxShared = ${T}(${I("threadShared[0]",h)});
        }
        workgroupBarrier();

        // find the rows sum
        var threadSum = ${T}(0.0);
        for (var col = lindex; col < cols; col += wg) {
          let subExp = exp(getValue(row, col, row_stride) - rowMaxShared);
          threadSum += subExp;
        }
        threadShared[lindex] = threadSum;
        workgroupBarrier();

        for (var currSize = wg >> 1;  currSize > 0; currSize = currSize >> 1) {
          if (lindex < currSize) {
            threadShared[lindex] = threadShared[lindex] + threadShared[lindex + currSize];
          }
          workgroupBarrier();
        }
        if (lindex == 0) {
          rowSumShared = ${T}(${ui("threadShared[0]",h)});
        }
        workgroupBarrier();

        // calculate final value for each element in the row
        for (var col = lindex; col < cols; col += wg) {
          let value = exp(getValue(row, col, row_stride) - rowMaxShared) / rowSumShared;
          setValue(row, col, row_stride, value);
        }
      }`,b=e.compute({name:"Softmax",shaderCache:{hint:`${h};${g}`,inputDependencies:["type"]},getRunData:()=>({outputs:[{dims:d,dataType:l.dataType}],dispatchGroup:{x:p},programUniforms:[{type:6,data:m}]}),getShaderSource:v},{inputs:[l],outputs:[a?-1:0]})[0];a&&e.compute(Sr(b,c),{inputs:[b]})},SS=(e,t)=>{OT(e.inputs),jT(e,t)},LS=e=>Xt({axis:e.axis})}),Zh,RT,NT,zT,DS,gV=He(()=>{Mt(),Ft(),Ot(),Zh=e=>Array.from(e.getBigInt64Array(),Number),RT=e=>{if(!e||e.length!==2)throw new Error("Tile requires 2 inputs.");if(e[0].dataType!==1&&e[0].dataType!==10&&e[0].dataType!==6&&e[0].dataType!==12)throw new Error("Tile only support float, float16, int32, and uint32 data types");if(e[1].dataType!==7)throw new Error("Tile `repeats` input should be of int64 data type");if(e[1].dims.length!==1)throw new Error("Tile `repeats` input should be 1-D");if(Zh(e[1]).length!==e[0].dims.length)throw new Error("Tile `repeats` input should have same number of elements as rank of input data tensor")},NT=(e,t)=>{let n=[];for(let r=0;r<e.length;++r)n.push(e[r]*t[r]);return n},zT=(e,t)=>{let n=e[0].dims,r=t??Zh(e[1]),s=NT(n,r),i=Ae.size(s),o=e[0].dataType,a=Re("input",o,n.length),l=ht("output",o,s.length),c=d=>`
      const inputShape = ${a.indices(...n)};
      ${d.registerUniform("output_size","u32").declareVariables(a,l)}
      ${d.mainStart()}
      ${d.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
      let output_indices = ${l.offsetToIndices("global_idx")};
      var input_indices: ${a.type.indices};
      for (var i = 0; i < ${n.length}; i++) {
        let input_dim_i = ${a.indicesGet("uniforms.input_shape","i")};
        let input_dim_value = ${l.indicesGet("output_indices","i")}  % input_dim_i;

        ${a.indicesSet("input_indices","i","input_dim_value")}
      }
      ${l.setByOffset("global_idx",a.getByIndices("input_indices"))}
    }`;return{name:"Tile",shaderCache:{hint:`${r}`,inputDependencies:["rank"]},getRunData:()=>({outputs:[{dims:s,dataType:e[0].dataType}],dispatchGroup:{x:Math.ceil(i/64)},programUniforms:[{type:12,data:i},...ft(e[0].dims,s)]}),getShaderSource:c}},DS=e=>{RT(e.inputs),e.compute(zT(e.inputs),{inputs:[0]})}}),BT,GT,$S,_V=He(()=>{Mt(),Ft(),Ot(),BT=(e,t,n,r,s)=>{let i=ht("output_data",s,n.length,4),o=Re("a_data",t[1].dataType,t[1].dims.length,4),a=Re("b_data",t[2].dataType,t[2].dims.length,4),l=Re("c_data",t[0].dataType,t[0].dims.length,4),c,d=(u,p,h)=>`select(${p}, ${u}, ${h})`;if(!r)c=i.setByOffset("global_idx",d(o.getByOffset("global_idx"),a.getByOffset("global_idx"),l.getByOffset("global_idx")));else{let u=(p,h,m="")=>{let g=`a_data[index_a${h}][component_a${h}]`,I=`b_data[index_b${h}][component_b${h}]`,f=`bool(c_data[index_c${h}] & (0xffu << (component_c${h} * 8)))`;return`
            let output_indices${h} = ${i.offsetToIndices(`global_idx * 4u + ${h}u`)};
            let offset_a${h} = ${o.broadcastedIndicesToOffset(`output_indices${h}`,i)};
            let offset_b${h} = ${a.broadcastedIndicesToOffset(`output_indices${h}`,i)};
            let offset_c${h} = ${l.broadcastedIndicesToOffset(`output_indices${h}`,i)};
            let index_a${h} = offset_a${h} / 4u;
            let index_b${h} = offset_b${h} / 4u;
            let index_c${h} = offset_c${h} / 4u;
            let component_a${h} = offset_a${h} % 4u;
            let component_b${h} = offset_b${h} % 4u;
            let component_c${h} = offset_c${h} % 4u;
            ${p}[${h}] = ${m}(${d(g,I,f)});
          `};s===9?c=`
            var data = vec4<u32>(0);
            ${u("data",0,"u32")}
            ${u("data",1,"u32")}
            ${u("data",2,"u32")}
            ${u("data",3,"u32")}
            output_data[global_idx] = dot(vec4<u32>(0x1, 0x100, 0x10000, 0x1000000), vec4<u32>(data));`:c=`
            ${u("output_data[global_idx]",0)}
            ${u("output_data[global_idx]",1)}
            ${u("output_data[global_idx]",2)}
            ${u("output_data[global_idx]",3)}
          `}return`
        ${e.registerUniform("vec_size","u32").declareVariables(l,o,a,i)}
        ${e.mainStart()}
        ${e.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.vec_size")}
        ${c}
      }`},GT=e=>{let t=e[1].dims,n=e[2].dims,r=e[0].dims,s=e[1].dataType,i=!(Ae.areEqual(t,n)&&Ae.areEqual(n,r)),o=t,a=Ae.size(t);if(i){let c=Vo.calcShape(Vo.calcShape(t,n,!1),r,!1);if(!c)throw new Error("Can't perform where op on the given tensors");o=c,a=Ae.size(o)}let l=Math.ceil(a/4);return{name:"Where",shaderCache:{inputDependencies:["rank","rank","rank"]},getShaderSource:c=>BT(c,e,o,i,s),getRunData:()=>({outputs:[{dims:o,dataType:s}],dispatchGroup:{x:Math.ceil(a/64/4)},programUniforms:[{type:12,data:l},...ft(r,t,n,o)]})}},$S=e=>{e.compute(GT(e.inputs))}}),FS,yV=He(()=>{LG(),Sg(),DG(),$G(),FG(),OG(),jG(),GG(),UG(),WG(),HG(),KG(),qG(),QG(),XG(),YG(),JG(),ZG(),eV(),tV(),nV(),rV(),sV(),iV(),oV(),eS(),aV(),lV(),cV(),uV(),dV(),Eg(),pV(),iS(),hV(),mV(),fV(),rS(),gV(),hi(),Lg(),_V(),FS=new Map([["Abs",[Ik]],["Acos",[kk]],["Acosh",[Ek]],["Add",[uE]],["ArgMax",[Ak,ef]],["ArgMin",[Mk,ef]],["Asin",[Sk]],["Asinh",[Lk]],["Atan",[Dk]],["Atanh",[$k]],["Attention",[Tk]],["AveragePool",[mS,hS]],["BatchNormalization",[Pk]],["BiasAdd",[Ck]],["BiasSplitGelu",[cE]],["Cast",[Ok,Fk]],["Ceil",[Rk]],["Clip",[jk]],["Concat",[bE,wE]],["Conv",[af,of]],["ConvTranspose",[SE,EE]],["Cos",[Nk]],["Cosh",[zk]],["CumSum",[LE,DE]],["DepthToSpace",[$E,FE]],["DequantizeLinear",[wS,vS]],["Div",[dE]],["Einsum",[OE,jE]],["Elu",[Bk,qa]],["Equal",[pE]],["Erf",[Gk]],["Exp",[Vk]],["Expand",[RE]],["FastGelu",[NE]],["Floor",[Uk]],["FusedConv",[af,of]],["Gather",[BE,zE]],["GatherElements",[KE,HE]],["GatherBlockQuantized",[UE,WE]],["GatherND",[GE,VE]],["Gelu",[Wk]],["Gemm",[QE,qE]],["GlobalAveragePool",[gS,fS]],["GlobalMaxPool",[bS,xS]],["Greater",[gE]],["GreaterOrEqual",[yE]],["GridSample",[XE,YE]],["GroupQueryAttention",[oS]],["HardSigmoid",[Zk,Jk]],["InstanceNormalization",[aS]],["LayerNormalization",[lS]],["LeakyRelu",[Hk,qa]],["Less",[_E]],["LessOrEqual",[xE]],["Log",[aE]],["MatMul",[cS]],["MatMulNBits",[uS,dS]],["MaxPool",[_S,yS]],["Mul",[hE]],["MultiHeadAttention",[ZE,JE]],["Neg",[qk]],["Not",[Kk]],["Pad",[pS]],["Pow",[mE]],["QuickGelu",[lE,qa]],["Range",[MS]],["Reciprocal",[Qk]],["ReduceMin",[yk]],["ReduceMean",[hk]],["ReduceMax",[_k]],["ReduceSum",[bk]],["ReduceProd",[xk]],["ReduceL1",[mk]],["ReduceL2",[fk]],["ReduceLogSum",[vk]],["ReduceLogSumExp",[gk]],["ReduceSumSquare",[wk]],["Relu",[Xk]],["Resize",[PS,CS]],["RotaryEmbedding",[sS]],["ScatterND",[TS,AS]],["Sigmoid",[Yk]],["Sin",[eE]],["Sinh",[tE]],["Slice",[kS,ES]],["SkipLayerNormalization",[IS]],["Split",[tS,nS]],["Sqrt",[nE]],["Softmax",[SS,LS]],["Sub",[fE]],["Tan",[rE]],["Tanh",[sE]],["ThresholdedRelu",[oE,qa]],["Tile",[DS]],["Transpose",[tk,nk]],["Where",[$S]]])}),OS,xV=He(()=>{ds(),Ws(),Ot(),OS=class{constructor(e){this.backend=e,this.repo=new Map,this.attributesBound=!1}getArtifact(e){return this.repo.get(e)}setArtifact(e,t){this.repo.set(e,t)}run(e,t,n,r,s){us(e.programInfo.name);let i=this.backend.device,o=this.backend.getComputePassEncoder();this.backend.writeTimestamp(this.backend.pendingDispatchNumber*2);let a=[];for(let c of t)a.push({binding:a.length,resource:{buffer:c.buffer}});for(let c of n)a.push({binding:a.length,resource:{buffer:c.buffer}});s&&a.push({binding:a.length,resource:s});let l=i.createBindGroup({layout:e.computePipeline.getBindGroupLayout(0),entries:a,label:e.programInfo.name});if(this.backend.sessionStatus==="capturing"){let c={kernelId:this.backend.currentKernelId,computePipeline:e.computePipeline,bindGroup:l,dispatchGroup:r};this.backend.capturedCommandList.get(this.backend.currentSessionId).push(c)}o.setPipeline(e.computePipeline),o.setBindGroup(0,l),o.dispatchWorkgroups(...r),this.backend.writeTimestamp(this.backend.pendingDispatchNumber*2+1),this.backend.pendingDispatchNumber++,(this.backend.pendingDispatchNumber>=this.backend.maxDispatchNumber||this.backend.queryType==="at-passes")&&this.backend.endComputePass(),this.backend.pendingDispatchNumber>=this.backend.maxDispatchNumber&&this.backend.flush(),Kr(e.programInfo.name)}dispose(){}build(e,t){us(e.name);let n=this.backend.device,r=[];[{feature:"shader-f16",extension:"f16"},{feature:"subgroups",extension:"subgroups"}].forEach(c=>{n.features.has(c.feature)&&r.push(`enable ${c.extension};`)});let s=ek(t,this.backend.device.limits),i=e.getShaderSource(s),o=`${r.join(`
`)}
${s.additionalImplementations}
${i}`,a=n.createShaderModule({code:o,label:e.name});Ut("verbose",()=>`[WebGPU] ${e.name} shader code: ${o}`);let l=n.createComputePipeline({compute:{module:a,entryPoint:"main"},layout:"auto",label:e.name});return Kr(e.name),{programInfo:e,computePipeline:l,uniformVariablesInfo:s.variablesInfo}}normalizeDispatchGroupSize(e){let t=typeof e=="number"?e:e.x,n=typeof e=="number"?1:e.y||1,r=typeof e=="number"?1:e.z||1,s=this.backend.device.limits.maxComputeWorkgroupsPerDimension;if(t<=s&&n<=s&&r<=s)return[t,n,r];let i=t*n*r,o=Math.ceil(Math.sqrt(i));if(o>s){if(o=Math.ceil(Math.cbrt(i)),o>s)throw new Error("Total dispatch size exceeds WebGPU maximum.");return[o,o,o]}else return[o,o,1]}}}),jS={};Zo(jS,{WebGpuBackend:()=>RS});var VT,UT,WT,RS,bV=He(()=>{ds(),Mt(),Ws(),QI(),EG(),yV(),xV(),VT=(e,t)=>{if(t.length!==e.length)throw new Error(`inputDependencies length ${t.length} is not equal to inputTensors length ${e.length}.`);let n=[];for(let r=0;r<e.length;++r){let s=e[r].dataType;switch(t[r]){case"none":{n.push("");break}case"type":{n.push(`${s}`);break}case"rank":{let i=e[r].dims.length;n.push(`${s};${i}`);break}case"dims":{let i=e[r].dims.join(",");n.push(`${s};${i}`);break}default:throw new Error(`unsupported input dependency: ${t[r]}`)}}return n.join("|")},UT=(e,t,n)=>{let r=e.name;return e.shaderCache?.hint&&(r+="["+e.shaderCache.hint+"]"),r+=":"+n+`:${VT(t,e.shaderCache?.inputDependencies??new Array(t.length).fill("dims"))}`,r},WT=class{constructor(e){e&&(this.architecture=e.architecture,this.vendor=e.vendor)}isArchitecture(e){return this.architecture===e}isVendor(e){return this.vendor===e}},RS=class{constructor(){this.currentSessionId=null,this.currentKernelId=null,this.commandEncoder=null,this.computePassEncoder=null,this.maxDispatchNumber=16,this.pendingDispatchNumber=0,this.pendingKernels=[],this.pendingQueries=new Map,this.sessionStatus="default",this.capturedCommandList=new Map,this.capturedPendingKernels=new Map,this.sessionExternalDataMapping=new Map}get currentKernelCustomData(){if(this.currentKernelId===null)throw new Error("currentKernelCustomData(): currentKernelId is null. (should not happen)");let e=this.kernelCustomData.get(this.currentKernelId);return e||(e={},this.kernelCustomData.set(this.currentKernelId,e)),e}async initialize(e,t){this.env=e;let n=[],r={requiredLimits:{maxComputeWorkgroupStorageSize:t.limits.maxComputeWorkgroupStorageSize,maxComputeWorkgroupsPerDimension:t.limits.maxComputeWorkgroupsPerDimension,maxStorageBufferBindingSize:t.limits.maxStorageBufferBindingSize,maxBufferSize:t.limits.maxBufferSize,maxComputeInvocationsPerWorkgroup:t.limits.maxComputeInvocationsPerWorkgroup,maxComputeWorkgroupSizeX:t.limits.maxComputeWorkgroupSizeX,maxComputeWorkgroupSizeY:t.limits.maxComputeWorkgroupSizeY,maxComputeWorkgroupSizeZ:t.limits.maxComputeWorkgroupSizeZ},requiredFeatures:n},s=i=>t.features.has(i)&&n.push(i)&&!0;s("chromium-experimental-timestamp-query-inside-passes")||s("timestamp-query"),s("shader-f16"),s("subgroups"),this.device=await t.requestDevice(r),this.adapterInfo=new WT(t.info||await t.requestAdapterInfo()),this.gpuDataManager=JI(this),this.programManager=new OS(this),this.kernels=new Map,this.kernelPersistentData=new Map,this.kernelCustomData=new Map,Pg(e.logLevel,!!e.debug),this.device.onuncapturederror=i=>{i.error instanceof GPUValidationError&&console.error(`An uncaught WebGPU validation error was raised: ${i.error.message}`)},Object.defineProperty(this.env.webgpu,"device",{value:this.device,writable:!1,enumerable:!0,configurable:!1}),Object.defineProperty(this.env.webgpu,"adapter",{value:t,writable:!1,enumerable:!0,configurable:!1}),this.setQueryType()}dispose(){typeof this.querySet<"u"&&this.querySet.destroy(),this.gpuDataManager.dispose()}getCommandEncoder(){return this.commandEncoder||(this.commandEncoder=this.device.createCommandEncoder()),this.commandEncoder}getComputePassEncoder(){if(!this.computePassEncoder){let e=this.getCommandEncoder(),t={};this.queryType==="at-passes"&&(t.timestampWrites={querySet:this.querySet,beginningOfPassWriteIndex:this.pendingDispatchNumber*2,endOfPassWriteIndex:this.pendingDispatchNumber*2+1}),this.computePassEncoder=e.beginComputePass(t)}return this.computePassEncoder}endComputePass(){this.computePassEncoder&&(this.computePassEncoder.end(),this.computePassEncoder=null)}flush(){if(!this.commandEncoder)return;us(),this.endComputePass();let e;this.queryType!=="none"&&(this.commandEncoder.resolveQuerySet(this.querySet,0,this.pendingDispatchNumber*2,this.queryResolveBuffer,0),e=this.device.createBuffer({size:this.pendingDispatchNumber*2*8,usage:GPUBufferUsage.MAP_READ|GPUBufferUsage.COPY_DST}),this.pendingQueries.set(e,this.pendingKernels),this.pendingKernels=[],this.commandEncoder.copyBufferToBuffer(this.queryResolveBuffer,0,e,0,this.pendingDispatchNumber*2*8)),this.device.queue.submit([this.commandEncoder.finish()]),this.gpuDataManager.refreshPendingBuffers(),this.commandEncoder=null,this.pendingDispatchNumber=0,this.queryType!=="none"&&e.mapAsync(GPUMapMode.READ).then(()=>{let t=new BigUint64Array(e.getMappedRange()),n=this.pendingQueries.get(e);for(let r=0;r<t.length/2;r++){let s=n[r],i=s.kernelId,o=this.kernels.get(i),a=o.kernelType,l=o.kernelName,c=s.programName,d=s.inputTensorViews,u=s.outputTensorViews,p=t[r*2],h=t[r*2+1];typeof this.queryTimeBase>"u"&&(this.queryTimeBase=p);let m=Number(p-this.queryTimeBase),g=Number(h-this.queryTimeBase);if(!Number.isSafeInteger(m)||!Number.isSafeInteger(g))throw new RangeError("incorrect timestamp range");if(this.env.webgpu.profiling?.ondata)this.env.webgpu.profiling.ondata({version:1,inputsMetadata:d.map(I=>({dims:I.dims,dataType:zs(I.dataType)})),outputsMetadata:u.map(I=>({dims:I.dims,dataType:zs(I.dataType)})),kernelId:i,kernelType:a,kernelName:l,programName:c,startTime:m,endTime:g});else{let I="";d.forEach((_,T)=>{I+=`input[${T}]: [${_.dims}] | ${zs(_.dataType)}, `});let f="";u.forEach((_,T)=>{f+=`output[${T}]: [${_.dims}] | ${zs(_.dataType)}, `}),console.log(`[profiling] kernel "${i}|${a}|${l}|${c}" ${I}${f}execution time: ${g-m} ns`)}fl("GPU",`${c}::${p}::${h}`)}e.unmap(),this.pendingQueries.delete(e)}),Kr()}run(e,t,n,r,s,i){us(e.name);let o=[];for(let _=0;_<t.length;++_){let T=t[_].data;if(T===0)continue;let M=this.gpuDataManager.get(T);if(!M)throw new Error(`no GPU data for input: ${T}`);o.push(M)}let{outputs:a,dispatchGroup:l,programUniforms:c}=e.getRunData(t),d=n.length===0?a.map((_,T)=>T):n;if(d.length!==a.length)throw new Error(`Output size ${d.length} must be equal to ${a.length}.`);let u=[],p=[];for(let _=0;_<a.length;++_){if(!Number.isInteger(d[_])||d[_]<-3||d[_]>=i)throw new Error(`Invalid output index: ${d[_]}`);if(d[_]===-3)continue;let T=d[_]===-1,M=d[_]===-2,v=T||M?s(a[_].dataType,a[_].dims):r(d[_],a[_].dataType,a[_].dims);if(u.push(v),v.data===0)continue;let b=this.gpuDataManager.get(v.data);if(!b)throw new Error(`no GPU data for output: ${v.data}`);if(T&&this.temporaryData.push(b),M){let A=this.kernelPersistentData.get(this.currentKernelId);A||(A=[],this.kernelPersistentData.set(this.currentKernelId,A)),A.push(b)}p.push(b)}if(o.length!==t.length||p.length!==u.length){if(p.length===0)return Kr(e.name),u;throw new Error(`Program ${e.name} has zero-sized tensor(s) in inputs or outputs. This is not supported now.`)}let h;if(c){let _=0,T=[];c.forEach(A=>{let k=typeof A.data=="number"?[A.data]:A.data;if(k.length===0)return;let F=A.type===10?2:4,L,G;A.type===10?(G=k.length>4?16:k.length>2?8:k.length*F,L=k.length>4?16:F*k.length):(G=k.length<=2?k.length*F:16,L=16),_=Math.ceil(_/G)*G,T.push(_);let j=A.type===10?8:4;_+=k.length>4?Math.ceil(k.length/j)*L:k.length*F});let M=16;_=Math.ceil(_/M)*M;let v=new ArrayBuffer(_);c.forEach((A,k)=>{let F=T[k],L=typeof A.data=="number"?[A.data]:A.data;if(A.type===6)new Int32Array(v,F,L.length).set(L);else if(A.type===12)new Uint32Array(v,F,L.length).set(L);else if(A.type===10)new Uint16Array(v,F,L.length).set(L);else if(A.type===1)new Float32Array(v,F,L.length).set(L);else throw new Error(`Unsupported uniform type: ${zs(A.type)}`)});let b=this.gpuDataManager.create(_,GPUBufferUsage.COPY_DST|GPUBufferUsage.UNIFORM);this.device.queue.writeBuffer(b.buffer,0,v,0,_),this.gpuDataManager.release(b.id),h={offset:0,size:_,buffer:b.buffer}}let m=this.programManager.normalizeDispatchGroupSize(l),g=m[1]===1&&m[2]===1,I=UT(e,t,g),f=this.programManager.getArtifact(I);if(f||(f=this.programManager.build(e,m),this.programManager.setArtifact(I,f),Ut("info",()=>`[artifact] key: ${I}, programName: ${e.name}`)),c&&f.uniformVariablesInfo){if(c.length!==f.uniformVariablesInfo.length)throw new Error(`Uniform variables count mismatch: expect ${f.uniformVariablesInfo.length}, got ${c.length} in program "${f.programInfo.name}".`);for(let _=0;_<c.length;_++){let T=c[_],M=T.type,v=typeof T.data=="number"?1:T.data.length,[b,A]=f.uniformVariablesInfo[_];if(M!==b||v!==A)throw new Error(`Uniform variable ${_} mismatch: expect type ${b} with size ${A}, got type ${M} with size ${v} in program "${f.programInfo.name}".`)}}if(Ut("info",()=>`[ProgramManager] run "${e.name}" (key=${I}) with ${m[0]}x${m[1]}x${m[2]}`),this.queryType!=="none"||this.sessionStatus==="capturing"){let _={kernelId:this.currentKernelId,programName:f.programInfo.name,inputTensorViews:t,outputTensorViews:u};this.pendingKernels.push(_),this.sessionStatus==="capturing"&&this.capturedPendingKernels.get(this.currentSessionId).push(_)}return this.programManager.run(f,o,p,m,h),Kr(e.name),u}upload(e,t){this.gpuDataManager.upload(e,t)}memcpy(e,t){this.gpuDataManager.memcpy(e,t)}async download(e,t){await this.gpuDataManager.download(e,t)}alloc(e){return this.gpuDataManager.create(e).id}free(e){return this.gpuDataManager.release(e)}createKernel(e,t,n,r){let s=FS.get(e);if(!s)throw new Error(`kernel not implemented: ${e}`);let i={kernelType:e,kernelName:r,kernelEntry:s[0],attributes:[s[1],n]};this.kernels.set(t,i)}releaseKernel(e){let t=this.kernelPersistentData.get(e);if(t){for(let n of t)this.gpuDataManager.release(n.id);this.kernelPersistentData.delete(e)}this.kernelCustomData.delete(e),this.kernels.delete(e)}computeKernel(e,t,n){let r=this.kernels.get(e);if(!r)throw new Error(`kernel not created: ${e}`);let s=r.kernelType,i=r.kernelName,o=r.kernelEntry,a=r.attributes;if(this.currentKernelId!==null)throw new Error(`kernel "[${s}] ${i}" is not allowed to be called recursively`);this.currentKernelId=e,a[0]&&(a[1]=a[0](a[1]),a[0]=void 0),Ut("info",()=>`[WebGPU] Start to run kernel "[${s}] ${i}"...`);let l=this.env.debug;this.temporaryData=[];try{return l&&this.device.pushErrorScope("validation"),o(t,a[1]),0}catch(c){return n.push(Promise.resolve(`[WebGPU] Kernel "[${s}] ${i}" failed. ${c}`)),1}finally{l&&n.push(this.device.popErrorScope().then(c=>c?`GPU validation error for kernel "[${s}] ${i}": ${c.message}`:null));for(let c of this.temporaryData)this.gpuDataManager.release(c.id);this.temporaryData=[],this.currentKernelId=null}}registerBuffer(e,t,n,r){let s=this.sessionExternalDataMapping.get(e);s||(s=new Map,this.sessionExternalDataMapping.set(e,s));let i=s.get(t),o=this.gpuDataManager.registerExternalBuffer(n,r,i);return s.set(t,[o,n]),o}unregisterBuffers(e){let t=this.sessionExternalDataMapping.get(e);t&&(t.forEach(n=>this.gpuDataManager.unregisterExternalBuffer(n[0])),this.sessionExternalDataMapping.delete(e))}getBuffer(e){let t=this.gpuDataManager.get(e);if(!t)throw new Error(`no GPU data for buffer: ${e}`);return t.buffer}createDownloader(e,t,n){return async()=>{let r=await Ym(this,e,t);return Cg(r.buffer,n)}}writeTimestamp(e){this.queryType==="inside-passes"&&this.computePassEncoder.writeTimestamp(this.querySet,e)}setQueryType(){this.queryType="none",(this.env.webgpu.profiling?.mode==="default"||(typeof this.env.trace>"u"?this.env.wasm.trace:this.env.trace))&&(this.device.features.has("chromium-experimental-timestamp-query-inside-passes")?this.queryType="inside-passes":this.device.features.has("timestamp-query")&&(this.queryType="at-passes"),this.queryType!=="none"&&typeof this.querySet>"u"&&(this.querySet=this.device.createQuerySet({type:"timestamp",count:this.maxDispatchNumber*2}),this.queryResolveBuffer=this.device.createBuffer({size:this.maxDispatchNumber*2*8,usage:GPUBufferUsage.COPY_SRC|GPUBufferUsage.QUERY_RESOLVE})))}captureBegin(){Ut("info","captureBegin"),this.capturedCommandList.get(this.currentSessionId)||this.capturedCommandList.set(this.currentSessionId,[]),this.capturedPendingKernels.get(this.currentSessionId)||this.capturedPendingKernels.set(this.currentSessionId,[]),this.flush(),this.sessionStatus="capturing"}captureEnd(){Ut("info","captureEnd"),this.flush(),this.sessionStatus="default"}replay(){Ut("info","replay"),this.sessionStatus="replaying";let e=this.capturedCommandList.get(this.currentSessionId),t=this.capturedPendingKernels.get(this.currentSessionId),n=e.length;this.pendingKernels=[];for(let r=0;r<n;r++){let s=this.getComputePassEncoder(),i=e[r];this.writeTimestamp(this.pendingDispatchNumber*2),s.setPipeline(i.computePipeline),s.setBindGroup(0,i.bindGroup),s.dispatchWorkgroups(...i.dispatchGroup),this.writeTimestamp(this.pendingDispatchNumber*2+1),this.pendingDispatchNumber++,this.queryType!=="none"&&this.pendingKernels.push(t[r]),(this.pendingDispatchNumber>=this.maxDispatchNumber||this.queryType==="at-passes")&&this.endComputePass(),this.pendingDispatchNumber>=this.maxDispatchNumber&&this.flush()}this.flush(),this.sessionStatus="default"}onCreateSession(){this.gpuDataManager.onCreateSession()}onReleaseSession(e){this.unregisterBuffers(e),this.capturedCommandList.has(e)&&this.capturedCommandList.delete(e),this.capturedPendingKernels.has(e)&&this.capturedPendingKernels.delete(e),this.gpuDataManager.onReleaseSession(e)}onRunStart(e){this.currentSessionId=e,this.setQueryType()}}}),NS={};Zo(NS,{init:()=>zS});var Nc,HT,zS,wV=He(()=>{Mt(),Ws(),Ft(),kG(),Nc=class BS{constructor(t,n,r,s){this.module=t,this.dataType=n,this.data=r,this.dims=s}getFloat32Array(){if(this.dataType!==1)throw new Error("Invalid data type");let t=Ae.size(this.dims);return t===0?new Float32Array:new Float32Array(this.module.HEAP8.buffer,this.data,t)}getBigInt64Array(){if(this.dataType!==7)throw new Error("Invalid data type");let t=Ae.size(this.dims);return t===0?new BigInt64Array:new BigInt64Array(this.module.HEAP8.buffer,this.data,t)}getInt32Array(){if(this.dataType!==6)throw new Error("Invalid data type");let t=Ae.size(this.dims);return t===0?new Int32Array:new Int32Array(this.module.HEAP8.buffer,this.data,t)}getUint16Array(){if(this.dataType!==10&&this.dataType!==4)throw new Error("Invalid data type");let t=Ae.size(this.dims);return t===0?new Uint16Array:new Uint16Array(this.module.HEAP8.buffer,this.data,t)}reshape(t){if(Ae.size(t)!==Ae.size(this.dims))throw new Error("Invalid new shape");return new BS(this.module,this.dataType,this.data,t)}},HT=class{constructor(e,t,n){this.module=e,this.backend=t,this.customDataOffset=0,this.customDataSize=0,this.adapterInfo=t.adapterInfo;let r=e.PTR_SIZE,s=n/e.PTR_SIZE,i=r===4?"i32":"i64";this.opKernelContext=Number(e.getValue(r*s++,i));let o=Number(e.getValue(r*s++,i));this.outputCount=Number(e.getValue(r*s++,i)),this.customDataOffset=Number(e.getValue(r*s++,"*")),this.customDataSize=Number(e.getValue(r*s++,i));let a=[];for(let l=0;l<o;l++){let c=Number(e.getValue(r*s++,i)),d=Number(e.getValue(r*s++,"*")),u=Number(e.getValue(r*s++,i)),p=[];for(let h=0;h<u;h++)p.push(Number(e.getValue(r*s++,i)));a.push(new Nc(e,c,d,p))}this.inputs=a}get kernelCustomData(){return this.backend.currentKernelCustomData}get customDataBuffer(){return this.module.HEAPU8.subarray(this.customDataOffset,this.customDataOffset+this.customDataSize)}compute(e,t){let n=t?.inputs?.map(o=>typeof o=="number"?this.inputs[o]:o)??this.inputs,r=t?.outputs??[],s=(o,a,l)=>new Nc(this.module,a,this.output(o,l),l),i=(o,a)=>{let l=Fi(o,a);if(!l)throw new Error(`Unsupported data type: ${o}`);let c=l>0?this.backend.gpuDataManager.create(l).id:0;return new Nc(this.module,o,c,a)};return this.backend.run(e,n,r,s,i,this.outputCount)}output(e,t){let n=this.module.stackSave();try{let r=this.module.PTR_SIZE,s=r===4?"i32":"i64",i=this.module.stackAlloc((1+t.length)*r);this.module.setValue(i,t.length,s);for(let o=0;o<t.length;o++)this.module.setValue(i+r*(o+1),t[o],s);return this.module._JsepOutput(this.opKernelContext,e,i)}catch(r){throw new Error(`Failed to generate kernel's output[${e}] with dims [${t}]. If you are running with pre-allocated output, please make sure the output type/dims are correct. Error: ${r}`)}finally{this.module.stackRestore(n)}}},zS=async(e,t,n,r)=>{let s=t.jsepInit;if(!s)throw new Error("Failed to initialize JSEP. The WebAssembly module is not built with JSEP support.");if(e==="webgpu"){let i=(bV(),ml(jS)).WebGpuBackend,o=new i;await o.initialize(n,r),s("webgpu",[o,a=>o.alloc(Number(a)),a=>o.free(a),(a,l,c,d=!1)=>{if(d)Ut("verbose",()=>`[WebGPU] jsepCopyGpuToGpu: src=${Number(a)}, dst=${Number(l)}, size=${Number(c)}`),o.memcpy(Number(a),Number(l));else{Ut("verbose",()=>`[WebGPU] jsepCopyCpuToGpu: dataOffset=${Number(a)}, gpuDataId=${Number(l)}, size=${Number(c)}`);let u=t.HEAPU8.subarray(Number(a>>>0),Number(a>>>0)+Number(c));o.upload(Number(l),u)}},async(a,l,c)=>{Ut("verbose",()=>`[WebGPU] jsepCopyGpuToCpu: gpuDataId=${a}, dataOffset=${l}, size=${c}`),await o.download(Number(a),()=>t.HEAPU8.subarray(Number(l)>>>0,Number(l+c)>>>0))},(a,l,c)=>o.createKernel(a,Number(l),c,t.UTF8ToString(t._JsepGetNodeName(Number(l)))),a=>o.releaseKernel(a),(a,l,c,d)=>{Ut("verbose",()=>`[WebGPU] jsepRun: sessionHandle=${c}, kernel=${a}, contextDataOffset=${l}`);let u=new HT(t,o,Number(l));return o.computeKernel(Number(a),u,d)},()=>o.captureBegin(),()=>o.captureEnd(),()=>o.replay()])}else{let i=new YI(n);s("webnn",[i,()=>i.reserveTensorId(),o=>i.releaseTensorId(o),async(o,a,l,c,d)=>i.ensureTensor(o,a,l,c,d),(o,a)=>{i.uploadTensor(o,a)},async(o,a)=>i.downloadTensor(o,a)])}}}),KT,Rg,Ng,ri,qT,em,Au,zg,Bg,tm,Gg,Vg,Ug,GS=He(()=>{PG(),CG(),Mt(),Yi(),wg(),WI(),KT=(e,t)=>{sn()._OrtInit(e,t)!==0&&en("Can't initialize onnxruntime.")},Rg=async e=>{KT(e.wasm.numThreads,xu(e.logLevel))},Ng=async(e,t)=>{sn().asyncInit?.();{let n=(wV(),ml(NS)).init;if(t==="webgpu"){if(typeof navigator>"u"||!navigator.gpu)throw new Error("WebGPU is not supported in current environment");let r=e.webgpu.adapter;if(r){if(typeof r.limits!="object"||typeof r.features!="object"||typeof r.requestDevice!="function")throw new Error("Invalid GPU adapter set in `env.webgpu.adapter`. It must be a GPUAdapter object.")}else{let s=e.webgpu.powerPreference;if(s!==void 0&&s!=="low-power"&&s!=="high-performance")throw new Error(`Invalid powerPreference setting: "${s}"`);let i=e.webgpu.forceFallbackAdapter;if(i!==void 0&&typeof i!="boolean")throw new Error(`Invalid forceFallbackAdapter setting: "${i}"`);if(r=await navigator.gpu.requestAdapter({powerPreference:s,forceFallbackAdapter:i}),!r)throw new Error('Failed to get GPU adapter. You may need to enable flag "--enable-unsafe-webgpu" if you are using Chrome.')}await n("webgpu",sn(),e,r)}if(t==="webnn"){if(typeof navigator>"u"||!navigator.ml)throw new Error("WebNN is not supported in current environment");await n("webnn",sn(),e)}}},ri=new Map,qT=e=>{let t=sn(),n=t.stackSave();try{let r=t.PTR_SIZE,s=t.stackAlloc(2*r);t._OrtGetInputOutputCount(e,s,s+r)!==0&&en("Can't get session input/output count.");let i=r===4?"i32":"i64";return[Number(t.getValue(s,i)),Number(t.getValue(s+r,i))]}finally{t.stackRestore(n)}},em=(e,t)=>{let n=sn(),r=n.stackSave(),s=0;try{let i=n.PTR_SIZE,o=n.stackAlloc(2*i);n._OrtGetInputOutputMetadata(e,t,o,o+i)!==0&&en("Can't get session input/output metadata.");let a=Number(n.getValue(o,"*"));s=Number(n.getValue(o+i,"*"));let l=n.HEAP32[s/4];if(l===0)return[a,0];let c=n.HEAPU32[s/4+1],d=[];for(let u=0;u<c;u++){let p=Number(n.getValue(s+8+u*i,"*"));d.push(p!==0?n.UTF8ToString(p):Number(n.getValue(s+8+(u+c)*i,"*")))}return[a,l,d]}finally{n.stackRestore(r),s!==0&&n._OrtFree(s)}},Au=e=>{let t=sn(),n=t._malloc(e.byteLength);if(n===0)throw new Error(`Can't create a session. failed to allocate a buffer of size ${e.byteLength}.`);return t.HEAPU8.set(e,n),[n,e.byteLength]},zg=async(e,t)=>{let n,r,s=sn();Array.isArray(e)?[n,r]=e:e.buffer===s.HEAPU8.buffer?[n,r]=[e.byteOffset,e.byteLength]:[n,r]=Au(e);let i=0,o=0,a=0,l=[],c=[],d=[];try{if([o,l]=await UI(t),t?.externalData&&s.mountExternalData){let M=[];for(let v of t.externalData){let b=typeof v=="string"?v:v.path;M.push(Tg(typeof v=="string"?v:v.data).then(A=>{s.mountExternalData(b,A)}))}await Promise.all(M)}for(let M of t?.executionProviders??[])if((typeof M=="string"?M:M.name)==="webnn"){if(s.shouldTransferToMLTensor=!1,typeof M!="string"){let v=M,b=v?.context,A=v?.gpuDevice,k=v?.deviceType,F=v?.powerPreference;b?s.currentContext=b:A?s.currentContext=await s.webnnCreateMLContext(A):s.currentContext=await s.webnnCreateMLContext({deviceType:k,powerPreference:F})}else s.currentContext=await s.webnnCreateMLContext();break}i=await s._OrtCreateSession(n,r,o),s.webgpuOnCreateSession?.(i),i===0&&en("Can't create a session."),s.jsepOnCreateSession?.(),s.currentContext&&(s.webnnRegisterMLContext(i,s.currentContext),s.currentContext=void 0,s.shouldTransferToMLTensor=!0);let[u,p]=qT(i),h=!!t?.enableGraphCapture,m=[],g=[],I=[],f=[],_=[];for(let M=0;M<u;M++){let[v,b,A]=em(i,M);v===0&&en("Can't get an input name."),c.push(v);let k=s.UTF8ToString(v);m.push(k),I.push(b===0?{name:k,isTensor:!1}:{name:k,isTensor:!0,type:zs(b),shape:A})}for(let M=0;M<p;M++){let[v,b,A]=em(i,M+u);v===0&&en("Can't get an output name."),d.push(v);let k=s.UTF8ToString(v);g.push(k),f.push(b===0?{name:k,isTensor:!1}:{name:k,isTensor:!0,type:zs(b),shape:A});{if(h&&t?.preferredOutputLocation===void 0){_.push("gpu-buffer");continue}let F=typeof t?.preferredOutputLocation=="string"?t.preferredOutputLocation:t?.preferredOutputLocation?.[k]??"cpu";if(F!=="cpu"&&F!=="cpu-pinned"&&F!=="gpu-buffer"&&F!=="ml-tensor")throw new Error(`Not supported preferred output location: ${F}.`);if(h&&F!=="gpu-buffer")throw new Error(`Not supported preferred output location: ${F}. Only 'gpu-buffer' location is supported when enableGraphCapture is true.`);_.push(F)}}let T=null;return _.some(M=>M==="gpu-buffer"||M==="ml-tensor")&&(a=s._OrtCreateBinding(i),a===0&&en("Can't create IO binding."),T={handle:a,outputPreferredLocations:_,outputPreferredLocationsEncoded:_.map(M=>Qm(M))}),ri.set(i,[i,c,d,T,h,!1]),[i,m,g,I,f]}catch(u){throw c.forEach(p=>s._OrtFree(p)),d.forEach(p=>s._OrtFree(p)),a!==0&&s._OrtReleaseBinding(a)!==0&&en("Can't release IO binding."),i!==0&&s._OrtReleaseSession(i)!==0&&en("Can't release session."),u}finally{s._free(n),o!==0&&s._OrtReleaseSessionOptions(o)!==0&&en("Can't release session options."),l.forEach(u=>s._free(u)),s.unmountExternalData?.()}},Bg=e=>{let t=sn(),n=ri.get(e);if(!n)throw new Error(`cannot release session. invalid session id: ${e}`);let[r,s,i,o,a]=n;o&&(a&&t._OrtClearBoundOutputs(o.handle)!==0&&en("Can't clear bound outputs."),t._OrtReleaseBinding(o.handle)!==0&&en("Can't release IO binding.")),t.jsepOnReleaseSession?.(e),t.webnnOnReleaseSession?.(e),t.webgpuOnReleaseSession?.(e),s.forEach(l=>t._OrtFree(l)),i.forEach(l=>t._OrtFree(l)),t._OrtReleaseSession(r)!==0&&en("Can't release session."),ri.delete(e)},tm=async(e,t,n,r,s,i,o=!1)=>{if(!e){t.push(0);return}let a=sn(),l=a.PTR_SIZE,c=e[0],d=e[1],u=e[3],p=u,h,m;if(c==="string"&&(u==="gpu-buffer"||u==="ml-tensor"))throw new Error("String tensor is not supported on GPU.");if(o&&u!=="gpu-buffer")throw new Error(`External buffer must be provided for input/output index ${i} when enableGraphCapture is true.`);if(u==="gpu-buffer"){let f=e[2].gpuBuffer;m=Fi(Io(c),d);{let _=a.jsepRegisterBuffer;if(!_)throw new Error('Tensor location "gpu-buffer" is not supported without using WebGPU.');h=_(r,i,f,m)}}else if(u==="ml-tensor"){let f=e[2].mlTensor;m=Fi(Io(c),d);let _=a.webnnRegisterMLTensor;if(!_)throw new Error('Tensor location "ml-tensor" is not supported without using WebNN.');h=_(r,f,Io(c),d)}else{let f=e[2];if(Array.isArray(f)){m=l*f.length,h=a._malloc(m),n.push(h);for(let _=0;_<f.length;_++){if(typeof f[_]!="string")throw new TypeError(`tensor data at index ${_} is not a string`);a.setValue(h+_*l,is(f[_],n),"*")}}else{let _=a.webnnIsGraphInput;if(c!=="string"&&_){let T=a.UTF8ToString(s);if(_(r,T)){let M=Io(c);m=Fi(M,d),p="ml-tensor";let v=a.webnnCreateTemporaryTensor,b=a.webnnUploadTensor;if(!v||!b)throw new Error('Tensor location "ml-tensor" is not supported without using WebNN.');let A=await v(r,M,d);b(A,new Uint8Array(f.buffer,f.byteOffset,f.byteLength)),h=A}else m=f.byteLength,h=a._malloc(m),n.push(h),a.HEAPU8.set(new Uint8Array(f.buffer,f.byteOffset,m),h)}else m=f.byteLength,h=a._malloc(m),n.push(h),a.HEAPU8.set(new Uint8Array(f.buffer,f.byteOffset,m),h)}}let g=a.stackSave(),I=a.stackAlloc(4*d.length);try{d.forEach((_,T)=>a.setValue(I+T*l,_,l===4?"i32":"i64"));let f=a._OrtCreateTensor(Io(c),h,m,I,d.length,Qm(p));f===0&&en(`Can't create tensor for input/output. session=${r}, index=${i}.`),t.push(f)}finally{a.stackRestore(g)}},Gg=async(e,t,n,r,s,i)=>{let o=sn(),a=o.PTR_SIZE,l=ri.get(e);if(!l)throw new Error(`cannot run inference. invalid session id: ${e}`);let c=l[0],d=l[1],u=l[2],p=l[3],h=l[4],m=l[5],g=t.length,I=r.length,f=0,_=[],T=[],M=[],v=[],b=o.stackSave(),A=o.stackAlloc(g*a),k=o.stackAlloc(g*a),F=o.stackAlloc(I*a),L=o.stackAlloc(I*a);try{[f,_]=VI(i);for(let R=0;R<g;R++)await tm(n[R],T,v,e,d[t[R]],t[R],h);for(let R=0;R<I;R++)await tm(s[R],M,v,e,u[r[R]],g+r[R],h);for(let R=0;R<g;R++)o.setValue(A+R*a,T[R],"*"),o.setValue(k+R*a,d[t[R]],"*");for(let R=0;R<I;R++)o.setValue(F+R*a,M[R],"*"),o.setValue(L+R*a,u[r[R]],"*");if(p&&!m){let{handle:R,outputPreferredLocations:K,outputPreferredLocationsEncoded:U}=p;if(d.length!==g)throw new Error(`input count from feeds (${g}) is expected to be always equal to model's input count (${d.length}).`);for(let Y=0;Y<g;Y++){let te=t[Y];await o._OrtBindInput(R,d[te],T[Y])!==0&&en(`Can't bind input[${Y}] for session=${e}.`)}for(let Y=0;Y<I;Y++){let te=r[Y];s[Y]?.[3]?o._OrtBindOutput(R,u[te],M[Y],0)!==0&&en(`Can't bind pre-allocated output[${Y}] for session=${e}.`):o._OrtBindOutput(R,u[te],0,U[te])!==0&&en(`Can't bind output[${Y}] to ${K[Y]} for session=${e}.`)}ri.set(e,[c,d,u,p,h,!0])}o.jsepOnRunStart?.(c),o.webnnOnRunStart?.(c);let G;p?G=await o._OrtRunWithBinding(c,p.handle,I,F,f):G=await o._OrtRun(c,k,A,g,L,I,F,f),G!==0&&en("failed to call OrtRun().");let j=[];for(let R=0;R<I;R++){let K=Number(o.getValue(F+R*a,"*"));if(K===M[R]){j.push(s[R]);continue}let U=o.stackSave(),Y=o.stackAlloc(4*a),te=!1,ne,le=0;try{o._OrtGetTensorData(K,Y,Y+a,Y+2*a,Y+3*a)!==0&&en(`Can't access output tensor data on index ${R}.`);let N=a===4?"i32":"i64",oe=Number(o.getValue(Y,N));le=o.getValue(Y+a,"*");let X=o.getValue(Y+a*2,"*"),D=Number(o.getValue(Y+a*3,N)),z=[];for(let $e=0;$e<D;$e++)z.push(Number(o.getValue(X+$e*a,N)));o._OrtFree(X)!==0&&en("Can't free memory for tensor dims.");let se=z.reduce(($e,ke)=>$e*ke,1);ne=zs(oe);let me=p?.outputPreferredLocations[r[R]];if(ne==="string"){if(me==="gpu-buffer"||me==="ml-tensor")throw new Error("String tensor is not supported on GPU.");let $e=[];for(let ke=0;ke<se;ke++){let Be=o.getValue(le+ke*a,"*"),Ce=o.getValue(le+(ke+1)*a,"*"),Z=ke===se-1?void 0:Ce-Be;$e.push(o.UTF8ToString(Be,Z))}j.push([ne,z,$e,"cpu"])}else if(me==="gpu-buffer"&&se>0){let $e=o.jsepGetBuffer;if(!$e)throw new Error('preferredLocation "gpu-buffer" is not supported without using WebGPU.');let ke=$e(le),Be=Fi(oe,se);if(Be===void 0||!Mg(ne))throw new Error(`Unsupported data type: ${ne}`);te=!0,j.push([ne,z,{gpuBuffer:ke,download:o.jsepCreateDownloader(ke,Be,ne),dispose:()=>{o._OrtReleaseTensor(K)!==0&&en("Can't release tensor.")}},"gpu-buffer"])}else if(me==="ml-tensor"&&se>0){let $e=o.webnnEnsureTensor,ke=o.webnnIsInt64Supported;if(!$e||!ke)throw new Error('preferredLocation "ml-tensor" is not supported without using WebNN.');if(Fi(oe,se)===void 0||!Ag(ne))throw new Error(`Unsupported data type: ${ne}`);if(ne==="int64"&&!ke(e))throw new Error('preferredLocation "ml-tensor" for int64 output is not supported by current WebNN Context.');let Be=await $e(e,le,oe,z,!1);te=!0,j.push([ne,z,{mlTensor:Be,download:o.webnnCreateMLTensorDownloader(le,ne),dispose:()=>{o.webnnReleaseTensorId(le),o._OrtReleaseTensor(K)}},"ml-tensor"])}else{let $e=vg(ne),ke=new $e(se);new Uint8Array(ke.buffer,ke.byteOffset,ke.byteLength).set(o.HEAPU8.subarray(le,le+ke.byteLength)),j.push([ne,z,ke,"cpu"])}}finally{o.stackRestore(U),ne==="string"&&le&&o._free(le),te||o._OrtReleaseTensor(K),o.webnnOnRunEnd?.(c)}}return p&&!h&&(o._OrtClearBoundOutputs(p.handle)!==0&&en("Can't clear bound outputs."),ri.set(e,[c,d,u,p,h,!1])),j}finally{o.stackRestore(b),T.forEach(G=>o._OrtReleaseTensor(G)),M.forEach(G=>o._OrtReleaseTensor(G)),v.forEach(G=>o._free(G)),f!==0&&o._OrtReleaseRunOptions(f),_.forEach(G=>o._free(G))}},Vg=e=>{let t=sn(),n=ri.get(e);if(!n)throw new Error("invalid session id");let r=n[0],s=t._OrtEndProfiling(r);s===0&&en("Can't get an profile file name."),t._OrtFree(s)},Ug=e=>{let t=[];for(let n of e){let r=n[2];!Array.isArray(r)&&"buffer"in r&&t.push(r.buffer)}return t}}),si,Rr,Mo,ja,Ra,zc,nm,Bc,Ei,Si,QT,VS,US,WS,HS,KS,qS,QS,XS=He(()=>{ds(),GS(),Yi(),xg(),si=()=>!!ln.wasm.proxy&&typeof document<"u",Mo=!1,ja=!1,Ra=!1,Bc=new Map,Ei=(e,t)=>{let n=Bc.get(e);n?n.push(t):Bc.set(e,[t])},Si=()=>{if(Mo||!ja||Ra||!Rr)throw new Error("worker not ready")},QT=e=>{switch(e.data.type){case"init-wasm":Mo=!1,e.data.err?(Ra=!0,nm[1](e.data.err)):(ja=!0,nm[0]()),zc&&(URL.revokeObjectURL(zc),zc=void 0);break;case"init-ep":case"copy-from":case"create":case"release":case"run":case"end-profiling":{let t=Bc.get(e.data.type);e.data.err?t.shift()[1](e.data.err):t.shift()[0](e.data.out);break}}},VS=async()=>{if(!ja){if(Mo)throw new Error("multiple calls to 'initWasm()' detected.");if(Ra)throw new Error("previous call to 'initWasm()' failed.");if(Mo=!0,si())return new Promise((e,t)=>{Rr?.terminate(),BI().then(([n,r])=>{try{Rr=r,Rr.onerror=i=>t(i),Rr.onmessage=QT,nm=[e,t];let s={type:"init-wasm",in:ln};!s.in.wasm.wasmPaths&&(n||qm)&&(s.in.wasm.wasmPaths={wasm:new URL(""+new URL("ort-wasm-simd-threaded.jsep-B0T3yYHD.wasm",import.meta.url).href,import.meta.url).href}),Rr.postMessage(s),zc=n}catch(s){t(s)}},t)});try{await bg(ln.wasm),await Rg(ln),ja=!0}catch(e){throw Ra=!0,e}finally{Mo=!1}}},US=async e=>{if(si())return Si(),new Promise((t,n)=>{Ei("init-ep",[t,n]);let r={type:"init-ep",in:{epName:e,env:ln}};Rr.postMessage(r)});await Ng(ln,e)},WS=async e=>si()?(Si(),new Promise((t,n)=>{Ei("copy-from",[t,n]);let r={type:"copy-from",in:{buffer:e}};Rr.postMessage(r,[e.buffer])})):Au(e),HS=async(e,t)=>{if(si()){if(t?.preferredOutputLocation)throw new Error('session option "preferredOutputLocation" is not supported for proxy.');return Si(),new Promise((n,r)=>{Ei("create",[n,r]);let s={type:"create",in:{model:e,options:{...t}}},i=[];e instanceof Uint8Array&&i.push(e.buffer),Rr.postMessage(s,i)})}else return zg(e,t)},KS=async e=>{if(si())return Si(),new Promise((t,n)=>{Ei("release",[t,n]);let r={type:"release",in:e};Rr.postMessage(r)});Bg(e)},qS=async(e,t,n,r,s,i)=>{if(si()){if(n.some(o=>o[3]!=="cpu"))throw new Error("input tensor on GPU is not supported for proxy.");if(s.some(o=>o))throw new Error("pre-allocated output tensor is not supported for proxy.");return Si(),new Promise((o,a)=>{Ei("run",[o,a]);let l=n,c={type:"run",in:{sessionId:e,inputIndices:t,inputs:l,outputIndices:r,options:i}};Rr.postMessage(c,Ug(l))})}else return Gg(e,t,n,r,s,i)},QS=async e=>{if(si())return Si(),new Promise((t,n)=>{Ei("end-profiling",[t,n]);let r={type:"end-profiling",in:e};Rr.postMessage(r)});Vg(e)}}),rm,XT,YS,vV=He(()=>{ds(),XS(),Mt(),yg(),WI(),rm=(e,t)=>{switch(e.location){case"cpu":return[e.type,e.dims,e.data,"cpu"];case"gpu-buffer":return[e.type,e.dims,{gpuBuffer:e.gpuBuffer},"gpu-buffer"];case"ml-tensor":return[e.type,e.dims,{mlTensor:e.mlTensor},"ml-tensor"];default:throw new Error(`invalid data location: ${e.location} for ${t()}`)}},XT=e=>{switch(e[3]){case"cpu":return new os(e[0],e[2],e[1]);case"gpu-buffer":{let t=e[0];if(!Mg(t))throw new Error(`not supported data type: ${t} for deserializing GPU tensor`);let{gpuBuffer:n,download:r,dispose:s}=e[2];return os.fromGpuBuffer(n,{dataType:t,dims:e[1],download:r,dispose:s})}case"ml-tensor":{let t=e[0];if(!Ag(t))throw new Error(`not supported data type: ${t} for deserializing MLTensor tensor`);let{mlTensor:n,download:r,dispose:s}=e[2];return os.fromMLTensor(n,{dataType:t,dims:e[1],download:r,dispose:s})}default:throw new Error(`invalid data location: ${e[3]}`)}},YS=class{async fetchModelAndCopyToWasmMemory(e){return WS(await Tg(e))}async loadModel(e,t){us();let n;typeof e=="string"?n=await this.fetchModelAndCopyToWasmMemory(e):n=e,[this.sessionId,this.inputNames,this.outputNames,this.inputMetadata,this.outputMetadata]=await HS(n,t),Kr()}async dispose(){return KS(this.sessionId)}async run(e,t,n){us();let r=[],s=[];Object.entries(e).forEach(u=>{let p=u[0],h=u[1],m=this.inputNames.indexOf(p);if(m===-1)throw new Error(`invalid input '${p}'`);r.push(h),s.push(m)});let i=[],o=[];Object.entries(t).forEach(u=>{let p=u[0],h=u[1],m=this.outputNames.indexOf(p);if(m===-1)throw new Error(`invalid output '${p}'`);i.push(h),o.push(m)});let a=r.map((u,p)=>rm(u,()=>`input "${this.inputNames[s[p]]}"`)),l=i.map((u,p)=>u?rm(u,()=>`output "${this.outputNames[o[p]]}"`):null),c=await qS(this.sessionId,s,a,o,l,n),d={};for(let u=0;u<c.length;u++)d[this.outputNames[o[u]]]=i[u]??XT(c[u]);return Kr(),d}startProfiling(){}endProfiling(){QS(this.sessionId)}}}),JS={};Zo(JS,{OnnxruntimeWebAssemblyBackend:()=>uf,initializeFlags:()=>cf,wasmBackend:()=>ZS});var cf,uf,ZS,MV=He(()=>{ds(),XS(),vV(),cf=()=>{(typeof ln.wasm.initTimeout!="number"||ln.wasm.initTimeout<0)&&(ln.wasm.initTimeout=0);let e=ln.wasm.simd;if(typeof e!="boolean"&&e!==void 0&&e!=="fixed"&&e!=="relaxed"&&(console.warn(`Property "env.wasm.simd" is set to unknown value "${e}". Reset it to \`false\` and ignore SIMD feature checking.`),ln.wasm.simd=!1),typeof ln.wasm.proxy!="boolean"&&(ln.wasm.proxy=!1),typeof ln.wasm.trace!="boolean"&&(ln.wasm.trace=!1),typeof ln.wasm.numThreads!="number"||!Number.isInteger(ln.wasm.numThreads)||ln.wasm.numThreads<=0)if(typeof self<"u"&&!self.crossOriginIsolated)ln.wasm.numThreads=1;else{let t=typeof navigator>"u"?cG("node:os").cpus().length:navigator.hardwareConcurrency;ln.wasm.numThreads=Math.min(4,Math.ceil((t||1)/2))}},uf=class{async init(e){cf(),await VS(),await US(e)}async createInferenceSessionHandler(e,t){let n=new YS;return await n.loadModel(e,t),n}},ZS=new uf});ds();ds();ds();var AV="1.22.0-dev.20250409-89f8206ba4",TV=FI;{let e=(MV(),ml(JS)).wasmBackend;Ri("webgpu",e,5),Ri("webnn",e,5),Ri("cpu",e,10),Ri("wasm",e,10)}Object.defineProperty(ln.versions,"web",{value:AV,enumerable:!0});const PV=Object.freeze(Object.defineProperty({__proto__:null,get InferenceSession(){return _g},get TRACE(){return fl},get TRACE_FUNC_BEGIN(){return us},get TRACE_FUNC_END(){return Kr},get Tensor(){return os},default:TV,get env(){return ln},get registerBackend(){return Ri}},Symbol.toStringTag,{value:"Module"}));var sm={},CV={"onnxruntime-common":(e=>{e.exports=iG}),"onnxruntime-web":(e=>{e.exports=PV}),"?2ce3":(()=>{}),"?7992":(()=>{}),"?5af5":(()=>{}),"?2b25":(()=>{}),"?db59":(()=>{}),"?383f":(()=>{}),"?fa4b":(()=>{}),"./node_modules/@huggingface/jinja/dist/index.js":((e,t,n)=>{n.r(t),n.d(t,{Environment:()=>Qe,Interpreter:()=>on,Template:()=>fs,parse:()=>$e,tokenize:()=>u});var r=Object.freeze({Text:"Text",NumericLiteral:"NumericLiteral",StringLiteral:"StringLiteral",Identifier:"Identifier",Equals:"Equals",OpenParen:"OpenParen",CloseParen:"CloseParen",OpenStatement:"OpenStatement",CloseStatement:"CloseStatement",OpenExpression:"OpenExpression",CloseExpression:"CloseExpression",OpenSquareBracket:"OpenSquareBracket",CloseSquareBracket:"CloseSquareBracket",OpenCurlyBracket:"OpenCurlyBracket",CloseCurlyBracket:"CloseCurlyBracket",Comma:"Comma",Dot:"Dot",Colon:"Colon",Pipe:"Pipe",CallOperator:"CallOperator",AdditiveBinaryOperator:"AdditiveBinaryOperator",MultiplicativeBinaryOperator:"MultiplicativeBinaryOperator",ComparisonBinaryOperator:"ComparisonBinaryOperator",UnaryOperator:"UnaryOperator",Comment:"Comment"}),s=class{constructor(S,ee){this.value=S,this.type=ee}};function i(S){return/\w/.test(S)}function o(S){return/[0-9]/.test(S)}function a(S){return/\s/.test(S)}var l=[["{%",r.OpenStatement],["%}",r.CloseStatement],["{{",r.OpenExpression],["}}",r.CloseExpression],["(",r.OpenParen],[")",r.CloseParen],["{",r.OpenCurlyBracket],["}",r.CloseCurlyBracket],["[",r.OpenSquareBracket],["]",r.CloseSquareBracket],[",",r.Comma],[".",r.Dot],[":",r.Colon],["|",r.Pipe],["<=",r.ComparisonBinaryOperator],[">=",r.ComparisonBinaryOperator],["==",r.ComparisonBinaryOperator],["!=",r.ComparisonBinaryOperator],["<",r.ComparisonBinaryOperator],[">",r.ComparisonBinaryOperator],["+",r.AdditiveBinaryOperator],["-",r.AdditiveBinaryOperator],["~",r.AdditiveBinaryOperator],["*",r.MultiplicativeBinaryOperator],["/",r.MultiplicativeBinaryOperator],["%",r.MultiplicativeBinaryOperator],["=",r.Equals]],c=new Map([["n",`
`],["t","	"],["r","\r"],["b","\b"],["f","\f"],["v","\v"],["'","'"],['"','"'],["\\","\\"]]);function d(S,ee={}){return S.endsWith(`
`)&&(S=S.slice(0,-1)),ee.lstrip_blocks&&(S=S.replace(/^[ \t]*({[#%-])/gm,"$1")),ee.trim_blocks&&(S=S.replace(/([#%-]})\n/g,"$1")),S.replace(/{%\s*(end)?generation\s*%}/gs,"")}function u(S,ee={}){const W=[],J=d(S,ee);let re=0,pe=0;const Se=Ze=>{let At="";for(;Ze(J[re]);){if(J[re]==="\\"){if(++re,re>=J.length)throw new SyntaxError("Unexpected end of input");const et=J[re++],Lt=c.get(et);if(Lt===void 0)throw new SyntaxError(`Unexpected escaped character: ${et}`);At+=Lt;continue}if(At+=J[re++],re>=J.length)throw new SyntaxError("Unexpected end of input")}return At},it=()=>{const Ze=W.at(-1);Ze&&Ze.type===r.Text&&(Ze.value=Ze.value.trimEnd(),Ze.value===""&&W.pop())},gt=()=>{for(;re<J.length&&a(J[re]);)++re};e:for(;re<J.length;){const Ze=W.at(-1)?.type;if(Ze===void 0||Ze===r.CloseStatement||Ze===r.CloseExpression||Ze===r.Comment){let et="";for(;re<J.length&&!(J[re]==="{"&&(J[re+1]==="%"||J[re+1]==="{"||J[re+1]==="#"));)et+=J[re++];if(et.length>0){W.push(new s(et,r.Text));continue}}if(J[re]==="{"&&J[re+1]==="#"){re+=2;const et=J[re]==="-";et&&++re;let Lt="";for(;J[re]!=="#"||J[re+1]!=="}";){if(re+2>=J.length)throw new SyntaxError("Missing end of comment tag");Lt+=J[re++]}const Rt=Lt.endsWith("-");Rt&&(Lt=Lt.slice(0,-1)),et&&it(),W.push(new s(Lt,r.Comment)),re+=2,Rt&&gt();continue}if(J.slice(re,re+3)==="{%-"){it(),W.push(new s("{%",r.OpenStatement)),re+=3;continue}if(J.slice(re,re+3)==="{{-"){it(),W.push(new s("{{",r.OpenExpression)),pe=0,re+=3;continue}if(Se(a),J.slice(re,re+3)==="-%}"){W.push(new s("%}",r.CloseStatement)),re+=3,gt();continue}if(J.slice(re,re+3)==="-}}"){W.push(new s("}}",r.CloseExpression)),re+=3,gt();continue}const At=J[re];if(At==="-"||At==="+"){const et=W.at(-1)?.type;if(et===r.Text||et===void 0)throw new SyntaxError(`Unexpected character: ${At}`);switch(et){case r.Identifier:case r.NumericLiteral:case r.StringLiteral:case r.CloseParen:case r.CloseSquareBracket:break;default:{++re;const Lt=Se(o);W.push(new s(`${At}${Lt}`,Lt.length>0?r.NumericLiteral:r.UnaryOperator));continue}}}for(const[et,Lt]of l){if(et==="}}"&&pe>0)continue;if(J.slice(re,re+et.length)===et){W.push(new s(et,Lt)),Lt===r.OpenExpression?pe=0:Lt===r.OpenCurlyBracket?++pe:Lt===r.CloseCurlyBracket&&--pe,re+=et.length;continue e}}if(At==="'"||At==='"'){++re;const et=Se(Lt=>Lt!==At);W.push(new s(et,r.StringLiteral)),++re;continue}if(o(At)){let et=Se(o);if(J[re]==="."&&o(J[re+1])){++re;const Lt=Se(o);et=`${et}.${Lt}`}W.push(new s(et,r.NumericLiteral));continue}if(i(At)){const et=Se(i);W.push(new s(et,r.Identifier));continue}throw new SyntaxError(`Unexpected character: ${At}`)}return W}var p=class{type="Statement"},h=class extends p{constructor(S){super(),this.body=S}type="Program"},m=class extends p{constructor(S,ee,W){super(),this.test=S,this.body=ee,this.alternate=W}type="If"},g=class extends p{constructor(S,ee,W,J){super(),this.loopvar=S,this.iterable=ee,this.body=W,this.defaultBlock=J}type="For"},I=class extends p{type="Break"},f=class extends p{type="Continue"},_=class extends p{constructor(S,ee,W){super(),this.assignee=S,this.value=ee,this.body=W}type="Set"},T=class extends p{constructor(S,ee,W){super(),this.name=S,this.args=ee,this.body=W}type="Macro"},M=class extends p{constructor(S){super(),this.value=S}type="Comment"},v=class extends p{type="Expression"},b=class extends v{constructor(S,ee,W){super(),this.object=S,this.property=ee,this.computed=W}type="MemberExpression"},A=class extends v{constructor(S,ee){super(),this.callee=S,this.args=ee}type="CallExpression"},k=class extends v{constructor(S){super(),this.value=S}type="Identifier"},F=class extends v{constructor(S){super(),this.value=S}type="Literal"},L=class extends F{type="IntegerLiteral"},G=class extends F{type="FloatLiteral"},j=class extends F{type="StringLiteral"},R=class extends F{type="ArrayLiteral"},K=class extends F{type="TupleLiteral"},U=class extends F{type="ObjectLiteral"},Y=class extends v{constructor(S,ee,W){super(),this.operator=S,this.left=ee,this.right=W}type="BinaryExpression"},te=class extends v{constructor(S,ee){super(),this.operand=S,this.filter=ee}type="FilterExpression"},ne=class extends p{constructor(S,ee){super(),this.filter=S,this.body=ee}type="FilterStatement"},le=class extends v{constructor(S,ee){super(),this.lhs=S,this.test=ee}type="SelectExpression"},N=class extends v{constructor(S,ee,W){super(),this.operand=S,this.negate=ee,this.test=W}type="TestExpression"},oe=class extends v{constructor(S,ee){super(),this.operator=S,this.argument=ee}type="UnaryExpression"},X=class extends v{constructor(S=void 0,ee=void 0,W=void 0){super(),this.start=S,this.stop=ee,this.step=W}type="SliceExpression"},D=class extends v{constructor(S,ee){super(),this.key=S,this.value=ee}type="KeywordArgumentExpression"},z=class extends v{constructor(S){super(),this.argument=S}type="SpreadExpression"},se=class extends p{constructor(S,ee,W){super(),this.call=S,this.callerArgs=ee,this.body=W}type="CallStatement"},me=class extends v{constructor(S,ee,W){super(),this.condition=S,this.trueExpr=ee,this.falseExpr=W}type="Ternary"};function $e(S){const ee=new h([]);let W=0;function J(Ue,Ke){const ut=S[W++];if(!ut||ut.type!==Ue)throw new Error(`Parser Error: ${Ke}. ${ut.type} !== ${Ue}.`);return ut}function re(Ue){if(!gt(Ue))throw new SyntaxError(`Expected ${Ue}`);++W}function pe(){switch(S[W].type){case r.Comment:return new M(S[W++].value);case r.Text:return Ze();case r.OpenStatement:return At();case r.OpenExpression:return et();default:throw new SyntaxError(`Unexpected token type: ${S[W].type}`)}}function Se(...Ue){return W+Ue.length<=S.length&&Ue.every((Ke,ut)=>Ke===S[W+ut].type)}function it(...Ue){return S[W]?.type===r.OpenStatement&&S[W+1]?.type===r.Identifier&&Ue.includes(S[W+1]?.value)}function gt(...Ue){return W+Ue.length<=S.length&&Ue.every((Ke,ut)=>S[W+ut].type==="Identifier"&&Ke===S[W+ut].value)}function Ze(){return new j(J(r.Text,"Expected text token").value)}function At(){if(J(r.OpenStatement,"Expected opening statement token"),S[W].type!==r.Identifier)throw new SyntaxError(`Unknown statement, got ${S[W].type}`);const Ue=S[W].value;let Ke;switch(Ue){case"set":++W,Ke=Lt();break;case"if":++W,Ke=Rt(),J(r.OpenStatement,"Expected {% token"),re("endif"),J(r.CloseStatement,"Expected %} token");break;case"macro":++W,Ke=Fn(),J(r.OpenStatement,"Expected {% token"),re("endmacro"),J(r.CloseStatement,"Expected %} token");break;case"for":++W,Ke=bn(),J(r.OpenStatement,"Expected {% token"),re("endfor"),J(r.CloseStatement,"Expected %} token");break;case"call":{++W;let ut=null;Se(r.OpenParen)&&(ut=yr());const tn=or();if(tn.type!=="Identifier")throw new SyntaxError("Expected identifier following call statement");const $s=yr();J(r.CloseStatement,"Expected closing statement token");const ws=[];for(;!it("endcall");)ws.push(pe());J(r.OpenStatement,"Expected '{%'"),re("endcall"),J(r.CloseStatement,"Expected closing statement token");const Fs=new A(tn,$s);Ke=new se(Fs,ut,ws);break}case"break":++W,J(r.CloseStatement,"Expected closing statement token"),Ke=new I;break;case"continue":++W,J(r.CloseStatement,"Expected closing statement token"),Ke=new f;break;case"filter":{++W;let ut=or();ut instanceof k&&Se(r.OpenParen)&&(ut=wn(ut)),J(r.CloseStatement,"Expected closing statement token");const tn=[];for(;!it("endfilter");)tn.push(pe());J(r.OpenStatement,"Expected '{%'"),re("endfilter"),J(r.CloseStatement,"Expected '%}'"),Ke=new ne(ut,tn);break}default:throw new SyntaxError(`Unknown statement type: ${Ue}`)}return Ke}function et(){J(r.OpenExpression,"Expected opening expression token");const Ue=Bn();return J(r.CloseExpression,"Expected closing expression token"),Ue}function Lt(){const Ue=Cn();let Ke=null;const ut=[];if(Se(r.Equals))++W,Ke=Cn();else{for(J(r.CloseStatement,"Expected %} token");!it("endset");)ut.push(pe());J(r.OpenStatement,"Expected {% token"),re("endset")}return J(r.CloseStatement,"Expected closing statement token"),new _(Ue,Ke,ut)}function Rt(){const Ue=Bn();J(r.CloseStatement,"Expected closing statement token");const Ke=[],ut=[];for(;!it("elif","else","endif");)Ke.push(pe());if(it("elif")){++W,++W;const tn=Rt();ut.push(tn)}else if(it("else"))for(++W,++W,J(r.CloseStatement,"Expected closing statement token");!it("endif");)ut.push(pe());return new m(Ue,Ke,ut)}function Fn(){const Ue=or();if(Ue.type!=="Identifier")throw new SyntaxError("Expected identifier following macro statement");const Ke=yr();J(r.CloseStatement,"Expected closing statement token");const ut=[];for(;!it("endmacro");)ut.push(pe());return new T(Ue,Ke,ut)}function Cn(Ue=!1){const Ke=Ue?or:Bn,ut=[Ke()],tn=Se(r.Comma);for(;tn&&(++W,ut.push(Ke()),!!Se(r.Comma)););return tn?new K(ut):ut[0]}function bn(){const Ue=Cn(!0);if(!(Ue instanceof k||Ue instanceof K))throw new SyntaxError(`Expected identifier/tuple for the loop variable, got ${Ue.type} instead`);if(!gt("in"))throw new SyntaxError("Expected `in` keyword following loop variable");++W;const Ke=Bn();J(r.CloseStatement,"Expected closing statement token");const ut=[];for(;!it("endfor","else");)ut.push(pe());const tn=[];if(it("else"))for(++W,++W,J(r.CloseStatement,"Expected closing statement token");!it("endfor");)tn.push(pe());return new g(Ue,Ke,ut,tn)}function Bn(){return gs()}function gs(){const Ue=tr();if(gt("if")){++W;const Ke=tr();if(gt("else")){++W;const ut=gs();return new me(Ke,Ue,ut)}else return new le(Ue,Ke)}return Ue}function tr(){let Ue=_s();for(;gt("or");){const Ke=S[W];++W;const ut=_s();Ue=new Y(Ke,Ue,ut)}return Ue}function _s(){let Ue=nr();for(;gt("and");){const Ke=S[W];++W;const ut=nr();Ue=new Y(Ke,Ue,ut)}return Ue}function nr(){let Ue;for(;gt("not");){const Ke=S[W];++W;const ut=nr();Ue=new oe(Ke,ut)}return Ue??rr()}function rr(){let Ue=Dr();for(;;){let Ke;if(gt("not","in"))Ke=new s("not in",r.Identifier),W+=2;else if(gt("in"))Ke=S[W++];else if(Se(r.ComparisonBinaryOperator))Ke=S[W++];else break;const ut=Dr();Ue=new Y(Ke,Ue,ut)}return Ue}function Dr(){let Ue=xs();for(;Se(r.AdditiveBinaryOperator);){const Ke=S[W];++W;const ut=xs();Ue=new Y(Ke,Ue,ut)}return Ue}function On(){const Ue=ys(or());return Se(r.OpenParen)?wn(Ue):Ue}function wn(Ue){let Ke=new A(Ue,yr());return Ke=ys(Ke),Se(r.OpenParen)&&(Ke=wn(Ke)),Ke}function yr(){J(r.OpenParen,"Expected opening parenthesis for arguments list");const Ue=$r();return J(r.CloseParen,"Expected closing parenthesis for arguments list"),Ue}function $r(){const Ue=[];for(;!Se(r.CloseParen);){let Ke;if(S[W].type===r.MultiplicativeBinaryOperator&&S[W].value==="*"){++W;const ut=Bn();Ke=new z(ut)}else if(Ke=Bn(),Se(r.Equals)){if(++W,!(Ke instanceof k))throw new SyntaxError("Expected identifier for keyword argument");const ut=Bn();Ke=new D(Ke,ut)}Ue.push(Ke),Se(r.Comma)&&++W}return Ue}function Ds(){const Ue=[];let Ke=!1;for(;!Se(r.CloseSquareBracket);)Se(r.Colon)?(Ue.push(void 0),++W,Ke=!0):(Ue.push(Bn()),Se(r.Colon)&&(++W,Ke=!0));if(Ue.length===0)throw new SyntaxError("Expected at least one argument for member/slice expression");if(Ke){if(Ue.length>3)throw new SyntaxError("Expected 0-3 arguments for slice expression");return new X(...Ue)}return Ue[0]}function ys(Ue){for(;Se(r.Dot)||Se(r.OpenSquareBracket);){const Ke=S[W];++W;let ut;const tn=Ke.type===r.OpenSquareBracket;if(tn)ut=Ds(),J(r.CloseSquareBracket,"Expected closing square bracket");else if(ut=or(),ut.type!=="Identifier")throw new SyntaxError("Expected identifier following dot operator");Ue=new b(Ue,ut,tn)}return Ue}function xs(){let Ue=bs();for(;Se(r.MultiplicativeBinaryOperator);){const Ke=S[W++],ut=bs();Ue=new Y(Ke,Ue,ut)}return Ue}function bs(){let Ue=Fr();for(;gt("is");){++W;const Ke=gt("not");Ke&&++W;const ut=or();if(!(ut instanceof k))throw new SyntaxError("Expected identifier for the test");Ue=new N(Ue,Ke,ut)}return Ue}function Fr(){let Ue=On();for(;Se(r.Pipe);){++W;let Ke=or();if(!(Ke instanceof k))throw new SyntaxError("Expected identifier for the filter");Se(r.OpenParen)&&(Ke=wn(Ke)),Ue=new te(Ue,Ke)}return Ue}function or(){const Ue=S[W++];switch(Ue.type){case r.NumericLiteral:{const Ke=Ue.value;return Ke.includes(".")?new G(Number(Ke)):new L(Number(Ke))}case r.StringLiteral:{let Ke=Ue.value;for(;Se(r.StringLiteral);)Ke+=S[W++].value;return new j(Ke)}case r.Identifier:return new k(Ue.value);case r.OpenParen:{const Ke=Cn();return J(r.CloseParen,"Expected closing parenthesis, got ${tokens[current].type} instead."),Ke}case r.OpenSquareBracket:{const Ke=[];for(;!Se(r.CloseSquareBracket);)Ke.push(Bn()),Se(r.Comma)&&++W;return++W,new R(Ke)}case r.OpenCurlyBracket:{const Ke=new Map;for(;!Se(r.CloseCurlyBracket);){const ut=Bn();J(r.Colon,"Expected colon between key and value in object literal");const tn=Bn();Ke.set(ut,tn),Se(r.Comma)&&++W}return++W,new U(Ke)}default:throw new SyntaxError(`Unexpected token: ${Ue.type}`)}}for(;W<S.length;)ee.body.push(pe());return ee}function ke(S,ee,W=1){ee===void 0&&(ee=S,S=0);const J=[];for(let re=S;re<ee;re+=W)J.push(re);return J}function Be(S,ee,W,J=1){const re=Math.sign(J);re>=0?(ee=(ee??=0)<0?Math.max(S.length+ee,0):Math.min(ee,S.length),W=(W??=S.length)<0?Math.max(S.length+W,0):Math.min(W,S.length)):(ee=(ee??=S.length-1)<0?Math.max(S.length+ee,-1):Math.min(ee,S.length-1),W=(W??=-1)<-1?Math.max(S.length+W,-1):Math.min(W,S.length-1));const pe=[];for(let Se=ee;re*Se<re*W;Se+=J)pe.push(S[Se]);return pe}function Ce(S){return S.replace(/\b\w/g,ee=>ee.toUpperCase())}function Z(S){return V(new Date,S)}function V(S,ee){const W=new Intl.DateTimeFormat(void 0,{month:"long"}),J=new Intl.DateTimeFormat(void 0,{month:"short"}),re=pe=>pe<10?"0"+pe:pe.toString();return ee.replace(/%[YmdbBHM%]/g,pe=>{switch(pe){case"%Y":return S.getFullYear().toString();case"%m":return re(S.getMonth()+1);case"%d":return re(S.getDate());case"%b":return J.format(S);case"%B":return W.format(S);case"%H":return re(S.getHours());case"%M":return re(S.getMinutes());case"%%":return"%";default:return pe}})}function fe(S){return S.replace(/[.*+?^${}()|[\]\\]/g,"\\$&")}function Te(S,ee,W,J){if(J===0)return S;let re=J==null||J<0?1/0:J;const pe=ee.length===0?new RegExp("(?=)","gu"):new RegExp(fe(ee),"gu");return S.replaceAll(pe,Se=>re>0?(--re,W):Se)}var Ie=class extends Error{},Ee=class extends Error{},De=class{type="RuntimeValue";value;builtins=new Map;constructor(S=void 0){this.value=S}__bool__(){return new Le(!!this.value)}toString(){return String(this.value)}},xe=class extends De{type="IntegerValue"},ze=class extends De{type="FloatValue";toString(){return this.value%1===0?this.value.toFixed(1):this.value.toString()}},_e=class extends De{type="StringValue";builtins=new Map([["upper",new ye(()=>new _e(this.value.toUpperCase()))],["lower",new ye(()=>new _e(this.value.toLowerCase()))],["strip",new ye(()=>new _e(this.value.trim()))],["title",new ye(()=>new _e(Ce(this.value)))],["capitalize",new ye(()=>new _e(this.value.charAt(0).toUpperCase()+this.value.slice(1)))],["length",new xe(this.value.length)],["rstrip",new ye(()=>new _e(this.value.trimEnd()))],["lstrip",new ye(()=>new _e(this.value.trimStart()))],["startswith",new ye(S=>{if(S.length===0)throw new Error("startswith() requires at least one argument");const ee=S[0];if(ee instanceof _e)return new Le(this.value.startsWith(ee.value));if(ee instanceof Ve){for(const W of ee.value){if(!(W instanceof _e))throw new Error("startswith() tuple elements must be strings");if(this.value.startsWith(W.value))return new Le(!0)}return new Le(!1)}throw new Error("startswith() argument must be a string or tuple of strings")})],["endswith",new ye(S=>{if(S.length===0)throw new Error("endswith() requires at least one argument");const ee=S[0];if(ee instanceof _e)return new Le(this.value.endsWith(ee.value));if(ee instanceof Ve){for(const W of ee.value){if(!(W instanceof _e))throw new Error("endswith() tuple elements must be strings");if(this.value.endsWith(W.value))return new Le(!0)}return new Le(!1)}throw new Error("endswith() argument must be a string or tuple of strings")})],["split",new ye(S=>{const ee=S[0]??new Pe;if(!(ee instanceof _e||ee instanceof Pe))throw new Error("sep argument must be a string or null");const W=S[1]??new xe(-1);if(!(W instanceof xe))throw new Error("maxsplit argument must be a number");let J=[];if(ee instanceof Pe){const re=this.value.trimStart();for(const{0:pe,index:Se}of re.matchAll(/\S+/g)){if(W.value!==-1&&J.length>=W.value&&Se!==void 0){J.push(pe+re.slice(Se+pe.length));break}J.push(pe)}}else{if(ee.value==="")throw new Error("empty separator");J=this.value.split(ee.value),W.value!==-1&&J.length>W.value&&J.push(J.splice(W.value).join(ee.value))}return new Ve(J.map(re=>new _e(re)))})],["replace",new ye(S=>{if(S.length<2)throw new Error("replace() requires at least two arguments");const ee=S[0],W=S[1];if(!(ee instanceof _e&&W instanceof _e))throw new Error("replace() arguments must be strings");let J;if(S.length>2?S[2].type==="KeywordArgumentsValue"?J=S[2].value.get("count")??new Pe:J=S[2]:J=new Pe,!(J instanceof xe||J instanceof Pe))throw new Error("replace() count argument must be a number or null");return new _e(Te(this.value,ee.value,W.value,J.value))})]])},Le=class extends De{type="BooleanValue"};function qe(S,ee,W,J=!0){const re=W??0;switch(S.type){case"NullValue":return"null";case"UndefinedValue":return J?"null":"undefined";case"IntegerValue":case"FloatValue":case"StringValue":case"BooleanValue":return JSON.stringify(S.value);case"ArrayValue":case"ObjectValue":{const pe=ee?" ".repeat(ee):"",Se=`
`+pe.repeat(re),it=Se+pe;if(S.type==="ArrayValue"){const gt=S.value.map(Ze=>qe(Ze,ee,re+1,J));return ee?`[${it}${gt.join(`,${it}`)}${Se}]`:`[${gt.join(", ")}]`}else{const gt=Array.from(S.value.entries()).map(([Ze,At])=>{const et=`"${Ze}": ${qe(At,ee,re+1,J)}`;return ee?`${it}${et}`:et});return ee?`{${gt.join(",")}${Se}}`:`{${gt.join(", ")}}`}}default:throw new Error(`Cannot convert to JSON: ${S.type}`)}}var Ne=class extends De{type="ObjectValue";__bool__(){return new Le(this.value.size>0)}builtins=new Map([["get",new ye(([S,ee])=>{if(!(S instanceof _e))throw new Error(`Object key must be a string: got ${S.type}`);return this.value.get(S.value)??ee??new Pe})],["items",new ye(()=>this.items())],["keys",new ye(()=>this.keys())],["values",new ye(()=>this.values())],["dictsort",new ye(S=>{let ee=new Map;const W=S.filter(it=>it instanceof ot?(ee=it.value,!1):!0),J=W.at(0)??ee.get("case_sensitive")??new Le(!1);if(!(J instanceof Le))throw new Error("case_sensitive must be a boolean");const re=W.at(1)??ee.get("by")??new _e("key");if(!(re instanceof _e))throw new Error("by must be a string");if(!["key","value"].includes(re.value))throw new Error("by must be either 'key' or 'value'");const pe=W.at(2)??ee.get("reverse")??new Le(!1);if(!(pe instanceof Le))throw new Error("reverse must be a boolean");const Se=Array.from(this.value.entries()).map(([it,gt])=>new Ve([new _e(it),gt])).sort((it,gt)=>{const Ze=re.value==="key"?0:1,At=it.value[Ze],et=gt.value[Ze],Lt=wt(At,et,J.value);return pe.value?-Lt:Lt});return new Ve(Se)})]]);items(){return new Ve(Array.from(this.value.entries()).map(([S,ee])=>new Ve([new _e(S),ee])))}keys(){return new Ve(Array.from(this.value.keys()).map(S=>new _e(S)))}values(){return new Ve(Array.from(this.value.values()))}toString(){return qe(this,null,0,!1)}},ot=class extends Ne{type="KeywordArgumentsValue"},Ve=class extends De{type="ArrayValue";builtins=new Map([["length",new xe(this.value.length)]]);__bool__(){return new Le(this.value.length>0)}toString(){return qe(this,null,0,!1)}},de=class extends Ve{type="TupleValue"},ye=class extends De{type="FunctionValue"},Pe=class extends De{type="NullValue"},ve=class extends De{type="UndefinedValue"},Qe=class{constructor(S){this.parent=S}variables=new Map([["namespace",new ye(S=>{if(S.length===0)return new Ne(new Map);if(S.length!==1||!(S[0]instanceof Ne))throw new Error("`namespace` expects either zero arguments or a single object argument");return S[0]})]]);tests=new Map([["boolean",S=>S.type==="BooleanValue"],["callable",S=>S instanceof ye],["odd",S=>{if(!(S instanceof xe))throw new Error(`cannot odd on ${S.type}`);return S.value%2!==0}],["even",S=>{if(!(S instanceof xe))throw new Error(`cannot even on ${S.type}`);return S.value%2===0}],["false",S=>S.type==="BooleanValue"&&!S.value],["true",S=>S.type==="BooleanValue"&&S.value],["none",S=>S.type==="NullValue"],["string",S=>S.type==="StringValue"],["number",S=>S instanceof xe||S instanceof ze],["integer",S=>S instanceof xe],["iterable",S=>S.type==="ArrayValue"||S.type==="StringValue"],["mapping",S=>S.type==="ObjectValue"],["lower",S=>{const ee=S.value;return S.type==="StringValue"&&ee===ee.toLowerCase()}],["upper",S=>{const ee=S.value;return S.type==="StringValue"&&ee===ee.toUpperCase()}],["none",S=>S.type==="NullValue"],["defined",S=>S.type!=="UndefinedValue"],["undefined",S=>S.type==="UndefinedValue"],["equalto",(S,ee)=>S.value===ee.value],["eq",(S,ee)=>S.value===ee.value]]);set(S,ee){return this.declareVariable(S,Tn(ee))}declareVariable(S,ee){if(this.variables.has(S))throw new SyntaxError(`Variable already declared: ${S}`);return this.variables.set(S,ee),ee}setVariable(S,ee){return this.variables.set(S,ee),ee}resolve(S){if(this.variables.has(S))return this;if(this.parent)return this.parent.resolve(S);throw new Error(`Unknown variable: ${S}`)}lookupVariable(S){try{return this.resolve(S).variables.get(S)??new ve}catch{return new ve}}};function ct(S){S.set("false",!1),S.set("true",!0),S.set("none",null),S.set("raise_exception",ee=>{throw new Error(ee)}),S.set("range",ke),S.set("strftime_now",Z),S.set("True",!0),S.set("False",!1),S.set("None",null)}function zt(S,ee){const W=ee.split(".");let J=S;for(const re of W)if(J instanceof Ne)J=J.value.get(re)??new ve;else if(J instanceof Ve){const pe=parseInt(re,10);if(!isNaN(pe)&&pe>=0&&pe<J.value.length)J=J.value[pe];else return new ve}else return new ve;return J}function wt(S,ee,W=!1){if(S instanceof Pe&&ee instanceof Pe)return 0;if(S instanceof Pe||ee instanceof Pe)throw new Error(`Cannot compare ${S.type} with ${ee.type}`);if(S instanceof ve&&ee instanceof ve)return 0;if(S instanceof ve||ee instanceof ve)throw new Error(`Cannot compare ${S.type} with ${ee.type}`);const J=pe=>pe instanceof xe||pe instanceof ze||pe instanceof Le,re=pe=>pe instanceof Le?pe.value?1:0:pe.value;if(J(S)&&J(ee)){const pe=re(S),Se=re(ee);return pe<Se?-1:pe>Se?1:0}if(S.type!==ee.type)throw new Error(`Cannot compare different types: ${S.type} and ${ee.type}`);if(S.type==="StringValue"){let pe=S.value,Se=ee.value;return W||(pe=pe.toLowerCase(),Se=Se.toLowerCase()),pe<Se?-1:pe>Se?1:0}else throw new Error(`Cannot compare type: ${S.type}`)}var on=class{global;constructor(S){this.global=S??new Qe}run(S){return this.evaluate(S,this.global)}evaluateBinaryExpression(S,ee){const W=this.evaluate(S.left,ee);switch(S.operator.value){case"and":return W.__bool__().value?this.evaluate(S.right,ee):W;case"or":return W.__bool__().value?W:this.evaluate(S.right,ee)}const J=this.evaluate(S.right,ee);switch(S.operator.value){case"==":return new Le(W.value==J.value);case"!=":return new Le(W.value!=J.value)}if(W instanceof ve||J instanceof ve){if(J instanceof ve&&["in","not in"].includes(S.operator.value))return new Le(S.operator.value==="not in");throw new Error(`Cannot perform operation ${S.operator.value} on undefined values`)}else{if(W instanceof Pe||J instanceof Pe)throw new Error("Cannot perform operation on null values");if(S.operator.value==="~")return new _e(W.value.toString()+J.value.toString());if((W instanceof xe||W instanceof ze)&&(J instanceof xe||J instanceof ze)){const re=W.value,pe=J.value;switch(S.operator.value){case"+":case"-":case"*":{const Se=S.operator.value==="+"?re+pe:S.operator.value==="-"?re-pe:re*pe;return W instanceof ze||J instanceof ze?new ze(Se):new xe(Se)}case"/":return new ze(re/pe);case"%":{const Se=re%pe;return W instanceof ze||J instanceof ze?new ze(Se):new xe(Se)}case"<":return new Le(re<pe);case">":return new Le(re>pe);case">=":return new Le(re>=pe);case"<=":return new Le(re<=pe)}}else if(W instanceof Ve&&J instanceof Ve){if(S.operator.value==="+")return new Ve(W.value.concat(J.value))}else if(J instanceof Ve){const re=J.value.find(pe=>pe.value===W.value)!==void 0;switch(S.operator.value){case"in":return new Le(re);case"not in":return new Le(!re)}}}if((W instanceof _e||J instanceof _e)&&S.operator.value==="+")return new _e(W.value.toString()+J.value.toString());if(W instanceof _e&&J instanceof _e)switch(S.operator.value){case"in":return new Le(J.value.includes(W.value));case"not in":return new Le(!J.value.includes(W.value))}if(W instanceof _e&&J instanceof Ne)switch(S.operator.value){case"in":return new Le(J.value.has(W.value));case"not in":return new Le(!J.value.has(W.value))}throw new SyntaxError(`Unknown operator "${S.operator.value}" between ${W.type} and ${J.type}`)}evaluateArguments(S,ee){const W=[],J=new Map;for(const re of S)if(re.type==="SpreadExpression"){const pe=re,Se=this.evaluate(pe.argument,ee);if(!(Se instanceof Ve))throw new Error(`Cannot unpack non-iterable type: ${Se.type}`);for(const it of Se.value)W.push(it)}else if(re.type==="KeywordArgumentExpression"){const pe=re;J.set(pe.key.value,this.evaluate(pe.value,ee))}else{if(J.size>0)throw new Error("Positional arguments must come before keyword arguments");W.push(this.evaluate(re,ee))}return[W,J]}applyFilter(S,ee,W){if(ee.type==="Identifier"){const J=ee;if(J.value==="tojson")return new _e(qe(S));if(S instanceof Ve)switch(J.value){case"list":return S;case"first":return S.value[0];case"last":return S.value[S.value.length-1];case"length":return new xe(S.value.length);case"reverse":return new Ve(S.value.slice().reverse());case"sort":return new Ve(S.value.slice().sort((re,pe)=>wt(re,pe,!1)));case"join":return new _e(S.value.map(re=>re.value).join(""));case"string":return new _e(qe(S,null,0,!1));case"unique":{const re=new Set,pe=[];for(const Se of S.value)re.has(Se.value)||(re.add(Se.value),pe.push(Se));return new Ve(pe)}default:throw new Error(`Unknown ArrayValue filter: ${J.value}`)}else if(S instanceof _e)switch(J.value){case"length":case"upper":case"lower":case"title":case"capitalize":{const re=S.builtins.get(J.value);if(re instanceof ye)return re.value([],W);if(re instanceof xe)return re;throw new Error(`Unknown StringValue filter: ${J.value}`)}case"trim":return new _e(S.value.trim());case"indent":return new _e(S.value.split(`
`).map((re,pe)=>pe===0||re.length===0?re:"    "+re).join(`
`));case"join":case"string":return S;case"int":{const re=parseInt(S.value,10);return new xe(isNaN(re)?0:re)}case"float":{const re=parseFloat(S.value);return new ze(isNaN(re)?0:re)}default:throw new Error(`Unknown StringValue filter: ${J.value}`)}else if(S instanceof xe||S instanceof ze)switch(J.value){case"abs":return S instanceof xe?new xe(Math.abs(S.value)):new ze(Math.abs(S.value));case"int":return new xe(Math.floor(S.value));case"float":return new ze(S.value);default:throw new Error(`Unknown NumericValue filter: ${J.value}`)}else if(S instanceof Ne)switch(J.value){case"items":return new Ve(Array.from(S.value.entries()).map(([re,pe])=>new Ve([new _e(re),pe])));case"length":return new xe(S.value.size);default:{const re=S.builtins.get(J.value);if(re)return re instanceof ye?re.value([],W):re;throw new Error(`Unknown ObjectValue filter: ${J.value}`)}}else if(S instanceof Le)switch(J.value){case"bool":return new Le(S.value);case"int":return new xe(S.value?1:0);case"float":return new ze(S.value?1:0);case"string":return new _e(S.value?"true":"false");default:throw new Error(`Unknown BooleanValue filter: ${J.value}`)}throw new Error(`Cannot apply filter "${J.value}" to type: ${S.type}`)}else if(ee.type==="CallExpression"){const J=ee;if(J.callee.type!=="Identifier")throw new Error(`Unknown filter: ${J.callee.type}`);const re=J.callee.value;if(re==="tojson"){const[,pe]=this.evaluateArguments(J.args,W),Se=pe.get("indent")??new Pe;if(!(Se instanceof xe||Se instanceof Pe))throw new Error("If set, indent must be a number");return new _e(qe(S,Se.value))}else if(re==="join"){let pe;if(S instanceof _e)pe=Array.from(S.value);else if(S instanceof Ve)pe=S.value.map(Ze=>Ze.value);else throw new Error(`Cannot apply filter "${re}" to type: ${S.type}`);const[Se,it]=this.evaluateArguments(J.args,W),gt=Se.at(0)??it.get("separator")??new _e("");if(!(gt instanceof _e))throw new Error("separator must be a string");return new _e(pe.join(gt.value))}else if(re==="int"||re==="float"){const[pe,Se]=this.evaluateArguments(J.args,W),it=pe.at(0)??Se.get("default")??(re==="int"?new xe(0):new ze(0));if(S instanceof _e){const gt=re==="int"?parseInt(S.value,10):parseFloat(S.value);return isNaN(gt)?it:re==="int"?new xe(gt):new ze(gt)}else{if(S instanceof xe||S instanceof ze)return S;if(S instanceof Le)return re==="int"?new xe(S.value?1:0):new ze(S.value?1:0);throw new Error(`Cannot apply filter "${re}" to type: ${S.type}`)}}else if(re==="default"){const[pe,Se]=this.evaluateArguments(J.args,W),it=pe[0]??new _e(""),gt=pe[1]??Se.get("boolean")??new Le(!1);if(!(gt instanceof Le))throw new Error("`default` filter flag must be a boolean");return S instanceof ve||gt.value&&!S.__bool__().value?it:S}if(S instanceof Ve){switch(re){case"sort":{const[pe,Se]=this.evaluateArguments(J.args,W),it=pe.at(0)??Se.get("reverse")??new Le(!1);if(!(it instanceof Le))throw new Error("reverse must be a boolean");const gt=pe.at(1)??Se.get("case_sensitive")??new Le(!1);if(!(gt instanceof Le))throw new Error("case_sensitive must be a boolean");const Ze=pe.at(2)??Se.get("attribute")??new Pe;if(!(Ze instanceof _e||Ze instanceof xe||Ze instanceof Pe))throw new Error("attribute must be a string, integer, or null");const At=et=>{if(Ze instanceof Pe)return et;const Lt=Ze instanceof xe?String(Ze.value):Ze.value;return zt(et,Lt)};return new Ve(S.value.slice().sort((et,Lt)=>{const Rt=At(et),Fn=At(Lt),Cn=wt(Rt,Fn,gt.value);return it.value?-Cn:Cn}))}case"selectattr":case"rejectattr":{const pe=re==="selectattr";if(S.value.some(et=>!(et instanceof Ne)))throw new Error(`\`${re}\` can only be applied to array of objects`);if(J.args.some(et=>et.type!=="StringLiteral"))throw new Error(`arguments of \`${re}\` must be strings`);const[Se,it,gt]=J.args.map(et=>this.evaluate(et,W));let Ze;if(it){const et=W.tests.get(it.value);if(!et)throw new Error(`Unknown test: ${it.value}`);Ze=et}else Ze=(...et)=>et[0].__bool__().value;const At=S.value.filter(et=>{const Lt=et.value.get(Se.value),Rt=Lt?Ze(Lt,gt):!1;return pe?Rt:!Rt});return new Ve(At)}case"map":{const[,pe]=this.evaluateArguments(J.args,W);if(pe.has("attribute")){const Se=pe.get("attribute");if(!(Se instanceof _e))throw new Error("attribute must be a string");const it=pe.get("default"),gt=S.value.map(Ze=>{if(!(Ze instanceof Ne))throw new Error("items in map must be an object");const At=zt(Ze,Se.value);return At instanceof ve?it??new ve:At});return new Ve(gt)}else throw new Error("`map` expressions without `attribute` set are not currently supported.")}}throw new Error(`Unknown ArrayValue filter: ${re}`)}else if(S instanceof _e){switch(re){case"indent":{const[pe,Se]=this.evaluateArguments(J.args,W),it=pe.at(0)??Se.get("width")??new xe(4);if(!(it instanceof xe))throw new Error("width must be a number");const gt=pe.at(1)??Se.get("first")??new Le(!1),Ze=pe.at(2)??Se.get("blank")??new Le(!1),At=S.value.split(`
`),et=" ".repeat(it.value),Lt=At.map((Rt,Fn)=>!gt.value&&Fn===0||!Ze.value&&Rt.length===0?Rt:et+Rt);return new _e(Lt.join(`
`))}case"replace":{const pe=S.builtins.get("replace");if(!(pe instanceof ye))throw new Error("replace filter not available");const[Se,it]=this.evaluateArguments(J.args,W);return pe.value([...Se,new ot(it)],W)}}throw new Error(`Unknown StringValue filter: ${re}`)}else if(S instanceof Ne){const pe=S.builtins.get(re);if(pe&&pe instanceof ye){const[Se,it]=this.evaluateArguments(J.args,W);return it.size>0&&Se.push(new ot(it)),pe.value(Se,W)}throw new Error(`Unknown ObjectValue filter: ${re}`)}else throw new Error(`Cannot apply filter "${re}" to type: ${S.type}`)}throw new Error(`Unknown filter: ${ee.type}`)}evaluateFilterExpression(S,ee){const W=this.evaluate(S.operand,ee);return this.applyFilter(W,S.filter,ee)}evaluateTestExpression(S,ee){const W=this.evaluate(S.operand,ee),J=ee.tests.get(S.test.value);if(!J)throw new Error(`Unknown test: ${S.test.value}`);const re=J(W);return new Le(S.negate?!re:re)}evaluateSelectExpression(S,ee){return this.evaluate(S.test,ee).__bool__().value?this.evaluate(S.lhs,ee):new ve}evaluateUnaryExpression(S,ee){const W=this.evaluate(S.argument,ee);if(S.operator.value==="not")return new Le(!W.value);throw new SyntaxError(`Unknown operator: ${S.operator.value}`)}evaluateTernaryExpression(S,ee){return this.evaluate(S.condition,ee).__bool__().value?this.evaluate(S.trueExpr,ee):this.evaluate(S.falseExpr,ee)}evalProgram(S,ee){return this.evaluateBlock(S.body,ee)}evaluateBlock(S,ee){let W="";for(const J of S){const re=this.evaluate(J,ee);re.type!=="NullValue"&&re.type!=="UndefinedValue"&&(W+=re.toString())}return new _e(W)}evaluateIdentifier(S,ee){return ee.lookupVariable(S.value)}evaluateCallExpression(S,ee){const[W,J]=this.evaluateArguments(S.args,ee);J.size>0&&W.push(new ot(J));const re=this.evaluate(S.callee,ee);if(re.type!=="FunctionValue")throw new Error(`Cannot call something that is not a function: got ${re.type}`);return re.value(W,ee)}evaluateSliceExpression(S,ee,W){if(!(S instanceof Ve||S instanceof _e))throw new Error("Slice object must be an array or string");const J=this.evaluate(ee.start,W),re=this.evaluate(ee.stop,W),pe=this.evaluate(ee.step,W);if(!(J instanceof xe||J instanceof ve))throw new Error("Slice start must be numeric or undefined");if(!(re instanceof xe||re instanceof ve))throw new Error("Slice stop must be numeric or undefined");if(!(pe instanceof xe||pe instanceof ve))throw new Error("Slice step must be numeric or undefined");return S instanceof Ve?new Ve(Be(S.value,J.value,re.value,pe.value)):new _e(Be(Array.from(S.value),J.value,re.value,pe.value).join(""))}evaluateMemberExpression(S,ee){const W=this.evaluate(S.object,ee);let J;if(S.computed){if(S.property.type==="SliceExpression")return this.evaluateSliceExpression(W,S.property,ee);J=this.evaluate(S.property,ee)}else J=new _e(S.property.value);let re;if(W instanceof Ne){if(!(J instanceof _e))throw new Error(`Cannot access property with non-string: got ${J.type}`);re=W.value.get(J.value)??W.builtins.get(J.value)}else if(W instanceof Ve||W instanceof _e)if(J instanceof xe)re=W.value.at(J.value),W instanceof _e&&(re=new _e(W.value.at(J.value)));else if(J instanceof _e)re=W.builtins.get(J.value);else throw new Error(`Cannot access property with non-string/non-number: got ${J.type}`);else{if(!(J instanceof _e))throw new Error(`Cannot access property with non-string: got ${J.type}`);re=W.builtins.get(J.value)}return re instanceof De?re:new ve}evaluateSet(S,ee){const W=S.value?this.evaluate(S.value,ee):this.evaluateBlock(S.body,ee);if(S.assignee.type==="Identifier"){const J=S.assignee.value;ee.setVariable(J,W)}else if(S.assignee.type==="TupleLiteral"){const J=S.assignee;if(!(W instanceof Ve))throw new Error(`Cannot unpack non-iterable type in set: ${W.type}`);const re=W.value;if(re.length!==J.value.length)throw new Error(`Too ${J.value.length>re.length?"few":"many"} items to unpack in set`);for(let pe=0;pe<J.value.length;++pe){const Se=J.value[pe];if(Se.type!=="Identifier")throw new Error(`Cannot unpack to non-identifier in set: ${Se.type}`);ee.setVariable(Se.value,re[pe])}}else if(S.assignee.type==="MemberExpression"){const J=S.assignee,re=this.evaluate(J.object,ee);if(!(re instanceof Ne))throw new Error("Cannot assign to member of non-object");if(J.property.type!=="Identifier")throw new Error("Cannot assign to member with non-identifier property");re.value.set(J.property.value,W)}else throw new Error(`Invalid LHS inside assignment expression: ${JSON.stringify(S.assignee)}`);return new Pe}evaluateIf(S,ee){const W=this.evaluate(S.test,ee);return this.evaluateBlock(W.__bool__().value?S.body:S.alternate,ee)}evaluateFor(S,ee){const W=new Qe(ee);let J,re;if(S.iterable.type==="SelectExpression"){const Ze=S.iterable;re=this.evaluate(Ze.lhs,W),J=Ze.test}else re=this.evaluate(S.iterable,W);if(!(re instanceof Ve||re instanceof Ne))throw new Error(`Expected iterable or object type in for loop: got ${re.type}`);re instanceof Ne&&(re=re.keys());const pe=[],Se=[];for(let Ze=0;Ze<re.value.length;++Ze){const At=new Qe(W),et=re.value[Ze];let Lt;if(S.loopvar.type==="Identifier")Lt=Rt=>Rt.setVariable(S.loopvar.value,et);else if(S.loopvar.type==="TupleLiteral"){const Rt=S.loopvar;if(et.type!=="ArrayValue")throw new Error(`Cannot unpack non-iterable type: ${et.type}`);const Fn=et;if(Rt.value.length!==Fn.value.length)throw new Error(`Too ${Rt.value.length>Fn.value.length?"few":"many"} items to unpack`);Lt=Cn=>{for(let bn=0;bn<Rt.value.length;++bn){if(Rt.value[bn].type!=="Identifier")throw new Error(`Cannot unpack non-identifier type: ${Rt.value[bn].type}`);Cn.setVariable(Rt.value[bn].value,Fn.value[bn])}}}else throw new Error(`Invalid loop variable(s): ${S.loopvar.type}`);J&&(Lt(At),!this.evaluate(J,At).__bool__().value)||(pe.push(et),Se.push(Lt))}let it="",gt=!0;for(let Ze=0;Ze<pe.length;++Ze){const At=new Map([["index",new xe(Ze+1)],["index0",new xe(Ze)],["revindex",new xe(pe.length-Ze)],["revindex0",new xe(pe.length-Ze-1)],["first",new Le(Ze===0)],["last",new Le(Ze===pe.length-1)],["length",new xe(pe.length)],["previtem",Ze>0?pe[Ze-1]:new ve],["nextitem",Ze<pe.length-1?pe[Ze+1]:new ve]]);W.setVariable("loop",new Ne(At)),Se[Ze](W);try{const et=this.evaluateBlock(S.body,W);it+=et.value}catch(et){if(et instanceof Ee)continue;if(et instanceof Ie)break;throw et}gt=!1}if(gt){const Ze=this.evaluateBlock(S.defaultBlock,W);it+=Ze.value}return new _e(it)}evaluateMacro(S,ee){return ee.setVariable(S.name.value,new ye((W,J)=>{const re=new Qe(J);W=W.slice();let pe;W.at(-1)?.type==="KeywordArgumentsValue"&&(pe=W.pop());for(let Se=0;Se<S.args.length;++Se){const it=S.args[Se],gt=W[Se];if(it.type==="Identifier"){const Ze=it;if(!gt)throw new Error(`Missing positional argument: ${Ze.value}`);re.setVariable(Ze.value,gt)}else if(it.type==="KeywordArgumentExpression"){const Ze=it,At=gt??pe?.value.get(Ze.key.value)??this.evaluate(Ze.value,re);re.setVariable(Ze.key.value,At)}else throw new Error(`Unknown argument type: ${it.type}`)}return this.evaluateBlock(S.body,re)})),new Pe}evaluateCallStatement(S,ee){const W=new ye((it,gt)=>{const Ze=new Qe(gt);if(S.callerArgs)for(let At=0;At<S.callerArgs.length;++At){const et=S.callerArgs[At];if(et.type!=="Identifier")throw new Error(`Caller parameter must be an identifier, got ${et.type}`);Ze.setVariable(et.value,it[At]??new ve)}return this.evaluateBlock(S.body,Ze)}),[J,re]=this.evaluateArguments(S.call.args,ee);J.push(new ot(re));const pe=this.evaluate(S.call.callee,ee);if(pe.type!=="FunctionValue")throw new Error(`Cannot call something that is not a function: got ${pe.type}`);const Se=new Qe(ee);return Se.setVariable("caller",W),pe.value(J,Se)}evaluateFilterStatement(S,ee){const W=this.evaluateBlock(S.body,ee);return this.applyFilter(W,S.filter,ee)}evaluate(S,ee){if(!S)return new ve;switch(S.type){case"Program":return this.evalProgram(S,ee);case"Set":return this.evaluateSet(S,ee);case"If":return this.evaluateIf(S,ee);case"For":return this.evaluateFor(S,ee);case"Macro":return this.evaluateMacro(S,ee);case"CallStatement":return this.evaluateCallStatement(S,ee);case"Break":throw new Ie;case"Continue":throw new Ee;case"IntegerLiteral":return new xe(S.value);case"FloatLiteral":return new ze(S.value);case"StringLiteral":return new _e(S.value);case"ArrayLiteral":return new Ve(S.value.map(W=>this.evaluate(W,ee)));case"TupleLiteral":return new de(S.value.map(W=>this.evaluate(W,ee)));case"ObjectLiteral":{const W=new Map;for(const[J,re]of S.value){const pe=this.evaluate(J,ee);if(!(pe instanceof _e))throw new Error(`Object keys must be strings: got ${pe.type}`);W.set(pe.value,this.evaluate(re,ee))}return new Ne(W)}case"Identifier":return this.evaluateIdentifier(S,ee);case"CallExpression":return this.evaluateCallExpression(S,ee);case"MemberExpression":return this.evaluateMemberExpression(S,ee);case"UnaryExpression":return this.evaluateUnaryExpression(S,ee);case"BinaryExpression":return this.evaluateBinaryExpression(S,ee);case"FilterExpression":return this.evaluateFilterExpression(S,ee);case"FilterStatement":return this.evaluateFilterStatement(S,ee);case"TestExpression":return this.evaluateTestExpression(S,ee);case"SelectExpression":return this.evaluateSelectExpression(S,ee);case"Ternary":return this.evaluateTernaryExpression(S,ee);case"Comment":return new Pe;default:throw new SyntaxError(`Unknown node type: ${S.type}`)}}};function Tn(S){switch(typeof S){case"number":return Number.isInteger(S)?new xe(S):new ze(S);case"string":return new _e(S);case"boolean":return new Le(S);case"undefined":return new ve;case"object":return S===null?new Pe:Array.isArray(S)?new Ve(S.map(Tn)):new Ne(new Map(Object.entries(S).map(([ee,W])=>[ee,Tn(W)])));case"function":return new ye((ee,W)=>{const J=S(...ee.map(re=>re.value))??null;return Tn(J)});default:throw new Error(`Cannot convert to runtime value: ${S}`)}}var kt=`
`,Pn="{%- ",Zn=" -%}";function Hs(S){switch(S.operator.type){case"MultiplicativeBinaryOperator":return 4;case"AdditiveBinaryOperator":return 3;case"ComparisonBinaryOperator":return 2;case"Identifier":return S.operator.value==="and"?1:S.operator.value==="in"||S.operator.value==="not in"?2:0}return 0}function Ks(S,ee="	"){const W=typeof ee=="number"?" ".repeat(ee):ee;return Kn(S.body,0,W).replace(/\n$/,"")}function xn(...S){return Pn+S.join(" ")+Zn}function Kn(S,ee,W){return S.map(J=>Ar(J,ee,W)).join(kt)}function Ar(S,ee,W){const J=W.repeat(ee);switch(S.type){case"Program":return Kn(S.body,ee,W);case"If":return Ls(S,ee,W);case"For":return vt(S,ee,W);case"Set":return rn(S,ee,W);case"Macro":return hs(S,ee,W);case"Break":return J+xn("break");case"Continue":return J+xn("continue");case"CallStatement":return ms(S,ee,W);case"FilterStatement":return _r(S,ee,W);case"Comment":return J+"{# "+S.value+" #}";default:return J+"{{- "+It(S)+" -}}"}}function Ls(S,ee,W){const J=W.repeat(ee),re=[];let pe=S;for(;pe&&(re.push({test:pe.test,body:pe.body}),pe.alternate.length===1&&pe.alternate[0].type==="If");)pe=pe.alternate[0];let Se=J+xn("if",It(re[0].test))+kt+Kn(re[0].body,ee+1,W);for(let it=1;it<re.length;++it)Se+=kt+J+xn("elif",It(re[it].test))+kt+Kn(re[it].body,ee+1,W);return pe&&pe.alternate.length>0&&(Se+=kt+J+xn("else")+kt+Kn(pe.alternate,ee+1,W)),Se+=kt+J+xn("endif"),Se}function vt(S,ee,W){const J=W.repeat(ee);let re="";if(S.iterable.type==="SelectExpression"){const Se=S.iterable;re=`${It(Se.lhs)} if ${It(Se.test)}`}else re=It(S.iterable);let pe=J+xn("for",It(S.loopvar),"in",re)+kt+Kn(S.body,ee+1,W);return S.defaultBlock.length>0&&(pe+=kt+J+xn("else")+kt+Kn(S.defaultBlock,ee+1,W)),pe+=kt+J+xn("endfor"),pe}function rn(S,ee,W){const J=W.repeat(ee),re=It(S.assignee),pe=S.value?It(S.value):"",Se=J+xn("set",`${re}${S.value?" = "+pe:""}`);return S.body.length===0?Se:Se+kt+Kn(S.body,ee+1,W)+kt+J+xn("endset")}function hs(S,ee,W){const J=W.repeat(ee),re=S.args.map(It).join(", ");return J+xn("macro",`${S.name.value}(${re})`)+kt+Kn(S.body,ee+1,W)+kt+J+xn("endmacro")}function ms(S,ee,W){const J=W.repeat(ee),re=S.callerArgs&&S.callerArgs.length>0?`(${S.callerArgs.map(It).join(", ")})`:"",pe=It(S.call);let Se=J+xn(`call${re}`,pe)+kt;return Se+=Kn(S.body,ee+1,W)+kt,Se+=J+xn("endcall"),Se}function _r(S,ee,W){const J=W.repeat(ee),re=S.filter.type==="Identifier"?S.filter.value:It(S.filter);let pe=J+xn("filter",re)+kt;return pe+=Kn(S.body,ee+1,W)+kt,pe+=J+xn("endfilter"),pe}function It(S,ee=-1){switch(S.type){case"SpreadExpression":return`*${It(S.argument)}`;case"Identifier":return S.value;case"IntegerLiteral":return`${S.value}`;case"FloatLiteral":return`${S.value}`;case"StringLiteral":return JSON.stringify(S.value);case"BinaryExpression":{const W=S,J=Hs(W),re=It(W.left,J),pe=It(W.right,J+1),Se=`${re} ${W.operator.value} ${pe}`;return J<ee?`(${Se})`:Se}case"UnaryExpression":{const W=S;return W.operator.value+(W.operator.value==="not"?" ":"")+It(W.argument,1/0)}case"CallExpression":{const W=S,J=W.args.map(It).join(", ");return`${It(W.callee)}(${J})`}case"MemberExpression":{const W=S;let J=It(W.object);["Identifier","MemberExpression","CallExpression","StringLiteral","IntegerLiteral","FloatLiteral","ArrayLiteral","TupleLiteral","ObjectLiteral"].includes(W.object.type)||(J=`(${J})`);let re=It(W.property);return!W.computed&&W.property.type!=="Identifier"&&(re=`(${re})`),W.computed?`${J}[${re}]`:`${J}.${re}`}case"FilterExpression":{const W=S,J=It(W.operand,1/0);return W.filter.type==="CallExpression"?`${J} | ${It(W.filter)}`:`${J} | ${W.filter.value}`}case"SelectExpression":{const W=S;return`${It(W.lhs)} if ${It(W.test)}`}case"TestExpression":{const W=S;return`${It(W.operand)} is${W.negate?" not":""} ${W.test.value}`}case"ArrayLiteral":case"TupleLiteral":{const W=S.value.map(It),J=S.type==="ArrayLiteral"?"[]":"()";return`${J[0]}${W.join(", ")}${J[1]}`}case"ObjectLiteral":return`{${Array.from(S.value.entries()).map(([J,re])=>`${It(J)}: ${It(re)}`).join(", ")}}`;case"SliceExpression":{const W=S,J=W.start?It(W.start):"",re=W.stop?It(W.stop):"",pe=W.step?`:${It(W.step)}`:"";return`${J}:${re}${pe}`}case"KeywordArgumentExpression":{const W=S;return`${W.key.value}=${It(W.value)}`}case"Ternary":{const W=S,J=`${It(W.trueExpr)} if ${It(W.condition,0)} else ${It(W.falseExpr)}`;return ee>-1?`(${J})`:J}default:throw new Error(`Unknown expression type: ${S.type}`)}}var fs=class{parsed;constructor(S){const ee=u(S,{lstrip_blocks:!0,trim_blocks:!0});this.parsed=$e(ee)}render(S){const ee=new Qe;if(ct(ee),S)for(const[re,pe]of Object.entries(S))ee.set(re,pe);return new on(ee).run(this.parsed).value}format(S){return Ks(this.parsed,S?.indent||"	")}}}),"./src/backends/onnx.js":((e,t,n)=>{var r;n.r(t),n.d(t,{Tensor:()=>a.Tensor,createInferenceSession:()=>I,deviceToExecutionProviders:()=>m,isONNXProxy:()=>b,isONNXTensor:()=>M,runInferenceSession:()=>T});var s=n("./src/env.js"),i=n("?2ce3"),o=n("onnxruntime-web"),a=n("onnxruntime-common");const l=Object.freeze({auto:null,gpu:null,cpu:"cpu",wasm:"wasm",webgpu:"webgpu",cuda:"cuda",dml:"dml",webnn:{name:"webnn",deviceType:"cpu"},"webnn-npu":{name:"webnn",deviceType:"npu"},"webnn-gpu":{name:"webnn",deviceType:"gpu"},"webnn-cpu":{name:"webnn",deviceType:"cpu"}}),c=[];let d,u;const p=Symbol.for("onnxruntime");if(p in globalThis)u=globalThis[p];else if(s.apis.IS_NODE_ENV){switch(u=i??(r||(r=n.t(i,2))),process.platform){case"win32":c.push("dml");break;case"linux":process.arch==="x64"&&c.push("cuda");break}c.push("cpu"),d=["cpu"]}else u=o,s.apis.IS_WEBNN_AVAILABLE&&c.push("webnn-npu","webnn-gpu","webnn-cpu","webnn"),s.apis.IS_WEBGPU_AVAILABLE&&c.push("webgpu"),c.push("wasm"),d=["wasm"];const h=u.InferenceSession;function m(A=null){if(!A)return d;switch(A){case"auto":return c;case"gpu":return c.filter(k=>["webgpu","cuda","dml","webnn-gpu"].includes(k))}if(c.includes(A))return[l[A]??A];throw new Error(`Unsupported device: "${A}". Should be one of: ${c.join(", ")}.`)}let g=null;async function I(A,k,F){g&&await g;const L=h.create(A,k);g??=L;const G=await L;return G.config=F,G}let f=Promise.resolve();const _=s.apis.IS_BROWSER_ENV||s.apis.IS_WEBWORKER_ENV;async function T(A,k){const F=()=>A.run(k);return await(_?f=f.then(F):F())}function M(A){return A instanceof u.Tensor}const v=u?.env;v?.wasm&&(!(typeof ServiceWorkerGlobalScope<"u"&&self instanceof ServiceWorkerGlobalScope)&&!v.wasm.wasmPaths&&(v.wasm.wasmPaths=`https://cdn.jsdelivr.net/npm/@huggingface/transformers@${s.env.version}/dist/`),v.wasm.proxy=!1),v?.webgpu&&(v.webgpu.powerPreference="high-performance");function b(){return v?.wasm?.proxy}s.env.backends.onnx=v}),"./src/base/feature_extraction_utils.js":((e,t,n)=>{n.r(t),n.d(t,{FeatureExtractor:()=>o,validate_audio_inputs:()=>a});var r=n("./src/utils/constants.js"),s=n("./src/utils/generic.js"),i=n("./src/utils/hub.js");class o extends s.Callable{constructor(c){super(),this.config=c}static async from_pretrained(c,d={}){const u=await(0,i.getModelJSON)(c,r.FEATURE_EXTRACTOR_NAME,!0,d);return new this(u)}}function a(l,c){if(!(l instanceof Float32Array||l instanceof Float64Array))throw new Error(`${c} expects input to be a Float32Array or a Float64Array, but got ${l?.constructor?.name??typeof l} instead. If using the feature extractor directly, remember to use \`read_audio(url, sampling_rate)\` to obtain the raw audio data of the file/url.`)}}),"./src/base/image_processors_utils.js":((e,t,n)=>{n.r(t),n.d(t,{ImageProcessor:()=>M,center_to_corners_format:()=>u,post_process_instance_segmentation:()=>T,post_process_object_detection:()=>p,post_process_panoptic_segmentation:()=>_,post_process_semantic_segmentation:()=>h});var r=n("./src/utils/generic.js"),s=n("./src/utils/tensor.js"),i=n("./src/utils/maths.js");n("./src/utils/image.js");var o=n("./src/utils/core.js"),a=n("./src/utils/hub.js"),l=n("./src/utils/constants.js");function c(v,b,A=0,k=null){const F=v/b;let L=(0,i.bankers_round)(F)*b;return k!==null&&L>k&&(L=Math.floor(F)*b),L<A&&(L=Math.ceil(F)*b),L}function d([v,b],A){return[Math.max(Math.floor(v/A),1)*A,Math.max(Math.floor(b/A),1)*A]}function u([v,b,A,k]){return[v-A/2,b-k/2,v+A/2,b+k/2]}function p(v,b=.5,A=null,k=!1){const F=v.logits,L=v.pred_boxes,[G,j,R]=F.dims;if(A!==null&&A.length!==G)throw Error("Make sure that you pass in as many target sizes as the batch dimension of the logits");let K=[];for(let U=0;U<G;++U){let Y=A!==null?A[U]:null,te={boxes:[],classes:[],scores:[]},ne=F[U],le=L[U];for(let N=0;N<j;++N){let oe=ne[N],X=[],D;if(k){D=oe.sigmoid().data;for(let z=0;z<D.length;++z)D[z]>b&&X.push(z)}else{let z=(0,i.max)(oe.data)[1];if(z===R-1||(D=(0,i.softmax)(oe.data),D[z]<b))continue;X.push(z)}for(const z of X){let se=le[N].data;se=u(se),Y!==null&&(se=se.map((me,$e)=>me*Y[($e+1)%2])),te.boxes.push(se),te.classes.push(z),te.scores.push(D[z])}}K.push(te)}return K}function h(v,b=null){const A=v.logits,k=A.dims[0];if(b!==null&&b.length!==k)throw Error("Make sure that you pass in as many target sizes as the batch dimension of the logits");const F=[];for(let L=0;L<k;++L){const G=b!==null?b[L]:null;let j=A[L];G!==null&&(j=(0,s.interpolate)(j,G,"bilinear",!1));const[R,K]=G??j.dims.slice(-2),U=new s.Tensor("int32",new Int32Array(R*K),[R,K]),Y=j[0].data,te=U.data;for(let N=1;N<j.dims[0];++N){const oe=j[N].data;for(let X=0;X<oe.length;++X)oe[X]>Y[X]&&(Y[X]=oe[X],te[X]=N)}const ne=new Array(j.dims[0]);for(let N=0;N<te.length;++N){const oe=te[N];ne[oe]=oe}const le=ne.filter(N=>N!==void 0);F.push({segmentation:U,labels:le})}return F}function m(v,b,A,k){const F=[],L=[],G=[];for(let j=0;j<v.dims[0];++j){const R=v[j],K=b[j],U=(0,i.max)(R.data)[1];if(U===k)continue;const te=(0,i.softmax)(R.data)[U];te>A&&(F.push(K),L.push(te),G.push(U))}return[F,L,G]}function g(v,b,A,k=.5,F=.8){const L=[];let G=0,j=0;const R=b[A].data;for(let U=0;U<v.length;++U)v[U]===A&&(L.push(U),++G),R[U]>=k&&++j;let K=G>0&&j>0;return K&&(K=G/j>F),[K,L]}function I(v,b,A,k,F,L=null,G=null){const[j,R]=G??v[0].dims,K=new s.Tensor("int32",new Int32Array(j*R),[j,R]),U=[];if(G!==null)for(let N=0;N<v.length;++N)v[N]=(0,s.interpolate)(v[N],G,"bilinear",!1);const Y=new Int32Array(v[0].data.length),te=new Float32Array(v[0].data.length);for(let N=0;N<v.length;++N){let oe=b[N];const X=v[N].data;for(let D=0;D<X.length;++D)X[D]*=oe,X[D]>te[D]&&(Y[D]=N,te[D]=X[D])}let ne=0;const le=K.data;for(let N=0;N<A.length;++N){const oe=A[N],[X,D]=g(Y,v,N,k,F);if(X){++ne;for(const z of D)le[z]=ne;U.push({id:ne,label_id:oe,score:b[N]})}}return[K,U]}function f(v,b,A=28,k=3136,F=784*1280){if(v<A||b<A)throw new Error(`height:${v} or width:${b} must be larger than factor:${A}`);if(Math.max(v,b)/Math.min(v,b)>200)throw new Error(`absolute aspect ratio must be smaller than 200, got ${Math.max(v,b)/Math.min(v,b)}`);let L=Math.round(v/A)*A,G=Math.round(b/A)*A;if(L*G>F){const j=Math.sqrt(v*b/F);L=Math.floor(v/j/A)*A,G=Math.floor(b/j/A)*A}else if(L*G<k){const j=Math.sqrt(k/(v*b));L=Math.ceil(v*j/A)*A,G=Math.ceil(b*j/A)*A}return[L,G]}function _(v,b=.5,A=.5,k=.8,F=null,L=null){F===null&&(console.warn("`label_ids_to_fuse` unset. No instance will be fused."),F=new Set);const G=v.class_queries_logits??v.logits,R=(v.masks_queries_logits??v.pred_masks).sigmoid();let[K,U,Y]=G.dims;if(Y-=1,L!==null&&L.length!==K)throw Error("Make sure that you pass in as many target sizes as the batch dimension of the logits");let te=[];for(let ne=0;ne<K;++ne){let le=L!==null?L[ne]:null,N=G[ne],oe=R[ne],[X,D,z]=m(N,oe,b,Y);if(z.length===0){let[$e,ke]=le??oe.dims.slice(-2),Be=new s.Tensor("int32",new Int32Array($e*ke).fill(-1),[$e,ke]);te.push({segmentation:Be,segments_info:[]});continue}let[se,me]=I(X,D,z,A,k,F,le);te.push({segmentation:se,segments_info:me})}return te}function T(v,b=.5,A=null){throw new Error("`post_process_instance_segmentation` is not yet implemented.")}class M extends r.Callable{constructor(b){super(),this.image_mean=b.image_mean??b.mean,this.image_std=b.image_std??b.std,this.resample=b.resample??2,this.do_rescale=b.do_rescale??!0,this.rescale_factor=b.rescale_factor??1/255,this.do_normalize=b.do_normalize,this.do_thumbnail=b.do_thumbnail,this.size=b.size??b.image_size,this.do_resize=b.do_resize??this.size!==void 0,this.size_divisibility=b.size_divisibility??b.size_divisor,this.do_center_crop=b.do_center_crop,this.crop_size=b.crop_size,this.do_convert_rgb=b.do_convert_rgb??!0,this.do_crop_margin=b.do_crop_margin,this.pad_size=b.pad_size,this.do_pad=b.do_pad,this.min_pixels=b.min_pixels,this.max_pixels=b.max_pixels,this.do_pad&&!this.pad_size&&this.size&&this.size.width!==void 0&&this.size.height!==void 0&&(this.pad_size=this.size),this.do_flip_channel_order=b.do_flip_channel_order??!1,this.config=b}async thumbnail(b,A,k=2){const F=b.height,L=b.width,G=A.height,j=A.width;let R=Math.min(F,G),K=Math.min(L,j);return R===F&&K===L?b:(F>L?K=Math.floor(L*R/F):L>F&&(R=Math.floor(F*K/L)),await b.resize(K,R,{resample:k}))}async crop_margin(b,A=200){const k=b.clone().grayscale(),F=(0,i.min)(k.data)[0],G=(0,i.max)(k.data)[0]-F;if(G===0)return b;const j=A/255;let R=k.width,K=k.height,U=0,Y=0;const te=k.data;for(let ne=0;ne<k.height;++ne){const le=ne*k.width;for(let N=0;N<k.width;++N)(te[le+N]-F)/G<j&&(R=Math.min(R,N),K=Math.min(K,ne),U=Math.max(U,N),Y=Math.max(Y,ne))}return b=await b.crop([R,K,U,Y]),b}pad_image(b,A,k,{mode:F="constant",center:L=!1,constant_values:G=0}={}){const[j,R,K]=A;let U,Y;if(typeof k=="number"?(U=k,Y=k):k==="square"?U=Y=Math.max(j,R):(U=k.width,Y=k.height),U!==R||Y!==j){const te=new Float32Array(U*Y*K);if(Array.isArray(G))for(let N=0;N<te.length;++N)te[N]=G[N%K];else G!==0&&te.fill(G);const[ne,le]=L?[Math.floor((U-R)/2),Math.floor((Y-j)/2)]:[0,0];for(let N=0;N<j;++N){const oe=(N+le)*U,X=N*R;for(let D=0;D<R;++D){const z=(oe+D+ne)*K,se=(X+D)*K;for(let me=0;me<K;++me)te[z+me]=b[se+me]}}if(F==="symmetric"){if(L)throw new Error("`center` padding is not supported when `mode` is set to `symmetric`.");const N=j-1,oe=R-1;for(let X=0;X<Y;++X){const D=X*U,z=(0,o.calculateReflectOffset)(X,N)*R;for(let se=0;se<U;++se){if(X<j&&se<R)continue;const me=(D+se)*K,$e=(z+(0,o.calculateReflectOffset)(se,oe))*K;for(let ke=0;ke<K;++ke)te[me+ke]=b[$e+ke]}}}b=te,A=[Y,U,K]}return[b,A]}rescale(b){for(let A=0;A<b.length;++A)b[A]=this.rescale_factor*b[A]}get_resize_output_image_size(b,A){const[k,F]=b.size;let L,G;if(this.do_thumbnail){const{height:j,width:R}=A;L=Math.min(j,R)}else Number.isInteger(A)?(L=A,G=this.config.max_size??L):A!==void 0&&(L=A.shortest_edge,G=A.longest_edge);if(L!==void 0||G!==void 0){const j=L===void 0?1:Math.max(L/k,L/F),R=k*j,K=F*j,U=G===void 0?1:Math.min(G/R,G/K);let Y=Math.floor(Number((R*U).toFixed(2))),te=Math.floor(Number((K*U).toFixed(2)));return this.size_divisibility!==void 0&&([Y,te]=d([Y,te],this.size_divisibility)),[Y,te]}else if(A!==void 0&&A.width!==void 0&&A.height!==void 0){let j=A.width,R=A.height;if(this.config.keep_aspect_ratio&&this.config.ensure_multiple_of){let K=R/F,U=j/k;Math.abs(1-U)<Math.abs(1-K)?K=U:U=K,R=c(K*F,this.config.ensure_multiple_of),j=c(U*k,this.config.ensure_multiple_of)}return[j,R]}else{if(this.size_divisibility!==void 0)return d([k,F],this.size_divisibility);if(this.min_pixels!==void 0&&this.max_pixels!==void 0){const j=this.config.patch_size*this.config.merge_size;return f(F,k,j,this.min_pixels,this.max_pixels)}else throw new Error(`Could not resize image due to unsupported \`this.size\` option in config: ${JSON.stringify(A)}`)}}async resize(b){const[A,k]=this.get_resize_output_image_size(b,this.size);return await b.resize(A,k,{resample:this.resample})}async preprocess(b,{do_normalize:A=null,do_pad:k=null,do_convert_rgb:F=null,do_convert_grayscale:L=null,do_flip_channel_order:G=null}={}){this.do_crop_margin&&(b=await this.crop_margin(b));const[j,R]=b.size;if(F??this.do_convert_rgb?b=b.rgb():L&&(b=b.grayscale()),this.do_resize&&(b=await this.resize(b)),this.do_thumbnail&&(b=await this.thumbnail(b,this.size,this.resample)),this.do_center_crop){let ne,le;Number.isInteger(this.crop_size)?(ne=this.crop_size,le=this.crop_size):(ne=this.crop_size.width,le=this.crop_size.height),b=await b.center_crop(ne,le)}const K=[b.height,b.width];let U=Float32Array.from(b.data),Y=[b.height,b.width,b.channels];if(this.do_rescale&&this.rescale(U),A??this.do_normalize){let ne=this.image_mean;Array.isArray(this.image_mean)||(ne=new Array(b.channels).fill(ne));let le=this.image_std;if(Array.isArray(this.image_std)||(le=new Array(b.channels).fill(le)),ne.length!==b.channels||le.length!==b.channels)throw new Error(`When set to arrays, the length of \`image_mean\` (${ne.length}) and \`image_std\` (${le.length}) must match the number of channels in the image (${b.channels}).`);for(let N=0;N<U.length;N+=b.channels)for(let oe=0;oe<b.channels;++oe)U[N+oe]=(U[N+oe]-ne[oe])/le[oe]}if(k??this.do_pad){if(this.pad_size)[U,Y]=this.pad_image(U,[b.height,b.width,b.channels],this.pad_size);else if(this.size_divisibility){const[ne,le]=d([Y[1],Y[0]],this.size_divisibility);[U,Y]=this.pad_image(U,Y,{width:ne,height:le})}}if(G??this.do_flip_channel_order){if(Y[2]!==3)throw new Error("Flipping channel order is only supported for RGB images.");for(let ne=0;ne<U.length;ne+=3){const le=U[ne];U[ne]=U[ne+2],U[ne+2]=le}}const te=new s.Tensor("float32",U,Y).permute(2,0,1);return{original_size:[R,j],reshaped_input_size:K,pixel_values:te}}async _call(b,...A){Array.isArray(b)||(b=[b]);const k=await Promise.all(b.map(L=>this.preprocess(L)));return{pixel_values:(0,s.stack)(k.map(L=>L.pixel_values),0),original_sizes:k.map(L=>L.original_size),reshaped_input_sizes:k.map(L=>L.reshaped_input_size)}}static async from_pretrained(b,A={}){const k=await(0,a.getModelJSON)(b,l.IMAGE_PROCESSOR_NAME,!0,A);return new this(k)}}}),"./src/base/processing_utils.js":((e,t,n)=>{n.r(t),n.d(t,{Processor:()=>o});var r=n("./src/utils/constants.js"),s=n("./src/utils/generic.js"),i=n("./src/utils/hub.js");class o extends s.Callable{static classes=["image_processor_class","tokenizer_class","feature_extractor_class"];static uses_processor_config=!1;static uses_chat_template_file=!1;constructor(l,c,d){super(),this.config=l,this.components=c,this.chat_template=d}get image_processor(){return this.components.image_processor}get tokenizer(){return this.components.tokenizer}get feature_extractor(){return this.components.feature_extractor}apply_chat_template(l,c={}){if(!this.tokenizer)throw new Error("Unable to apply chat template without a tokenizer.");return this.tokenizer.apply_chat_template(l,{tokenize:!1,chat_template:this.chat_template??void 0,...c})}batch_decode(...l){if(!this.tokenizer)throw new Error("Unable to decode without a tokenizer.");return this.tokenizer.batch_decode(...l)}decode(...l){if(!this.tokenizer)throw new Error("Unable to decode without a tokenizer.");return this.tokenizer.decode(...l)}async _call(l,...c){for(const d of[this.image_processor,this.feature_extractor,this.tokenizer])if(d)return d(l,...c);throw new Error("No image processor, feature extractor, or tokenizer found.")}static async from_pretrained(l,c={}){const[d,u,p]=await Promise.all([this.uses_processor_config?(0,i.getModelJSON)(l,r.PROCESSOR_NAME,!0,c):{},Promise.all(this.classes.filter(h=>h in this).map(async h=>{const m=await this[h].from_pretrained(l,c);return[h.replace(/_class$/,""),m]})).then(Object.fromEntries),this.uses_chat_template_file?(0,i.getModelText)(l,r.CHAT_TEMPLATE_NAME,!0,c):null]);return new this(d,u,p)}}}),"./src/configs.js":((e,t,n)=>{n.r(t),n.d(t,{AutoConfig:()=>d,PretrainedConfig:()=>c,getCacheShapes:()=>a});var r=n("./src/utils/core.js"),s=n("./src/utils/hub.js");async function i(u,p){return await(0,s.getModelJSON)(u,"config.json",!0,p)}function o(u){const p={};let h={};switch(u.model_type){case"llava":case"paligemma":case"gemma3":case"florence2":case"llava_onevision":case"idefics3":case"ultravox":case"voxtral":case"smolvlm":case"gemma3n":case"mistral3":h=o(u.text_config);break;case"moondream1":h=o(u.phi_config);break;case"musicgen":h=o(u.decoder);break;case"multi_modality":h=o(u.language_config);break;case"gpt2":case"gptj":case"jais":case"codegen":case"gpt_bigcode":p.num_heads="n_head",p.num_layers="n_layer",p.hidden_size="n_embd";break;case"gpt_neox":case"stablelm":case"opt":case"falcon":case"modernbert-decoder":p.num_heads="num_attention_heads",p.num_layers="num_hidden_layers",p.hidden_size="hidden_size";break;case"llama":case"llama4_text":case"nanochat":case"arcee":case"lfm2":case"smollm3":case"olmo":case"olmo2":case"mobilellm":case"granite":case"granitemoehybrid":case"cohere":case"mistral":case"starcoder2":case"qwen2":case"qwen2_vl":case"phi":case"phi3":case"phi3_v":case"llava_qwen2":p.num_heads="num_key_value_heads",p.num_layers="num_hidden_layers",p.hidden_size="hidden_size",p.num_attention_heads="num_attention_heads",p.dim_kv="head_dim";break;case"qwen3":case"gemma":case"gemma2":case"vaultgemma":case"gemma3_text":case"gemma3n_text":case"glm":case"helium":case"ernie4_5":case"ministral":case"ministral3":p.num_heads="num_key_value_heads",p.num_layers="num_hidden_layers",p.dim_kv="head_dim";break;case"openelm":p.num_heads="num_kv_heads",p.num_layers="num_transformer_layers",p.dim_kv="head_dim";break;case"gpt_neo":case"donut-swin":p.num_heads="num_heads",p.num_layers="num_layers",p.hidden_size="hidden_size";break;case"bloom":p.num_heads="n_head",p.num_layers="n_layer",p.hidden_size="hidden_size";break;case"mpt":p.num_heads="n_heads",p.num_layers="n_layers",p.hidden_size="d_model";break;case"exaone":p.num_heads="num_key_value_heads",p.num_layers="num_layers",p.dim_kv="head_dim",p.num_attention_heads="num_attention_heads";break;case"t5":case"mt5":case"longt5":p.num_decoder_layers="num_decoder_layers",p.num_decoder_heads="num_heads",p.decoder_dim_kv="d_kv",p.num_encoder_layers="num_layers",p.num_encoder_heads="num_heads",p.encoder_dim_kv="d_kv";break;case"bart":case"mbart":case"marian":case"whisper":case"lite-whisper":case"m2m_100":case"blenderbot":case"blenderbot-small":case"florence2_language":p.num_decoder_layers="decoder_layers",p.num_decoder_heads="decoder_attention_heads",p.decoder_hidden_size="d_model",p.num_encoder_layers="encoder_layers",p.num_encoder_heads="encoder_attention_heads",p.encoder_hidden_size="d_model";break;case"speecht5":p.num_decoder_layers="decoder_layers",p.num_decoder_heads="decoder_attention_heads",p.decoder_hidden_size="hidden_size",p.num_encoder_layers="encoder_layers",p.num_encoder_heads="encoder_attention_heads",p.encoder_hidden_size="hidden_size";break;case"trocr":p.num_encoder_layers=p.num_decoder_layers="decoder_layers",p.num_encoder_heads=p.num_decoder_heads="decoder_attention_heads",p.encoder_hidden_size=p.decoder_hidden_size="d_model";break;case"musicgen_decoder":p.num_encoder_layers=p.num_decoder_layers="num_hidden_layers",p.num_encoder_heads=p.num_decoder_heads="num_attention_heads",p.encoder_hidden_size=p.decoder_hidden_size="hidden_size";break;case"moonshine":p.num_decoder_layers="decoder_num_hidden_layers",p.num_decoder_heads="decoder_num_key_value_heads",p.num_encoder_layers="encoder_num_hidden_layers",p.num_encoder_heads="encoder_num_key_value_heads",p.encoder_hidden_size=p.decoder_hidden_size="hidden_size";break;case"vision-encoder-decoder":const g=o(u.decoder),I="num_decoder_layers"in g,f=(0,r.pick)(u,["model_type","is_encoder_decoder"]);return I?(f.num_decoder_layers=g.num_decoder_layers,f.num_decoder_heads=g.num_decoder_heads,f.decoder_hidden_size=g.decoder_hidden_size,f.num_encoder_layers=g.num_encoder_layers,f.num_encoder_heads=g.num_encoder_heads,f.encoder_hidden_size=g.encoder_hidden_size):(f.num_layers=g.num_layers,f.num_heads=g.num_heads,f.hidden_size=g.hidden_size),f}const m={...h,...(0,r.pick)(u,["model_type","multi_query","is_encoder_decoder"])};for(const g in p)m[g]=u[p[g]];return m}function a(u,p){if(u.model_type==="lfm2"){const h=p?.prefix??"past_key_values",m=h==="present"?"present":"past",g={},{layer_types:I,num_attention_heads:f,num_key_value_heads:_,hidden_size:T,conv_L_cache:M}=u,v=T/f,b=p?.batch_size??1;for(let A=0;A<I.length;++A)if(I[A]==="full_attention")for(const k of["key","value"])g[`${h}.${A}.${k}`]=[b,_,0,v];else if(I[A]==="conv")g[`${m}_conv.${A}`]=[b,T,M];else throw new Error(`Unsupported layer type: ${I[A]}`);return g}return l(u,p)}function l(u,{prefix:p="past_key_values",batch_size:h=1}={}){const m={},g=u.normalized_config;if(g.is_encoder_decoder&&"num_encoder_heads"in g&&"num_decoder_heads"in g){const I=g.encoder_dim_kv??g.encoder_hidden_size/g.num_encoder_heads,f=g.decoder_dim_kv??g.decoder_hidden_size/g.num_decoder_heads,_=[h,g.num_encoder_heads,0,I],T=[h,g.num_decoder_heads,0,f];for(let M=0;M<g.num_decoder_layers;++M)m[`${p}.${M}.encoder.key`]=_,m[`${p}.${M}.encoder.value`]=_,m[`${p}.${M}.decoder.key`]=T,m[`${p}.${M}.decoder.value`]=T}else{const I=g.num_heads,f=g.num_layers,_=g.dim_kv??g.hidden_size/(g.num_attention_heads??I);if(g.model_type==="falcon"){const T=[h*I,0,_];for(let M=0;M<f;++M)m[`${p}.${M}.key`]=T,m[`${p}.${M}.value`]=T}else if(g.multi_query){const T=[h*I,0,2*_];for(let M=0;M<f;++M)m[`${p}.${M}.key_value`]=T}else if(g.model_type==="bloom"){const T=[h*I,_,0],M=[h*I,0,_];for(let v=0;v<f;++v)m[`${p}.${v}.key`]=T,m[`${p}.${v}.value`]=M}else if(g.model_type==="openelm")for(let T=0;T<f;++T){const M=[h,I[T],0,_];m[`${p}.${T}.key`]=M,m[`${p}.${T}.value`]=M}else{const T=[h,I,0,_];for(let M=0;M<f;++M)m[`${p}.${M}.key`]=T,m[`${p}.${M}.value`]=T}}return m}class c{model_type=null;is_encoder_decoder=!1;max_position_embeddings;"transformers.js_config";constructor(p){Object.assign(this,p),this.normalized_config=o(this)}static async from_pretrained(p,{progress_callback:h=null,config:m=null,cache_dir:g=null,local_files_only:I=!1,revision:f="main"}={}){m&&!(m instanceof c)&&(m=new c(m));const _=m??await i(p,{progress_callback:h,config:m,cache_dir:g,local_files_only:I,revision:f});return new this(_)}}class d{static async from_pretrained(...p){return c.from_pretrained(...p)}}}),"./src/env.js":((e,t,n)=>{n.r(t),n.d(t,{apis:()=>f,env:()=>A});var r=n("?db59"),s=n("?383f"),i=n("?fa4b");const o="3.8.1",a=typeof window<"u"&&typeof window.document<"u",l=typeof self<"u"&&["DedicatedWorkerGlobalScope","ServiceWorkerGlobalScope","SharedWorkerGlobalScope"].includes(self.constructor?.name),c=typeof self<"u"&&"caches"in self,d=typeof navigator<"u"&&"gpu"in navigator,u=typeof navigator<"u"&&"ml"in navigator,p=typeof process<"u",h=p&&process?.release?.name==="node",m=!k(r),g=!k(s),I=typeof globalThis.Deno<"u",f=Object.freeze({IS_BROWSER_ENV:a,IS_WEBWORKER_ENV:l,IS_WEB_CACHE_AVAILABLE:c,IS_WEBGPU_AVAILABLE:d,IS_WEBNN_AVAILABLE:u,IS_PROCESS_AVAILABLE:p,IS_NODE_ENV:h,IS_FS_AVAILABLE:m,IS_PATH_AVAILABLE:g}),_=m&&g;let T="./";if(_){const F=Object(import.meta).url;F?T=s.dirname(s.dirname(i.fileURLToPath(F))):typeof __dirname<"u"&&(T=s.dirname(__dirname))}const M=_?s.join(T,"/.cache/"):null,v="/models/",b=_?s.join(T,v):v,A={version:o,backends:{onnx:{}},allowRemoteModels:!0,remoteHost:"https://huggingface.co/",remotePathTemplate:"{model}/resolve/{revision}/",allowLocalModels:!(a||l),localModelPath:b,useFS:m,useBrowserCache:c&&!I,useFSCache:m,cacheDir:M,useCustomCache:!1,customCache:null};function k(F){return Object.keys(F).length===0}}),"./src/generation/configuration_utils.js":((e,t,n)=>{n.r(t),n.d(t,{GenerationConfig:()=>s});var r=n("./src/utils/core.js");class s{max_length=20;max_new_tokens=null;min_length=0;min_new_tokens=null;early_stopping=!1;max_time=null;do_sample=!1;num_beams=1;num_beam_groups=1;penalty_alpha=null;use_cache=!0;temperature=1;top_k=50;top_p=1;typical_p=1;epsilon_cutoff=0;eta_cutoff=0;diversity_penalty=0;repetition_penalty=1;encoder_repetition_penalty=1;length_penalty=1;no_repeat_ngram_size=0;bad_words_ids=null;force_words_ids=null;renormalize_logits=!1;constraints=null;forced_bos_token_id=null;forced_eos_token_id=null;remove_invalid_values=!1;exponential_decay_length_penalty=null;suppress_tokens=null;streamer=null;begin_suppress_tokens=null;forced_decoder_ids=null;guidance_scale=null;num_return_sequences=1;output_attentions=!1;output_hidden_states=!1;output_scores=!1;return_dict_in_generate=!1;pad_token_id=null;bos_token_id=null;eos_token_id=null;encoder_no_repeat_ngram_size=0;decoder_start_token_id=null;generation_kwargs={};constructor(o){Object.assign(this,(0,r.pick)(o,Object.getOwnPropertyNames(this)))}}}),"./src/generation/logits_process.js":((e,t,n)=>{n.r(t),n.d(t,{ClassifierFreeGuidanceLogitsProcessor:()=>f,ForcedBOSTokenLogitsProcessor:()=>l,ForcedEOSTokenLogitsProcessor:()=>c,LogitsProcessor:()=>i,LogitsProcessorList:()=>a,LogitsWarper:()=>o,MinLengthLogitsProcessor:()=>m,MinNewTokensLengthLogitsProcessor:()=>g,NoBadWordsLogitsProcessor:()=>I,NoRepeatNGramLogitsProcessor:()=>p,RepetitionPenaltyLogitsProcessor:()=>h,SuppressTokensAtBeginLogitsProcessor:()=>d,TemperatureLogitsWarper:()=>_,TopKLogitsWarper:()=>M,TopPLogitsWarper:()=>T,WhisperTimeStampLogitsProcessor:()=>u});var r=n("./src/utils/generic.js");n("./src/utils/tensor.js");var s=n("./src/utils/maths.js");class i extends r.Callable{_call(b,A){throw Error("`_call` should be implemented in a subclass")}}class o extends r.Callable{_call(b,A){throw Error("`_call` should be implemented in a subclass")}}class a extends r.Callable{constructor(){super(),this.processors=[]}push(b){this.processors.push(b)}extend(b){this.processors.push(...b)}_call(b,A){let k=A;for(const F of this.processors)k=F(b,k);return k}[Symbol.iterator](){return this.processors.values()}}class l extends i{constructor(b){super(),this.bos_token_id=b}_call(b,A){for(let k=0;k<b.length;++k)if(b[k].length===1){const F=A[k].data;F.fill(-1/0),F[this.bos_token_id]=0}return A}}class c extends i{constructor(b,A){super(),this.max_length=b,this.eos_token_id=Array.isArray(A)?A:[A]}_call(b,A){for(let k=0;k<b.length;++k)if(b[k].length===this.max_length-1){const F=A[k].data;F.fill(-1/0);for(const L of this.eos_token_id)F[L]=0}return A}}class d extends i{constructor(b,A){super(),this.begin_suppress_tokens=b,this.begin_index=A}_call(b,A){for(let k=0;k<b.length;++k)if(b[k].length===this.begin_index){const F=A[k].data;for(const L of this.begin_suppress_tokens)F[L]=-1/0}return A}}class u extends i{constructor(b,A){super(),this.eos_token_id=Array.isArray(b.eos_token_id)?b.eos_token_id[0]:b.eos_token_id,this.no_timestamps_token_id=b.no_timestamps_token_id,this.timestamp_begin=this.no_timestamps_token_id+1,this.begin_index=A.length,A.at(-1)===this.no_timestamps_token_id&&(this.begin_index-=1),this.max_initial_timestamp_index=b.max_initial_timestamp_index}_call(b,A){for(let k=0;k<b.length;++k){const F=A[k].data;if(F[this.no_timestamps_token_id]=-1/0,b[k].length===this.begin_index-1){F.fill(-1/0),F[this.timestamp_begin]=0;continue}const L=b[k].slice(this.begin_index),G=L.length>=1&&L[L.length-1]>=this.timestamp_begin,j=L.length<2||L[L.length-2]>=this.timestamp_begin;if(G&&(j?F.subarray(this.timestamp_begin).fill(-1/0):F.subarray(0,this.eos_token_id).fill(-1/0)),b[k].length===this.begin_index&&this.max_initial_timestamp_index!==null){const Y=this.timestamp_begin+this.max_initial_timestamp_index;F.subarray(Y+1).fill(-1/0)}const R=(0,s.log_softmax)(F),K=Math.log(R.subarray(this.timestamp_begin).map(Math.exp).reduce((Y,te)=>Y+te)),U=(0,s.max)(R.subarray(0,this.timestamp_begin))[0];K>U&&F.subarray(0,this.timestamp_begin).fill(-1/0)}return A}}class p extends i{constructor(b){super(),this.no_repeat_ngram_size=b}getNgrams(b){const A=b.length,k=[];for(let L=0;L<A+1-this.no_repeat_ngram_size;++L){const G=[];for(let j=0;j<this.no_repeat_ngram_size;++j)G.push(b[L+j]);k.push(G.map(Number))}const F=new Map;for(const L of k){const G=L.slice(0,L.length-1),j=JSON.stringify(G),R=F.get(j)??[];R.push(L[L.length-1]),F.set(j,R)}return F}getGeneratedNgrams(b,A){const k=A.slice(A.length+1-this.no_repeat_ngram_size,A.length);return b.get(JSON.stringify(k.map(Number)))??[]}calcBannedNgramTokens(b){const A=[];if(b.length+1<this.no_repeat_ngram_size)return A;{const k=this.getNgrams(b);return this.getGeneratedNgrams(k,b)}}_call(b,A){for(let k=0;k<b.length;++k){const F=A[k].data,L=this.calcBannedNgramTokens(b[k]);for(const G of L)F[G]=-1/0}return A}}class h extends i{constructor(b){super(),this.penalty=b}_call(b,A){for(let k=0;k<b.length;++k){const F=A[k].data;for(const L of new Set(b[k])){const G=Number(L);F[G]<0?F[G]*=this.penalty:F[G]/=this.penalty}}return A}}class m extends i{constructor(b,A){super(),this.min_length=b,this.eos_token_id=Array.isArray(A)?A:[A]}_call(b,A){for(let k=0;k<b.length;++k)if(b[k].length<this.min_length){const F=A[k].data;for(const L of this.eos_token_id)F[L]=-1/0}return A}}class g extends i{constructor(b,A,k){super(),this.prompt_length_to_skip=b,this.min_new_tokens=A,this.eos_token_id=Array.isArray(k)?k:[k]}_call(b,A){for(let k=0;k<b.length;++k)if(b[k].length-this.prompt_length_to_skip<this.min_new_tokens){const L=A[k].data;for(const G of this.eos_token_id)L[G]=-1/0}return A}}class I extends i{constructor(b,A){super(),this.bad_words_ids=b,this.eos_token_id=Array.isArray(A)?A:[A]}_call(b,A){for(let k=0;k<b.length;++k){const F=A[k].data,L=b[k];for(const G of this.bad_words_ids){if(L.length<G.length-1)continue;let j=!0;for(let R=1;R<=G.length-1;++R)if(G.at(-R-1)!=L.at(-R)){j=!1;break}j&&(F[G.at(-1)]=-1/0)}}return A}}class f extends i{constructor(b){if(super(),b<=1)throw new Error(`Require guidance scale >1 to use the classifier free guidance processor, got guidance scale ${b}.`);this.guidance_scale=b}_call(b,A){if(A.dims[0]!==2*b.length)throw new Error(`Logits should have twice the batch size of the input ids, the first half of batches corresponding to the conditional inputs, and the second half of batches corresponding to the unconditional inputs. Got batch size ${A.dims[0]} for the logits and ${b.length} for the input ids.`);const k=b.length,F=A.slice([0,k],null),L=A.slice([k,A.dims[0]],null);for(let G=0;G<L.data.length;++G)L.data[G]+=(F.data[G]-L.data[G])*this.guidance_scale;return L}}class _ extends o{constructor(b){super(),this.temperature=b}_call(b,A){const k=A.data;for(let F=0;F<k.length;++F)k[F]/=this.temperature;return A}}class T extends o{constructor(b,{filter_value:A=-1/0,min_tokens_to_keep:k=1}={}){if(super(),b<0||b>1)throw new Error(`\`top_p\` must be a float > 0 and < 1, but is ${b}`);if(!Number.isInteger(k)||k<1)throw new Error(`\`min_tokens_to_keep\` must be a positive integer, but is ${k}`);this.top_p=b,this.filter_value=A,this.min_tokens_to_keep=k}}class M extends o{constructor(b,{filter_value:A=-1/0,min_tokens_to_keep:k=1}={}){if(super(),!Number.isInteger(b)||b<0)throw new Error(`\`top_k\` must be a positive integer, but is ${b}`);this.top_k=Math.max(b,k),this.filter_value=A}}}),"./src/generation/logits_sampler.js":((e,t,n)=>{n.r(t),n.d(t,{LogitsSampler:()=>o});var r=n("./src/utils/generic.js"),s=n("./src/utils/tensor.js"),i=n("./src/utils/maths.js");n("./src/generation/configuration_utils.js");class o extends r.Callable{constructor(u){super(),this.generation_config=u}async _call(u){return this.sample(u)}async sample(u){throw Error("sample should be implemented in subclasses.")}getLogits(u,p){let h=u.dims.at(-1),m=u.data;if(p===-1)m=m.slice(-h);else{let g=p*h;m=m.slice(g,g+h)}return m}randomSelect(u){let p=0;for(let m=0;m<u.length;++m)p+=u[m];let h=Math.random()*p;for(let m=0;m<u.length;++m)if(h-=u[m],h<=0)return m;return 0}static getSampler(u){if(u.do_sample)return new l(u);if(u.num_beams>1)return new c(u);if(u.num_return_sequences>1)throw Error(`num_return_sequences has to be 1 when doing greedy search, but is ${u.num_return_sequences}.`);return new a(u)}}class a extends o{async sample(u){const p=(0,i.max)(u.data)[1];return[[BigInt(p),0]]}}class l extends o{async sample(u){let p=u.dims.at(-1);this.generation_config.top_k>0&&(p=Math.min(this.generation_config.top_k,p));const[h,m]=await(0,s.topk)(u,p),g=(0,i.softmax)(h.data);return Array.from({length:this.generation_config.num_beams},()=>{const I=this.randomSelect(g);return[m.data[I],Math.log(g[I])]})}}class c extends o{async sample(u){let p=u.dims.at(-1);this.generation_config.top_k>0&&(p=Math.min(this.generation_config.top_k,p));const[h,m]=await(0,s.topk)(u,p),g=(0,i.softmax)(h.data);return Array.from({length:this.generation_config.num_beams},(I,f)=>[m.data[f],Math.log(g[f])])}}}),"./src/generation/stopping_criteria.js":((e,t,n)=>{n.r(t),n.d(t,{EosTokenCriteria:()=>a,InterruptableStoppingCriteria:()=>l,MaxLengthCriteria:()=>o,StoppingCriteria:()=>s,StoppingCriteriaList:()=>i});var r=n("./src/utils/generic.js");class s extends r.Callable{_call(d,u){throw Error("StoppingCriteria needs to be subclassed")}}class i extends r.Callable{constructor(){super(),this.criteria=[]}push(d){this.criteria.push(d)}extend(d){d instanceof i?d=d.criteria:d instanceof s&&(d=[d]),this.criteria.push(...d)}_call(d,u){const p=new Array(d.length).fill(!1);for(const h of this.criteria){const m=h(d,u);for(let g=0;g<p.length;++g)p[g]||=m[g]}return p}[Symbol.iterator](){return this.criteria.values()}}class o extends s{constructor(d,u=null){super(),this.max_length=d,this.max_position_embeddings=u}_call(d){return d.map(u=>u.length>=this.max_length)}}class a extends s{constructor(d){super(),Array.isArray(d)||(d=[d]),this.eos_token_id=d}_call(d,u){return d.map(p=>{const h=p.at(-1);return this.eos_token_id.some(m=>h==m)})}}class l extends s{constructor(){super(),this.interrupted=!1}interrupt(){this.interrupted=!0}reset(){this.interrupted=!1}_call(d,u){return new Array(d.length).fill(this.interrupted)}}}),"./src/generation/streamers.js":((e,t,n)=>{n.r(t),n.d(t,{BaseStreamer:()=>o,TextStreamer:()=>l,WhisperTextStreamer:()=>c});var r=n("./src/utils/core.js"),s=n("./src/tokenizers.js"),i=n("./src/env.js");class o{put(u){throw Error("Not implemented")}end(){throw Error("Not implemented")}}const a=i.apis.IS_PROCESS_AVAILABLE?d=>process.stdout.write(d):d=>console.log(d);class l extends o{constructor(u,{skip_prompt:p=!1,callback_function:h=null,token_callback_function:m=null,skip_special_tokens:g=!0,decode_kwargs:I={},...f}={}){super(),this.tokenizer=u,this.skip_prompt=p,this.callback_function=h??a,this.token_callback_function=m,this.decode_kwargs={skip_special_tokens:g,...I,...f},this.token_cache=[],this.print_len=0,this.next_tokens_are_prompt=!0}put(u){if(u.length>1)throw Error("TextStreamer only supports batch size of 1");const p=this.next_tokens_are_prompt;if(p&&(this.next_tokens_are_prompt=!1,this.skip_prompt))return;const h=u[0];this.token_callback_function?.(h),this.token_cache=(0,r.mergeArrays)(this.token_cache,h);const m=this.tokenizer.decode(this.token_cache,this.decode_kwargs);let g;p||m.endsWith(`
`)?(g=m.slice(this.print_len),this.token_cache=[],this.print_len=0):m.length>0&&(0,s.is_chinese_char)(m.charCodeAt(m.length-1))?(g=m.slice(this.print_len),this.print_len+=g.length):(g=m.slice(this.print_len,m.lastIndexOf(" ")+1),this.print_len+=g.length),this.on_finalized_text(g,!1)}end(){let u;this.token_cache.length>0?(u=this.tokenizer.decode(this.token_cache,this.decode_kwargs).slice(this.print_len),this.token_cache=[],this.print_len=0):u="",this.next_tokens_are_prompt=!0,this.on_finalized_text(u,!0)}on_finalized_text(u,p){u.length>0&&this.callback_function?.(u),p&&this.callback_function===a&&i.apis.IS_PROCESS_AVAILABLE&&this.callback_function?.(`
`)}}class c extends l{constructor(u,{skip_prompt:p=!1,callback_function:h=null,token_callback_function:m=null,on_chunk_start:g=null,on_chunk_end:I=null,on_finalize:f=null,time_precision:_=.02,skip_special_tokens:T=!0,decode_kwargs:M={}}={}){super(u,{skip_prompt:p,skip_special_tokens:T,callback_function:h,token_callback_function:m,decode_kwargs:M}),this.timestamp_begin=u.timestamp_begin,this.on_chunk_start=g,this.on_chunk_end=I,this.on_finalize=f,this.time_precision=_,this.waiting_for_timestamp=!1}put(u){if(u.length>1)throw Error("WhisperTextStreamer only supports batch size of 1");const p=u[0];if(p.length===1){const h=Number(p[0])-this.timestamp_begin;if(h>=0){const m=h*this.time_precision;this.waiting_for_timestamp?this.on_chunk_end?.(m):this.on_chunk_start?.(m),this.waiting_for_timestamp=!this.waiting_for_timestamp,this.token_callback_function?.(p);return}}return super.put(u)}end(){super.end(),this.on_finalize?.()}}}),"./src/models.js":((e,t,n)=>{n.r(t),n.d(t,{ASTForAudioClassification:()=>oa,ASTModel:()=>ia,ASTPreTrainedModel:()=>sa,AlbertForMaskedLM:()=>Je,AlbertForQuestionAnswering:()=>st,AlbertForSequenceClassification:()=>rt,AlbertModel:()=>nt,AlbertPreTrainedModel:()=>je,ArceeForCausalLM:()=>t_,ArceeModel:()=>e_,ArceePreTrainedModel:()=>Ju,AutoModel:()=>U3,AutoModelForAudioClassification:()=>cL,AutoModelForAudioFrameClassification:()=>dL,AutoModelForAudioTextToText:()=>bL,AutoModelForCTC:()=>lL,AutoModelForCausalLM:()=>Y3,AutoModelForDepthEstimation:()=>fL,AutoModelForDocumentQuestionAnswering:()=>pL,AutoModelForImageClassification:()=>tL,AutoModelForImageFeatureExtraction:()=>yL,AutoModelForImageMatting:()=>hL,AutoModelForImageSegmentation:()=>nL,AutoModelForImageTextToText:()=>xL,AutoModelForImageToImage:()=>mL,AutoModelForMaskGeneration:()=>aL,AutoModelForMaskedLM:()=>J3,AutoModelForNormalEstimation:()=>gL,AutoModelForObjectDetection:()=>iL,AutoModelForPoseEstimation:()=>_L,AutoModelForQuestionAnswering:()=>Z3,AutoModelForSemanticSegmentation:()=>rL,AutoModelForSeq2SeqLM:()=>K3,AutoModelForSequenceClassification:()=>W3,AutoModelForSpeechSeq2Seq:()=>q3,AutoModelForTextToSpectrogram:()=>Q3,AutoModelForTextToWaveform:()=>X3,AutoModelForTokenClassification:()=>H3,AutoModelForUniversalSegmentation:()=>sL,AutoModelForVision2Seq:()=>eL,AutoModelForXVector:()=>uL,AutoModelForZeroShotObjectDetection:()=>oL,BartForConditionalGeneration:()=>un,BartForSequenceClassification:()=>Sn,BartModel:()=>qr,BartPretrainedModel:()=>Gn,BaseModelOutput:()=>Te,BeitForImageClassification:()=>M0,BeitModel:()=>v0,BeitPreTrainedModel:()=>Sd,BertForMaskedLM:()=>De,BertForQuestionAnswering:()=>_e,BertForSequenceClassification:()=>xe,BertForTokenClassification:()=>ze,BertModel:()=>Ee,BertPreTrainedModel:()=>Ie,BlenderbotForConditionalGeneration:()=>Ln,BlenderbotModel:()=>dn,BlenderbotPreTrainedModel:()=>Nn,BlenderbotSmallForConditionalGeneration:()=>qs,BlenderbotSmallModel:()=>dr,BlenderbotSmallPreTrainedModel:()=>vn,BloomForCausalLM:()=>W_,BloomModel:()=>U_,BloomPreTrainedModel:()=>xd,CLIPModel:()=>ua,CLIPPreTrainedModel:()=>Ms,CLIPSegForImageSegmentation:()=>fa,CLIPSegModel:()=>ma,CLIPSegPreTrainedModel:()=>bi,CLIPTextModel:()=>Ql,CLIPTextModelWithProjection:()=>da,CLIPVisionModel:()=>Xl,CLIPVisionModelWithProjection:()=>Yl,CamembertForMaskedLM:()=>pe,CamembertForQuestionAnswering:()=>gt,CamembertForSequenceClassification:()=>Se,CamembertForTokenClassification:()=>it,CamembertModel:()=>re,CamembertPreTrainedModel:()=>J,CausalLMOutput:()=>Zs,CausalLMOutputWithPast:()=>vL,ChineseCLIPModel:()=>ha,ChineseCLIPPreTrainedModel:()=>Jl,ClapAudioModelWithProjection:()=>zx,ClapModel:()=>Rx,ClapPreTrainedModel:()=>ac,ClapTextModelWithProjection:()=>Nx,CodeGenForCausalLM:()=>We,CodeGenModel:()=>Fe,CodeGenPreTrainedModel:()=>ue,CohereForCausalLM:()=>M_,CohereModel:()=>v_,CoherePreTrainedModel:()=>cd,ConvBertForMaskedLM:()=>vt,ConvBertForQuestionAnswering:()=>ms,ConvBertForSequenceClassification:()=>rn,ConvBertForTokenClassification:()=>hs,ConvBertModel:()=>Ls,ConvBertPreTrainedModel:()=>Ar,ConvNextForImageClassification:()=>_y,ConvNextModel:()=>gy,ConvNextPreTrainedModel:()=>Hd,ConvNextV2ForImageClassification:()=>xy,ConvNextV2Model:()=>yy,ConvNextV2PreTrainedModel:()=>Kd,DFineForObjectDetection:()=>O0,DFineModel:()=>F0,DFinePreTrainedModel:()=>jd,DINOv3ConvNextModel:()=>Cy,DINOv3ConvNextPreTrainedModel:()=>Py,DINOv3ViTModel:()=>Ty,DINOv3ViTPreTrainedModel:()=>Ay,DPTForDepthEstimation:()=>J0,DPTModel:()=>Y0,DPTPreTrainedModel:()=>Vd,DacDecoderModel:()=>Ib,DacDecoderOutput:()=>Tb,DacEncoderModel:()=>Cb,DacEncoderOutput:()=>Ab,DacModel:()=>Pb,DacPreTrainedModel:()=>mc,DebertaForMaskedLM:()=>et,DebertaForQuestionAnswering:()=>Fn,DebertaForSequenceClassification:()=>Lt,DebertaForTokenClassification:()=>Rt,DebertaModel:()=>At,DebertaPreTrainedModel:()=>Ze,DebertaV2ForMaskedLM:()=>Bn,DebertaV2ForQuestionAnswering:()=>_s,DebertaV2ForSequenceClassification:()=>gs,DebertaV2ForTokenClassification:()=>tr,DebertaV2Model:()=>bn,DebertaV2PreTrainedModel:()=>Cn,DecisionTransformerModel:()=>ab,DecisionTransformerPreTrainedModel:()=>ob,DeiTForImageClassification:()=>B0,DeiTModel:()=>z0,DeiTPreTrainedModel:()=>Nd,DepthAnythingForDepthEstimation:()=>ey,DepthAnythingPreTrainedModel:()=>Z0,DepthProForDepthEstimation:()=>iy,DepthProPreTrainedModel:()=>sy,DetrForObjectDetection:()=>T0,DetrForSegmentation:()=>Ld,DetrModel:()=>A0,DetrObjectDetectionOutput:()=>Dd,DetrPreTrainedModel:()=>ec,DetrSegmentationOutput:()=>P0,Dinov2ForImageClassification:()=>wy,Dinov2Model:()=>by,Dinov2PreTrainedModel:()=>qd,Dinov2WithRegistersForImageClassification:()=>My,Dinov2WithRegistersModel:()=>vy,Dinov2WithRegistersPreTrainedModel:()=>Qd,DistilBertForMaskedLM:()=>yr,DistilBertForQuestionAnswering:()=>wn,DistilBertForSequenceClassification:()=>Dr,DistilBertForTokenClassification:()=>On,DistilBertModel:()=>rr,DistilBertPreTrainedModel:()=>nr,DonutSwinModel:()=>fy,DonutSwinPreTrainedModel:()=>my,EdgeTamModel:()=>Ry,EfficientNetForImageClassification:()=>Kx,EfficientNetModel:()=>Hx,EfficientNetPreTrainedModel:()=>cp,ElectraForMaskedLM:()=>fs,ElectraForQuestionAnswering:()=>W,ElectraForSequenceClassification:()=>S,ElectraForTokenClassification:()=>ee,ElectraModel:()=>It,ElectraPreTrainedModel:()=>_r,Ernie4_5ForCausalLM:()=>Dx,Ernie4_5Model:()=>Lx,Ernie4_5PreTrainedModel:()=>sp,EsmForMaskedLM:()=>ys,EsmForSequenceClassification:()=>xs,EsmForTokenClassification:()=>bs,EsmModel:()=>Ds,EsmPreTrainedModel:()=>$r,ExaoneForCausalLM:()=>d_,ExaoneModel:()=>u_,ExaonePreTrainedModel:()=>rd,FalconForCausalLM:()=>jx,FalconModel:()=>Ox,FalconPreTrainedModel:()=>op,FastViTForImageClassification:()=>d0,FastViTModel:()=>u0,FastViTPreTrainedModel:()=>Pd,Florence2ForConditionalGeneration:()=>Wl,Florence2PreTrainedModel:()=>Ul,GLPNForDepthEstimation:()=>hy,GLPNModel:()=>py,GLPNPreTrainedModel:()=>Wd,GPT2LMHeadModel:()=>ga,GPT2Model:()=>uo,GPT2PreTrainedModel:()=>co,GPTBigCodeForCausalLM:()=>q,GPTBigCodeModel:()=>B,GPTBigCodePreTrainedModel:()=>O,GPTJForCausalLM:()=>E,GPTJModel:()=>x,GPTJPreTrainedModel:()=>fo,GPTNeoForCausalLM:()=>ya,GPTNeoModel:()=>ho,GPTNeoPreTrainedModel:()=>vi,GPTNeoXForCausalLM:()=>ba,GPTNeoXModel:()=>xa,GPTNeoXPreTrainedModel:()=>mo,Gemma2ForCausalLM:()=>C_,Gemma2Model:()=>P_,Gemma2PreTrainedModel:()=>dd,Gemma3ForCausalLM:()=>S_,Gemma3Model:()=>E_,Gemma3PreTrainedModel:()=>hd,Gemma3nForConditionalGeneration:()=>so,Gemma3nPreTrainedModel:()=>Ys,GemmaForCausalLM:()=>T_,GemmaModel:()=>A_,GemmaPreTrainedModel:()=>ud,GlmForCausalLM:()=>c_,GlmModel:()=>l_,GlmPreTrainedModel:()=>nd,GraniteForCausalLM:()=>x_,GraniteModel:()=>y_,GraniteMoeHybridForCausalLM:()=>w_,GraniteMoeHybridModel:()=>b_,GraniteMoeHybridPreTrainedModel:()=>ld,GranitePreTrainedModel:()=>ad,GroundingDinoForObjectDetection:()=>ky,GroundingDinoPreTrainedModel:()=>Iy,GroupViTModel:()=>c0,GroupViTPreTrainedModel:()=>l0,HeliumForCausalLM:()=>a_,HeliumModel:()=>o_,HeliumPreTrainedModel:()=>td,HieraForImageClassification:()=>V0,HieraModel:()=>G0,HieraPreTrainedModel:()=>zd,HubertForCTC:()=>dx,HubertForSequenceClassification:()=>px,HubertModel:()=>ux,HubertPreTrainedModel:()=>S3,IJepaForImageClassification:()=>Z_,IJepaModel:()=>J_,IJepaPreTrainedModel:()=>Md,Idefics3ForConditionalGeneration:()=>io,Idefics3PreTrainedModel:()=>ql,ImageMattingOutput:()=>iw,JAISLMHeadModel:()=>wi,JAISModel:()=>_a,JAISPreTrainedModel:()=>po,JinaCLIPModel:()=>yi,JinaCLIPPreTrainedModel:()=>_i,JinaCLIPTextModel:()=>Pr,JinaCLIPVisionModel:()=>xi,Lfm2ForCausalLM:()=>r_,Lfm2Model:()=>n_,Lfm2PreTrainedModel:()=>Zu,LiteWhisperForConditionalGeneration:()=>Bl,Llama4ForCausalLM:()=>Yt,Llama4PreTrainedModel:()=>Vt,LlamaForCausalLM:()=>Et,LlamaModel:()=>dt,LlamaPreTrainedModel:()=>tt,LlavaForConditionalGeneration:()=>ro,LlavaOnevisionForConditionalGeneration:()=>Gl,LlavaPreTrainedModel:()=>la,LlavaQwen2ForCausalLM:()=>ca,LongT5ForConditionalGeneration:()=>gn,LongT5Model:()=>an,LongT5PreTrainedModel:()=>jn,M2M100ForConditionalGeneration:()=>Vy,M2M100Model:()=>Gy,M2M100PreTrainedModel:()=>Jd,MBartForCausalLM:()=>jr,MBartForConditionalGeneration:()=>Qr,MBartForSequenceClassification:()=>Rn,MBartModel:()=>Tr,MBartPreTrainedModel:()=>qn,MPNetForMaskedLM:()=>ws,MPNetForQuestionAnswering:()=>he,MPNetForSequenceClassification:()=>Fs,MPNetForTokenClassification:()=>Zi,MPNetModel:()=>$s,MPNetPreTrainedModel:()=>tn,MT5ForConditionalGeneration:()=>Or,MT5Model:()=>xr,MT5PreTrainedModel:()=>nn,MarianMTModel:()=>By,MarianModel:()=>zy,MarianPreTrainedModel:()=>Yd,MaskFormerForInstanceSegmentation:()=>dy,MaskFormerModel:()=>uy,MaskFormerPreTrainedModel:()=>Ud,MaskedLMOutput:()=>Qn,Metric3DForDepthEstimation:()=>ay,Metric3DPreTrainedModel:()=>oy,Metric3Dv2ForDepthEstimation:()=>cy,Metric3Dv2PreTrainedModel:()=>ly,MgpstrForSceneTextRecognition:()=>pb,MgpstrModelOutput:()=>ub,MgpstrPreTrainedModel:()=>db,MimiDecoderModel:()=>Mb,MimiDecoderOutput:()=>bb,MimiEncoderModel:()=>vb,MimiEncoderOutput:()=>xb,MimiModel:()=>wb,MimiPreTrainedModel:()=>hc,Ministral3ForCausalLM:()=>Sx,Ministral3Model:()=>Ex,Ministral3PreTrainedModel:()=>rp,MinistralForCausalLM:()=>kx,MinistralModel:()=>Ix,MinistralPreTrainedModel:()=>np,Mistral3ForConditionalGeneration:()=>mi,MistralForCausalLM:()=>Cx,MistralModel:()=>Px,MistralPreTrainedModel:()=>tp,MobileBertForMaskedLM:()=>Ue,MobileBertForQuestionAnswering:()=>ut,MobileBertForSequenceClassification:()=>Ke,MobileBertModel:()=>or,MobileBertPreTrainedModel:()=>Fr,MobileLLMForCausalLM:()=>h_,MobileLLMModel:()=>p_,MobileLLMPreTrainedModel:()=>sd,MobileNetV1ForImageClassification:()=>Qx,MobileNetV1ForSemanticSegmentation:()=>Xx,MobileNetV1Model:()=>qx,MobileNetV1PreTrainedModel:()=>cc,MobileNetV2ForImageClassification:()=>Jx,MobileNetV2ForSemanticSegmentation:()=>Zx,MobileNetV2Model:()=>Yx,MobileNetV2PreTrainedModel:()=>uc,MobileNetV3ForImageClassification:()=>tb,MobileNetV3ForSemanticSegmentation:()=>nb,MobileNetV3Model:()=>eb,MobileNetV3PreTrainedModel:()=>dc,MobileNetV4ForImageClassification:()=>sb,MobileNetV4ForSemanticSegmentation:()=>ib,MobileNetV4Model:()=>rb,MobileNetV4PreTrainedModel:()=>pc,MobileViTForImageClassification:()=>f0,MobileViTModel:()=>m0,MobileViTPreTrainedModel:()=>Cd,MobileViTV2ForImageClassification:()=>_0,MobileViTV2Model:()=>g0,MobileViTV2PreTrainedModel:()=>Id,ModelOutput:()=>fe,ModernBertDecoderForCausalLM:()=>on,ModernBertDecoderModel:()=>wt,ModernBertDecoderPreTrainedModel:()=>zt,ModernBertForMaskedLM:()=>ve,ModernBertForSequenceClassification:()=>Qe,ModernBertForTokenClassification:()=>ct,ModernBertModel:()=>Pe,ModernBertPreTrainedModel:()=>ye,Moondream1ForConditionalGeneration:()=>Vl,MoonshineForConditionalGeneration:()=>to,MoonshineModel:()=>Yu,MoonshinePreTrainedModel:()=>aa,MptForCausalLM:()=>K_,MptModel:()=>H_,MptPreTrainedModel:()=>bd,MultiModalityCausalLM:()=>cb,MultiModalityPreTrainedModel:()=>lb,MusicgenForCausalLM:()=>F3,MusicgenForConditionalGeneration:()=>dp,MusicgenModel:()=>$3,MusicgenPreTrainedModel:()=>up,NanoChatForCausalLM:()=>Zl,NanoChatModel:()=>js,NanoChatPreTrainedModel:()=>zn,NeoBertForMaskedLM:()=>Ne,NeoBertForQuestionAnswering:()=>de,NeoBertForSequenceClassification:()=>ot,NeoBertForTokenClassification:()=>Ve,NeoBertModel:()=>qe,NeoBertPreTrainedModel:()=>Le,NomicBertModel:()=>kt,NomicBertPreTrainedModel:()=>Tn,OPTForCausalLM:()=>Q_,OPTModel:()=>q_,OPTPreTrainedModel:()=>wd,Olmo2ForCausalLM:()=>__,Olmo2Model:()=>g_,Olmo2PreTrainedModel:()=>od,OlmoForCausalLM:()=>f_,OlmoModel:()=>m_,OlmoPreTrainedModel:()=>id,OpenELMForCausalLM:()=>D_,OpenELMModel:()=>L_,OpenELMPreTrainedModel:()=>md,OwlViTForObjectDetection:()=>x0,OwlViTModel:()=>y0,OwlViTPreTrainedModel:()=>kd,Owlv2ForObjectDetection:()=>w0,Owlv2Model:()=>b0,Owlv2PreTrainedModel:()=>Ed,PaliGemmaForConditionalGeneration:()=>Kl,PaliGemmaPreTrainedModel:()=>Hl,ParakeetForCTC:()=>Qy,ParakeetPreTrainedModel:()=>qy,PatchTSMixerForPrediction:()=>gb,PatchTSMixerModel:()=>fb,PatchTSMixerPreTrainedModel:()=>hp,PatchTSTForPrediction:()=>mb,PatchTSTModel:()=>hb,PatchTSTPreTrainedModel:()=>pp,Phi3ForCausalLM:()=>V_,Phi3Model:()=>G_,Phi3PreTrainedModel:()=>yd,Phi3VForCausalLM:()=>ao,Phi3VPreTrainedModel:()=>oo,PhiForCausalLM:()=>B_,PhiModel:()=>z_,PhiPreTrainedModel:()=>_d,PreTrainedModel:()=>V,PretrainedMixin:()=>Jt,PvtForImageClassification:()=>r0,PvtModel:()=>n0,PvtPreTrainedModel:()=>Ad,PyAnnoteForAudioFrameClassification:()=>Yy,PyAnnoteModel:()=>Xy,PyAnnotePreTrainedModel:()=>Zd,QuestionAnsweringModelOutput:()=>sr,Qwen2ForCausalLM:()=>F_,Qwen2Model:()=>$_,Qwen2PreTrainedModel:()=>fd,Qwen2VLForConditionalGeneration:()=>N_,Qwen2VLPreTrainedModel:()=>R_,Qwen3ForCausalLM:()=>j_,Qwen3Model:()=>O_,Qwen3PreTrainedModel:()=>gd,RFDetrForObjectDetection:()=>D0,RFDetrModel:()=>L0,RFDetrObjectDetectionOutput:()=>$0,RFDetrPreTrainedModel:()=>Od,RTDetrForObjectDetection:()=>I0,RTDetrModel:()=>C0,RTDetrObjectDetectionOutput:()=>wa,RTDetrPreTrainedModel:()=>$d,RTDetrV2ForObjectDetection:()=>E0,RTDetrV2Model:()=>k0,RTDetrV2ObjectDetectionOutput:()=>S0,RTDetrV2PreTrainedModel:()=>Fd,ResNetForImageClassification:()=>W0,ResNetModel:()=>U0,ResNetPreTrainedModel:()=>Bd,RoFormerForMaskedLM:()=>Hs,RoFormerForQuestionAnswering:()=>Kn,RoFormerForSequenceClassification:()=>Ks,RoFormerForTokenClassification:()=>xn,RoFormerModel:()=>Zn,RoFormerPreTrainedModel:()=>Pn,RobertaForMaskedLM:()=>kl,RobertaForQuestionAnswering:()=>Ll,RobertaForSequenceClassification:()=>El,RobertaForTokenClassification:()=>Sl,RobertaModel:()=>ea,RobertaPreTrainedModel:()=>vs,Sam2ImageSegmentationOutput:()=>Oy,Sam2Model:()=>rc,Sam2PreTrainedModel:()=>jy,Sam3TrackerModel:()=>Ny,SamImageSegmentationOutput:()=>Fy,SamModel:()=>$y,SamPreTrainedModel:()=>Dy,SapiensForDepthEstimation:()=>ny,SapiensForNormalEstimation:()=>ry,SapiensForSemanticSegmentation:()=>ty,SapiensPreTrainedModel:()=>nc,SegformerForImageClassification:()=>Gx,SegformerForSemanticSegmentation:()=>Vx,SegformerModel:()=>D3,SegformerPreTrainedModel:()=>lc,Seq2SeqLMOutput:()=>wL,SequenceClassifierOutput:()=>St,SiglipModel:()=>pa,SiglipPreTrainedModel:()=>lo,SiglipTextModel:()=>gi,SiglipVisionModel:()=>_t,SmolLM3ForCausalLM:()=>i_,SmolLM3Model:()=>s_,SmolLM3PreTrainedModel:()=>ed,SmolVLMForConditionalGeneration:()=>fi,SnacDecoderModel:()=>Sb,SnacEncoderModel:()=>Eb,SnacModel:()=>kb,SnacPreTrainedModel:()=>fc,SpeechT5ForSpeechToText:()=>bx,SpeechT5ForTextToSpeech:()=>wx,SpeechT5HifiGan:()=>vx,SpeechT5Model:()=>L3,SpeechT5PreTrainedModel:()=>oc,SqueezeBertForMaskedLM:()=>ie,SqueezeBertForQuestionAnswering:()=>we,SqueezeBertForSequenceClassification:()=>ae,SqueezeBertModel:()=>Q,SqueezeBertPreTrainedModel:()=>$,StableLmForCausalLM:()=>Wx,StableLmModel:()=>Ux,StableLmPreTrainedModel:()=>lp,Starcoder2ForCausalLM:()=>Fx,Starcoder2Model:()=>$x,Starcoder2PreTrainedModel:()=>ip,StyleTextToSpeech2Model:()=>xx,StyleTextToSpeech2PreTrainedModel:()=>yx,SupertonicForConditionalGeneration:()=>ep,SupertonicPreTrainedModel:()=>Mx,Swin2SRForImageSuperResolution:()=>X0,Swin2SRModel:()=>Q0,Swin2SRPreTrainedModel:()=>Gd,SwinForImageClassification:()=>K0,SwinForSemanticSegmentation:()=>q0,SwinModel:()=>H0,SwinPreTrainedModel:()=>tc,T5ForConditionalGeneration:()=>fn,T5Model:()=>Dt,T5PreTrainedModel:()=>jt,TableTransformerForObjectDetection:()=>R0,TableTransformerModel:()=>j0,TableTransformerObjectDetectionOutput:()=>N0,TableTransformerPreTrainedModel:()=>Rd,TokenClassifierOutput:()=>Vn,TrOCRForCausalLM:()=>Tx,TrOCRPreTrainedModel:()=>Ax,UltravoxModel:()=>mp,UltravoxPreTrainedModel:()=>_b,UniSpeechForCTC:()=>tx,UniSpeechForSequenceClassification:()=>nx,UniSpeechModel:()=>ex,UniSpeechPreTrainedModel:()=>sc,UniSpeechSatForAudioFrameClassification:()=>ox,UniSpeechSatForCTC:()=>sx,UniSpeechSatForSequenceClassification:()=>ix,UniSpeechSatModel:()=>rx,UniSpeechSatPreTrainedModel:()=>va,VaultGemmaForCausalLM:()=>k_,VaultGemmaModel:()=>I_,VaultGemmaPreTrainedModel:()=>pd,ViTForImageClassification:()=>Y_,ViTMAEModel:()=>i0,ViTMAEPreTrainedModel:()=>s0,ViTMSNForImageClassification:()=>a0,ViTMSNModel:()=>o0,ViTMSNPreTrainedModel:()=>Td,ViTModel:()=>X_,ViTPreTrainedModel:()=>vd,VisionEncoderDecoderModel:()=>no,VitMatteForImageMatting:()=>h0,VitMattePreTrainedModel:()=>p0,VitPoseForPoseEstimation:()=>t0,VitPosePreTrainedModel:()=>e0,VitsModel:()=>ap,VitsModelOutput:()=>ow,VitsPreTrainedModel:()=>Bx,VoxtralForConditionalGeneration:()=>yb,Wav2Vec2BertForCTC:()=>lx,Wav2Vec2BertForSequenceClassification:()=>cx,Wav2Vec2BertModel:()=>ax,Wav2Vec2BertPreTrainedModel:()=>ic,Wav2Vec2ForAudioFrameClassification:()=>Ky,Wav2Vec2ForCTC:()=>Wy,Wav2Vec2ForSequenceClassification:()=>Hy,Wav2Vec2Model:()=>Uy,Wav2Vec2PreTrainedModel:()=>Js,WavLMForAudioFrameClassification:()=>_x,WavLMForCTC:()=>mx,WavLMForSequenceClassification:()=>fx,WavLMForXVector:()=>gx,WavLMModel:()=>hx,WavLMPreTrainedModel:()=>go,WeSpeakerResNetModel:()=>Zy,WeSpeakerResNetPreTrainedModel:()=>Jy,WhisperForConditionalGeneration:()=>eo,WhisperModel:()=>zl,WhisperPreTrainedModel:()=>Os,XLMForQuestionAnswering:()=>jl,XLMForSequenceClassification:()=>Fl,XLMForTokenClassification:()=>Ol,XLMModel:()=>Dl,XLMPreTrainedModel:()=>Qs,XLMRobertaForMaskedLM:()=>ta,XLMRobertaForQuestionAnswering:()=>Nl,XLMRobertaForSequenceClassification:()=>na,XLMRobertaForTokenClassification:()=>ra,XLMRobertaModel:()=>Rl,XLMRobertaPreTrainedModel:()=>Xs,XLMWithLMHeadModel:()=>$l,XVectorOutput:()=>sw,YolosForObjectDetection:()=>Sy,YolosModel:()=>Ey,YolosObjectDetectionOutput:()=>Ly,YolosPreTrainedModel:()=>Xd});var r=n("./src/configs.js"),s=n("./src/backends/onnx.js"),i=n("./src/utils/dtypes.js"),o=n("./src/utils/generic.js"),a=n("./src/utils/core.js"),l=n("./src/utils/hub.js"),c=n("./src/utils/constants.js"),d=n("./src/generation/logits_process.js"),u=n("./src/generation/configuration_utils.js"),p=n("./src/utils/tensor.js"),h=n("./src/utils/image.js"),m=n("./src/utils/maths.js"),g=n("./src/generation/stopping_criteria.js"),I=n("./src/generation/logits_sampler.js"),f=n("./src/env.js"),_=n("./src/models/whisper/generation_whisper.js"),T=n("./src/models/whisper/common_whisper.js");const M={EncoderOnly:0,EncoderDecoder:1,Seq2Seq:2,Vision2Seq:3,DecoderOnly:4,MaskGeneration:5,ImageTextToText:6,Musicgen:7,MultiModality:8,Phi3V:9,AudioTextToText:10,AutoEncoder:11,ImageAudioTextToText:12,Supertonic:13},v=new Map,b=new Map,A=new Map;async function k(P,C,H){let ce=H.config?.["transformers.js_config"]??{},ge=H.device??ce.device;ge&&typeof ge!="string"&&(ge.hasOwnProperty(C)?ge=ge[C]:(console.warn(`device not specified for "${C}". Using the default device.`),ge=null));const be=ge??(f.apis.IS_NODE_ENV?"cpu":"wasm"),Oe=(0,s.deviceToExecutionProviders)(be),Ge=ce.device_config??{};Ge.hasOwnProperty(be)&&(ce={...ce,...Ge[be]});let Xe=H.dtype??ce.dtype;if(typeof Xe!="string"&&(Xe&&Xe.hasOwnProperty(C)?Xe=Xe[C]:(Xe=i.DEFAULT_DEVICE_DTYPE_MAPPING[be]??i.DATA_TYPES.fp32,console.warn(`dtype not specified for "${C}". Using the default dtype (${Xe}) for this device (${be}).`))),Xe===i.DATA_TYPES.auto){let Nt=ce.dtype;typeof Nt!="string"&&(Nt=Nt?.[C]),Nt&&Nt!==i.DATA_TYPES.auto&&i.DATA_TYPES.hasOwnProperty(Nt)?Xe=Nt:Xe=i.DEFAULT_DEVICE_DTYPE_MAPPING[be]??i.DATA_TYPES.fp32}const at=Xe;if(i.DEFAULT_DTYPE_SUFFIX_MAPPING.hasOwnProperty(at)){if(at===i.DATA_TYPES.fp16&&be==="webgpu"&&!await(0,i.isWebGpuFp16Supported)())throw new Error(`The device (${be}) does not support fp16.`)}else throw new Error(`Invalid dtype: ${at}. Should be one of: ${Object.keys(i.DATA_TYPES).join(", ")}`);const Pt=ce.kv_cache_dtype,Ct=Pt?typeof Pt=="string"?Pt:Pt[at]??"float32":void 0;if(Ct&&!["float32","float16"].includes(Ct))throw new Error(`Invalid kv_cache_dtype: ${Ct}. Should be one of: float32, float16`);const $t={dtype:at,kv_cache_dtype:Ct,device:be},xt=i.DEFAULT_DTYPE_SUFFIX_MAPPING[at],Ht=`${C}${xt}.onnx`,bt=`${H.subfolder??""}/${Ht}`,yt={...H.session_options};yt.executionProviders??=Oe;const Gt=ce.free_dimension_overrides;Gt?yt.freeDimensionOverrides??=Gt:be.startsWith("webnn")&&!yt.freeDimensionOverrides&&console.warn(`WebNN does not currently support dynamic shapes and requires 'free_dimension_overrides' to be set in config.json, preferably as a field within config["transformers.js_config"]["device_config"]["${be}"]. When 'free_dimension_overrides' is not set, you may experience significant performance degradation.`);const Zt=f.apis.IS_NODE_ENV&&f.env.useFSCache,pn=(0,l.getModelFile)(P,bt,!0,H,Zt),_n=H.use_external_data_format??ce.use_external_data_format;let In=[];if(_n){let Nt;typeof _n=="object"?_n.hasOwnProperty(Ht)?Nt=_n[Ht]:_n.hasOwnProperty(C)?Nt=_n[C]:Nt=!1:Nt=_n;const Mn=+Nt;if(Mn>l.MAX_EXTERNAL_DATA_CHUNKS)throw new Error(`The number of external data chunks (${Mn}) exceeds the maximum allowed value (${l.MAX_EXTERNAL_DATA_CHUNKS}).`);for(let Xn=0;Xn<Mn;++Xn){const Mi=`${Ht}_data${Xn===0?"":"_"+Xn}`,pr=`${H.subfolder??""}/${Mi}`;In.push(new Promise(async(Xr,yo)=>{const xo=await(0,l.getModelFile)(P,pr,!0,H,Zt);Xr(xo instanceof Uint8Array?{path:Mi,data:xo}:Mi)}))}}else yt.externalData!==void 0&&(In=yt.externalData.map(async Nt=>{if(typeof Nt.data=="string"){const Mn=await(0,l.getModelFile)(P,Nt.data,!0,H);return{...Nt,data:Mn}}return Nt}));if(In.length>0){const Nt=await Promise.all(In);f.apis.IS_NODE_ENV||(yt.externalData=Nt)}if(be==="webgpu"){const Nt=(0,r.getCacheShapes)(H.config,{prefix:"present"});if(Object.keys(Nt).length>0&&!(0,s.isONNXProxy)()){const Mn={};for(const Xn in Nt)Mn[Xn]="gpu-buffer";yt.preferredOutputLocation=Mn}}return{buffer_or_path:await pn,session_options:yt,session_config:$t}}async function F(P,C,H){return Object.fromEntries(await Promise.all(Object.keys(C).map(async ce=>{const{buffer_or_path:ge,session_options:be,session_config:Oe}=await k(P,C[ce],H),Ge=await(0,s.createInferenceSession)(ge,be,Oe);return[ce,Ge]})))}async function L(P,C,H){return Object.fromEntries(await Promise.all(Object.keys(C).map(async ce=>{const ge=await(0,l.getModelJSON)(P,C[ce],!1,H);return[ce,ge]})))}function G(P,C){const H=Object.create(null),ce=[];for(const Oe of P.inputNames){const Ge=C[Oe];if(!(Ge instanceof p.Tensor)){ce.push(Oe);continue}H[Oe]=(0,s.isONNXProxy)()?Ge.clone():Ge}if(ce.length>0)throw new Error(`An error occurred during model execution: "Missing the following inputs: ${ce.join(", ")}.`);const ge=Object.keys(C).length,be=P.inputNames.length;if(ge>be){let Oe=Object.keys(C).filter(Ge=>!P.inputNames.includes(Ge));console.warn(`WARNING: Too many inputs were provided (${ge} > ${be}). The following inputs will be ignored: "${Oe.join(", ")}".`)}return H}async function j(P,C){const H=G(P,C);try{const ce=Object.fromEntries(Object.entries(H).map(([be,Oe])=>[be,Oe.ort_tensor])),ge=await(0,s.runInferenceSession)(P,ce);return R(ge)}catch(ce){const ge=Object.fromEntries(Object.entries(H).map(([be,Oe])=>{const Ge={type:Oe.type,dims:Oe.dims,location:Oe.location};return Ge.location!=="gpu-buffer"&&(Ge.data=Oe.data),[be,Ge]}));throw console.error(`An error occurred during model execution: "${ce}".`),console.error("Inputs given to model:",ge),ce}}function R(P){for(let C in P)(0,s.isONNXTensor)(P[C])?P[C]=new p.Tensor(P[C]):typeof P[C]=="object"&&R(P[C]);return P}function K(P){if(P instanceof p.Tensor)return P;if(P.length===0)throw Error("items must be non-empty");if(Array.isArray(P[0])){if(P.some(C=>C.length!==P[0].length))throw Error("Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' and/or 'truncation=True' to have batched tensors with the same length.");return new p.Tensor("int64",BigInt64Array.from(P.flat().map(C=>BigInt(C))),[P.length,P[0].length])}else return new p.Tensor("int64",BigInt64Array.from(P.map(C=>BigInt(C))),[1,P.length])}function U(P){return new p.Tensor("bool",[P],[1])}async function Y(P,C){let{encoder_outputs:H,input_ids:ce,decoder_input_ids:ge,...be}=C;if(!H){const Ge=(0,a.pick)(C,P.sessions.model.inputNames);H=(await te(P,Ge)).last_hidden_state}return be.input_ids=ge,be.encoder_hidden_states=H,P.sessions.decoder_model_merged.inputNames.includes("encoder_attention_mask")&&(be.encoder_attention_mask=C.attention_mask),await le(P,be,!0)}async function te(P,C){const H=P.sessions.model,ce=(0,a.pick)(C,H.inputNames);if(H.inputNames.includes("inputs_embeds")&&!ce.inputs_embeds){if(!C.input_ids)throw new Error("Both `input_ids` and `inputs_embeds` are missing in the model inputs.");ce.inputs_embeds=await P.encode_text({input_ids:C.input_ids})}if(H.inputNames.includes("token_type_ids")&&!ce.token_type_ids){if(!ce.input_ids)throw new Error("Both `input_ids` and `token_type_ids` are missing in the model inputs.");ce.token_type_ids=(0,p.zeros_like)(ce.input_ids)}if(H.inputNames.includes("pixel_mask")&&!ce.pixel_mask){if(!ce.pixel_values)throw new Error("Both `pixel_values` and `pixel_mask` are missing in the model inputs.");const ge=ce.pixel_values.dims;ce.pixel_mask=(0,p.ones)([ge[0],ge[2],ge[3]])}return await j(H,ce)}async function ne(P,C){const H=await P.encode(C);return await P.decode(H)}async function le(P,C,H=!1){const ce=P.sessions[H?"decoder_model_merged":"model"],{past_key_values:ge,...be}=C;if(ce.inputNames.includes("use_cache_branch")&&(be.use_cache_branch=U(!!ge)),ce.inputNames.includes("position_ids")&&be.attention_mask&&!be.position_ids){const Ge=["paligemma","gemma3_text","gemma3"].includes(P.config.model_type)?1:0;be.position_ids=$e(be,ge,Ge)}P.addPastKeyValues(be,ge);const Oe=(0,a.pick)(be,ce.inputNames);return await j(ce,Oe)}function N({modality_token_id:P,inputs_embeds:C,modality_features:H,input_ids:ce,attention_mask:ge}){const be=ce.tolist().map(at=>at.reduce((Pt,Ct,$t)=>(Ct==P&&Pt.push($t),Pt),[])),Oe=be.reduce((at,Pt)=>at+Pt.length,0),Ge=H.dims[0];if(Oe!==Ge)throw new Error(`Number of tokens and features do not match: tokens: ${Oe}, features ${Ge}`);let Xe=0;for(let at=0;at<be.length;++at){const Pt=be[at],Ct=C[at];for(let $t=0;$t<Pt.length;++$t)Ct[Pt[$t]].data.set(H[Xe++].data)}return{inputs_embeds:C,attention_mask:ge}}function oe({image_token_id:P,inputs_embeds:C,image_features:H,input_ids:ce,attention_mask:ge}){return N({modality_token_id:P,inputs_embeds:C,modality_features:H,input_ids:ce,attention_mask:ge})}function X({audio_token_id:P,inputs_embeds:C,audio_features:H,input_ids:ce,attention_mask:ge}){return N({modality_token_id:P,inputs_embeds:C,modality_features:H,input_ids:ce,attention_mask:ge})}async function D(P,{encode_function:C,merge_function:H,modality_input_name:ce,modality_output_name:ge,input_ids:be=null,attention_mask:Oe=null,position_ids:Ge=null,inputs_embeds:Xe=null,past_key_values:at=null,generation_config:Pt=null,logits_processor:Ct=null,...$t}){const xt=$t[ce];if(!Xe){if(Xe=await P.encode_text({input_ids:be,...$t}),xt&&be.dims[1]!==1){const bt=await C({[ce]:xt,...$t});({inputs_embeds:Xe,attention_mask:Oe}=H({[ge]:bt,inputs_embeds:Xe,input_ids:be,attention_mask:Oe}))}else if(at&&xt&&be.dims[1]===1){const bt=be.dims[1],yt=Object.values(at)[0].dims.at(-2);Oe=(0,p.cat)([(0,p.ones)([be.dims[0],yt]),Oe.slice(null,[Oe.dims[1]-bt,Oe.dims[1]])],1)}}if(!Ge&&P.config.model_type==="qwen2_vl"){const{image_grid_thw:bt,video_grid_thw:yt}=$t;[Ge]=P.get_rope_index(be,bt,yt,Oe)}return await le(P,{inputs_embeds:Xe,past_key_values:at,attention_mask:Oe,position_ids:Ge,generation_config:Pt,logits_processor:Ct},!0)}async function z(P,C){return await D(P,{...C,modality_input_name:"audio_values",modality_output_name:"audio_features",encode_function:P.encode_audio.bind(P),merge_function:P._merge_input_ids_with_audio_features.bind(P)})}async function se(P,C){return await D(P,{...C,modality_input_name:"pixel_values",modality_output_name:"image_features",encode_function:P.encode_image.bind(P),merge_function:P._merge_input_ids_with_image_features.bind(P)})}function me(P,C=0){const[H,ce]=P.dims,ge=P.data,be=new BigInt64Array(ge.length);for(let Oe=0;Oe<H;++Oe){const Ge=Oe*ce;let Xe=BigInt(C);for(let at=0;at<ce;++at){const Pt=Ge+at;ge[Pt]===0n?be[Pt]=BigInt(1):(be[Pt]=Xe,Xe+=ge[Pt])}}return{data:be,dims:P.dims}}function $e(P,C=null,H=0){const{input_ids:ce,inputs_embeds:ge,attention_mask:be}=P,{data:Oe,dims:Ge}=me(be,H);let Xe=new p.Tensor("int64",Oe,Ge);if(C){const at=-(ce??ge).dims.at(1);Xe=Xe.slice(null,[at,null])}return Xe}function ke(P,C,H,ce){const ge=H.past_key_values?Object.values(H.past_key_values)[0].dims.at(-2):0;if(!H.attention_mask){let be;for(const Oe of["input_ids","inputs_embeds","position_ids"])if(H[Oe]){be=H[Oe].dims;break}if(!be)throw new Error("attention_mask is not provided, and unable to infer its shape from model inputs.");H.attention_mask=(0,p.ones)([be[0],ge+be[1]])}if(H.past_key_values){const{input_ids:be,attention_mask:Oe}=H;Oe&&Oe.dims[1]>be.dims[1]||ge<be.dims[1]&&(H.input_ids=be.slice(null,[ge,null]))}return H}function Be(P,C,H,ce){return H.past_key_values&&(C=C.map(ge=>[ge.at(-1)])),{...H,decoder_input_ids:K(C)}}function Ce(P,...C){return P.config.is_encoder_decoder?Be(P,...C):ke(P,...C)}function Z(P,C,H,ce){const ge=!!H.past_key_values;return ce.guidance_scale!==null&&ce.guidance_scale>1&&(ge?H.input_ids=(0,p.cat)([H.input_ids,H.input_ids],0):(H.input_ids=(0,p.cat)([H.input_ids,(0,p.full_like)(H.input_ids,BigInt(ce.pad_token_id))],0),H.attention_mask=(0,p.cat)([H.attention_mask,(0,p.full_like)(H.attention_mask,0n)],0))),(ge||!H.pixel_values)&&(H.pixel_values=(0,p.full)([0,0,3,384,384],1)),ge&&(H.images_seq_mask=new p.Tensor("bool",new Array(1).fill(!0).fill(!1,0,1),[1,1]),H.images_emb_mask=new p.Tensor("bool",new Array(0).fill(!1),[1,1,0])),H}class V extends o.Callable{main_input_name="input_ids";forward_params=["input_ids","attention_mask"];constructor(C,H,ce){super(),this.config=C,this.sessions=H,this.configs=ce;const ge=A.get(this.constructor),be=v.get(ge);switch(this.can_generate=!1,this._forward=null,this._prepare_inputs_for_generation=null,be){case M.DecoderOnly:this.can_generate=!0,this._forward=le,this._prepare_inputs_for_generation=ke;break;case M.Seq2Seq:case M.Vision2Seq:case M.Musicgen:this.can_generate=!0,this._forward=Y,this._prepare_inputs_for_generation=Be;break;case M.EncoderDecoder:this._forward=Y;break;case M.ImageTextToText:this.can_generate=!0,this._forward=se,this._prepare_inputs_for_generation=Ce;break;case M.AudioTextToText:this.can_generate=!0,this._forward=z,this._prepare_inputs_for_generation=Ce;break;case M.Phi3V:case M.ImageAudioTextToText:this.can_generate=!0,this._prepare_inputs_for_generation=Ce;break;case M.MultiModality:this.can_generate=!0,this._prepare_inputs_for_generation=Z;break;case M.AutoEncoder:this._forward=ne;break;default:this._forward=te;break}this.can_generate&&this.forward_params.push("past_key_values"),this.custom_config=this.config["transformers.js_config"]??{}}async dispose(){const C=[];for(const H of Object.values(this.sessions))H?.handler?.dispose&&C.push(H.handler.dispose());return await Promise.all(C)}static async from_pretrained(C,{progress_callback:H=null,config:ce=null,cache_dir:ge=null,local_files_only:be=!1,revision:Oe="main",model_file_name:Ge=null,subfolder:Xe="onnx",device:at=null,dtype:Pt=null,use_external_data_format:Ct=null,session_options:$t={}}={}){let xt={progress_callback:H,config:ce,cache_dir:ge,local_files_only:be,revision:Oe,model_file_name:Ge,subfolder:Xe,device:at,dtype:Pt,use_external_data_format:Ct,session_options:$t};const Ht=A.get(this),bt=v.get(Ht);ce=xt.config=await r.AutoConfig.from_pretrained(C,xt);let yt;if(bt===M.DecoderOnly)yt=await Promise.all([F(C,{model:xt.model_file_name??"model"},xt),L(C,{generation_config:"generation_config.json"},xt)]);else if(bt===M.Seq2Seq||bt===M.Vision2Seq)yt=await Promise.all([F(C,{model:"encoder_model",decoder_model_merged:"decoder_model_merged"},xt),L(C,{generation_config:"generation_config.json"},xt)]);else if(bt===M.MaskGeneration)yt=await Promise.all([F(C,{model:"vision_encoder",prompt_encoder_mask_decoder:"prompt_encoder_mask_decoder"},xt)]);else if(bt===M.EncoderDecoder)yt=await Promise.all([F(C,{model:"encoder_model",decoder_model_merged:"decoder_model_merged"},xt)]);else if(bt===M.ImageTextToText){const Gt={embed_tokens:"embed_tokens",vision_encoder:"vision_encoder",decoder_model_merged:"decoder_model_merged"};ce.is_encoder_decoder&&(Gt.model="encoder_model"),yt=await Promise.all([F(C,Gt,xt),L(C,{generation_config:"generation_config.json"},xt)])}else if(bt===M.AudioTextToText){const Gt={embed_tokens:"embed_tokens",audio_encoder:"audio_encoder",decoder_model_merged:"decoder_model_merged"};yt=await Promise.all([F(C,Gt,xt),L(C,{generation_config:"generation_config.json"},xt)])}else if(bt===M.ImageAudioTextToText){const Gt={embed_tokens:"embed_tokens",audio_encoder:"audio_encoder",vision_encoder:"vision_encoder",decoder_model_merged:"decoder_model_merged"};yt=await Promise.all([F(C,Gt,xt),L(C,{generation_config:"generation_config.json"},xt)])}else if(bt===M.Musicgen)yt=await Promise.all([F(C,{model:"text_encoder",decoder_model_merged:"decoder_model_merged",encodec_decode:"encodec_decode"},xt),L(C,{generation_config:"generation_config.json"},xt)]);else if(bt===M.MultiModality)yt=await Promise.all([F(C,{prepare_inputs_embeds:"prepare_inputs_embeds",model:"language_model",lm_head:"lm_head",gen_head:"gen_head",gen_img_embeds:"gen_img_embeds",image_decode:"image_decode"},xt),L(C,{generation_config:"generation_config.json"},xt)]);else if(bt===M.Phi3V)yt=await Promise.all([F(C,{prepare_inputs_embeds:"prepare_inputs_embeds",model:"model",vision_encoder:"vision_encoder"},xt),L(C,{generation_config:"generation_config.json"},xt)]);else if(bt===M.AutoEncoder)yt=await Promise.all([F(C,{encoder_model:"encoder_model",decoder_model:"decoder_model"},xt)]);else if(bt===M.Supertonic)yt=await Promise.all([F(C,{text_encoder:"text_encoder",latent_denoiser:"latent_denoiser",voice_decoder:"voice_decoder"},xt)]);else{if(bt!==M.EncoderOnly){const Gt=Ht??ce?.model_type;Gt!=="custom"&&console.warn(`Model type for '${Gt}' not found, assuming encoder-only architecture. Please report this at ${c.GITHUB_ISSUE_URL}.`)}yt=await Promise.all([F(C,{model:xt.model_file_name??"model"},xt)])}return new this(ce,...yt)}async _call(C){return await this.forward(C)}async forward(C){return await this._forward(this,C)}get generation_config(){return this.configs?.generation_config??null}_get_logits_processor(C,H,ce=null){const ge=new d.LogitsProcessorList;if(C.repetition_penalty!==null&&C.repetition_penalty!==1&&ge.push(new d.RepetitionPenaltyLogitsProcessor(C.repetition_penalty)),C.no_repeat_ngram_size!==null&&C.no_repeat_ngram_size>0&&ge.push(new d.NoRepeatNGramLogitsProcessor(C.no_repeat_ngram_size)),C.bad_words_ids!==null&&ge.push(new d.NoBadWordsLogitsProcessor(C.bad_words_ids,C.eos_token_id)),C.min_length!==null&&C.eos_token_id!==null&&C.min_length>0&&ge.push(new d.MinLengthLogitsProcessor(C.min_length,C.eos_token_id)),C.min_new_tokens!==null&&C.eos_token_id!==null&&C.min_new_tokens>0&&ge.push(new d.MinNewTokensLengthLogitsProcessor(H,C.min_new_tokens,C.eos_token_id)),C.forced_bos_token_id!==null&&ge.push(new d.ForcedBOSTokenLogitsProcessor(C.forced_bos_token_id)),C.forced_eos_token_id!==null&&ge.push(new d.ForcedEOSTokenLogitsProcessor(C.max_length,C.forced_eos_token_id)),C.begin_suppress_tokens!==null){const be=H>1||C.forced_bos_token_id===null?H:H+1;ge.push(new d.SuppressTokensAtBeginLogitsProcessor(C.begin_suppress_tokens,be))}return C.guidance_scale!==null&&C.guidance_scale>1&&ge.push(new d.ClassifierFreeGuidanceLogitsProcessor(C.guidance_scale)),C.temperature===0&&C.do_sample&&(console.warn("`do_sample` changed to false because `temperature: 0` implies greedy sampling (always selecting the most likely token), which is incompatible with `do_sample: true`."),C.do_sample=!1),C.do_sample&&C.temperature!==null&&C.temperature!==1&&ge.push(new d.TemperatureLogitsWarper(C.temperature)),ce!==null&&ge.extend(ce),ge}_prepare_generation_config(C,H,ce=u.GenerationConfig){const ge={...this.config};for(const Oe of["decoder","generator","text_config"])Oe in ge&&Object.assign(ge,ge[Oe]);const be=new ce(ge);return Object.assign(be,this.generation_config??{}),C&&Object.assign(be,C),H&&Object.assign(be,(0,a.pick)(H,Object.getOwnPropertyNames(be))),be}_get_stopping_criteria(C,H=null){const ce=new g.StoppingCriteriaList;return C.max_length!==null&&ce.push(new g.MaxLengthCriteria(C.max_length,this.config.max_position_embeddings??null)),C.eos_token_id!==null&&ce.push(new g.EosTokenCriteria(C.eos_token_id)),H&&ce.extend(H),ce}_validate_model_class(){if(!this.can_generate){const C=[_p,yp,gp,fp],H=A.get(this.constructor),ce=new Set,ge=this.config.model_type;for(const Oe of C){const Ge=Oe.get(ge);Ge&&ce.add(Ge[0])}let be=`The current model class (${H}) is not compatible with \`.generate()\`, as it doesn't have a language model head.`;throw ce.size>0&&(be+=` Please use the following class instead: ${[...ce].join(", ")}`),Error(be)}}prepare_inputs_for_generation(...C){return this._prepare_inputs_for_generation(this,...C)}_update_model_kwargs_for_generation({generated_input_ids:C,outputs:H,model_inputs:ce,is_encoder_decoder:ge}){return ce.past_key_values=this.getPastKeyValues(H,ce.past_key_values),ce.input_ids=new p.Tensor("int64",C.flat(),[C.length,1]),ge||(ce.attention_mask=(0,p.cat)([ce.attention_mask,(0,p.ones)([ce.attention_mask.dims[0],1])],1)),ce.position_ids=null,ce}_prepare_model_inputs({inputs:C,bos_token_id:H,model_kwargs:ce}){const ge=(0,a.pick)(ce,this.forward_params),be=this.main_input_name;if(be in ge){if(C)throw new Error("`inputs`: {inputs}` were passed alongside {input_name} which is not allowed. Make sure to either pass {inputs} or {input_name}=...")}else ge[be]=C;return{inputs_tensor:ge[be],model_inputs:ge,model_input_name:be}}async _prepare_encoder_decoder_kwargs_for_generation({inputs_tensor:C,model_inputs:H,model_input_name:ce,generation_config:ge}){if(this.sessions.model.inputNames.includes("inputs_embeds")&&!H.inputs_embeds&&"_prepare_inputs_embeds"in this){const{input_ids:Oe,pixel_values:Ge,attention_mask:Xe,...at}=H,Pt=await this._prepare_inputs_embeds(H);H={...at,...(0,a.pick)(Pt,["inputs_embeds","attention_mask"])}}let{last_hidden_state:be}=await te(this,H);if(ge.guidance_scale!==null&&ge.guidance_scale>1)be=(0,p.cat)([be,(0,p.full_like)(be,0)],0),"attention_mask"in H&&(H.attention_mask=(0,p.cat)([H.attention_mask,(0,p.zeros_like)(H.attention_mask)],0));else if(H.decoder_input_ids){const Oe=K(H.decoder_input_ids).dims[0];if(Oe!==be.dims[0]){if(be.dims[0]!==1)throw new Error(`The encoder outputs have a different batch size (${be.dims[0]}) than the decoder inputs (${Oe}).`);be=(0,p.cat)(Array.from({length:Oe},()=>be),0)}}return H.encoder_outputs=be,H}_prepare_decoder_input_ids_for_generation({batch_size:C,model_input_name:H,model_kwargs:ce,decoder_start_token_id:ge,bos_token_id:be,generation_config:Oe}){let{decoder_input_ids:Ge,...Xe}=ce;if(!(Ge instanceof p.Tensor)){if(Ge)Array.isArray(Ge[0])||(Ge=Array.from({length:C},()=>Ge));else if(ge??=be,this.config.model_type==="musicgen")Ge=Array.from({length:C*this.config.decoder.num_codebooks},()=>[ge]);else if(Array.isArray(ge)){if(ge.length!==C)throw new Error(`\`decoder_start_token_id\` expcted to have length ${C} but got ${ge.length}`);Ge=ge}else Ge=Array.from({length:C},()=>[ge]);Ge=K(Ge)}return ce.decoder_attention_mask=(0,p.ones_like)(Ge),{input_ids:Ge,model_inputs:Xe}}async generate({inputs:C=null,generation_config:H=null,logits_processor:ce=null,stopping_criteria:ge=null,streamer:be=null,...Oe}){this._validate_model_class(),H=this._prepare_generation_config(H,Oe);let{inputs_tensor:Ge,model_inputs:Xe,model_input_name:at}=this._prepare_model_inputs({inputs:C,model_kwargs:Oe});const Pt=this.config.is_encoder_decoder;Pt&&("encoder_outputs"in Xe||(Xe=await this._prepare_encoder_decoder_kwargs_for_generation({inputs_tensor:Ge,model_inputs:Xe,model_input_name:at,generation_config:H})));let Ct;Pt?{input_ids:Ct,model_inputs:Xe}=this._prepare_decoder_input_ids_for_generation({batch_size:Xe[at].dims.at(0),model_input_name:at,model_kwargs:Xe,decoder_start_token_id:H.decoder_start_token_id,bos_token_id:H.bos_token_id,generation_config:H}):Ct=Xe[at];let $t=Ct.dims.at(-1);H.max_new_tokens!==null&&(H.max_length=$t+H.max_new_tokens);const xt=this._get_logits_processor(H,$t,ce),Ht=this._get_stopping_criteria(H,ge),bt=Xe[at].dims.at(0),yt=I.LogitsSampler.getSampler(H),Gt=new Array(bt).fill(0),Zt=Ct.tolist();be&&be.put(Zt);let pn,_n={};for(;;){if(Xe=this.prepare_inputs_for_generation(Zt,Xe,H),pn=await this.forward(Xe),H.output_attentions&&H.return_dict_in_generate){const pr=this.getAttentions(pn);for(const Xr in pr)Xr in _n||(_n[Xr]=[]),_n[Xr].push(pr[Xr])}const Nt=pn.logits.slice(null,-1,null),Mn=xt(Zt,Nt),Xn=[];for(let pr=0;pr<Mn.dims.at(0);++pr){const Xr=Mn[pr],yo=await yt(Xr);for(const[xo,gc]of yo){const Ma=BigInt(xo);Gt[pr]+=gc,Zt[pr].push(Ma),Xn.push([Ma]);break}}if(be&&be.put(Xn),Ht(Zt).every(pr=>pr))break;Xe=this._update_model_kwargs_for_generation({generated_input_ids:Xn,outputs:pn,model_inputs:Xe,is_encoder_decoder:Pt})}be&&be.end();const In=this.getPastKeyValues(pn,Xe.past_key_values,!0),Dn=new p.Tensor("int64",Zt.flat(),[Zt.length,Zt[0].length]);if(H.return_dict_in_generate)return{sequences:Dn,past_key_values:In,..._n};for(const Nt of Object.values(pn))Nt.location==="gpu-buffer"&&Nt.dispose();return Dn}getPastKeyValues(C,H,ce=!1){const ge=Object.create(null);for(const be in C)if(be.startsWith("present")){const Oe=be.replace("present_conv","past_conv").replace("present","past_key_values"),Ge=be.includes("encoder");if(Ge&&H?ge[Oe]=H[Oe]:ge[Oe]=C[be],H&&(!Ge||ce)){const Xe=H[Oe];Xe.location==="gpu-buffer"&&Xe.dispose()}}return ge}getAttentions(C){const H={};for(const ce of["cross_attentions","encoder_attentions","decoder_attentions"])for(const ge in C)ge.startsWith(ce)&&(ce in H||(H[ce]=[]),H[ce].push(C[ge]));return H}addPastKeyValues(C,H){if(H)Object.assign(C,H);else{const ce=this.sessions.decoder_model_merged??this.sessions.model,ge=(C[this.main_input_name]??C.attention_mask)?.dims?.[0]??1,be=ce?.config?.kv_cache_dtype??"float32",Oe=be==="float16"?p.DataTypeMap.float16:p.DataTypeMap.float32,Ge=(0,r.getCacheShapes)(this.config,{batch_size:ge});for(const Xe in Ge){const at=Ge[Xe].reduce((Pt,Ct)=>Pt*Ct,1);C[Xe]=new p.Tensor(be,new Oe(at),Ge[Xe])}}}async encode_image({pixel_values:C}){return(await j(this.sessions.vision_encoder,{pixel_values:C})).image_features}async encode_text({input_ids:C}){return(await j(this.sessions.embed_tokens,{input_ids:C})).inputs_embeds}async encode_audio({audio_values:C}){return(await j(this.sessions.audio_encoder,{audio_values:C})).audio_features}}class fe{}class Te extends fe{constructor({last_hidden_state:C,hidden_states:H=null,attentions:ce=null}){super(),this.last_hidden_state=C,this.hidden_states=H,this.attentions=ce}}class Ie extends V{}class Ee extends Ie{}class De extends Ie{async _call(C){return new Qn(await super._call(C))}}class xe extends Ie{async _call(C){return new St(await super._call(C))}}class ze extends Ie{async _call(C){return new Vn(await super._call(C))}}class _e extends Ie{async _call(C){return new sr(await super._call(C))}}class Le extends V{}class qe extends Le{}class Ne extends Le{async _call(C){return new Qn(await super._call(C))}}class ot extends Le{async _call(C){return new St(await super._call(C))}}class Ve extends Le{async _call(C){return new Vn(await super._call(C))}}class de extends Le{async _call(C){return new sr(await super._call(C))}}class ye extends V{}class Pe extends ye{}class ve extends ye{async _call(C){return new Qn(await super._call(C))}}class Qe extends ye{async _call(C){return new St(await super._call(C))}}class ct extends ye{async _call(C){return new Vn(await super._call(C))}}class zt extends V{}class wt extends zt{}class on extends zt{}class Tn extends V{}class kt extends Tn{}class Pn extends V{}class Zn extends Pn{}class Hs extends Pn{async _call(C){return new Qn(await super._call(C))}}class Ks extends Pn{async _call(C){return new St(await super._call(C))}}class xn extends Pn{async _call(C){return new Vn(await super._call(C))}}class Kn extends Pn{async _call(C){return new sr(await super._call(C))}}class Ar extends V{}class Ls extends Ar{}class vt extends Ar{async _call(C){return new Qn(await super._call(C))}}class rn extends Ar{async _call(C){return new St(await super._call(C))}}class hs extends Ar{async _call(C){return new Vn(await super._call(C))}}class ms extends Ar{async _call(C){return new sr(await super._call(C))}}class _r extends V{}class It extends _r{}class fs extends _r{async _call(C){return new Qn(await super._call(C))}}class S extends _r{async _call(C){return new St(await super._call(C))}}class ee extends _r{async _call(C){return new Vn(await super._call(C))}}class W extends _r{async _call(C){return new sr(await super._call(C))}}class J extends V{}class re extends J{}class pe extends J{async _call(C){return new Qn(await super._call(C))}}class Se extends J{async _call(C){return new St(await super._call(C))}}class it extends J{async _call(C){return new Vn(await super._call(C))}}class gt extends J{async _call(C){return new sr(await super._call(C))}}class Ze extends V{}class At extends Ze{}class et extends Ze{async _call(C){return new Qn(await super._call(C))}}class Lt extends Ze{async _call(C){return new St(await super._call(C))}}class Rt extends Ze{async _call(C){return new Vn(await super._call(C))}}class Fn extends Ze{async _call(C){return new sr(await super._call(C))}}class Cn extends V{}class bn extends Cn{}class Bn extends Cn{async _call(C){return new Qn(await super._call(C))}}class gs extends Cn{async _call(C){return new St(await super._call(C))}}class tr extends Cn{async _call(C){return new Vn(await super._call(C))}}class _s extends Cn{async _call(C){return new sr(await super._call(C))}}class nr extends V{}class rr extends nr{}class Dr extends nr{async _call(C){return new St(await super._call(C))}}class On extends nr{async _call(C){return new Vn(await super._call(C))}}class wn extends nr{async _call(C){return new sr(await super._call(C))}}class yr extends nr{async _call(C){return new Qn(await super._call(C))}}class $r extends V{}class Ds extends $r{}class ys extends $r{async _call(C){return new Qn(await super._call(C))}}class xs extends $r{async _call(C){return new St(await super._call(C))}}class bs extends $r{async _call(C){return new Vn(await super._call(C))}}class Fr extends V{}class or extends Fr{}class Ue extends Fr{async _call(C){return new Qn(await super._call(C))}}class Ke extends Fr{async _call(C){return new St(await super._call(C))}}class ut extends Fr{async _call(C){return new sr(await super._call(C))}}class tn extends V{}class $s extends tn{}class ws extends tn{async _call(C){return new Qn(await super._call(C))}}class Fs extends tn{async _call(C){return new St(await super._call(C))}}class Zi extends tn{async _call(C){return new Vn(await super._call(C))}}class he extends tn{async _call(C){return new sr(await super._call(C))}}class $ extends V{}class Q extends ${}class ie extends ${async _call(C){return new Qn(await super._call(C))}}class ae extends ${async _call(C){return new St(await super._call(C))}}class we extends ${async _call(C){return new sr(await super._call(C))}}class je extends V{}class nt extends je{}class rt extends je{async _call(C){return new St(await super._call(C))}}class st extends je{async _call(C){return new sr(await super._call(C))}}class Je extends je{async _call(C){return new Qn(await super._call(C))}}class jt extends V{forward_params=["input_ids","attention_mask","encoder_outputs","decoder_input_ids","decoder_attention_mask","past_key_values"]}class Dt extends jt{}class fn extends jt{}class jn extends V{}class an extends jn{}class gn extends jn{}class nn extends V{}class xr extends nn{}class Or extends nn{}class Gn extends V{}class qr extends Gn{}class un extends Gn{}class Sn extends Gn{async _call(C){return new St(await super._call(C))}}class qn extends V{}class Tr extends qn{}class Qr extends qn{}class Rn extends qn{async _call(C){return new St(await super._call(C))}}class jr extends qn{}class Nn extends V{}class dn extends Nn{}class Ln extends Nn{}class vn extends V{}class dr extends vn{}class qs extends vn{}class vs extends V{}class ea extends vs{}class kl extends vs{async _call(C){return new Qn(await super._call(C))}}class El extends vs{async _call(C){return new St(await super._call(C))}}class Sl extends vs{async _call(C){return new Vn(await super._call(C))}}class Ll extends vs{async _call(C){return new sr(await super._call(C))}}class Qs extends V{}class Dl extends Qs{}class $l extends Qs{async _call(C){return new Qn(await super._call(C))}}class Fl extends Qs{async _call(C){return new St(await super._call(C))}}class Ol extends Qs{async _call(C){return new Vn(await super._call(C))}}class jl extends Qs{async _call(C){return new sr(await super._call(C))}}class Xs extends V{}class Rl extends Xs{}class ta extends Xs{async _call(C){return new Qn(await super._call(C))}}class na extends Xs{async _call(C){return new St(await super._call(C))}}class ra extends Xs{async _call(C){return new Vn(await super._call(C))}}class Nl extends Xs{async _call(C){return new sr(await super._call(C))}}class sa extends V{}class ia extends sa{}class oa extends sa{}class Os extends V{requires_attention_mask=!1;main_input_name="input_features";forward_params=["input_features","attention_mask","decoder_input_ids","decoder_attention_mask","past_key_values"]}class zl extends Os{}class eo extends Os{_prepare_generation_config(C,H){return super._prepare_generation_config(C,H,_.WhisperGenerationConfig)}_retrieve_init_tokens(C){const H=[C.decoder_start_token_id];let ce=C.language;const ge=C.task;if(C.is_multilingual){ce||(console.warn("No language specified - defaulting to English (en)."),ce="en");const Oe=`<|${(0,T.whisper_language_to_code)(ce)}|>`;H.push(C.lang_to_id[Oe]),H.push(C.task_to_id[ge??"transcribe"])}else if(ce||ge)throw new Error("Cannot specify `task` or `language` for an English-only model. If the model is intended to be multilingual, pass `is_multilingual=true` to generate, or update the generation config.");return!C.return_timestamps&&C.no_timestamps_token_id&&H.at(-1)!==C.no_timestamps_token_id?H.push(C.no_timestamps_token_id):C.return_timestamps&&H.at(-1)===C.no_timestamps_token_id&&(console.warn("<|notimestamps|> prompt token is removed from generation_config since `return_timestamps` is set to `true`."),H.pop()),H.filter(be=>be!=null)}async generate({inputs:C=null,generation_config:H=null,logits_processor:ce=null,stopping_criteria:ge=null,...be}){H=this._prepare_generation_config(H,be);const Oe=be.decoder_input_ids??this._retrieve_init_tokens(H);if(H.return_timestamps&&(ce??=new d.LogitsProcessorList,ce.push(new d.WhisperTimeStampLogitsProcessor(H,Oe))),H.begin_suppress_tokens&&(ce??=new d.LogitsProcessorList,ce.push(new d.SuppressTokensAtBeginLogitsProcessor(H.begin_suppress_tokens,Oe.length))),H.return_token_timestamps){if(!H.alignment_heads)throw new Error("Model generation config has no `alignment_heads`, token-level timestamps not available. See https://gist.github.com/hollance/42e32852f24243b748ae6bc1f985b13a on how to add this property to the generation config.");H.task==="translate"&&console.warn("Token-level timestamps may not be reliable for task 'translate'."),H.output_attentions=!0,H.return_dict_in_generate=!0}const Ge=await super.generate({inputs:C,generation_config:H,logits_processor:ce,decoder_input_ids:Oe,...be});return H.return_token_timestamps&&(Ge.token_timestamps=this._extract_token_timestamps(Ge,H.alignment_heads,H.num_frames)),Ge}_extract_token_timestamps(C,H,ce=null,ge=.02){if(!C.cross_attentions)throw new Error("Model outputs must contain cross attentions to extract timestamps. This is most likely because the model was not exported with `output_attentions=True`.");ce==null&&console.warn("`num_frames` has not been set, meaning the entire audio will be analyzed. This may lead to inaccurate token-level timestamps for short audios (< 30 seconds).");let be=this.config.median_filter_width;be===void 0&&(console.warn("Model config has no `median_filter_width`, using default value of 7."),be=7);const Oe=C.cross_attentions,Ge=Array.from({length:this.config.decoder_layers},(bt,yt)=>(0,p.cat)(Oe.map(Gt=>Gt[yt]),2)),Xe=(0,p.stack)(H.map(([bt,yt])=>{if(bt>=Ge.length)throw new Error(`Layer index ${bt} is out of bounds for cross attentions (length ${Ge.length}).`);return ce?Ge[bt].slice(null,yt,null,[0,ce]):Ge[bt].slice(null,yt)})).transpose(1,0,2,3),[at,Pt]=(0,p.std_mean)(Xe,-2,0,!0),Ct=Xe.clone();for(let bt=0;bt<Ct.dims[0];++bt){const yt=Ct[bt];for(let Gt=0;Gt<yt.dims[0];++Gt){const Zt=yt[Gt],pn=at[bt][Gt][0].data,_n=Pt[bt][Gt][0].data;for(let In=0;In<Zt.dims[0];++In){let Dn=Zt[In].data;for(let Nt=0;Nt<Dn.length;++Nt)Dn[Nt]=(Dn[Nt]-_n[Nt])/pn[Nt];Dn.set((0,m.medianFilter)(Dn,be))}}}const $t=[(0,p.mean)(Ct,1)],xt=C.sequences.dims,Ht=new p.Tensor("float32",new Float32Array(xt[0]*xt[1]),xt);for(let bt=0;bt<xt[0];++bt){const yt=$t[bt].neg().squeeze_(0),[Gt,Zt]=(0,m.dynamic_time_warping)(yt.tolist()),pn=Array.from({length:Gt.length-1},(Dn,Nt)=>Gt[Nt+1]-Gt[Nt]),_n=(0,a.mergeArrays)([1],pn).map(Dn=>!!Dn),In=[];for(let Dn=0;Dn<_n.length;++Dn)_n[Dn]&&In.push(Zt[Dn]*ge);Ht[bt].data.set(In,1)}return Ht}}class Bl extends eo{}class aa extends V{requires_attention_mask=!1;main_input_name="input_values";forward_params=["input_values","decoder_input_ids","past_key_values"]}class Yu extends aa{}class to extends aa{}class no extends V{main_input_name="pixel_values";forward_params=["pixel_values","decoder_input_ids","encoder_hidden_states","past_key_values"]}class la extends V{forward_params=["input_ids","attention_mask","pixel_values","position_ids","past_key_values"]}class ro extends la{_merge_input_ids_with_image_features(C){const H=C.image_features.dims.at(-1),ce=C.image_features.view(-1,H);return oe({image_token_id:this.config.image_token_index,...C,image_features:ce})}}class Gl extends ro{}class Vl extends ro{}class Ul extends V{forward_params=["input_ids","inputs_embeds","attention_mask","pixel_values","encoder_outputs","decoder_input_ids","decoder_inputs_embeds","decoder_attention_mask","past_key_values"];main_input_name="inputs_embeds"}class Wl extends Ul{_merge_input_ids_with_image_features({inputs_embeds:C,image_features:H,input_ids:ce,attention_mask:ge}){return{inputs_embeds:(0,p.cat)([H,C],1),attention_mask:(0,p.cat)([(0,p.ones)(H.dims.slice(0,2)),ge],1)}}async _prepare_inputs_embeds({input_ids:C,pixel_values:H,inputs_embeds:ce,attention_mask:ge}){if(!C&&!H)throw new Error("Either `input_ids` or `pixel_values` should be provided.");let be,Oe;return C&&(be=await this.encode_text({input_ids:C})),H&&(Oe=await this.encode_image({pixel_values:H})),be&&Oe?{inputs_embeds:ce,attention_mask:ge}=this._merge_input_ids_with_image_features({inputs_embeds:be,image_features:Oe,input_ids:C,attention_mask:ge}):ce=be||Oe,{inputs_embeds:ce,attention_mask:ge}}async forward({input_ids:C,pixel_values:H,attention_mask:ce,decoder_input_ids:ge,decoder_attention_mask:be,encoder_outputs:Oe,past_key_values:Ge,inputs_embeds:Xe,decoder_inputs_embeds:at}){if(Xe||({inputs_embeds:Xe,attention_mask:ce}=await this._prepare_inputs_embeds({input_ids:C,pixel_values:H,inputs_embeds:Xe,attention_mask:ce})),!Oe){let{last_hidden_state:$t}=await te(this,{inputs_embeds:Xe,attention_mask:ce});Oe=$t}if(!at){if(!ge)throw new Error("Either `decoder_input_ids` or `decoder_inputs_embeds` should be provided.");at=await this.encode_text({input_ids:ge})}return await le(this,{inputs_embeds:at,attention_mask:be,encoder_attention_mask:ce,encoder_hidden_states:Oe,past_key_values:Ge},!0)}}class Hl extends V{forward_params=["input_ids","attention_mask","pixel_values","position_ids","past_key_values"]}class Kl extends Hl{_merge_input_ids_with_image_features(C){const H=C.image_features.dims.at(-1),ce=C.image_features.view(-1,H);return oe({image_token_id:this.config.image_token_index,...C,image_features:ce})}}class ca extends la{_merge_input_ids_with_image_features(C){const H=C.image_features.dims.at(-1),ce=C.image_features.view(-1,H);return oe({image_token_id:this.config.image_token_index,...C,image_features:ce})}}class mi extends ca{}class Ys extends V{forward_params=["input_ids","attention_mask","inputs_embeds","per_layer_inputs","position_ids","pixel_values","input_features","input_features_mask","past_key_values"]}class so extends Ys{async forward({input_ids:C=null,attention_mask:H=null,pixel_values:ce=null,input_features:ge=null,input_features_mask:be=null,position_ids:Oe=null,inputs_embeds:Ge=null,per_layer_inputs:Xe=null,past_key_values:at=null,generation_config:Pt=null,logits_processor:Ct=null,...$t}){if((!Ge||!Xe)&&({inputs_embeds:Ge,per_layer_inputs:Xe}=await j(this.sessions.embed_tokens,{input_ids:C}),C.dims[1]!==1)){if(ce){const{image_features:Ht}=await j(this.sessions.vision_encoder,{pixel_values:ce});({inputs_embeds:Ge,attention_mask:H}=this._merge_input_ids_with_image_features({image_features:Ht,inputs_embeds:Ge,input_ids:C,attention_mask:H}))}if(ge){const{audio_features:Ht}=await j(this.sessions.audio_encoder,{input_features:ge,input_features_mask:be});({inputs_embeds:Ge,attention_mask:H}=this._merge_input_ids_with_audio_features({audio_features:Ht,inputs_embeds:Ge,input_ids:C,attention_mask:H}))}}return await le(this,{inputs_embeds:Ge,per_layer_inputs:Xe,past_key_values:at,attention_mask:H,position_ids:Oe,generation_config:Pt,logits_processor:Ct},!0)}_merge_input_ids_with_image_features(C){const H=C.image_features.dims.at(-1),ce=C.image_features.view(-1,H);return oe({image_token_id:this.config.image_token_id,...C,image_features:ce})}_merge_input_ids_with_audio_features(C){const H=C.audio_features.dims.at(-1),ce=C.audio_features.view(-1,H);return X({audio_token_id:this.config.audio_token_id,...C,audio_features:ce})}}class ql extends V{forward_params=["input_ids","attention_mask","pixel_values","pixel_attention_mask","position_ids","past_key_values"]}class io extends ql{async encode_image({pixel_values:C,pixel_attention_mask:H}){return(await j(this.sessions.vision_encoder,{pixel_values:C,pixel_attention_mask:H})).image_features}_merge_input_ids_with_image_features(C){const H=C.image_features.dims.at(-1),ce=C.image_features.view(-1,H);return oe({image_token_id:this.config.image_token_id,...C,image_features:ce})}}class fi extends io{}class oo extends V{forward_params=["input_ids","inputs_embeds","attention_mask","position_ids","pixel_values","image_sizes","past_key_values"]}class ao extends oo{async forward({input_ids:C=null,attention_mask:H=null,pixel_values:ce=null,image_sizes:ge=null,position_ids:be=null,inputs_embeds:Oe=null,past_key_values:Ge=null,generation_config:Xe=null,logits_processor:at=null,...Pt}){if(!Oe){let $t;if(ce&&C.dims[1]!==1){if(!ge)throw new Error("`image_sizes` must be provided when `pixel_values` is provided.");({image_features:$t}=await j(this.sessions.vision_encoder,{pixel_values:ce,image_sizes:ge}))}else{const xt=this.config.normalized_config.hidden_size;$t=new p.Tensor("float32",[],[0,xt])}({inputs_embeds:Oe}=await j(this.sessions.prepare_inputs_embeds,{input_ids:C,image_features:$t}))}return await le(this,{inputs_embeds:Oe,past_key_values:Ge,attention_mask:H,position_ids:be,generation_config:Xe,logits_processor:at},!1)}}class Ms extends V{}class ua extends Ms{}class Ql extends Ms{static async from_pretrained(C,H={}){return super.from_pretrained(C,{...H,model_file_name:H.model_file_name??"text_model"})}}class da extends Ms{static async from_pretrained(C,H={}){return super.from_pretrained(C,{...H,model_file_name:H.model_file_name??"text_model"})}}class Xl extends Ms{static async from_pretrained(C,H={}){return super.from_pretrained(C,{...H,model_file_name:H.model_file_name??"vision_model"})}}class Yl extends Ms{static async from_pretrained(C,H={}){return super.from_pretrained(C,{...H,model_file_name:H.model_file_name??"vision_model"})}}class lo extends V{}class pa extends lo{}class gi extends lo{static async from_pretrained(C,H={}){return super.from_pretrained(C,{...H,model_file_name:H.model_file_name??"text_model"})}}class _t extends Ms{static async from_pretrained(C,H={}){return super.from_pretrained(C,{...H,model_file_name:H.model_file_name??"vision_model"})}}class Jl extends V{}class ha extends Jl{}class _i extends V{}class yi extends _i{async forward(C){const H=!C.input_ids,ce=!C.pixel_values;if(H&&ce)throw new Error("Either `input_ids` or `pixel_values` should be provided.");if(H&&(C.input_ids=(0,p.ones)([C.pixel_values.dims[0],1])),ce){const{image_size:at}=this.config.vision_config;C.pixel_values=(0,p.full)([0,3,at,at],0)}const{text_embeddings:ge,image_embeddings:be,l2norm_text_embeddings:Oe,l2norm_image_embeddings:Ge}=await super.forward(C),Xe={};return H||(Xe.text_embeddings=ge,Xe.l2norm_text_embeddings=Oe),ce||(Xe.image_embeddings=be,Xe.l2norm_image_embeddings=Ge),Xe}}class Pr extends _i{static async from_pretrained(C,H={}){return super.from_pretrained(C,{...H,model_file_name:H.model_file_name??"text_model"})}}class xi extends _i{static async from_pretrained(C,H={}){return super.from_pretrained(C,{...H,model_file_name:H.model_file_name??"vision_model"})}}class bi extends V{}class ma extends bi{}class fa extends bi{}class co extends V{}class uo extends co{}class ga extends co{}class po extends V{}class _a extends po{}class wi extends po{}class vi extends V{}class ho extends vi{}class ya extends vi{}class mo extends V{}class xa extends mo{}class ba extends mo{}class fo extends V{}class x extends fo{}class E extends fo{}class O extends V{}class B extends O{}class q extends O{}class ue extends V{}class Fe extends ue{}class We extends ue{}class tt extends V{}class dt extends tt{}class Et extends tt{}class Vt extends V{}class Yt extends Vt{}class zn extends V{}class js extends zn{}class Zl extends zn{}class Ju extends V{}class e_ extends Ju{}class t_ extends Ju{}class Zu extends V{}class n_ extends Zu{}class r_ extends Zu{}class ed extends V{}class s_ extends ed{}class i_ extends ed{}class td extends V{}class o_ extends td{}class a_ extends td{}class nd extends V{}class l_ extends nd{}class c_ extends nd{}class rd extends V{}class u_ extends rd{}class d_ extends rd{}class sd extends V{}class p_ extends sd{}class h_ extends sd{}class id extends V{}class m_ extends id{}class f_ extends id{}class od extends V{}class g_ extends od{}class __ extends od{}class ad extends V{}class y_ extends ad{}class x_ extends ad{}class ld extends V{}class b_ extends ld{}class w_ extends ld{}class cd extends V{}class v_ extends cd{}class M_ extends cd{}class ud extends V{}class A_ extends ud{}class T_ extends ud{}class dd extends V{}class P_ extends dd{}class C_ extends dd{}class pd extends V{}class I_ extends pd{}class k_ extends pd{}class hd extends V{}class E_ extends hd{}class S_ extends hd{}class md extends V{}class L_ extends md{}class D_ extends md{}class fd extends V{}class $_ extends fd{}class F_ extends fd{}class gd extends V{}class O_ extends gd{}class j_ extends gd{}class R_ extends V{forward_params=["input_ids","attention_mask","position_ids","past_key_values","pixel_values","image_grid_thw"]}class N_ extends R_{get_rope_index(C,H,ce,ge){const{vision_config:be,image_token_id:Oe,video_token_id:Ge,vision_start_token_id:Xe}=this.config,at=be.spatial_merge_size??2,Pt=[];if(H||ce){let Ct=C.tolist();ge||(ge=(0,p.ones_like)(C));const $t=ge.tolist(),xt=Array.from({length:3},Zt=>Array.from({length:C.dims[0]},pn=>Array.from({length:C.dims[1]},_n=>1))),Ht=H?H.tolist():[],bt=ce?ce.tolist():[];let yt=0,Gt=0;for(let Zt=0;Zt<Ct.length;++Zt){const pn=Ct[Zt].filter((hn,Un)=>$t[Zt][Un]==1),In=pn.reduce((hn,Un,ei)=>(Un==Xe&&hn.push(ei),hn),[]).map(hn=>pn[hn+1]),Dn=In.filter(hn=>hn==Oe).length,Nt=In.filter(hn=>hn==Ge).length;let Mn=[],Xn=0,Mi=Dn,pr=Nt;for(let hn=0;hn<In.length;++hn){const Un=pn.findIndex((Ti,Yr)=>Yr>Xn&&Ti==Oe),ei=pn.findIndex((Ti,Yr)=>Yr>Xn&&Ti==Ge),Ai=Mi>0&&Un!==-1?Un:pn.length+1,bo=pr>0&&ei!==-1?ei:pn.length+1;let _c,xp,bp,wp;Ai<bo?([xp,bp,wp]=Ht[yt],++yt,--Mi,_c=Ai):([xp,bp,wp]=bt[Gt],++Gt,--pr,_c=bo);const[AL,vp,yc]=[Number(xp),Math.floor(Number(bp)/at),Math.floor(Number(wp)/at)],Mp=_c-Xn,aw=Mn.length>0?(0,m.max)(Mn.at(-1))[0]+1:0;Mn.push(Array.from({length:3*Mp},(Ti,Yr)=>aw+Yr%Mp));const Ap=Mp+aw,xc=AL*vp*yc,TL=Array.from({length:xc},(Ti,Yr)=>Ap+Math.floor(Yr/(vp*yc))),PL=Array.from({length:xc},(Ti,Yr)=>Ap+Math.floor(Yr/yc)%vp),CL=Array.from({length:xc},(Ti,Yr)=>Ap+Yr%yc);Mn.push([TL,PL,CL].flat()),Xn=_c+xc}if(Xn<pn.length){const hn=Mn.length>0?(0,m.max)(Mn.at(-1))[0]+1:0,Un=pn.length-Xn;Mn.push(Array.from({length:3*Un},(ei,Ai)=>hn+Ai%Un))}const Xr=Mn.reduce((hn,Un)=>hn+Un.length,0),yo=new Array(Xr);let xo=0;for(let hn=0;hn<3;++hn)for(let Un=0;Un<Mn.length;++Un){const ei=Mn[Un],Ai=ei.length/3;for(let bo=hn*Ai;bo<(hn+1)*Ai;++bo)yo[xo++]=ei[bo]}let gc=0;const Ma=$t[Zt];for(let hn=0;hn<Ma.length;++hn)if(Ma[hn]==1){for(let Un=0;Un<3;++Un)xt[Un][Zt][hn]=yo[Un*Xr/3+gc];++gc}const ML=(0,m.max)(yo)[0];Pt.push(ML+1-Ct[Zt].length)}return[new p.Tensor("int64",xt.flat(1/0),[3,C.dims[0],C.dims[1]]),new p.Tensor("int64",Pt,[Pt.length,1])]}else if(ge){const{data:Ct,dims:$t}=me(ge),xt=BigInt64Array.from({length:3*Ct.length},(bt,yt)=>Ct[yt%Ct.length]),Ht=Array.from({length:$t[0]},(bt,yt)=>(0,m.max)(Ct.subarray($t[1]*yt,$t[1]*(yt+1)))[0]+1n+BigInt($t[1]));return[new p.Tensor("int64",xt,[3,...$t]),new p.Tensor("int64",Ht,[Ht.length,1])]}else{const[Ct,$t]=C.dims,xt=BigInt64Array.from({length:3*Ct*$t},(Ht,bt)=>BigInt(Math.floor(bt%$t/Ct)));return[new p.Tensor("int64",xt,[3,...C.dims]),(0,p.zeros)([Ct,1])]}}async encode_image({pixel_values:C,image_grid_thw:H}){return(await j(this.sessions.vision_encoder,{pixel_values:C,grid_thw:H})).image_features}_merge_input_ids_with_image_features(C){return oe({image_token_id:this.config.image_token_id,...C})}prepare_inputs_for_generation(C,H,ce){if(H.attention_mask&&!H.position_ids)if(!H.past_key_values)[H.position_ids,H.rope_deltas]=this.get_rope_index(H.input_ids,H.image_grid_thw,H.video_grid_thw,H.attention_mask);else{H.pixel_values=null;const ge=BigInt(Object.values(H.past_key_values)[0].dims.at(-2)),be=H.rope_deltas.map(Oe=>ge+Oe);H.position_ids=(0,p.stack)([be,be,be],0)}return H}}class _d extends V{}class z_ extends _d{}class B_ extends _d{}class yd extends V{}class G_ extends yd{}class V_ extends yd{}class xd extends V{}class U_ extends xd{}class W_ extends xd{}class bd extends V{}class H_ extends bd{}class K_ extends bd{}class wd extends V{}class q_ extends wd{}class Q_ extends wd{}class vd extends V{}class X_ extends vd{}class Y_ extends vd{async _call(C){return new St(await super._call(C))}}class Md extends V{}class J_ extends Md{}class Z_ extends Md{async _call(C){return new St(await super._call(C))}}class e0 extends V{}class t0 extends e0{}class Ad extends V{}class n0 extends Ad{}class r0 extends Ad{async _call(C){return new St(await super._call(C))}}class s0 extends V{}class i0 extends s0{}class Td extends V{}class o0 extends Td{}class a0 extends Td{async _call(C){return new St(await super._call(C))}}class l0 extends V{}class c0 extends l0{}class Pd extends V{}class u0 extends Pd{}class d0 extends Pd{async _call(C){return new St(await super._call(C))}}class p0 extends V{}class h0 extends p0{async _call(C){return new iw(await super._call(C))}}class Cd extends V{}class m0 extends Cd{}class f0 extends Cd{async _call(C){return new St(await super._call(C))}}class Id extends V{}class g0 extends Id{}class _0 extends Id{async _call(C){return new St(await super._call(C))}}class kd extends V{}class y0 extends kd{}class x0 extends kd{}class Ed extends V{}class b0 extends Ed{}class w0 extends Ed{}class Sd extends V{}class v0 extends Sd{}class M0 extends Sd{async _call(C){return new St(await super._call(C))}}class ec extends V{}class A0 extends ec{}class T0 extends ec{async _call(C){return new Dd(await super._call(C))}}class Ld extends ec{async _call(C){return new P0(await super._call(C))}}class Dd extends fe{constructor({logits:C,pred_boxes:H}){super(),this.logits=C,this.pred_boxes=H}}class P0 extends fe{constructor({logits:C,pred_boxes:H,pred_masks:ce}){super(),this.logits=C,this.pred_boxes=H,this.pred_masks=ce}}class $d extends V{}class C0 extends $d{}class I0 extends $d{async _call(C){return new wa(await super._call(C))}}class wa extends fe{constructor({logits:C,pred_boxes:H}){super(),this.logits=C,this.pred_boxes=H}}class Fd extends V{}class k0 extends Fd{}class E0 extends Fd{async _call(C){return new S0(await super._call(C))}}class S0 extends wa{}class Od extends V{}class L0 extends Od{}class D0 extends Od{async _call(C){return new $0(await super._call(C))}}class $0 extends wa{}class jd extends V{}class F0 extends jd{}class O0 extends jd{async _call(C){return new wa(await super._call(C))}}class Rd extends V{}class j0 extends Rd{}class R0 extends Rd{async _call(C){return new N0(await super._call(C))}}class N0 extends Dd{}class Nd extends V{}class z0 extends Nd{}class B0 extends Nd{async _call(C){return new St(await super._call(C))}}class zd extends V{}class G0 extends zd{}class V0 extends zd{async _call(C){return new St(await super._call(C))}}class Bd extends V{}class U0 extends Bd{}class W0 extends Bd{async _call(C){return new St(await super._call(C))}}class tc extends V{}class H0 extends tc{}class K0 extends tc{async _call(C){return new St(await super._call(C))}}class q0 extends tc{}class Gd extends V{}class Q0 extends Gd{}class X0 extends Gd{}class Vd extends V{}class Y0 extends Vd{}class J0 extends Vd{}class Z0 extends V{}class ey extends Z0{}class nc extends V{}class ty extends nc{}class ny extends nc{}class ry extends nc{}class sy extends V{}class iy extends sy{}class oy extends V{}class ay extends oy{}class ly extends V{}class cy extends ly{}class Ud extends V{}class uy extends Ud{}class dy extends Ud{}class Wd extends V{}class py extends Wd{}class hy extends Wd{}class my extends V{}class fy extends my{}class Hd extends V{}class gy extends Hd{}class _y extends Hd{async _call(C){return new St(await super._call(C))}}class Kd extends V{}class yy extends Kd{}class xy extends Kd{async _call(C){return new St(await super._call(C))}}class qd extends V{}class by extends qd{}class wy extends qd{async _call(C){return new St(await super._call(C))}}class Qd extends V{}class vy extends Qd{}class My extends Qd{async _call(C){return new St(await super._call(C))}}class Ay extends V{}class Ty extends Ay{}class Py extends V{}class Cy extends Py{}class Iy extends V{}class ky extends Iy{}class Xd extends V{}class Ey extends Xd{}class Sy extends Xd{async _call(C){return new Ly(await super._call(C))}}class Ly extends fe{constructor({logits:C,pred_boxes:H}){super(),this.logits=C,this.pred_boxes=H}}class Dy extends V{}class $y extends Dy{async get_image_embeddings({pixel_values:C}){return await te(this,{pixel_values:C})}async forward(C){!C.image_embeddings||!C.image_positional_embeddings?C={...C,...await this.get_image_embeddings(C)}:C={...C},C.input_labels??=(0,p.ones)(C.input_points.dims.slice(0,-1));const H={image_embeddings:C.image_embeddings,image_positional_embeddings:C.image_positional_embeddings};return C.input_points&&(H.input_points=C.input_points),C.input_labels&&(H.input_labels=C.input_labels),C.input_boxes&&(H.input_boxes=C.input_boxes),await j(this.sessions.prompt_encoder_mask_decoder,H)}async _call(C){return new Fy(await super._call(C))}}class Fy extends fe{constructor({iou_scores:C,pred_masks:H}){super(),this.iou_scores=C,this.pred_masks=H}}class Oy extends fe{constructor({iou_scores:C,pred_masks:H,object_score_logits:ce}){super(),this.iou_scores=C,this.pred_masks=H,this.object_score_logits=ce}}class jy extends V{}class rc extends jy{async get_image_embeddings({pixel_values:C}){return await te(this,{pixel_values:C})}async forward(C){const{num_feature_levels:H}=this.config.vision_config;if(Array.from({length:H},(Oe,Ge)=>`image_embeddings.${Ge}`).some(Oe=>!C[Oe])?C={...C,...await this.get_image_embeddings(C)}:C={...C},C.input_points){if(C.input_boxes&&C.input_boxes.dims[1]!==1)throw new Error("When both `input_points` and `input_boxes` are provided, the number of boxes per image must be 1.");const Oe=C.input_points.dims;C.input_labels??=(0,p.ones)(Oe.slice(0,-1)),C.input_boxes??=(0,p.full)([Oe[0],0,4],0)}else if(C.input_boxes){const Oe=C.input_boxes.dims;C.input_labels=(0,p.full)([Oe[0],Oe[1],0],-1n),C.input_points=(0,p.full)([Oe[0],1,0,2],0)}else throw new Error("At least one of `input_points` or `input_boxes` must be provided.");const ge=this.sessions.prompt_encoder_mask_decoder,be=(0,a.pick)(C,ge.inputNames);return await j(ge,be)}async _call(C){return new Oy(await super._call(C))}}class Ry extends rc{}class Ny extends rc{}class Yd extends V{}class zy extends Yd{}class By extends Yd{}class Jd extends V{}class Gy extends Jd{}class Vy extends Jd{}class Js extends V{}class Uy extends Js{}class Wy extends Js{async _call(C){return new Zs(await super._call(C))}}class Hy extends Js{async _call(C){return new St(await super._call(C))}}class Ky extends Js{async _call(C){return new Vn(await super._call(C))}}class qy extends V{}class Qy extends qy{async _call(C){return new Zs(await super._call(C))}}class Zd extends V{}class Xy extends Zd{}class Yy extends Zd{async _call(C){return new Vn(await super._call(C))}}class Jy extends V{}class Zy extends Jy{}class sc extends V{}class ex extends sc{}class tx extends sc{async _call(C){return new Zs(await super._call(C))}}class nx extends sc{async _call(C){return new St(await super._call(C))}}class va extends V{}class rx extends va{}class sx extends va{async _call(C){return new Zs(await super._call(C))}}class ix extends va{async _call(C){return new St(await super._call(C))}}class ox extends va{async _call(C){return new Vn(await super._call(C))}}class ic extends V{}class ax extends ic{}class lx extends ic{async _call(C){return new Zs(await super._call(C))}}class cx extends ic{async _call(C){return new St(await super._call(C))}}class S3 extends V{}class ux extends Js{}class dx extends Js{async _call(C){return new Zs(await super._call(C))}}class px extends Js{async _call(C){return new St(await super._call(C))}}class go extends V{}class hx extends go{}class mx extends go{async _call(C){return new Zs(await super._call(C))}}class fx extends go{async _call(C){return new St(await super._call(C))}}class gx extends go{async _call(C){return new sw(await super._call(C))}}class _x extends go{async _call(C){return new Vn(await super._call(C))}}class yx extends V{}class xx extends yx{}class oc extends V{}class L3 extends oc{}class bx extends oc{}class wx extends oc{async generate_speech(C,H,{threshold:ce=.5,minlenratio:ge=0,maxlenratio:be=20,vocoder:Oe=null}={}){const Ge={input_ids:C},{encoder_outputs:Xe,encoder_attention_mask:at}=await te(this,Ge),Pt=Xe.dims[1]/this.config.reduction_factor,Ct=Math.floor(Pt*be),$t=Math.floor(Pt*ge),xt=this.config.num_mel_bins;let Ht=[],bt=null,yt=null,Gt=0;for(;;){++Gt;const _n=U(!!yt);let In;yt?In=yt.output_sequence_out:In=new p.Tensor("float32",new Float32Array(xt),[1,1,xt]);let Dn={use_cache_branch:_n,output_sequence:In,encoder_attention_mask:at,speaker_embeddings:H,encoder_hidden_states:Xe};this.addPastKeyValues(Dn,bt),yt=await j(this.sessions.decoder_model_merged,Dn),bt=this.getPastKeyValues(yt,bt);const{prob:Nt,spectrum:Mn}=yt;if(Ht.push(Mn),Gt>=$t&&(Array.from(Nt.data).filter(Xn=>Xn>=ce).length>0||Gt>=Ct))break}const Zt=(0,p.cat)(Ht),{waveform:pn}=await j(Oe.sessions.model,{spectrogram:Zt});return{spectrogram:Zt,waveform:pn}}}class vx extends V{main_input_name="spectrogram"}class Mx extends V{}class ep extends Mx{async generate_speech({input_ids:C,attention_mask:H,style:ce,num_inference_steps:ge=5,speed:be=1.05}){const{sampling_rate:Oe,chunk_compress_factor:Ge,base_chunk_size:Xe,latent_dim:at}=this.config,{last_hidden_state:Pt,durations:Ct}=await j(this.sessions.text_encoder,{input_ids:C,attention_mask:H,style:ce});Ct.div_(be);const $t=Ct.max().item()*Oe,xt=Xe*Ge,Ht=Math.floor(($t+xt-1)/xt),bt=C.dims[0],yt=(0,p.ones)([bt,Ht]),Gt=(0,p.full)([bt],ge);let Zt=(0,p.randn)([bt,at*Ge,Ht]);for(let _n=0;_n<ge;++_n){const In=(0,p.full)([bt],_n);({denoised_latents:Zt}=await j(this.sessions.latent_denoiser,{style:ce,noisy_latents:Zt,latent_mask:yt,encoder_outputs:Pt,attention_mask:H,timestep:In,num_inference_steps:Gt}))}const{waveform:pn}=await j(this.sessions.voice_decoder,{latents:Zt});return{waveform:pn,durations:Ct}}}class Ax extends V{}class Tx extends Ax{}class tp extends V{}class Px extends tp{}class Cx extends tp{}class np extends V{}class Ix extends np{}class kx extends np{}class rp extends V{}class Ex extends rp{}class Sx extends rp{}class sp extends V{}class Lx extends sp{}class Dx extends sp{}class ip extends V{}class $x extends ip{}class Fx extends ip{}class op extends V{}class Ox extends op{}class jx extends op{}class ac extends V{}class Rx extends ac{}class Nx extends ac{static async from_pretrained(C,H={}){return super.from_pretrained(C,{...H,model_file_name:H.model_file_name??"text_model"})}}class zx extends ac{static async from_pretrained(C,H={}){return super.from_pretrained(C,{...H,model_file_name:H.model_file_name??"audio_model"})}}class Bx extends V{}class ap extends Bx{async _call(C){return new ow(await super._call(C))}}class lc extends V{}class D3 extends lc{}class Gx extends lc{}class Vx extends lc{}class lp extends V{}class Ux extends lp{}class Wx extends lp{}class cp extends V{}class Hx extends cp{}class Kx extends cp{async _call(C){return new St(await super._call(C))}}class up extends V{}class $3 extends up{}class F3 extends up{}class dp extends V{forward_params=["input_ids","attention_mask","encoder_outputs","decoder_input_ids","decoder_attention_mask","past_key_values"];_apply_and_filter_by_delay_pattern_mask(C){const[H,ce]=C.dims,ge=this.config.decoder.num_codebooks,be=ce-ge;let Oe=0;for(let at=0;at<C.size;++at){if(C.data[at]===this.config.decoder.pad_token_id)continue;const Pt=at%ce,Ct=Math.floor(at/ce)%ge,$t=Pt-Ct;$t>0&&$t<=be&&(C.data[Oe++]=C.data[at])}const Ge=Math.floor(H/ge),Xe=Oe/(Ge*ge);return new p.Tensor(C.type,C.data.slice(0,Oe),[Ge,ge,Xe])}prepare_inputs_for_generation(C,H,ce){let ge=structuredClone(C);for(let Oe=0;Oe<ge.length;++Oe)for(let Ge=0;Ge<ge[Oe].length;++Ge)Oe%this.config.decoder.num_codebooks>=Ge&&(ge[Oe][Ge]=BigInt(this.config.decoder.pad_token_id));return ce.guidance_scale!==null&&ce.guidance_scale>1&&(ge=ge.concat(ge)),super.prepare_inputs_for_generation(ge,H,ce)}async generate(C){const H=await super.generate(C),ce=this._apply_and_filter_by_delay_pattern_mask(H).unsqueeze_(0),{audio_values:ge}=await j(this.sessions.encodec_decode,{audio_codes:ce});return ge}}class cc extends V{}class qx extends cc{}class Qx extends cc{async _call(C){return new St(await super._call(C))}}class Xx extends cc{}class uc extends V{}class Yx extends uc{}class Jx extends uc{async _call(C){return new St(await super._call(C))}}class Zx extends uc{}class dc extends V{}class eb extends dc{}class tb extends dc{async _call(C){return new St(await super._call(C))}}class nb extends dc{}class pc extends V{}class rb extends pc{}class sb extends pc{async _call(C){return new St(await super._call(C))}}class ib extends pc{}class ob extends V{}class ab extends ob{}class lb extends V{}class cb extends lb{forward_params=["input_ids","pixel_values","images_seq_mask","images_emb_mask","attention_mask","position_ids","past_key_values"];constructor(...C){super(...C),this._generation_mode="text"}async forward(C){const H=this._generation_mode??"text";let ce;if(H==="text"||!C.past_key_values){const Xe=this.sessions.prepare_inputs_embeds,at=(0,a.pick)(C,Xe.inputNames);ce=await j(Xe,at)}else{const Xe=this.sessions.gen_img_embeds,at=(0,a.pick)({image_ids:C.input_ids},Xe.inputNames);ce=await j(Xe,at)}const ge={...C,...ce},be=await le(this,ge),Oe=this.sessions[H==="text"?"lm_head":"gen_head"];if(!Oe)throw new Error(`Unable to find "${Oe}" generation head`);const Ge=await j(Oe,(0,a.pick)(be,Oe.inputNames));return{...ce,...be,...Ge}}async generate(C){return this._generation_mode="text",super.generate(C)}async generate_images(C){this._generation_mode="image";const H=(C.inputs??C[this.main_input_name]).dims[1],ge=(await super.generate(C)).slice(null,[H,null]),be=this.sessions.image_decode,{decoded_image:Oe}=await j(be,{generated_tokens:ge}),Ge=Oe.add_(1).mul_(255/2).clamp_(0,255).to("uint8"),Xe=[];for(const at of Ge){const Pt=h.RawImage.fromTensor(at);Xe.push(Pt)}return Xe}}class ub extends fe{constructor({char_logits:C,bpe_logits:H,wp_logits:ce}){super(),this.char_logits=C,this.bpe_logits=H,this.wp_logits=ce}get logits(){return[this.char_logits,this.bpe_logits,this.wp_logits]}}class db extends V{}class pb extends db{async _call(C){return new ub(await super._call(C))}}class pp extends V{}class hb extends pp{}class mb extends pp{}class hp extends V{}class fb extends hp{}class gb extends hp{}class _b extends V{forward_params=["input_ids","attention_mask","position_ids","audio_values","past_key_values"]}class mp extends _b{_merge_input_ids_with_audio_features(C){const H=C.audio_features.dims.at(-1),ce=C.audio_features.view(-1,H);return X({audio_token_id:this.config.ignore_index??this.config.audio_token_id,...C,audio_features:ce})}}class yb extends mp{}class hc extends V{main_input_name="input_values";forward_params=["input_values"]}class xb extends fe{constructor({audio_codes:C}){super(),this.audio_codes=C}}class bb extends fe{constructor({audio_values:C}){super(),this.audio_values=C}}class wb extends hc{async encode(C){return new xb(await j(this.sessions.encoder_model,C))}async decode(C){return new bb(await j(this.sessions.decoder_model,C))}}class vb extends hc{static async from_pretrained(C,H={}){return super.from_pretrained(C,{...H,model_file_name:H.model_file_name??"encoder_model"})}}class Mb extends hc{static async from_pretrained(C,H={}){return super.from_pretrained(C,{...H,model_file_name:H.model_file_name??"decoder_model"})}}class mc extends V{main_input_name="input_values";forward_params=["input_values"]}class Ab extends fe{constructor({audio_codes:C}){super(),this.audio_codes=C}}class Tb extends fe{constructor({audio_values:C}){super(),this.audio_values=C}}class Pb extends mc{async encode(C){return new Ab(await j(this.sessions.encoder_model,C))}async decode(C){return new Tb(await j(this.sessions.decoder_model,C))}}class Cb extends mc{static async from_pretrained(C,H={}){return super.from_pretrained(C,{...H,model_file_name:H.model_file_name??"encoder_model"})}}class Ib extends mc{static async from_pretrained(C,H={}){return super.from_pretrained(C,{...H,model_file_name:H.model_file_name??"decoder_model"})}}class fc extends V{main_input_name="input_values";forward_params=["input_values"]}class kb extends fc{async encode(C){return await j(this.sessions.encoder_model,C)}async decode(C){return await j(this.sessions.decoder_model,C)}}class Eb extends fc{static async from_pretrained(C,H={}){return super.from_pretrained(C,{...H,model_file_name:H.model_file_name??"encoder_model"})}}class Sb extends fc{static async from_pretrained(C,H={}){return super.from_pretrained(C,{...H,model_file_name:H.model_file_name??"decoder_model"})}}class Jt{static MODEL_CLASS_MAPPINGS=null;static BASE_IF_FAIL=!1;static async from_pretrained(C,{progress_callback:H=null,config:ce=null,cache_dir:ge=null,local_files_only:be=!1,revision:Oe="main",model_file_name:Ge=null,subfolder:Xe="onnx",device:at=null,dtype:Pt=null,use_external_data_format:Ct=null,session_options:$t={}}={}){const xt={progress_callback:H,config:ce,cache_dir:ge,local_files_only:be,revision:Oe,model_file_name:Ge,subfolder:Xe,device:at,dtype:Pt,use_external_data_format:Ct,session_options:$t};if(xt.config=await r.AutoConfig.from_pretrained(C,xt),!this.MODEL_CLASS_MAPPINGS)throw new Error("`MODEL_CLASS_MAPPINGS` not implemented for this type of `AutoClass`: "+this.name);const Ht=xt.config.model_type;for(const bt of this.MODEL_CLASS_MAPPINGS){let yt=bt.get(Ht);if(!yt){for(const Gt of bt.values())if(Gt[0]===Ht){yt=Gt;break}if(!yt)continue}return await yt[1].from_pretrained(C,xt)}if(this.BASE_IF_FAIL)return rw.has(Ht)||console.warn(`Unknown model class "${Ht}", attempting to construct from base class.`),await V.from_pretrained(C,xt);throw Error(`Unsupported model type: ${Ht}`)}}const O3=new Map([["bert",["BertModel",Ee]],["neobert",["NeoBertModel",qe]],["modernbert",["ModernBertModel",Pe]],["nomic_bert",["NomicBertModel",kt]],["roformer",["RoFormerModel",Zn]],["electra",["ElectraModel",It]],["esm",["EsmModel",Ds]],["convbert",["ConvBertModel",Ls]],["camembert",["CamembertModel",re]],["deberta",["DebertaModel",At]],["deberta-v2",["DebertaV2Model",bn]],["mpnet",["MPNetModel",$s]],["albert",["AlbertModel",nt]],["distilbert",["DistilBertModel",rr]],["roberta",["RobertaModel",ea]],["xlm",["XLMModel",Dl]],["xlm-roberta",["XLMRobertaModel",Rl]],["clap",["ClapModel",Rx]],["clip",["CLIPModel",ua]],["clipseg",["CLIPSegModel",ma]],["chinese_clip",["ChineseCLIPModel",ha]],["siglip",["SiglipModel",pa]],["jina_clip",["JinaCLIPModel",yi]],["mobilebert",["MobileBertModel",or]],["squeezebert",["SqueezeBertModel",Q]],["wav2vec2",["Wav2Vec2Model",Uy]],["wav2vec2-bert",["Wav2Vec2BertModel",ax]],["unispeech",["UniSpeechModel",ex]],["unispeech-sat",["UniSpeechSatModel",rx]],["hubert",["HubertModel",ux]],["wavlm",["WavLMModel",hx]],["audio-spectrogram-transformer",["ASTModel",ia]],["vits",["VitsModel",ap]],["pyannote",["PyAnnoteModel",Xy]],["wespeaker-resnet",["WeSpeakerResNetModel",Zy]],["detr",["DetrModel",A0]],["rt_detr",["RTDetrModel",C0]],["rt_detr_v2",["RTDetrV2Model",k0]],["rf_detr",["RFDetrModel",L0]],["d_fine",["DFineModel",F0]],["table-transformer",["TableTransformerModel",j0]],["vit",["ViTModel",X_]],["ijepa",["IJepaModel",J_]],["pvt",["PvtModel",n0]],["vit_msn",["ViTMSNModel",o0]],["vit_mae",["ViTMAEModel",i0]],["groupvit",["GroupViTModel",c0]],["fastvit",["FastViTModel",u0]],["mobilevit",["MobileViTModel",m0]],["mobilevitv2",["MobileViTV2Model",g0]],["owlvit",["OwlViTModel",y0]],["owlv2",["Owlv2Model",b0]],["beit",["BeitModel",v0]],["deit",["DeiTModel",z0]],["hiera",["HieraModel",G0]],["convnext",["ConvNextModel",gy]],["convnextv2",["ConvNextV2Model",yy]],["dinov2",["Dinov2Model",by]],["dinov2_with_registers",["Dinov2WithRegistersModel",vy]],["dinov3_vit",["DINOv3ViTModel",Ty]],["dinov3_convnext",["DINOv3ConvNextModel",Cy]],["resnet",["ResNetModel",U0]],["swin",["SwinModel",H0]],["swin2sr",["Swin2SRModel",Q0]],["donut-swin",["DonutSwinModel",fy]],["yolos",["YolosModel",Ey]],["dpt",["DPTModel",Y0]],["glpn",["GLPNModel",py]],["hifigan",["SpeechT5HifiGan",vx]],["efficientnet",["EfficientNetModel",Hx]],["decision_transformer",["DecisionTransformerModel",ab]],["patchtst",["PatchTSTForPrediction",hb]],["patchtsmixer",["PatchTSMixerForPrediction",fb]],["mobilenet_v1",["MobileNetV1Model",qx]],["mobilenet_v2",["MobileNetV2Model",Yx]],["mobilenet_v3",["MobileNetV3Model",eb]],["mobilenet_v4",["MobileNetV4Model",rb]],["maskformer",["MaskFormerModel",uy]],["mgp-str",["MgpstrForSceneTextRecognition",pb]],["style_text_to_speech_2",["StyleTextToSpeech2Model",xx]]]),j3=new Map([["t5",["T5Model",Dt]],["longt5",["LongT5Model",an]],["mt5",["MT5Model",xr]],["bart",["BartModel",qr]],["mbart",["MBartModel",Tr]],["marian",["MarianModel",zy]],["whisper",["WhisperModel",zl]],["m2m_100",["M2M100Model",Gy]],["blenderbot",["BlenderbotModel",dn]],["blenderbot-small",["BlenderbotSmallModel",dr]]]),R3=new Map([["mimi",["MimiModel",wb]],["dac",["DacModel",Pb]],["snac",["SnacModel",kb]]]),N3=new Map([["bloom",["BloomModel",U_]],["jais",["JAISModel",_a]],["gpt2",["GPT2Model",uo]],["gptj",["GPTJModel",x]],["gpt_bigcode",["GPTBigCodeModel",B]],["gpt_neo",["GPTNeoModel",ho]],["gpt_neox",["GPTNeoXModel",xa]],["codegen",["CodeGenModel",Fe]],["llama",["LlamaModel",dt]],["nanochat",["NanoChatModel",js]],["arcee",["ArceeModel",e_]],["lfm2",["Lfm2Model",n_]],["smollm3",["SmolLM3Model",s_]],["exaone",["ExaoneModel",u_]],["olmo",["OlmoModel",m_]],["olmo2",["Olmo2Model",g_]],["mobilellm",["MobileLLMModel",p_]],["granite",["GraniteModel",y_]],["granitemoehybrid",["GraniteMoeHybridModel",b_]],["cohere",["CohereModel",v_]],["gemma",["GemmaModel",A_]],["gemma2",["Gemma2Model",P_]],["vaultgemma",["VaultGemmaModel",I_]],["gemma3_text",["Gemma3Model",E_]],["helium",["HeliumModel",o_]],["glm",["GlmModel",l_]],["openelm",["OpenELMModel",L_]],["qwen2",["Qwen2Model",$_]],["qwen3",["Qwen3Model",O_]],["phi",["PhiModel",z_]],["phi3",["Phi3Model",G_]],["mpt",["MptModel",H_]],["opt",["OPTModel",q_]],["mistral",["MistralModel",Px]],["ministral",["MinistralModel",Ix]],["ministral3",["Ministral3Model",Ex]],["ernie4_5",["Ernie4_5Model",Lx]],["starcoder2",["Starcoder2Model",$x]],["falcon",["FalconModel",Ox]],["stablelm",["StableLmModel",Ux]],["modernbert-decoder",["ModernBertDecoderModel",wt]]]),fp=new Map([["speecht5",["SpeechT5ForSpeechToText",bx]],["whisper",["WhisperForConditionalGeneration",eo]],["lite-whisper",["LiteWhisperForConditionalGeneration",Bl]],["moonshine",["MoonshineForConditionalGeneration",to]]]),Lb=new Map([["speecht5",["SpeechT5ForTextToSpeech",wx]]]),Db=new Map([["vits",["VitsModel",ap]],["musicgen",["MusicgenForConditionalGeneration",dp]],["supertonic",["SupertonicForConditionalGeneration",ep]]]),$b=new Map([["bert",["BertForSequenceClassification",xe]],["neobert",["NeoBertForSequenceClassification",ot]],["modernbert",["ModernBertForSequenceClassification",Qe]],["roformer",["RoFormerForSequenceClassification",Ks]],["electra",["ElectraForSequenceClassification",S]],["esm",["EsmForSequenceClassification",xs]],["convbert",["ConvBertForSequenceClassification",rn]],["camembert",["CamembertForSequenceClassification",Se]],["deberta",["DebertaForSequenceClassification",Lt]],["deberta-v2",["DebertaV2ForSequenceClassification",gs]],["mpnet",["MPNetForSequenceClassification",Fs]],["albert",["AlbertForSequenceClassification",rt]],["distilbert",["DistilBertForSequenceClassification",Dr]],["roberta",["RobertaForSequenceClassification",El]],["xlm",["XLMForSequenceClassification",Fl]],["xlm-roberta",["XLMRobertaForSequenceClassification",na]],["bart",["BartForSequenceClassification",Sn]],["mbart",["MBartForSequenceClassification",Rn]],["mobilebert",["MobileBertForSequenceClassification",Ke]],["squeezebert",["SqueezeBertForSequenceClassification",ae]]]),Fb=new Map([["bert",["BertForTokenClassification",ze]],["neobert",["NeoBertForTokenClassification",Ve]],["modernbert",["ModernBertForTokenClassification",ct]],["roformer",["RoFormerForTokenClassification",xn]],["electra",["ElectraForTokenClassification",ee]],["esm",["EsmForTokenClassification",bs]],["convbert",["ConvBertForTokenClassification",hs]],["camembert",["CamembertForTokenClassification",it]],["deberta",["DebertaForTokenClassification",Rt]],["deberta-v2",["DebertaV2ForTokenClassification",tr]],["mpnet",["MPNetForTokenClassification",Zi]],["distilbert",["DistilBertForTokenClassification",On]],["roberta",["RobertaForTokenClassification",Sl]],["xlm",["XLMForTokenClassification",Ol]],["xlm-roberta",["XLMRobertaForTokenClassification",ra]]]),gp=new Map([["t5",["T5ForConditionalGeneration",fn]],["longt5",["LongT5ForConditionalGeneration",gn]],["mt5",["MT5ForConditionalGeneration",Or]],["bart",["BartForConditionalGeneration",un]],["mbart",["MBartForConditionalGeneration",Qr]],["marian",["MarianMTModel",By]],["m2m_100",["M2M100ForConditionalGeneration",Vy]],["blenderbot",["BlenderbotForConditionalGeneration",Ln]],["blenderbot-small",["BlenderbotSmallForConditionalGeneration",qs]]]),_p=new Map([["bloom",["BloomForCausalLM",W_]],["gpt2",["GPT2LMHeadModel",ga]],["jais",["JAISLMHeadModel",wi]],["gptj",["GPTJForCausalLM",E]],["gpt_bigcode",["GPTBigCodeForCausalLM",q]],["gpt_neo",["GPTNeoForCausalLM",ya]],["gpt_neox",["GPTNeoXForCausalLM",ba]],["codegen",["CodeGenForCausalLM",We]],["llama",["LlamaForCausalLM",Et]],["nanochat",["NanoChatForCausalLM",Zl]],["llama4_text",["Llama4ForCausalLM",Yt]],["arcee",["ArceeForCausalLM",t_]],["lfm2",["Lfm2ForCausalLM",r_]],["smollm3",["SmolLM3ForCausalLM",i_]],["exaone",["ExaoneForCausalLM",d_]],["olmo",["OlmoForCausalLM",f_]],["olmo2",["Olmo2ForCausalLM",__]],["mobilellm",["MobileLLMForCausalLM",h_]],["granite",["GraniteForCausalLM",x_]],["granitemoehybrid",["GraniteMoeHybridForCausalLM",w_]],["cohere",["CohereForCausalLM",M_]],["gemma",["GemmaForCausalLM",T_]],["gemma2",["Gemma2ForCausalLM",C_]],["vaultgemma",["VaultGemmaForCausalLM",k_]],["gemma3_text",["Gemma3ForCausalLM",S_]],["helium",["HeliumForCausalLM",a_]],["glm",["GlmForCausalLM",c_]],["openelm",["OpenELMForCausalLM",D_]],["qwen2",["Qwen2ForCausalLM",F_]],["qwen3",["Qwen3ForCausalLM",j_]],["phi",["PhiForCausalLM",B_]],["phi3",["Phi3ForCausalLM",V_]],["mpt",["MptForCausalLM",K_]],["opt",["OPTForCausalLM",Q_]],["mbart",["MBartForCausalLM",jr]],["mistral",["MistralForCausalLM",Cx]],["ministral",["MinistralForCausalLM",kx]],["ministral3",["Ministral3ForCausalLM",Sx]],["ernie4_5",["Ernie4_5ForCausalLM",Dx]],["starcoder2",["Starcoder2ForCausalLM",Fx]],["falcon",["FalconForCausalLM",jx]],["trocr",["TrOCRForCausalLM",Tx]],["stablelm",["StableLmForCausalLM",Wx]],["modernbert-decoder",["ModernBertDecoderForCausalLM",on]],["phi3_v",["Phi3VForCausalLM",ao]]]),z3=new Map([["multi_modality",["MultiModalityCausalLM",cb]]]),Ob=new Map([["bert",["BertForMaskedLM",De]],["neobert",["NeoBertForMaskedLM",Ne]],["modernbert",["ModernBertForMaskedLM",ve]],["roformer",["RoFormerForMaskedLM",Hs]],["electra",["ElectraForMaskedLM",fs]],["esm",["EsmForMaskedLM",ys]],["convbert",["ConvBertForMaskedLM",vt]],["camembert",["CamembertForMaskedLM",pe]],["deberta",["DebertaForMaskedLM",et]],["deberta-v2",["DebertaV2ForMaskedLM",Bn]],["mpnet",["MPNetForMaskedLM",ws]],["albert",["AlbertForMaskedLM",Je]],["distilbert",["DistilBertForMaskedLM",yr]],["roberta",["RobertaForMaskedLM",kl]],["xlm",["XLMWithLMHeadModel",$l]],["xlm-roberta",["XLMRobertaForMaskedLM",ta]],["mobilebert",["MobileBertForMaskedLM",Ue]],["squeezebert",["SqueezeBertForMaskedLM",ie]]]),jb=new Map([["bert",["BertForQuestionAnswering",_e]],["neobert",["NeoBertForQuestionAnswering",de]],["roformer",["RoFormerForQuestionAnswering",Kn]],["electra",["ElectraForQuestionAnswering",W]],["convbert",["ConvBertForQuestionAnswering",ms]],["camembert",["CamembertForQuestionAnswering",gt]],["deberta",["DebertaForQuestionAnswering",Fn]],["deberta-v2",["DebertaV2ForQuestionAnswering",_s]],["mpnet",["MPNetForQuestionAnswering",he]],["albert",["AlbertForQuestionAnswering",st]],["distilbert",["DistilBertForQuestionAnswering",wn]],["roberta",["RobertaForQuestionAnswering",Ll]],["xlm",["XLMForQuestionAnswering",jl]],["xlm-roberta",["XLMRobertaForQuestionAnswering",Nl]],["mobilebert",["MobileBertForQuestionAnswering",ut]],["squeezebert",["SqueezeBertForQuestionAnswering",we]]]),yp=new Map([["vision-encoder-decoder",["VisionEncoderDecoderModel",no]],["idefics3",["Idefics3ForConditionalGeneration",io]],["smolvlm",["SmolVLMForConditionalGeneration",fi]]]),Rb=new Map([["llava",["LlavaForConditionalGeneration",ro]],["llava_onevision",["LlavaOnevisionForConditionalGeneration",Gl]],["moondream1",["Moondream1ForConditionalGeneration",Vl]],["florence2",["Florence2ForConditionalGeneration",Wl]],["qwen2-vl",["Qwen2VLForConditionalGeneration",N_]],["idefics3",["Idefics3ForConditionalGeneration",io]],["smolvlm",["SmolVLMForConditionalGeneration",fi]],["paligemma",["PaliGemmaForConditionalGeneration",Kl]],["llava_qwen2",["LlavaQwen2ForCausalLM",ca]],["gemma3n",["Gemma3nForConditionalGeneration",so]],["mistral3",["Mistral3ForConditionalGeneration",mi]]]),Nb=new Map([["ultravox",["UltravoxModel",mp]],["voxtral",["VoxtralForConditionalGeneration",yb]]]),B3=new Map([["vision-encoder-decoder",["VisionEncoderDecoderModel",no]]]),zb=new Map([["vit",["ViTForImageClassification",Y_]],["ijepa",["IJepaForImageClassification",Z_]],["pvt",["PvtForImageClassification",r0]],["vit_msn",["ViTMSNForImageClassification",a0]],["fastvit",["FastViTForImageClassification",d0]],["mobilevit",["MobileViTForImageClassification",f0]],["mobilevitv2",["MobileViTV2ForImageClassification",_0]],["beit",["BeitForImageClassification",M0]],["deit",["DeiTForImageClassification",B0]],["hiera",["HieraForImageClassification",V0]],["convnext",["ConvNextForImageClassification",_y]],["convnextv2",["ConvNextV2ForImageClassification",xy]],["dinov2",["Dinov2ForImageClassification",wy]],["dinov2_with_registers",["Dinov2WithRegistersForImageClassification",My]],["resnet",["ResNetForImageClassification",W0]],["swin",["SwinForImageClassification",K0]],["segformer",["SegformerForImageClassification",Gx]],["efficientnet",["EfficientNetForImageClassification",Kx]],["mobilenet_v1",["MobileNetV1ForImageClassification",Qx]],["mobilenet_v2",["MobileNetV2ForImageClassification",Jx]],["mobilenet_v3",["MobileNetV3ForImageClassification",tb]],["mobilenet_v4",["MobileNetV4ForImageClassification",sb]]]),Bb=new Map([["detr",["DetrForObjectDetection",T0]],["rt_detr",["RTDetrForObjectDetection",I0]],["rt_detr_v2",["RTDetrV2ForObjectDetection",E0]],["rf_detr",["RFDetrForObjectDetection",D0]],["d_fine",["DFineForObjectDetection",O0]],["table-transformer",["TableTransformerForObjectDetection",R0]],["yolos",["YolosForObjectDetection",Sy]]]),Gb=new Map([["owlvit",["OwlViTForObjectDetection",x0]],["owlv2",["Owlv2ForObjectDetection",w0]],["grounding-dino",["GroundingDinoForObjectDetection",ky]]]),_o=new Map([["detr",["DetrForSegmentation",Ld]],["clipseg",["CLIPSegForImageSegmentation",fa]]]),Vb=new Map([["segformer",["SegformerForSemanticSegmentation",Vx]],["sapiens",["SapiensForSemanticSegmentation",ty]],["swin",["SwinForSemanticSegmentation",q0]],["mobilenet_v1",["MobileNetV1ForSemanticSegmentation",Xx]],["mobilenet_v2",["MobileNetV2ForSemanticSegmentation",Zx]],["mobilenet_v3",["MobileNetV3ForSemanticSegmentation",nb]],["mobilenet_v4",["MobileNetV4ForSemanticSegmentation",ib]]]),Ub=new Map([["detr",["DetrForSegmentation",Ld]],["maskformer",["MaskFormerForInstanceSegmentation",dy]]]),Wb=new Map([["sam",["SamModel",$y]],["sam2",["Sam2Model",rc]],["edgetam",["EdgeTamModel",Ry]],["sam3_tracker",["Sam3TrackerModel",Ny]]]),Hb=new Map([["wav2vec2",["Wav2Vec2ForCTC",Wy]],["wav2vec2-bert",["Wav2Vec2BertForCTC",lx]],["unispeech",["UniSpeechForCTC",tx]],["unispeech-sat",["UniSpeechSatForCTC",sx]],["wavlm",["WavLMForCTC",mx]],["hubert",["HubertForCTC",dx]],["parakeet_ctc",["ParakeetForCTC",Qy]]]),Kb=new Map([["wav2vec2",["Wav2Vec2ForSequenceClassification",Hy]],["wav2vec2-bert",["Wav2Vec2BertForSequenceClassification",cx]],["unispeech",["UniSpeechForSequenceClassification",nx]],["unispeech-sat",["UniSpeechSatForSequenceClassification",ix]],["wavlm",["WavLMForSequenceClassification",fx]],["hubert",["HubertForSequenceClassification",px]],["audio-spectrogram-transformer",["ASTForAudioClassification",oa]]]),qb=new Map([["wavlm",["WavLMForXVector",gx]]]),Qb=new Map([["unispeech-sat",["UniSpeechSatForAudioFrameClassification",ox]],["wavlm",["WavLMForAudioFrameClassification",_x]],["wav2vec2",["Wav2Vec2ForAudioFrameClassification",Ky]],["pyannote",["PyAnnoteForAudioFrameClassification",Yy]]]),Xb=new Map([["vitmatte",["VitMatteForImageMatting",h0]]]),G3=new Map([["patchtst",["PatchTSTForPrediction",mb]],["patchtsmixer",["PatchTSMixerForPrediction",gb]]]),Yb=new Map([["swin2sr",["Swin2SRForImageSuperResolution",X0]]]),Jb=new Map([["dpt",["DPTForDepthEstimation",J0]],["depth_anything",["DepthAnythingForDepthEstimation",ey]],["glpn",["GLPNForDepthEstimation",hy]],["sapiens",["SapiensForDepthEstimation",ny]],["depth_pro",["DepthProForDepthEstimation",iy]],["metric3d",["Metric3DForDepthEstimation",ay]],["metric3dv2",["Metric3Dv2ForDepthEstimation",cy]]]),Zb=new Map([["sapiens",["SapiensForNormalEstimation",ry]]]),ew=new Map([["vitpose",["VitPoseForPoseEstimation",t0]]]),tw=new Map([["clip",["CLIPVisionModelWithProjection",Yl]],["siglip",["SiglipVisionModel",_t]],["jina_clip",["JinaCLIPVisionModel",xi]]]),nw=[[O3,M.EncoderOnly],[j3,M.EncoderDecoder],[N3,M.DecoderOnly],[R3,M.AutoEncoder],[$b,M.EncoderOnly],[Fb,M.EncoderOnly],[gp,M.Seq2Seq],[fp,M.Seq2Seq],[_p,M.DecoderOnly],[z3,M.MultiModality],[Ob,M.EncoderOnly],[jb,M.EncoderOnly],[yp,M.Vision2Seq],[Rb,M.ImageTextToText],[Nb,M.AudioTextToText],[zb,M.EncoderOnly],[_o,M.EncoderOnly],[Ub,M.EncoderOnly],[Vb,M.EncoderOnly],[Xb,M.EncoderOnly],[G3,M.EncoderOnly],[Yb,M.EncoderOnly],[Jb,M.EncoderOnly],[Zb,M.EncoderOnly],[ew,M.EncoderOnly],[Bb,M.EncoderOnly],[Gb,M.EncoderOnly],[Wb,M.MaskGeneration],[Hb,M.EncoderOnly],[Kb,M.EncoderOnly],[Lb,M.Seq2Seq],[Db,M.EncoderOnly],[qb,M.EncoderOnly],[Qb,M.EncoderOnly],[tw,M.EncoderOnly]];for(const[P,C]of nw)for(const[H,ce]of P.values())v.set(H,C),A.set(ce,H),b.set(H,ce);const V3=[["MusicgenForConditionalGeneration",dp,M.Musicgen],["Phi3VForCausalLM",ao,M.Phi3V],["CLIPTextModelWithProjection",da,M.EncoderOnly],["SiglipTextModel",gi,M.EncoderOnly],["JinaCLIPTextModel",Pr,M.EncoderOnly],["ClapTextModelWithProjection",Nx,M.EncoderOnly],["ClapAudioModelWithProjection",zx,M.EncoderOnly],["DacEncoderModel",Cb,M.EncoderOnly],["DacDecoderModel",Ib,M.EncoderOnly],["MimiEncoderModel",vb,M.EncoderOnly],["MimiDecoderModel",Mb,M.EncoderOnly],["SnacEncoderModel",Eb,M.EncoderOnly],["SnacDecoderModel",Sb,M.EncoderOnly],["Gemma3nForConditionalGeneration",so,M.ImageAudioTextToText],["SupertonicForConditionalGeneration",ep,M.Supertonic]];for(const[P,C,H]of V3)v.set(P,H),A.set(C,P),b.set(P,C);const rw=new Map([["modnet",_o],["birefnet",_o],["isnet",_o],["ben",_o]]);for(const[P,C]of rw.entries())C.set(P,["PreTrainedModel",V]),v.set(P,M.EncoderOnly),A.set(V,P),b.set(P,V);class U3 extends Jt{static MODEL_CLASS_MAPPINGS=nw.map(C=>C[0]);static BASE_IF_FAIL=!0}class W3 extends Jt{static MODEL_CLASS_MAPPINGS=[$b]}class H3 extends Jt{static MODEL_CLASS_MAPPINGS=[Fb]}class K3 extends Jt{static MODEL_CLASS_MAPPINGS=[gp]}class q3 extends Jt{static MODEL_CLASS_MAPPINGS=[fp]}class Q3 extends Jt{static MODEL_CLASS_MAPPINGS=[Lb]}class X3 extends Jt{static MODEL_CLASS_MAPPINGS=[Db]}class Y3 extends Jt{static MODEL_CLASS_MAPPINGS=[_p]}class J3 extends Jt{static MODEL_CLASS_MAPPINGS=[Ob]}class Z3 extends Jt{static MODEL_CLASS_MAPPINGS=[jb]}class eL extends Jt{static MODEL_CLASS_MAPPINGS=[yp]}class tL extends Jt{static MODEL_CLASS_MAPPINGS=[zb]}class nL extends Jt{static MODEL_CLASS_MAPPINGS=[_o]}class rL extends Jt{static MODEL_CLASS_MAPPINGS=[Vb]}class sL extends Jt{static MODEL_CLASS_MAPPINGS=[Ub]}class iL extends Jt{static MODEL_CLASS_MAPPINGS=[Bb]}class oL extends Jt{static MODEL_CLASS_MAPPINGS=[Gb]}class aL extends Jt{static MODEL_CLASS_MAPPINGS=[Wb]}class lL extends Jt{static MODEL_CLASS_MAPPINGS=[Hb]}class cL extends Jt{static MODEL_CLASS_MAPPINGS=[Kb]}class uL extends Jt{static MODEL_CLASS_MAPPINGS=[qb]}class dL extends Jt{static MODEL_CLASS_MAPPINGS=[Qb]}class pL extends Jt{static MODEL_CLASS_MAPPINGS=[B3]}class hL extends Jt{static MODEL_CLASS_MAPPINGS=[Xb]}class mL extends Jt{static MODEL_CLASS_MAPPINGS=[Yb]}class fL extends Jt{static MODEL_CLASS_MAPPINGS=[Jb]}class gL extends Jt{static MODEL_CLASS_MAPPINGS=[Zb]}class _L extends Jt{static MODEL_CLASS_MAPPINGS=[ew]}class yL extends Jt{static MODEL_CLASS_MAPPINGS=[tw]}class xL extends Jt{static MODEL_CLASS_MAPPINGS=[Rb]}class bL extends Jt{static MODEL_CLASS_MAPPINGS=[Nb]}class wL extends fe{constructor({logits:C,past_key_values:H,encoder_outputs:ce,decoder_attentions:ge=null,cross_attentions:be=null}){super(),this.logits=C,this.past_key_values=H,this.encoder_outputs=ce,this.decoder_attentions=ge,this.cross_attentions=be}}class St extends fe{constructor({logits:C,...H}){super(),this.logits=C;const ce=Object.values(H);ce.length>0&&(this.attentions=ce)}}class sw extends fe{constructor({logits:C,embeddings:H}){super(),this.logits=C,this.embeddings=H}}class Vn extends fe{constructor({logits:C}){super(),this.logits=C}}class Qn extends fe{constructor({logits:C}){super(),this.logits=C}}class sr extends fe{constructor({start_logits:C,end_logits:H}){super(),this.start_logits=C,this.end_logits=H}}class Zs extends fe{constructor({logits:C}){super(),this.logits=C}}class vL extends fe{constructor({logits:C,past_key_values:H}){super(),this.logits=C,this.past_key_values=H}}class iw extends fe{constructor({alphas:C}){super(),this.alphas=C}}class ow extends fe{constructor({waveform:C,spectrogram:H}){super(),this.waveform=C,this.spectrogram=H}}}),"./src/models/audio_spectrogram_transformer/feature_extraction_audio_spectrogram_transformer.js":((e,t,n)=>{n.r(t),n.d(t,{ASTFeatureExtractor:()=>i});var r=n("./src/base/feature_extraction_utils.js");n("./src/utils/tensor.js");var s=n("./src/utils/audio.js");class i extends r.FeatureExtractor{constructor(a){super(a);const l=this.config.sampling_rate,c=(0,s.mel_filter_bank)(257,this.config.num_mel_bins,20,Math.floor(l/2),l,null,"kaldi",!0);this.mel_filters=c,this.window=(0,s.window_function)(400,"hann",{periodic:!1}),this.mean=this.config.mean,this.std=this.config.std}async _extract_fbank_features(a,l){return(0,s.spectrogram)(a,this.window,400,160,{fft_length:512,power:2,center:!1,preemphasis:.97,mel_filters:this.mel_filters,log_mel:"log",mel_floor:1192092955078125e-22,remove_dc_offset:!0,max_num_frames:l,transpose:!0})}async _call(a){(0,r.validate_audio_inputs)(a,"ASTFeatureExtractor");const l=await this._extract_fbank_features(a,this.config.max_length);if(this.config.do_normalize){const c=this.std*2,d=l.data;for(let u=0;u<d.length;++u)d[u]=(d[u]-this.mean)/c}return{input_values:l.unsqueeze_(0)}}}}),"./src/models/auto/feature_extraction_auto.js":((e,t,n)=>{n.r(t),n.d(t,{AutoFeatureExtractor:()=>o});var r=n("./src/utils/constants.js"),s=n("./src/utils/hub.js");n("./src/base/feature_extraction_utils.js");var i=n("./src/models/feature_extractors.js");class o{static async from_pretrained(l,c={}){const d=await(0,s.getModelJSON)(l,r.FEATURE_EXTRACTOR_NAME,!0,c),u=d.feature_extractor_type,p=i[u];if(!p)throw new Error(`Unknown feature_extractor_type: '${u}'. Please report this at ${r.GITHUB_ISSUE_URL}.`);return new p(d)}}}),"./src/models/auto/image_processing_auto.js":((e,t,n)=>{n.r(t),n.d(t,{AutoImageProcessor:()=>a});var r=n("./src/utils/constants.js"),s=n("./src/utils/hub.js"),i=n("./src/base/image_processors_utils.js"),o=n("./src/models/image_processors.js");class a{static async from_pretrained(c,d={}){const u=await(0,s.getModelJSON)(c,r.IMAGE_PROCESSOR_NAME,!0,d),p=u.image_processor_type??u.feature_extractor_type;let h=o[p?.replace(/Fast$/,"")];return h||(p!==void 0&&console.warn(`Image processor type '${p}' not found, assuming base ImageProcessor. Please report this at ${r.GITHUB_ISSUE_URL}.`),h=i.ImageProcessor),new h(u)}}}),"./src/models/auto/processing_auto.js":((e,t,n)=>{n.r(t),n.d(t,{AutoProcessor:()=>c});var r=n("./src/utils/constants.js"),s=n("./src/utils/hub.js"),i=n("./src/base/processing_utils.js"),o=n("./src/models/processors.js"),a=n("./src/models/image_processors.js"),l=n("./src/models/feature_extractors.js");class c{static async from_pretrained(u,p={}){const h=await(0,s.getModelJSON)(u,r.IMAGE_PROCESSOR_NAME,!0,p),{image_processor_type:m,feature_extractor_type:g,processor_class:I}=h;if(I&&o[I])return o[I].from_pretrained(u,p);if(!m&&!g)throw new Error("No `image_processor_type` or `feature_extractor_type` found in the config.");const f={};if(m){const T=a[m.replace(/Fast$/,"")];if(!T)throw new Error(`Unknown image_processor_type: '${m}'.`);f.image_processor=new T(h)}if(g){const T=a[g];if(T)f.image_processor=new T(h);else{const M=l[g];if(!M)throw new Error(`Unknown feature_extractor_type: '${g}'.`);f.feature_extractor=new M(h)}}const _={};return new i.Processor(_,f,null)}}}),"./src/models/beit/image_processing_beit.js":((e,t,n)=>{n.r(t),n.d(t,{BeitFeatureExtractor:()=>s});var r=n("./src/base/image_processors_utils.js");class s extends r.ImageProcessor{}}),"./src/models/bit/image_processing_bit.js":((e,t,n)=>{n.r(t),n.d(t,{BitImageProcessor:()=>s});var r=n("./src/base/image_processors_utils.js");class s extends r.ImageProcessor{}}),"./src/models/chinese_clip/image_processing_chinese_clip.js":((e,t,n)=>{n.r(t),n.d(t,{ChineseCLIPFeatureExtractor:()=>s});var r=n("./src/base/image_processors_utils.js");class s extends r.ImageProcessor{}}),"./src/models/clap/feature_extraction_clap.js":((e,t,n)=>{n.r(t),n.d(t,{ClapFeatureExtractor:()=>i});var r=n("./src/base/feature_extraction_utils.js");n("./src/utils/tensor.js");var s=n("./src/utils/audio.js");class i extends r.FeatureExtractor{constructor(a){super(a),this.mel_filters=(0,s.mel_filter_bank)(this.config.nb_frequency_bins,this.config.feature_size,this.config.frequency_min,this.config.frequency_max,this.config.sampling_rate,null,"htk"),this.mel_filters_slaney=(0,s.mel_filter_bank)(this.config.nb_frequency_bins,this.config.feature_size,this.config.frequency_min,this.config.frequency_max,this.config.sampling_rate,"slaney","slaney"),this.window=(0,s.window_function)(this.config.fft_window_size,"hann")}async _get_input_mel(a,l,c,d){let u;const p=a.length-l;if(p>0)if(c==="rand_trunc"){const h=Math.floor(Math.random()*(p+1));a=a.subarray(h,h+l),u=await this._extract_fbank_features(a,this.mel_filters_slaney,this.config.nb_max_samples)}else throw new Error(`Truncation strategy "${c}" not implemented`);else{if(p<0){let h=new Float64Array(l);if(h.set(a),d==="repeat")for(let m=a.length;m<l;m+=a.length)h.set(a.subarray(0,Math.min(a.length,l-m)),m);else if(d==="repeatpad")for(let m=a.length;m<-p;m+=a.length)h.set(a,m);a=h}if(c==="fusion")throw new Error(`Truncation strategy "${c}" not implemented`);u=await this._extract_fbank_features(a,this.mel_filters_slaney,this.config.nb_max_samples)}return u.unsqueeze_(0)}async _extract_fbank_features(a,l,c=null){return(0,s.spectrogram)(a,this.window,this.config.fft_window_size,this.config.hop_length,{power:2,mel_filters:l,log_mel:"dB",max_num_frames:c,do_pad:!1,transpose:!0})}async _call(a,{max_length:l=null}={}){return(0,r.validate_audio_inputs)(a,"ClapFeatureExtractor"),{input_features:(await this._get_input_mel(a,l??this.config.nb_max_samples,this.config.truncation,this.config.padding)).unsqueeze_(0)}}}}),"./src/models/clip/image_processing_clip.js":((e,t,n)=>{n.r(t),n.d(t,{CLIPFeatureExtractor:()=>i,CLIPImageProcessor:()=>s});var r=n("./src/base/image_processors_utils.js");class s extends r.ImageProcessor{}class i extends s{}}),"./src/models/convnext/image_processing_convnext.js":((e,t,n)=>{n.r(t),n.d(t,{ConvNextFeatureExtractor:()=>i,ConvNextImageProcessor:()=>s});var r=n("./src/base/image_processors_utils.js");class s extends r.ImageProcessor{constructor(a){super(a),this.crop_pct=this.config.crop_pct??224/256}async resize(a){const l=this.size?.shortest_edge;if(l===void 0)throw new Error("Size dictionary must contain 'shortest_edge' key.");if(l<384){const c=Math.floor(l/this.crop_pct),[d,u]=this.get_resize_output_image_size(a,{shortest_edge:c});a=await a.resize(d,u,{resample:this.resample}),a=await a.center_crop(l,l)}else a=await a.resize(l,l,{resample:this.resample});return a}}class i extends s{}}),"./src/models/dac/feature_extraction_dac.js":((e,t,n)=>{n.r(t),n.d(t,{DacFeatureExtractor:()=>s});var r=n("./src/models/encodec/feature_extraction_encodec.js");class s extends r.EncodecFeatureExtractor{}}),"./src/models/deit/image_processing_deit.js":((e,t,n)=>{n.r(t),n.d(t,{DeiTFeatureExtractor:()=>i,DeiTImageProcessor:()=>s});var r=n("./src/base/image_processors_utils.js");class s extends r.ImageProcessor{}class i extends s{}}),"./src/models/detr/image_processing_detr.js":((e,t,n)=>{n.r(t),n.d(t,{DetrFeatureExtractor:()=>o,DetrImageProcessor:()=>i});var r=n("./src/base/image_processors_utils.js"),s=n("./src/utils/tensor.js");class i extends r.ImageProcessor{async _call(l){const c=await super._call(l),d=[c.pixel_values.dims[0],64,64],u=(0,s.full)(d,1n);return{...c,pixel_mask:u}}post_process_object_detection(...l){return(0,r.post_process_object_detection)(...l)}post_process_panoptic_segmentation(...l){return(0,r.post_process_panoptic_segmentation)(...l)}post_process_instance_segmentation(...l){return(0,r.post_process_instance_segmentation)(...l)}}class o extends i{}}),"./src/models/dinov3_vit/image_processing_dinov3_vit.js":((e,t,n)=>{n.r(t),n.d(t,{DINOv3ViTImageProcessor:()=>s});var r=n("./src/base/image_processors_utils.js");class s extends r.ImageProcessor{}}),"./src/models/donut/image_processing_donut.js":((e,t,n)=>{n.r(t),n.d(t,{DonutFeatureExtractor:()=>i,DonutImageProcessor:()=>s});var r=n("./src/base/image_processors_utils.js");class s extends r.ImageProcessor{pad_image(a,l,c,d={}){const[u,p,h]=l;let m=this.image_mean;Array.isArray(this.image_mean)||(m=new Array(h).fill(m));let g=this.image_std;Array.isArray(g)||(g=new Array(h).fill(m));const I=m.map((f,_)=>-f/g[_]);return super.pad_image(a,l,c,{center:!0,constant_values:I,...d})}}class i extends s{}}),"./src/models/dpt/image_processing_dpt.js":((e,t,n)=>{n.r(t),n.d(t,{DPTFeatureExtractor:()=>i,DPTImageProcessor:()=>s});var r=n("./src/base/image_processors_utils.js");class s extends r.ImageProcessor{}class i extends s{}}),"./src/models/efficientnet/image_processing_efficientnet.js":((e,t,n)=>{n.r(t),n.d(t,{EfficientNetImageProcessor:()=>s});var r=n("./src/base/image_processors_utils.js");class s extends r.ImageProcessor{constructor(o){super(o),this.include_top=this.config.include_top??!0,this.include_top&&(this.image_std=this.image_std.map(a=>a*a))}}}),"./src/models/encodec/feature_extraction_encodec.js":((e,t,n)=>{n.r(t),n.d(t,{EncodecFeatureExtractor:()=>i});var r=n("./src/base/feature_extraction_utils.js"),s=n("./src/utils/tensor.js");class i extends r.FeatureExtractor{async _call(a){(0,r.validate_audio_inputs)(a,"EncodecFeatureExtractor"),a instanceof Float64Array&&(a=new Float32Array(a));const l=this.config.feature_size;if(a.length%l!==0)throw new Error(`The length of the audio data must be a multiple of the number of channels (${l}).`);const c=[1,l,a.length/l];return{input_values:new s.Tensor("float32",a,c)}}}}),"./src/models/feature_extractors.js":((e,t,n)=>{n.r(t),n.d(t,{ASTFeatureExtractor:()=>r.ASTFeatureExtractor,ClapFeatureExtractor:()=>i.ClapFeatureExtractor,DacFeatureExtractor:()=>o.DacFeatureExtractor,EncodecFeatureExtractor:()=>s.EncodecFeatureExtractor,Gemma3nAudioFeatureExtractor:()=>a.Gemma3nAudioFeatureExtractor,ImageFeatureExtractor:()=>f.ImageProcessor,MoonshineFeatureExtractor:()=>l.MoonshineFeatureExtractor,ParakeetFeatureExtractor:()=>c.ParakeetFeatureExtractor,PyAnnoteFeatureExtractor:()=>d.PyAnnoteFeatureExtractor,SeamlessM4TFeatureExtractor:()=>u.SeamlessM4TFeatureExtractor,SnacFeatureExtractor:()=>p.SnacFeatureExtractor,SpeechT5FeatureExtractor:()=>h.SpeechT5FeatureExtractor,Wav2Vec2FeatureExtractor:()=>m.Wav2Vec2FeatureExtractor,WeSpeakerFeatureExtractor:()=>g.WeSpeakerFeatureExtractor,WhisperFeatureExtractor:()=>I.WhisperFeatureExtractor});var r=n("./src/models/audio_spectrogram_transformer/feature_extraction_audio_spectrogram_transformer.js"),s=n("./src/models/encodec/feature_extraction_encodec.js"),i=n("./src/models/clap/feature_extraction_clap.js"),o=n("./src/models/dac/feature_extraction_dac.js"),a=n("./src/models/gemma3n/feature_extraction_gemma3n.js"),l=n("./src/models/moonshine/feature_extraction_moonshine.js"),c=n("./src/models/parakeet/feature_extraction_parakeet.js"),d=n("./src/models/pyannote/feature_extraction_pyannote.js"),u=n("./src/models/seamless_m4t/feature_extraction_seamless_m4t.js"),p=n("./src/models/snac/feature_extraction_snac.js"),h=n("./src/models/speecht5/feature_extraction_speecht5.js"),m=n("./src/models/wav2vec2/feature_extraction_wav2vec2.js"),g=n("./src/models/wespeaker/feature_extraction_wespeaker.js"),I=n("./src/models/whisper/feature_extraction_whisper.js"),f=n("./src/base/image_processors_utils.js")}),"./src/models/florence2/processing_florence2.js":((e,t,n)=>{n.r(t),n.d(t,{Florence2Processor:()=>o});var r=n("./src/base/processing_utils.js"),s=n("./src/models/auto/image_processing_auto.js"),i=n("./src/tokenizers.js");class o extends r.Processor{static tokenizer_class=i.AutoTokenizer;static image_processor_class=s.AutoImageProcessor;constructor(l,c,d){super(l,c,d);const{tasks_answer_post_processing_type:u,task_prompts_without_inputs:p,task_prompts_with_input:h}=this.image_processor.config;this.tasks_answer_post_processing_type=new Map(Object.entries(u??{})),this.task_prompts_without_inputs=new Map(Object.entries(p??{})),this.task_prompts_with_input=new Map(Object.entries(h??{})),this.regexes={quad_boxes:/(.+?)<loc_(\d+)><loc_(\d+)><loc_(\d+)><loc_(\d+)><loc_(\d+)><loc_(\d+)><loc_(\d+)><loc_(\d+)>/gm,bboxes:/([^<]+)?<loc_(\d+)><loc_(\d+)><loc_(\d+)><loc_(\d+)>/gm},this.size_per_bin=1e3}construct_prompts(l){typeof l=="string"&&(l=[l]);const c=[];for(const d of l)if(this.task_prompts_without_inputs.has(d))c.push(this.task_prompts_without_inputs.get(d));else{for(const[u,p]of this.task_prompts_with_input)if(d.includes(u)){c.push(p.replaceAll("{input}",d).replaceAll(u,""));break}c.length!==l.length&&c.push(d)}return c}post_process_generation(l,c,d){const u=this.tasks_answer_post_processing_type.get(c)??"pure_text";l=l.replaceAll("<s>","").replaceAll("</s>","");let p;switch(u){case"pure_text":p=l;break;case"description_with_bboxes":case"bboxes":case"phrase_grounding":case"ocr":const h=u==="ocr"?"quad_boxes":"bboxes",m=l.matchAll(this.regexes[h]),g=[],I=[];for(const[f,_,...T]of m)g.push(_?_.trim():g.at(-1)??""),I.push(T.map((M,v)=>(Number(M)+.5)/this.size_per_bin*d[v%2]));p={labels:g,[h]:I};break;default:throw new Error(`Task "${c}" (of type "${u}") not yet implemented.`)}return{[c]:p}}async _call(l,c=null,d={}){if(!l&&!c)throw new Error("Either text or images must be provided");const u=await this.image_processor(l,d),p=c?this.tokenizer(this.construct_prompts(c),d):{};return{...u,...p}}}}),"./src/models/gemma3n/feature_extraction_gemma3n.js":((e,t,n)=>{n.r(t),n.d(t,{Gemma3nAudioFeatureExtractor:()=>o});var r=n("./src/base/feature_extraction_utils.js"),s=n("./src/utils/tensor.js"),i=n("./src/utils/audio.js");class o extends r.FeatureExtractor{constructor(l){super(l);const{fft_length:c,feature_size:d,min_frequency:u,max_frequency:p,sampling_rate:h,frame_length:m}=this.config,g=(0,i.mel_filter_bank)(Math.floor(1+c/2),d,u,p,h,null,"htk",!1);this.mel_filters=g,this.window=(0,i.window_function)(m,"hann")}async _extract_fbank_features(l,c){return(0,i.spectrogram)(l,this.window,this.config.frame_length,this.config.hop_length,{fft_length:this.config.fft_length,center:!1,onesided:!0,preemphasis:this.config.preemphasis,preemphasis_htk_flavor:this.config.preemphasis_htk_flavor,mel_filters:this.mel_filters,log_mel:"log",mel_floor:this.config.mel_floor,remove_dc_offset:!1,transpose:!0})}async _call(l,{max_length:c=48e4,truncation:d=!0,padding:u=!0,pad_to_multiple_of:p=128}={}){if((0,r.validate_audio_inputs)(l,"Gemma3nAudioFeatureExtractor"),d&&l.length>c&&(l=l.slice(0,c)),u&&l.length%p!==0){const g=p-l.length%p,I=new Float64Array(l.length+g);I.set(l),this.config.padding_value!==0&&I.fill(this.config.padding_value,l.length),l=I}const h=await this._extract_fbank_features(l,this.config.max_length),m=(0,s.full)([1,h.dims[0]],!0);return{input_features:h.unsqueeze_(0),input_features_mask:m}}}}),"./src/models/gemma3n/processing_gemma3n.js":((e,t,n)=>{n.r(t),n.d(t,{Gemma3nProcessor:()=>a});var r=n("./src/base/processing_utils.js"),s=n("./src/models/auto/image_processing_auto.js"),i=n("./src/models/auto/feature_extraction_auto.js"),o=n("./src/tokenizers.js");n("./src/utils/image.js"),n("./src/utils/audio.js");class a extends r.Processor{static image_processor_class=s.AutoImageProcessor;static feature_extractor_class=i.AutoFeatureExtractor;static tokenizer_class=o.AutoTokenizer;static uses_processor_config=!0;static uses_chat_template_file=!0;constructor(c,d,u){super(c,d,u),this.audio_seq_length=this.config.audio_seq_length,this.image_seq_length=this.config.image_seq_length;const{audio_token_id:p,boa_token:h,audio_token:m,eoa_token:g,image_token_id:I,boi_token:f,image_token:_,eoi_token:T}=this.tokenizer.config;this.audio_token_id=p,this.boa_token=h,this.audio_token=m;const M=m.repeat(this.audio_seq_length);this.full_audio_sequence=`

${h}${M}${g}

`,this.image_token_id=I,this.boi_token=f,this.image_token=_;const v=_.repeat(this.image_seq_length);this.full_image_sequence=`

${f}${v}${T}

`}async _call(c,d=null,u=null,p={}){typeof c=="string"&&(c=[c]);let h;u&&(h=await this.feature_extractor(u,p),c=c.map(I=>I.replaceAll(this.audio_token,this.full_audio_sequence)));let m;return d&&(m=await this.image_processor(d,p),c=c.map(I=>I.replaceAll(this.image_token,this.full_image_sequence))),{...this.tokenizer(c,p),...m,...h}}}}),"./src/models/glpn/image_processing_glpn.js":((e,t,n)=>{n.r(t),n.d(t,{GLPNFeatureExtractor:()=>s});var r=n("./src/base/image_processors_utils.js");class s extends r.ImageProcessor{}}),"./src/models/grounding_dino/image_processing_grounding_dino.js":((e,t,n)=>{n.r(t),n.d(t,{GroundingDinoImageProcessor:()=>i});var r=n("./src/base/image_processors_utils.js"),s=n("./src/utils/tensor.js");class i extends r.ImageProcessor{async _call(a){const l=await super._call(a),c=l.pixel_values.dims,d=(0,s.ones)([c[0],c[2],c[3]]);return{...l,pixel_mask:d}}}}),"./src/models/grounding_dino/processing_grounding_dino.js":((e,t,n)=>{n.r(t),n.d(t,{GroundingDinoProcessor:()=>l});var r=n("./src/base/processing_utils.js"),s=n("./src/models/auto/image_processing_auto.js"),i=n("./src/tokenizers.js"),o=n("./src/base/image_processors_utils.js");function a(c,d){const p=c.dims.at(-1)-1,h=c.tolist();h.fill(!1,0,1),h.fill(!1,p);const m=d.tolist();return h.map((g,I)=>g?I:null).filter(g=>g!==null).map(g=>m[g])}class l extends r.Processor{static tokenizer_class=i.AutoTokenizer;static image_processor_class=s.AutoImageProcessor;async _call(d,u,p={}){const h=d?await this.image_processor(d,p):{};return{...u?this.tokenizer(u,p):{},...h}}post_process_grounded_object_detection(d,u,{box_threshold:p=.25,text_threshold:h=.25,target_sizes:m=null}={}){const{logits:g,pred_boxes:I}=d,f=g.dims[0];if(m!==null&&m.length!==f)throw Error("Make sure that you pass in as many target sizes as the batch dimension of the logits");const _=g.dims.at(1),T=g.sigmoid(),M=T.max(-1).tolist(),v=I.tolist().map(A=>A.map(k=>(0,o.center_to_corners_format)(k))),b=[];for(let A=0;A<f;++A){const k=m!==null?m[A]:null;k!==null&&(v[A]=v[A].map(R=>R.map((K,U)=>K*k[(U+1)%2])));const F=M[A],L=[],G=[],j=[];for(let R=0;R<_;++R){const K=F[R];if(K<=p)continue;const U=v[A][R],Y=T[A][R];L.push(K),j.push(U);const te=a(Y.gt(h),u[A]);G.push(te)}b.push({scores:L,boxes:j,labels:this.batch_decode(G)})}return b}}}),"./src/models/idefics3/image_processing_idefics3.js":((e,t,n)=>{n.r(t),n.d(t,{Idefics3ImageProcessor:()=>i});var r=n("./src/base/image_processors_utils.js"),s=n("./src/utils/tensor.js");class i extends r.ImageProcessor{constructor(a){super(a),this.do_image_splitting=a.do_image_splitting??!0,this.max_image_size=a.max_image_size}get_resize_for_vision_encoder(a,l){let[c,d]=a.dims.slice(-2);const u=d/c;return d>=c?(d=Math.ceil(d/l)*l,c=Math.floor(d/u),c=Math.ceil(c/l)*l):(c=Math.ceil(c/l)*l,d=Math.floor(c*u),d=Math.ceil(d/l)*l),{height:c,width:d}}async _call(a,{do_image_splitting:l=null,return_row_col_info:c=!1}={}){let d;if(!Array.isArray(a))d=[[a]];else{if(a.length===0||!a[0])throw new Error("No images provided.");Array.isArray(a[0])?d=a:d=[a]}let u=[],p=[],h=[];const m=[],g=[];for(const A of d){let k=await Promise.all(A.map(G=>this.preprocess(G)));m.push(...k.map(G=>G.original_size)),g.push(...k.map(G=>G.reshaped_input_size)),k.forEach(G=>G.pixel_values.unsqueeze_(0));const{longest_edge:F}=this.max_image_size;let L;if(l??this.do_image_splitting){let G=new Array(k.length),j=new Array(k.length);L=await Promise.all(k.map(async(R,K)=>{const U=this.get_resize_for_vision_encoder(R.pixel_values,F),Y=await(0,s.interpolate_4d)(R.pixel_values,{size:[U.height,U.width]}),{frames:te,num_splits_h:ne,num_splits_w:le}=await this.split_image(Y,this.max_image_size);return G[K]=ne,j[K]=le,(0,s.cat)(te,0)})),p.push(G),h.push(j)}else{const G=[F,F];L=await Promise.all(k.map(j=>(0,s.interpolate_4d)(j.pixel_values,{size:G}))),p.push(new Array(k.length).fill(0)),h.push(new Array(k.length).fill(0))}u.push((0,s.cat)(L,0))}const I=u.length,[f,_,T,M]=u[0].dims;let v,b;if(I===1)v=u[0].unsqueeze_(0),b=(0,s.full)([I,f,T,M],!0);else{const A=Math.max(...u.map(L=>L.dims.at(0)));b=(0,s.full)([I,A,T,M],!0);const k=b.data,F=A*T*M;for(let L=0;L<I;++L){const G=u[L].dims[0];if(G<A){u[L]=(0,s.cat)([u[L],(0,s.full)([A-G,_,T,M],0)],0);const j=L*F+G*T*M,R=(L+1)*F;k.fill(!1,j,R)}}v=(0,s.stack)(u,0)}return{pixel_values:v,pixel_attention_mask:b,original_sizes:m,reshaped_input_sizes:g,...c?{rows:p,cols:h}:{}}}async split_image(a,{longest_edge:l}){const c=l,d=l,u=[],[p,h]=a.dims.slice(-2);let m=0,g=0;if(p>c||h>d){m=Math.ceil(p/c),g=Math.ceil(h/d);const I=Math.ceil(p/m),f=Math.ceil(h/g);for(let M=0;M<m;++M)for(let v=0;v<g;++v){let b,A,k,F;M===m-1?(A=p-I,F=p):(A=M*I,F=(M+1)*I),v===g-1?(b=h-f,k=h):(b=v*f,k=(v+1)*f);const L=[A,b],G=[F,k],j=await(0,s.slice)(a,L,G,[2,3]);u.push(j)}const _=c,T=d;(p!==_||h!==T)&&(a=await(0,s.interpolate_4d)(a,{size:[_,T]}))}return u.push(a),{frames:u,num_splits_h:m,num_splits_w:g}}}}),"./src/models/idefics3/processing_idefics3.js":((e,t,n)=>{n.r(t),n.d(t,{Idefics3Processor:()=>d});var r=n("./src/base/processing_utils.js"),s=n("./src/models/auto/image_processing_auto.js"),i=n("./src/tokenizers.js");n("./src/utils/image.js");var o=n("./src/utils/core.js");function a(u,p,h,m,g,I){let f="";for(let _=0;_<p;++_){for(let T=0;T<h;++T)f+=m+`<row_${_+1}_col_${T+1}>`+g.repeat(u);f+=`
`}return f+=`
${m}${I}`+g.repeat(u)+`${m}`,f}function l(u,p,h,m){return`${p}${m}`+h.repeat(u)+`${p}`}function c(u,p,h,m,g,I){return u===0&&p===0?l(h,m,g,I):a(h,u,p,m,g,I)}class d extends r.Processor{static image_processor_class=s.AutoImageProcessor;static tokenizer_class=i.AutoTokenizer;static uses_processor_config=!0;fake_image_token="<fake_token_around_image>";image_token="<image>";global_img_token="<global-img>";async _call(p,h=null,m={}){m.return_row_col_info??=!0;let g;h&&(g=await this.image_processor(h,m)),Array.isArray(p)||(p=[p]);const I=g.rows??[new Array(p.length).fill(0)],f=g.cols??[new Array(p.length).fill(0)],_=this.config.image_seq_len,T=[],M=[];for(let b=0;b<p.length;++b){const A=p[b],k=I[b],F=f[b];T.push((0,o.count)(A,this.image_token));const L=k.map((R,K)=>c(R,F[K],_,this.fake_image_token,this.image_token,this.global_img_token)),G=A.split(this.image_token);if(G.length===0)throw new Error("The image token should be present in the text.");let j=G[0];for(let R=0;R<L.length;++R)j+=L[R]+G[R+1];M.push(j)}return{...this.tokenizer(M),...g}}}}),"./src/models/image_processors.js":((e,t,n)=>{n.r(t),n.d(t,{BeitFeatureExtractor:()=>r.BeitFeatureExtractor,BitImageProcessor:()=>s.BitImageProcessor,CLIPFeatureExtractor:()=>o.CLIPFeatureExtractor,CLIPImageProcessor:()=>o.CLIPImageProcessor,ChineseCLIPFeatureExtractor:()=>i.ChineseCLIPFeatureExtractor,ConvNextFeatureExtractor:()=>a.ConvNextFeatureExtractor,ConvNextImageProcessor:()=>a.ConvNextImageProcessor,DINOv3ViTImageProcessor:()=>d.DINOv3ViTImageProcessor,DPTFeatureExtractor:()=>p.DPTFeatureExtractor,DPTImageProcessor:()=>p.DPTImageProcessor,DeiTFeatureExtractor:()=>l.DeiTFeatureExtractor,DeiTImageProcessor:()=>l.DeiTImageProcessor,DetrFeatureExtractor:()=>c.DetrFeatureExtractor,DetrImageProcessor:()=>c.DetrImageProcessor,DonutFeatureExtractor:()=>u.DonutFeatureExtractor,DonutImageProcessor:()=>u.DonutImageProcessor,EfficientNetImageProcessor:()=>h.EfficientNetImageProcessor,GLPNFeatureExtractor:()=>m.GLPNFeatureExtractor,GroundingDinoImageProcessor:()=>g.GroundingDinoImageProcessor,Idefics3ImageProcessor:()=>I.Idefics3ImageProcessor,JinaCLIPImageProcessor:()=>_.JinaCLIPImageProcessor,LlavaOnevisionImageProcessor:()=>T.LlavaOnevisionImageProcessor,Mask2FormerImageProcessor:()=>M.Mask2FormerImageProcessor,MaskFormerFeatureExtractor:()=>v.MaskFormerFeatureExtractor,MaskFormerImageProcessor:()=>v.MaskFormerImageProcessor,MobileNetV1FeatureExtractor:()=>b.MobileNetV1FeatureExtractor,MobileNetV1ImageProcessor:()=>b.MobileNetV1ImageProcessor,MobileNetV2FeatureExtractor:()=>A.MobileNetV2FeatureExtractor,MobileNetV2ImageProcessor:()=>A.MobileNetV2ImageProcessor,MobileNetV3FeatureExtractor:()=>k.MobileNetV3FeatureExtractor,MobileNetV3ImageProcessor:()=>k.MobileNetV3ImageProcessor,MobileNetV4FeatureExtractor:()=>F.MobileNetV4FeatureExtractor,MobileNetV4ImageProcessor:()=>F.MobileNetV4ImageProcessor,MobileViTFeatureExtractor:()=>L.MobileViTFeatureExtractor,MobileViTImageProcessor:()=>L.MobileViTImageProcessor,NougatImageProcessor:()=>G.NougatImageProcessor,OwlViTFeatureExtractor:()=>R.OwlViTFeatureExtractor,OwlViTImageProcessor:()=>R.OwlViTImageProcessor,Owlv2ImageProcessor:()=>j.Owlv2ImageProcessor,Phi3VImageProcessor:()=>K.Phi3VImageProcessor,PixtralImageProcessor:()=>U.PixtralImageProcessor,PvtImageProcessor:()=>Y.PvtImageProcessor,Qwen2VLImageProcessor:()=>te.Qwen2VLImageProcessor,RTDetrImageProcessor:()=>ne.RTDetrImageProcessor,Sam2ImageProcessor:()=>N.Sam2ImageProcessor,Sam3ImageProcessor:()=>oe.Sam3ImageProcessor,SamImageProcessor:()=>le.SamImageProcessor,SegformerFeatureExtractor:()=>X.SegformerFeatureExtractor,SegformerImageProcessor:()=>X.SegformerImageProcessor,SiglipImageProcessor:()=>D.SiglipImageProcessor,SmolVLMImageProcessor:()=>z.SmolVLMImageProcessor,Swin2SRImageProcessor:()=>se.Swin2SRImageProcessor,VLMImageProcessor:()=>f.VLMImageProcessor,ViTFeatureExtractor:()=>me.ViTFeatureExtractor,ViTImageProcessor:()=>me.ViTImageProcessor,VitMatteImageProcessor:()=>$e.VitMatteImageProcessor,VitPoseImageProcessor:()=>ke.VitPoseImageProcessor,YolosFeatureExtractor:()=>Be.YolosFeatureExtractor,YolosImageProcessor:()=>Be.YolosImageProcessor});var r=n("./src/models/beit/image_processing_beit.js"),s=n("./src/models/bit/image_processing_bit.js"),i=n("./src/models/chinese_clip/image_processing_chinese_clip.js"),o=n("./src/models/clip/image_processing_clip.js"),a=n("./src/models/convnext/image_processing_convnext.js"),l=n("./src/models/deit/image_processing_deit.js"),c=n("./src/models/detr/image_processing_detr.js"),d=n("./src/models/dinov3_vit/image_processing_dinov3_vit.js"),u=n("./src/models/donut/image_processing_donut.js"),p=n("./src/models/dpt/image_processing_dpt.js"),h=n("./src/models/efficientnet/image_processing_efficientnet.js"),m=n("./src/models/glpn/image_processing_glpn.js"),g=n("./src/models/grounding_dino/image_processing_grounding_dino.js"),I=n("./src/models/idefics3/image_processing_idefics3.js"),f=n("./src/models/janus/image_processing_janus.js"),_=n("./src/models/jina_clip/image_processing_jina_clip.js"),T=n("./src/models/llava_onevision/image_processing_llava_onevision.js"),M=n("./src/models/mask2former/image_processing_mask2former.js"),v=n("./src/models/maskformer/image_processing_maskformer.js"),b=n("./src/models/mobilenet_v1/image_processing_mobilenet_v1.js"),A=n("./src/models/mobilenet_v2/image_processing_mobilenet_v2.js"),k=n("./src/models/mobilenet_v3/image_processing_mobilenet_v3.js"),F=n("./src/models/mobilenet_v4/image_processing_mobilenet_v4.js"),L=n("./src/models/mobilevit/image_processing_mobilevit.js"),G=n("./src/models/nougat/image_processing_nougat.js"),j=n("./src/models/owlv2/image_processing_owlv2.js"),R=n("./src/models/owlvit/image_processing_owlvit.js"),K=n("./src/models/phi3_v/image_processing_phi3_v.js"),U=n("./src/models/pixtral/image_processing_pixtral.js"),Y=n("./src/models/pvt/image_processing_pvt.js"),te=n("./src/models/qwen2_vl/image_processing_qwen2_vl.js"),ne=n("./src/models/rt_detr/image_processing_rt_detr.js"),le=n("./src/models/sam/image_processing_sam.js"),N=n("./src/models/sam2/image_processing_sam2.js"),oe=n("./src/models/sam3/image_processing_sam3.js"),X=n("./src/models/segformer/image_processing_segformer.js"),D=n("./src/models/siglip/image_processing_siglip.js"),z=n("./src/models/smolvlm/image_processing_smolvlm.js"),se=n("./src/models/swin2sr/image_processing_swin2sr.js"),me=n("./src/models/vit/image_processing_vit.js"),$e=n("./src/models/vitmatte/image_processing_vitmatte.js"),ke=n("./src/models/vitpose/image_processing_vitpose.js"),Be=n("./src/models/yolos/image_processing_yolos.js")}),"./src/models/janus/image_processing_janus.js":((e,t,n)=>{n.r(t),n.d(t,{VLMImageProcessor:()=>s});var r=n("./src/base/image_processors_utils.js");class s extends r.ImageProcessor{constructor(o){super({do_pad:!0,pad_size:{width:o.image_size,height:o.image_size},...o}),this.constant_values=this.config.background_color.map(a=>a*this.rescale_factor)}pad_image(o,a,l,c){return super.pad_image(o,a,l,{constant_values:this.constant_values,center:!0,...c})}}}),"./src/models/janus/processing_janus.js":((e,t,n)=>{n.r(t),n.d(t,{VLChatProcessor:()=>c});var r=n("./src/base/processing_utils.js"),s=n("./src/models/auto/image_processing_auto.js"),i=n("./src/tokenizers.js"),o=n("./src/utils/core.js"),a=n("./src/utils/tensor.js"),l=n("./src/utils/image.js");class c extends r.Processor{static image_processor_class=s.AutoImageProcessor;static tokenizer_class=i.AutoTokenizer;static uses_processor_config=!0;constructor(u,p,h){super(u,p,h),this.image_tag=this.config.image_tag,this.image_start_tag=this.config.image_start_tag,this.image_end_tag=this.config.image_end_tag,this.num_image_tokens=this.config.num_image_tokens}async _call(u,{images:p=null,chat_template:h="default"}={}){p?Array.isArray(p)||(p=[p]):p=await Promise.all(u.filter(L=>L.images).flatMap(L=>L.images).map(L=>l.RawImage.read(L)));const m=this.tokenizer,g=m.apply_chat_template(u,{tokenize:!1,add_generation_prompt:!0,chat_template:h}),I=L=>m.encode(L,{add_special_tokens:!1}),f=g.split(this.image_tag),_=f.length-1;if(p.length!==_)throw new Error(`Number of images provided (${p.length}) does not match number of "${this.image_tag}" image tags (${_})`);const[T,M,v]=m.model.convert_tokens_to_ids([this.image_tag,this.image_start_tag,this.image_end_tag]);let b=I(f[0]),A=new Array(b.length).fill(!1);for(let L=1;L<f.length;++L){const G=new Array(this.num_image_tokens).fill(T),j=I(f[L]);b=(0,o.mergeArrays)(b,[M],G,[v],j);const R=new Array(this.num_image_tokens).fill(!0);A=(0,o.mergeArrays)(A,[!1],R,[!1],new Array(j.length).fill(!1))}const k=[1,b.length],F={input_ids:new a.Tensor("int64",b,k),attention_mask:new a.Tensor("int64",new Array(b.length).fill(1),k),images_seq_mask:new a.Tensor("bool",A,k),images_emb_mask:new a.Tensor("bool",new Array(_*this.num_image_tokens).fill(!0),[1,_,this.num_image_tokens])};if(p&&p.length>0){const L=await this.image_processor(p);return L.pixel_values.unsqueeze_(0),{...F,...L}}return F}}}),"./src/models/jina_clip/image_processing_jina_clip.js":((e,t,n)=>{n.r(t),n.d(t,{JinaCLIPImageProcessor:()=>s});var r=n("./src/base/image_processors_utils.js");class s extends r.ImageProcessor{constructor(o){const{resize_mode:a,fill_color:l,interpolation:c,size:d,...u}=o,p=a==="squash"?{width:d,height:d}:a==="shortest"?{shortest_edge:d}:{longest_edge:d},h=c==="bicubic"?3:2;super({...u,size:p,resample:h,do_center_crop:!0,crop_size:d,do_normalize:!0})}}}),"./src/models/jina_clip/processing_jina_clip.js":((e,t,n)=>{n.r(t),n.d(t,{JinaCLIPProcessor:()=>o});var r=n("./src/base/processing_utils.js"),s=n("./src/models/auto/image_processing_auto.js"),i=n("./src/tokenizers.js");class o extends r.Processor{static tokenizer_class=i.AutoTokenizer;static image_processor_class=s.AutoImageProcessor;async _call(l=null,c=null,d={}){if(!l&&!c)throw new Error("Either text or images must be provided");const u=l?this.tokenizer(l,d):{},p=c?await this.image_processor(c,d):{};return{...u,...p}}}}),"./src/models/llava/processing_llava.js":((e,t,n)=>{n.r(t),n.d(t,{LlavaProcessor:()=>o});var r=n("./src/base/processing_utils.js"),s=n("./src/models/auto/image_processing_auto.js"),i=n("./src/tokenizers.js");class o extends r.Processor{static tokenizer_class=i.AutoTokenizer;static image_processor_class=s.AutoImageProcessor;static uses_processor_config=!0;async _call(l,c=null,d={}){const u=await this.image_processor(l,d);if(c){const[h,m]=u.pixel_values.dims.slice(-2),{image_token:g,patch_size:I,num_additional_image_tokens:f}=this.config,_=Math.floor(h/I)*Math.floor(m/I)+f;c=structuredClone(c),Array.isArray(c)||(c=[c]);for(let T=0;T<c.length;++T)c[T]=c[T].replace(g,g.repeat(_))}const p=c?this.tokenizer(c,d):{};return{...u,...p}}}}),"./src/models/llava_onevision/image_processing_llava_onevision.js":((e,t,n)=>{n.r(t),n.d(t,{LlavaOnevisionImageProcessor:()=>s});var r=n("./src/base/image_processors_utils.js");class s extends r.ImageProcessor{}}),"./src/models/mask2former/image_processing_mask2former.js":((e,t,n)=>{n.r(t),n.d(t,{Mask2FormerImageProcessor:()=>s});var r=n("./src/models/maskformer/image_processing_maskformer.js");class s extends r.MaskFormerImageProcessor{}}),"./src/models/maskformer/image_processing_maskformer.js":((e,t,n)=>{n.r(t),n.d(t,{MaskFormerFeatureExtractor:()=>i,MaskFormerImageProcessor:()=>s});var r=n("./src/base/image_processors_utils.js");class s extends r.ImageProcessor{post_process_panoptic_segmentation(...a){return(0,r.post_process_panoptic_segmentation)(...a)}post_process_instance_segmentation(...a){return(0,r.post_process_instance_segmentation)(...a)}}class i extends s{}}),"./src/models/mgp_str/processing_mgp_str.js":((e,t,n)=>{n.r(t),n.d(t,{MgpstrProcessor:()=>l});var r=n("./src/base/processing_utils.js"),s=n("./src/models/auto/image_processing_auto.js"),i=n("./src/tokenizers.js"),o=n("./src/utils/maths.js");const a={char:["char_decode",1],bpe:["bpe_decode",2],wp:["wp_decode",102]};class l extends r.Processor{static tokenizer_class=i.AutoTokenizer;static image_processor_class=s.AutoImageProcessor;get char_tokenizer(){return this.components.char_tokenizer}get bpe_tokenizer(){return this.components.bpe_tokenizer}get wp_tokenizer(){return this.components.wp_tokenizer}_decode_helper(d,u){if(!a.hasOwnProperty(u))throw new Error(`Format ${u} is not supported.`);const[p,h]=a[u],m=this[p].bind(this),[g,I]=d.dims,f=[],_=[],T=d.tolist();for(let v=0;v<g;++v){const b=T[v],A=[],k=[];for(let L=1;L<I;++L){const[G,j]=(0,o.max)((0,o.softmax)(b[L]));if(k.push(G),j==h)break;A.push(j)}const F=k.length>0?k.reduce((L,G)=>L*G,1):0;_.push(A),f.push(F)}return[m(_),f]}char_decode(d){return this.char_tokenizer.batch_decode(d).map(u=>u.replaceAll(" ",""))}bpe_decode(d){return this.bpe_tokenizer.batch_decode(d)}wp_decode(d){return this.wp_tokenizer.batch_decode(d).map(u=>u.replaceAll(" ",""))}batch_decode([d,u,p]){const[h,m]=this._decode_helper(d,"char"),[g,I]=this._decode_helper(u,"bpe"),[f,_]=this._decode_helper(p,"wp"),T=[],M=[];for(let v=0;v<h.length;++v){const[b,A]=(0,o.max)([m[v],I[v],_[v]]);T.push([h[v],g[v],f[v]][A]),M.push(b)}return{generated_text:T,scores:M,char_preds:h,bpe_preds:g,wp_preds:f}}static async from_pretrained(...d){const u=await super.from_pretrained(...d),p=await i.AutoTokenizer.from_pretrained("Xenova/gpt2"),h=await i.AutoTokenizer.from_pretrained("Xenova/bert-base-uncased");return u.components={image_processor:u.image_processor,char_tokenizer:u.tokenizer,bpe_tokenizer:p,wp_tokenizer:h},u}async _call(d,u=null){const p=await this.image_processor(d);return u&&(p.labels=this.tokenizer(u).input_ids),p}}}),"./src/models/mobilenet_v1/image_processing_mobilenet_v1.js":((e,t,n)=>{n.r(t),n.d(t,{MobileNetV1FeatureExtractor:()=>i,MobileNetV1ImageProcessor:()=>s});var r=n("./src/base/image_processors_utils.js");class s extends r.ImageProcessor{}class i extends s{}}),"./src/models/mobilenet_v2/image_processing_mobilenet_v2.js":((e,t,n)=>{n.r(t),n.d(t,{MobileNetV2FeatureExtractor:()=>i,MobileNetV2ImageProcessor:()=>s});var r=n("./src/base/image_processors_utils.js");class s extends r.ImageProcessor{}class i extends s{}}),"./src/models/mobilenet_v3/image_processing_mobilenet_v3.js":((e,t,n)=>{n.r(t),n.d(t,{MobileNetV3FeatureExtractor:()=>i,MobileNetV3ImageProcessor:()=>s});var r=n("./src/base/image_processors_utils.js");class s extends r.ImageProcessor{}class i extends s{}}),"./src/models/mobilenet_v4/image_processing_mobilenet_v4.js":((e,t,n)=>{n.r(t),n.d(t,{MobileNetV4FeatureExtractor:()=>i,MobileNetV4ImageProcessor:()=>s});var r=n("./src/base/image_processors_utils.js");class s extends r.ImageProcessor{}class i extends s{}}),"./src/models/mobilevit/image_processing_mobilevit.js":((e,t,n)=>{n.r(t),n.d(t,{MobileViTFeatureExtractor:()=>i,MobileViTImageProcessor:()=>s});var r=n("./src/base/image_processors_utils.js");class s extends r.ImageProcessor{}class i extends s{}}),"./src/models/moonshine/feature_extraction_moonshine.js":((e,t,n)=>{n.r(t),n.d(t,{MoonshineFeatureExtractor:()=>i});var r=n("./src/base/feature_extraction_utils.js"),s=n("./src/utils/tensor.js");class i extends r.FeatureExtractor{async _call(a){(0,r.validate_audio_inputs)(a,"MoonshineFeatureExtractor"),a instanceof Float64Array&&(a=new Float32Array(a));const l=[1,a.length];return{input_values:new s.Tensor("float32",a,l)}}}}),"./src/models/moonshine/processing_moonshine.js":((e,t,n)=>{n.r(t),n.d(t,{MoonshineProcessor:()=>o});var r=n("./src/models/auto/feature_extraction_auto.js"),s=n("./src/tokenizers.js"),i=n("./src/base/processing_utils.js");class o extends i.Processor{static tokenizer_class=s.AutoTokenizer;static feature_extractor_class=r.AutoFeatureExtractor;async _call(l){return await this.feature_extractor(l)}}}),"./src/models/nougat/image_processing_nougat.js":((e,t,n)=>{n.r(t),n.d(t,{NougatImageProcessor:()=>s});var r=n("./src/models/donut/image_processing_donut.js");class s extends r.DonutImageProcessor{}}),"./src/models/owlv2/image_processing_owlv2.js":((e,t,n)=>{n.r(t),n.d(t,{Owlv2ImageProcessor:()=>s});var r=n("./src/models/owlvit/image_processing_owlvit.js");class s extends r.OwlViTImageProcessor{}}),"./src/models/owlvit/image_processing_owlvit.js":((e,t,n)=>{n.r(t),n.d(t,{OwlViTFeatureExtractor:()=>i,OwlViTImageProcessor:()=>s});var r=n("./src/base/image_processors_utils.js");class s extends r.ImageProcessor{post_process_object_detection(...a){return(0,r.post_process_object_detection)(...a)}}class i extends s{}}),"./src/models/owlvit/processing_owlvit.js":((e,t,n)=>{n.r(t),n.d(t,{OwlViTProcessor:()=>o});var r=n("./src/base/processing_utils.js"),s=n("./src/models/auto/image_processing_auto.js"),i=n("./src/tokenizers.js");class o extends r.Processor{static tokenizer_class=i.AutoTokenizer;static image_processor_class=s.AutoImageProcessor}}),"./src/models/paligemma/processing_paligemma.js":((e,t,n)=>{n.r(t),n.d(t,{PaliGemmaProcessor:()=>l});var r=n("./src/base/processing_utils.js"),s=n("./src/models/auto/image_processing_auto.js"),i=n("./src/tokenizers.js");const o="<image>";function a(c,d,u,p,h){return`${p.repeat(u*h)}${d}${c}
`}class l extends r.Processor{static tokenizer_class=i.AutoTokenizer;static image_processor_class=s.AutoImageProcessor;static uses_processor_config=!1;async _call(d,u=null,p={}){u||(console.warn("You are using PaliGemma without a text prefix. It will perform as a picture-captioning model."),u=""),Array.isArray(d)||(d=[d]),Array.isArray(u)||(u=[u]);const h=this.tokenizer.bos_token,m=this.image_processor.config.image_seq_length;let g;u.some(_=>_.includes(o))?g=u.map(_=>{const T=_.replaceAll(o,o.repeat(m)),M=T.lastIndexOf(o),v=M===-1?0:M+o.length;return T.slice(0,v)+h+T.slice(v)+`
`}):(console.warn("You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens."),g=u.map(_=>a(_,h,m,o,d.length)));const I=this.tokenizer(g,p);return{...await this.image_processor(d,p),...I}}}}),"./src/models/parakeet/feature_extraction_parakeet.js":((e,t,n)=>{n.r(t),n.d(t,{ParakeetFeatureExtractor:()=>a});var r=n("./src/base/feature_extraction_utils.js"),s=n("./src/utils/tensor.js"),i=n("./src/utils/audio.js");const o=1e-5;class a extends r.FeatureExtractor{constructor(c){super(c),this.config.mel_filters??=(0,i.mel_filter_bank)(Math.floor(1+this.config.n_fft/2),this.config.feature_size,0,this.config.sampling_rate/2,this.config.sampling_rate,"slaney","slaney");const d=(0,i.window_function)(this.config.win_length,"hann",{periodic:!1});this.window=new Float64Array(this.config.n_fft);const u=Math.floor((this.config.n_fft-this.config.win_length)/2);this.window.set(d,u)}async _extract_fbank_features(c){const d=this.config.preemphasis;c=new Float64Array(c);for(let p=c.length-1;p>=1;--p)c[p]-=d*c[p-1];return await(0,i.spectrogram)(c,this.window,this.window.length,this.config.hop_length,{fft_length:this.config.n_fft,power:2,mel_filters:this.config.mel_filters,log_mel:"log",mel_floor:-1/0,pad_mode:"constant",center:!0,transpose:!0,mel_offset:2**-24})}async _call(c){(0,r.validate_audio_inputs)(c,"ParakeetFeatureExtractor");const d=await this._extract_fbank_features(c),u=Math.floor((c.length+Math.floor(this.config.n_fft/2)*2-this.config.n_fft)/this.config.hop_length),p=d.data;p.fill(0,u*d.dims[1]);const[h,m]=d.dims,g=new Float64Array(m),I=new Float64Array(m);for(let T=0;T<u;++T){const M=T*m;for(let v=0;v<m;++v){const b=p[M+v];g[v]+=b,I[v]+=b*b}}const f=u>1?u-1:1;for(let T=0;T<m;++T){const M=g[T]/u,v=(I[T]-u*M*M)/f,A=1/(Math.sqrt(v)+o);for(let k=0;k<u;++k){const F=k*m+T;p[F]=(p[F]-M)*A}}const _=new BigInt64Array(h);return _.fill(1n,0,u),{input_features:d.unsqueeze_(0),attention_mask:new s.Tensor("int64",_,[1,h])}}}}),"./src/models/phi3_v/image_processing_phi3_v.js":((e,t,n)=>{n.r(t),n.d(t,{Phi3VImageProcessor:()=>d});var r=n("./src/base/image_processors_utils.js"),s=n("./src/utils/tensor.js");const i=336,o=[2,3],{ceil:a,floor:l,sqrt:c}=Math;class d extends r.ImageProcessor{constructor(p){super({...p,do_normalize:!0,do_pad:!0,pad_size:"custom",do_convert_rgb:!0,do_resize:!0}),this._num_crops=p.num_crops}calc_num_image_tokens_from_image_size(p,h){const{num_img_tokens:m}=this.config;return l((l(h/i)*l(p/i)+1)*m+1+(l(h/i)+1)*c(m))}get_resize_output_image_size(p,h){const m=this._num_crops,[g,I]=p.size;let f=g/I,_=1;for(;_*Math.ceil(_/f)<=m;)_+=1;_-=1;const T=Math.floor(_*336),M=Math.floor(T/f);return[T,M]}pad_image(p,h,m,g={}){const[I,f]=h,_=i*a(I/i),T=i*a(f/i),M=[1,1,1].map((v,b)=>(v-this.image_mean[b])/this.image_std[b]);return super.pad_image(p,h,{width:T,height:_},{center:!0,constant_values:M,...g})}async _call(p,{num_crops:h=null}={}){if(this._num_crops=h??=this.config.num_crops,h<4||c(h)%1!==0)throw new Error("num_crops must be a square number >= 4");Array.isArray(p)||(p=[p]);const m=p.length,g=await Promise.all(p.map(A=>this.preprocess(A))),I=g.map(A=>A.original_size),f=g.map(A=>A.reshaped_input_size),_=[];for(const{pixel_values:A}of g){A.unsqueeze_(0);const[k,F]=A.dims.slice(-2),L=await(0,s.interpolate_4d)(A,{size:[i,i],mode:"bicubic"});if(h>0){const G=[],j=c(h),R=l(F/j),K=l(k/j);for(let Y=0;Y<j;++Y)for(let te=0;te<j;++te){let ne,le,N,oe;Y===j-1?(le=k-K,oe=k):(le=Y*K,oe=(Y+1)*K),te===j-1?(ne=F-R,N=F):(ne=te*R,N=(te+1)*R);const X=[le,ne],D=[oe,N],z=await(0,s.slice)(A,X,D,o);G.push(z)}const U=await(0,s.interpolate_4d)((0,s.cat)(G,0),{size:[i,i],mode:"bicubic"});_.push((0,s.cat)([L,U],0))}else _.push(L)}const T=(0,s.stack)(_,0),M=f.map(A=>A.map(k=>i*a(k/i))),v=new s.Tensor("int64",M.flat(),[m,2]),b=M.map(([A,k])=>this.calc_num_image_tokens_from_image_size(k,A));return{pixel_values:T,original_sizes:I,reshaped_input_sizes:f,image_sizes:v,num_img_tokens:b}}}}),"./src/models/phi3_v/processing_phi3_v.js":((e,t,n)=>{n.r(t),n.d(t,{Phi3VProcessor:()=>l});var r=n("./src/base/processing_utils.js"),s=n("./src/models/auto/image_processing_auto.js"),i=n("./src/tokenizers.js");n("./src/utils/image.js");const o="<|image|>",a=/<\|image_\d+\|>/g;class l extends r.Processor{static image_processor_class=s.AutoImageProcessor;static tokenizer_class=i.AutoTokenizer;async _call(d,u=null,{padding:p=!0,truncation:h=!0,num_crops:m=null}={}){Array.isArray(d)||(d=[d]);let g,I;if(u){I=await this.image_processor(u,{num_crops:m});const{num_img_tokens:f}=I,_=d.map((M,v)=>M.split(a).join(o.repeat(f[v])));g=this.tokenizer(_,{padding:p,truncation:h});const T=this.tokenizer.model.convert_tokens_to_ids([o])[0];g.input_ids.map_(M=>M==T?-M:M)}else g=this.tokenizer(d);return{...g,...I}}}}),"./src/models/pixtral/image_processing_pixtral.js":((e,t,n)=>{n.r(t),n.d(t,{PixtralImageProcessor:()=>s});var r=n("./src/base/image_processors_utils.js");class s extends r.ImageProcessor{get_resize_output_image_size(o,a){const{longest_edge:l}=a;if(l===void 0)throw new Error("size must contain 'longest_edge'");const[c,d]=o.size,u=Math.max(c,d)/l;let p=c,h=d;u>1&&(p=Math.floor(c/u),h=Math.floor(d/u));const{patch_size:m,spatial_merge_size:g}=this.config;if(!g)throw new Error("config must contain 'spatial_merge_size'");const I=m*g,f=Math.floor((p-1)/I)+1,_=Math.floor((h-1)/I)+1;return[f*I,_*I]}}}),"./src/models/pixtral/processing_pixtral.js":((e,t,n)=>{n.r(t),n.d(t,{PixtralProcessor:()=>o});var r=n("./src/base/processing_utils.js"),s=n("./src/models/auto/image_processing_auto.js"),i=n("./src/tokenizers.js");class o extends r.Processor{static tokenizer_class=i.AutoTokenizer;static image_processor_class=s.AutoImageProcessor;static uses_processor_config=!0;async _call(l,c=null,d={}){const u=await this.image_processor(l,d);if(c){const[h,m]=u.pixel_values.dims.slice(-2),{image_token:g,image_break_token:I,image_end_token:f,patch_size:_,spatial_merge_size:T}=this.config,M=_*T,v=Math.floor(h/M),b=Math.floor(m/M);c=structuredClone(c),Array.isArray(c)||(c=[c]);for(let A=0;A<c.length;++A){const k=g.repeat(b),F=k+I,L=k+f,G=F.repeat(v-1)+L;c[A]=c[A].replace(g,G)}}const p=c?this.tokenizer(c,d):{};return{...u,...p}}}}),"./src/models/processors.js":((e,t,n)=>{n.r(t),n.d(t,{Florence2Processor:()=>r.Florence2Processor,Gemma3nProcessor:()=>s.Gemma3nProcessor,GroundingDinoProcessor:()=>i.GroundingDinoProcessor,Idefics3Processor:()=>o.Idefics3Processor,JinaCLIPProcessor:()=>l.JinaCLIPProcessor,LlavaProcessor:()=>c.LlavaProcessor,MgpstrProcessor:()=>d.MgpstrProcessor,MoonshineProcessor:()=>u.MoonshineProcessor,OwlViTProcessor:()=>p.OwlViTProcessor,PaliGemmaProcessor:()=>h.PaliGemmaProcessor,Phi3VProcessor:()=>m.Phi3VProcessor,PixtralProcessor:()=>g.PixtralProcessor,PyAnnoteProcessor:()=>I.PyAnnoteProcessor,Qwen2VLProcessor:()=>f.Qwen2VLProcessor,Sam2Processor:()=>T.Sam2Processor,Sam2VideoProcessor:()=>T.Sam2VideoProcessor,SamProcessor:()=>_.SamProcessor,SmolVLMProcessor:()=>M.SmolVLMProcessor,SpeechT5Processor:()=>v.SpeechT5Processor,UltravoxProcessor:()=>b.UltravoxProcessor,VLChatProcessor:()=>a.VLChatProcessor,VoxtralProcessor:()=>A.VoxtralProcessor,Wav2Vec2Processor:()=>k.Wav2Vec2Processor,Wav2Vec2ProcessorWithLM:()=>F.Wav2Vec2ProcessorWithLM,WhisperProcessor:()=>L.WhisperProcessor});var r=n("./src/models/florence2/processing_florence2.js"),s=n("./src/models/gemma3n/processing_gemma3n.js"),i=n("./src/models/grounding_dino/processing_grounding_dino.js"),o=n("./src/models/idefics3/processing_idefics3.js"),a=n("./src/models/janus/processing_janus.js"),l=n("./src/models/jina_clip/processing_jina_clip.js"),c=n("./src/models/llava/processing_llava.js"),d=n("./src/models/mgp_str/processing_mgp_str.js"),u=n("./src/models/moonshine/processing_moonshine.js"),p=n("./src/models/owlvit/processing_owlvit.js"),h=n("./src/models/paligemma/processing_paligemma.js"),m=n("./src/models/phi3_v/processing_phi3_v.js"),g=n("./src/models/pixtral/processing_pixtral.js"),I=n("./src/models/pyannote/processing_pyannote.js"),f=n("./src/models/qwen2_vl/processing_qwen2_vl.js"),_=n("./src/models/sam/processing_sam.js"),T=n("./src/models/sam2/processing_sam2.js"),M=n("./src/models/smolvlm/processing_smolvlm.js"),v=n("./src/models/speecht5/processing_speecht5.js"),b=n("./src/models/ultravox/processing_ultravox.js"),A=n("./src/models/voxtral/processing_voxtral.js"),k=n("./src/models/wav2vec2/processing_wav2vec2.js"),F=n("./src/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.js"),L=n("./src/models/whisper/processing_whisper.js")}),"./src/models/pvt/image_processing_pvt.js":((e,t,n)=>{n.r(t),n.d(t,{PvtImageProcessor:()=>s});var r=n("./src/base/image_processors_utils.js");class s extends r.ImageProcessor{}}),"./src/models/pyannote/feature_extraction_pyannote.js":((e,t,n)=>{n.r(t),n.d(t,{PyAnnoteFeatureExtractor:()=>o});var r=n("./src/base/feature_extraction_utils.js"),s=n("./src/utils/tensor.js"),i=n("./src/utils/maths.js");class o extends r.FeatureExtractor{async _call(l){(0,r.validate_audio_inputs)(l,"PyAnnoteFeatureExtractor"),l instanceof Float64Array&&(l=new Float32Array(l));const c=[1,1,l.length];return{input_values:new s.Tensor("float32",l,c)}}samples_to_frames(l){return(l-this.config.offset)/this.config.step}post_process_speaker_diarization(l,c){const d=c/this.samples_to_frames(c)/this.config.sampling_rate,u=[];for(const p of l.tolist()){const h=[];let m=-1;for(let g=0;g<p.length;++g){const I=(0,i.softmax)(p[g]),[f,_]=(0,i.max)(I),[T,M]=[g,g+1];_!==m?(m=_,h.push({id:_,start:T,end:M,score:f})):(h.at(-1).end=M,h.at(-1).score+=f)}u.push(h.map(({id:g,start:I,end:f,score:_})=>({id:g,start:I*d,end:f*d,confidence:_/(f-I)})))}return u}}}),"./src/models/pyannote/processing_pyannote.js":((e,t,n)=>{n.r(t),n.d(t,{PyAnnoteProcessor:()=>i});var r=n("./src/base/processing_utils.js"),s=n("./src/models/pyannote/feature_extraction_pyannote.js");class i extends r.Processor{static feature_extractor_class=s.PyAnnoteFeatureExtractor;async _call(a){return await this.feature_extractor(a)}post_process_speaker_diarization(...a){return this.feature_extractor.post_process_speaker_diarization(...a)}get sampling_rate(){return this.feature_extractor.config.sampling_rate}}}),"./src/models/qwen2_vl/image_processing_qwen2_vl.js":((e,t,n)=>{n.r(t),n.d(t,{Qwen2VLImageProcessor:()=>i});var r=n("./src/base/image_processors_utils.js"),s=n("./src/utils/tensor.js");class i extends r.ImageProcessor{async _call(a,...l){const{pixel_values:c,original_sizes:d,reshaped_input_sizes:u}=await super._call(a,...l);let p=c;const{temporal_patch_size:h,merge_size:m,patch_size:g}=this.config;p.dims[0]===1&&(p=(0,s.cat)(Array.from({length:h},()=>p),0));const I=p.dims[0]/h,f=p.dims[1],_=Math.floor(p.dims[2]/g),T=Math.floor(p.dims[3]/g),M=p.view(I,h,f,Math.floor(_/m),m,g,Math.floor(T/m),m,g).permute(0,3,6,4,7,2,1,5,8).view(I*_*T,f*h*g*g),v=new s.Tensor("int64",[I,_,T],[1,3]);return{pixel_values:M,image_grid_thw:v,original_sizes:d,reshaped_input_sizes:u}}}}),"./src/models/qwen2_vl/processing_qwen2_vl.js":((e,t,n)=>{n.r(t),n.d(t,{Qwen2VLProcessor:()=>o});var r=n("./src/base/processing_utils.js"),s=n("./src/models/auto/image_processing_auto.js"),i=n("./src/tokenizers.js");n("./src/utils/image.js");class o extends r.Processor{static image_processor_class=s.AutoImageProcessor;static tokenizer_class=i.AutoTokenizer;async _call(l,c=null,...d){Array.isArray(l)||(l=[l]);let u,p;if(c&&(u=await this.image_processor(c),p=u.image_grid_thw),p){let m=this.image_processor.config.merge_size**2,g=0;const I=p.tolist();l=l.map(f=>{for(;f.includes("<|image_pad|>");){const _=Number(I[g++].reduce((T,M)=>T*M,1n));f=f.replace("<|image_pad|>","<|placeholder|>".repeat(Math.floor(_/m)))}return f.replaceAll("<|placeholder|>","<|image_pad|>")})}return{...this.tokenizer(l),...u}}}}),"./src/models/rt_detr/image_processing_rt_detr.js":((e,t,n)=>{n.r(t),n.d(t,{RTDetrImageProcessor:()=>s});var r=n("./src/base/image_processors_utils.js");class s extends r.ImageProcessor{post_process_object_detection(...o){return(0,r.post_process_object_detection)(...o)}}}),"./src/models/sam/image_processing_sam.js":((e,t,n)=>{n.r(t),n.d(t,{SamImageProcessor:()=>o});var r=n("./src/base/image_processors_utils.js"),s=n("./src/utils/core.js"),i=n("./src/utils/tensor.js");class o extends r.ImageProcessor{reshape_input_points(l,c,d,u=!1){l=structuredClone(l);let p=(0,s.calculateDimensions)(l);if(p.length===3)u||(p=[1,...p]),l=[l];else if(p.length!==4)throw Error("The input_points must be a 4D tensor of shape `batch_size`, `point_batch_size`, `nb_points_per_image`, `2`.");for(let h=0;h<l.length;++h){const[m,g]=c[h],[I,f]=d[h],_=[f/g,I/m];for(let T=0;T<l[h].length;++T)for(let M=0;M<l[h][T].length;++M)for(let v=0;v<l[h][T][M].length;++v)l[h][T][M][v]*=_[v%2]}return new i.Tensor("float32",Float32Array.from(l.flat(1/0)),p)}add_input_labels(l,c){let d=(0,s.calculateDimensions)(l);if(d.length===2)d=[1,...d],l=[l];else if(d.length!==3)throw Error("The input_points must be a 4D tensor of shape `batch_size`, `point_batch_size`, `nb_points_per_image`, `2`.");if(d.some((u,p)=>u!==c.dims[p]))throw Error(`The first ${d.length} dimensions of 'input_points' and 'input_labels' must be the same.`);return new i.Tensor("int64",l.flat(1/0).map(BigInt),d)}async _call(l,{input_points:c=null,input_labels:d=null,input_boxes:u=null}={}){const p=await super._call(l);if(c&&(p.input_points=this.reshape_input_points(c,p.original_sizes,p.reshaped_input_sizes)),d){if(!p.input_points)throw Error("`input_points` must be provided if `input_labels` are provided.");p.input_labels=this.add_input_labels(d,p.input_points)}return u&&(p.input_boxes=this.reshape_input_points(u,p.original_sizes,p.reshaped_input_sizes,!0)),p}async post_process_masks(l,c,d,{mask_threshold:u=0,binarize:p=!0,pad_size:h=null}={}){const m=[];h=h??this.pad_size??this.size;const g=[h.height,h.width];for(let I=0;I<c.length;++I){const f=c[I],_=d[I];let T=await(0,i.interpolate_4d)(l[I],{mode:"bilinear",size:g});if(T=T.slice(null,null,[0,_[0]],[0,_[1]]),T=await(0,i.interpolate_4d)(T,{mode:"bilinear",size:f}),p){const M=T.data,v=new Uint8Array(M.length);for(let b=0;b<M.length;++b)M[b]>u&&(v[b]=1);T=new i.Tensor("bool",v,T.dims)}m.push(T)}return m}generate_crop_boxes(l,c,{crop_n_layers:d=0,overlap_ratio:u=512/1500,points_per_crop:p=32,crop_n_points_downscale_factor:h=1}={}){}}}),"./src/models/sam/processing_sam.js":((e,t,n)=>{n.r(t),n.d(t,{SamProcessor:()=>i});var r=n("./src/base/processing_utils.js"),s=n("./src/models/auto/image_processing_auto.js");class i extends r.Processor{static image_processor_class=s.AutoImageProcessor;async _call(...a){return await this.image_processor(...a)}post_process_masks(...a){return this.image_processor.post_process_masks(...a)}reshape_input_points(...a){return this.image_processor.reshape_input_points(...a)}}}),"./src/models/sam2/image_processing_sam2.js":((e,t,n)=>{n.r(t),n.d(t,{Sam2ImageProcessor:()=>r.SamImageProcessor});var r=n("./src/models/sam/image_processing_sam.js")}),"./src/models/sam2/processing_sam2.js":((e,t,n)=>{n.r(t),n.d(t,{Sam2Processor:()=>s,Sam2VideoProcessor:()=>i});var r=n("./src/models/sam/processing_sam.js");class s extends r.SamProcessor{}class i extends s{}}),"./src/models/sam3/image_processing_sam3.js":((e,t,n)=>{n.r(t),n.d(t,{Sam3ImageProcessor:()=>r.Sam2ImageProcessor});var r=n("./src/models/sam2/image_processing_sam2.js")}),"./src/models/seamless_m4t/feature_extraction_seamless_m4t.js":((e,t,n)=>{n.r(t),n.d(t,{SeamlessM4TFeatureExtractor:()=>o});var r=n("./src/base/feature_extraction_utils.js"),s=n("./src/utils/tensor.js"),i=n("./src/utils/audio.js");class o extends r.FeatureExtractor{constructor(l){super(l);const c=this.config.sampling_rate,d=(0,i.mel_filter_bank)(257,this.config.num_mel_bins,20,Math.floor(c/2),c,null,"kaldi",!0);this.mel_filters=d,this.window=(0,i.window_function)(400,"povey",{periodic:!1})}async _extract_fbank_features(l,c){return l=l.map(d=>d*32768),(0,i.spectrogram)(l,this.window,400,160,{fft_length:512,power:2,center:!1,preemphasis:.97,mel_filters:this.mel_filters,log_mel:"log",mel_floor:1192092955078125e-22,remove_dc_offset:!0,max_num_frames:c,transpose:!0})}async _call(l,{padding:c=!0,pad_to_multiple_of:d=2,do_normalize_per_mel_bins:u=!0,return_attention_mask:p=!0}={}){(0,r.validate_audio_inputs)(l,"SeamlessM4TFeatureExtractor");let h=await this._extract_fbank_features(l,this.config.max_length);if(u){const[v,b]=h.dims,A=h.data;for(let k=0;k<b;++k){let F=0;for(let R=0;R<v;++R)F+=A[R*b+k];const L=F/v;let G=0;for(let R=0;R<v;++R)G+=(A[R*b+k]-L)**2;G/=v-1;const j=Math.sqrt(G+1e-7);for(let R=0;R<v;++R){const K=R*b+k;A[K]=(A[K]-L)/j}}}let m;if(c){const[v,b]=h.dims,A=h.data,k=v%d;if(k>0){const F=new Float32Array(b*(v+k));F.set(A),F.fill(this.config.padding_value,A.length);const L=v+k;h=new s.Tensor(h.type,F,[L,b]),p&&(m=new s.Tensor("int64",new BigInt64Array(L),[1,L]),m.data.fill(1n,0,v))}}const[g,I]=h.dims,f=this.config.stride;if(g%f!==0)throw new Error(`The number of frames (${g}) must be a multiple of the stride (${f}).`);const T=h.view(1,Math.floor(g/f),I*f),M={input_features:T};if(p){const v=T.dims[1],b=new BigInt64Array(v);if(m){const A=m.data;for(let k=1,F=0;k<g;k+=f,++F)b[F]=A[k]}else b.fill(1n);M.attention_mask=new s.Tensor("int64",b,[1,v])}return M}}}),"./src/models/segformer/image_processing_segformer.js":((e,t,n)=>{n.r(t),n.d(t,{SegformerFeatureExtractor:()=>i,SegformerImageProcessor:()=>s});var r=n("./src/base/image_processors_utils.js");class s extends r.ImageProcessor{post_process_semantic_segmentation(...a){return(0,r.post_process_semantic_segmentation)(...a)}}class i extends s{}}),"./src/models/siglip/image_processing_siglip.js":((e,t,n)=>{n.r(t),n.d(t,{SiglipImageProcessor:()=>s});var r=n("./src/base/image_processors_utils.js");class s extends r.ImageProcessor{}}),"./src/models/smolvlm/image_processing_smolvlm.js":((e,t,n)=>{n.r(t),n.d(t,{SmolVLMImageProcessor:()=>r.Idefics3ImageProcessor});var r=n("./src/models/idefics3/image_processing_idefics3.js")}),"./src/models/smolvlm/processing_smolvlm.js":((e,t,n)=>{n.r(t),n.d(t,{SmolVLMProcessor:()=>r.Idefics3Processor});var r=n("./src/models/idefics3/processing_idefics3.js")}),"./src/models/snac/feature_extraction_snac.js":((e,t,n)=>{n.r(t),n.d(t,{SnacFeatureExtractor:()=>s});var r=n("./src/models/dac/feature_extraction_dac.js");class s extends r.DacFeatureExtractor{}}),"./src/models/speecht5/feature_extraction_speecht5.js":((e,t,n)=>{n.r(t),n.d(t,{SpeechT5FeatureExtractor:()=>s});var r=n("./src/base/feature_extraction_utils.js");class s extends r.FeatureExtractor{}}),"./src/models/speecht5/processing_speecht5.js":((e,t,n)=>{n.r(t),n.d(t,{SpeechT5Processor:()=>o});var r=n("./src/base/processing_utils.js"),s=n("./src/tokenizers.js"),i=n("./src/models/auto/feature_extraction_auto.js");class o extends r.Processor{static tokenizer_class=s.AutoTokenizer;static feature_extractor_class=i.AutoFeatureExtractor;async _call(l){return await this.feature_extractor(l)}}}),"./src/models/swin2sr/image_processing_swin2sr.js":((e,t,n)=>{n.r(t),n.d(t,{Swin2SRImageProcessor:()=>s});var r=n("./src/base/image_processors_utils.js");class s extends r.ImageProcessor{pad_image(o,a,l,c={}){const[d,u,p]=a;return super.pad_image(o,a,{width:u+(l-u%l)%l,height:d+(l-d%l)%l},{mode:"symmetric",center:!1,constant_values:-1,...c})}}}),"./src/models/ultravox/processing_ultravox.js":((e,t,n)=>{n.r(t),n.d(t,{UltravoxProcessor:()=>o});var r=n("./src/models/auto/feature_extraction_auto.js"),s=n("./src/tokenizers.js"),i=n("./src/base/processing_utils.js");class o extends i.Processor{static tokenizer_class=s.AutoTokenizer;static feature_extractor_class=r.AutoFeatureExtractor;static uses_processor_config=!0;async _call(l,c=null,d={}){if(Array.isArray(l))throw new Error("Batched inputs are not supported yet.");let u={};if(c){const h=c.length,{input_features:m}=await this.feature_extractor(c,{...d,max_length:h}),g=Math.round(h/this.config.encoder_ds_factor+1e-4),I=1+Math.ceil(g/this.config.stack_factor);u.audio_token_len=[I],u.audio_values=m;const f=this.config.audio_placeholder;if(!l.includes(f))throw new Error(`The input text does not contain the image token ${f}.`);l=l.replaceAll(f,f.repeat(I))}return{...this.tokenizer(l,{add_special_tokens:!1,...d}),...u}}}}),"./src/models/vit/image_processing_vit.js":((e,t,n)=>{n.r(t),n.d(t,{ViTFeatureExtractor:()=>i,ViTImageProcessor:()=>s});var r=n("./src/base/image_processors_utils.js");class s extends r.ImageProcessor{}class i extends s{}}),"./src/models/vitmatte/image_processing_vitmatte.js":((e,t,n)=>{n.r(t),n.d(t,{VitMatteImageProcessor:()=>i});var r=n("./src/base/image_processors_utils.js"),s=n("./src/utils/tensor.js");class i extends r.ImageProcessor{async _call(a,l){Array.isArray(a)||(a=[a]),Array.isArray(l)||(l=[l]);const c=await Promise.all(a.map(p=>this.preprocess(p))),d=await Promise.all(l.map(p=>this.preprocess(p,{do_normalize:!1,do_convert_rgb:!1,do_convert_grayscale:!0})));return{pixel_values:(0,s.stack)(c.map((p,h)=>(0,s.cat)([p.pixel_values,d[h].pixel_values],0)),0),original_sizes:c.map(p=>p.original_size),reshaped_input_sizes:c.map(p=>p.reshaped_input_size)}}}}),"./src/models/vitpose/image_processing_vitpose.js":((e,t,n)=>{n.r(t),n.d(t,{VitPoseImageProcessor:()=>s});var r=n("./src/base/image_processors_utils.js");class s extends r.ImageProcessor{post_process_pose_estimation(o,a,{threshold:l=null}={}){const c=o.tolist(),[d,u,p,h]=o.dims,m=[];for(let g=0;g<d;++g){const I=c[g],f=a[g],_=[];for(let T=0;T<f.length;++T){const M=f[T],v=[],b=[],A=[],k=M.at(-2)/h,F=M.at(-1)/p;for(let L=0;L<I.length;++L){let[G,j]=[0,0],R=0,K=-1/0;const U=I[L];for(let te=0;te<U.length;++te){const ne=U[te];for(let le=0;le<ne.length;++le){const N=ne[le];R+=N,K=Math.max(K,N),G+=(le+.5)*N,j+=te*N}}if(l!=null&&K<l)continue;const Y=[k*G/R,F*j/R];v.push(Y),A.push(L),b.push(K)}_.push({bbox:M,scores:b,labels:A,keypoints:v})}m.push(_)}return m}}}),"./src/models/voxtral/processing_voxtral.js":((e,t,n)=>{n.r(t),n.d(t,{VoxtralProcessor:()=>u});var r=n("./src/models/auto/feature_extraction_auto.js"),s=n("./src/tokenizers.js"),i=n("./src/base/processing_utils.js"),o=n("./src/utils/tensor.js");const a="[AUDIO]",l="[BEGIN_AUDIO]",c=375;function d(p,h){const m=[];for(let g=0;g<p.length;g+=h)m.push(p.subarray(g,Math.min(g+h,p.length)));return m}class u extends i.Processor{static tokenizer_class=s.AutoTokenizer;static feature_extractor_class=r.AutoFeatureExtractor;static uses_processor_config=!1;async _call(h,m=null,g={}){if(Array.isArray(h))throw new Error("Batched inputs are not supported yet.");const I={};if(m){if(!h.includes(a))throw new Error(`The input text does not contain the audio token ${a}.`);Array.isArray(m)||(m=[m]);const _=h.split(a),T=_.length-1;if(T!==m.length)throw new Error(`The number of audio inputs (${m.length}) does not match the number of audio tokens in the text (${T}).`);const M=this.feature_extractor.config.n_samples,v=m.map(L=>d(L,M)),b=v.map(L=>L.length),A=v.flat(),k=(await Promise.all(A.map(L=>this.feature_extractor(L,g)))).map(L=>L.input_features);I.audio_values=k.length>1?(0,o.cat)(k,0):k[0];let F=_[0];for(let L=0;L<b.length;++L){F+=l;for(let G=0;G<b[L];++G)F+=a.repeat(c);F+=_[L+1]}h=F}return{...this.tokenizer(h,{add_special_tokens:!1,...g}),...I}}}}),"./src/models/wav2vec2/feature_extraction_wav2vec2.js":((e,t,n)=>{n.r(t),n.d(t,{Wav2Vec2FeatureExtractor:()=>i});var r=n("./src/base/feature_extraction_utils.js"),s=n("./src/utils/tensor.js");class i extends r.FeatureExtractor{_zero_mean_unit_var_norm(a){const c=a.reduce((u,p)=>u+p,0)/a.length,d=a.reduce((u,p)=>u+(p-c)**2,0)/a.length;return a.map(u=>(u-c)/Math.sqrt(d+1e-7))}async _call(a){(0,r.validate_audio_inputs)(a,"Wav2Vec2FeatureExtractor"),a instanceof Float64Array&&(a=new Float32Array(a));let l=a;this.config.do_normalize&&(l=this._zero_mean_unit_var_norm(l));const c=[1,l.length];return{input_values:new s.Tensor("float32",l,c),attention_mask:new s.Tensor("int64",new BigInt64Array(l.length).fill(1n),c)}}}}),"./src/models/wav2vec2/processing_wav2vec2.js":((e,t,n)=>{n.r(t),n.d(t,{Wav2Vec2Processor:()=>o});var r=n("./src/tokenizers.js"),s=n("./src/models/auto/feature_extraction_auto.js"),i=n("./src/base/processing_utils.js");class o extends i.Processor{static tokenizer_class=r.AutoTokenizer;static feature_extractor_class=s.AutoFeatureExtractor;async _call(l){return await this.feature_extractor(l)}}}),"./src/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.js":((e,t,n)=>{n.r(t),n.d(t,{Wav2Vec2ProcessorWithLM:()=>o});var r=n("./src/tokenizers.js"),s=n("./src/models/auto/feature_extraction_auto.js"),i=n("./src/base/processing_utils.js");class o extends i.Processor{static tokenizer_class=r.AutoTokenizer;static feature_extractor_class=s.AutoFeatureExtractor;async _call(l){return await this.feature_extractor(l)}}}),"./src/models/wespeaker/feature_extraction_wespeaker.js":((e,t,n)=>{n.r(t),n.d(t,{WeSpeakerFeatureExtractor:()=>i});var r=n("./src/base/feature_extraction_utils.js");n("./src/utils/tensor.js");var s=n("./src/utils/audio.js");class i extends r.FeatureExtractor{constructor(a){super(a);const l=this.config.sampling_rate,c=(0,s.mel_filter_bank)(257,this.config.num_mel_bins,20,Math.floor(l/2),l,null,"kaldi",!0);this.mel_filters=c,this.window=(0,s.window_function)(400,"hamming",{periodic:!1}),this.min_num_frames=this.config.min_num_frames}async _extract_fbank_features(a){return a=a.map(l=>l*32768),(0,s.spectrogram)(a,this.window,400,160,{fft_length:512,power:2,center:!1,preemphasis:.97,mel_filters:this.mel_filters,log_mel:"log",mel_floor:1192092955078125e-22,remove_dc_offset:!0,transpose:!0,min_num_frames:this.min_num_frames})}async _call(a){(0,r.validate_audio_inputs)(a,"WeSpeakerFeatureExtractor");const l=(await this._extract_fbank_features(a)).unsqueeze_(0);if(this.config.fbank_centering_span===null){const c=l.mean(1).data,d=l.data,[u,p,h]=l.dims;for(let m=0;m<u;++m){const g=m*p*h,I=m*h;for(let f=0;f<p;++f){const _=g+f*h;for(let T=0;T<h;++T)d[_+T]-=c[I+T]}}}return{input_features:l}}}}),"./src/models/whisper/common_whisper.js":((e,t,n)=>{n.r(t),n.d(t,{WHISPER_LANGUAGE_MAPPING:()=>s,WHISPER_TO_LANGUAGE_CODE_MAPPING:()=>i,whisper_language_to_code:()=>o});const r=[["en","english"],["zh","chinese"],["de","german"],["es","spanish"],["ru","russian"],["ko","korean"],["fr","french"],["ja","japanese"],["pt","portuguese"],["tr","turkish"],["pl","polish"],["ca","catalan"],["nl","dutch"],["ar","arabic"],["sv","swedish"],["it","italian"],["id","indonesian"],["hi","hindi"],["fi","finnish"],["vi","vietnamese"],["he","hebrew"],["uk","ukrainian"],["el","greek"],["ms","malay"],["cs","czech"],["ro","romanian"],["da","danish"],["hu","hungarian"],["ta","tamil"],["no","norwegian"],["th","thai"],["ur","urdu"],["hr","croatian"],["bg","bulgarian"],["lt","lithuanian"],["la","latin"],["mi","maori"],["ml","malayalam"],["cy","welsh"],["sk","slovak"],["te","telugu"],["fa","persian"],["lv","latvian"],["bn","bengali"],["sr","serbian"],["az","azerbaijani"],["sl","slovenian"],["kn","kannada"],["et","estonian"],["mk","macedonian"],["br","breton"],["eu","basque"],["is","icelandic"],["hy","armenian"],["ne","nepali"],["mn","mongolian"],["bs","bosnian"],["kk","kazakh"],["sq","albanian"],["sw","swahili"],["gl","galician"],["mr","marathi"],["pa","punjabi"],["si","sinhala"],["km","khmer"],["sn","shona"],["yo","yoruba"],["so","somali"],["af","afrikaans"],["oc","occitan"],["ka","georgian"],["be","belarusian"],["tg","tajik"],["sd","sindhi"],["gu","gujarati"],["am","amharic"],["yi","yiddish"],["lo","lao"],["uz","uzbek"],["fo","faroese"],["ht","haitian creole"],["ps","pashto"],["tk","turkmen"],["nn","nynorsk"],["mt","maltese"],["sa","sanskrit"],["lb","luxembourgish"],["my","myanmar"],["bo","tibetan"],["tl","tagalog"],["mg","malagasy"],["as","assamese"],["tt","tatar"],["haw","hawaiian"],["ln","lingala"],["ha","hausa"],["ba","bashkir"],["jw","javanese"],["su","sundanese"]],s=new Map(r),i=new Map([...r.map(([a,l])=>[l,a]),["burmese","my"],["valencian","ca"],["flemish","nl"],["haitian","ht"],["letzeburgesch","lb"],["pushto","ps"],["panjabi","pa"],["moldavian","ro"],["moldovan","ro"],["sinhalese","si"],["castilian","es"]]);function o(a){a=a.toLowerCase();let l=i.get(a);if(l===void 0){const c=a.match(/^<\|([a-z]{2})\|>$/);if(c&&(a=c[1]),s.has(a))l=a;else{const u=a.length===2?s.keys():s.values();throw new Error(`Language "${a}" is not supported. Must be one of: ${JSON.stringify(Array.from(u))}`)}}return l}}),"./src/models/whisper/feature_extraction_whisper.js":((e,t,n)=>{n.r(t),n.d(t,{WhisperFeatureExtractor:()=>o});var r=n("./src/base/feature_extraction_utils.js");n("./src/utils/tensor.js");var s=n("./src/utils/audio.js"),i=n("./src/utils/maths.js");class o extends r.FeatureExtractor{constructor(l){super(l),this.config.mel_filters??=(0,s.mel_filter_bank)(Math.floor(1+this.config.n_fft/2),this.config.feature_size,0,8e3,this.config.sampling_rate,"slaney","slaney"),this.window=(0,s.window_function)(this.config.n_fft,"hann")}async _extract_fbank_features(l){const c=await(0,s.spectrogram)(l,this.window,this.config.n_fft,this.config.hop_length,{power:2,mel_filters:this.config.mel_filters,log_mel:"log10",max_num_frames:Math.min(Math.floor(l.length/this.config.hop_length),this.config.nb_max_frames)}),d=c.data,u=(0,i.max)(d)[0];for(let p=0;p<d.length;++p)d[p]=(Math.max(d[p],u-8)+4)/4;return c}async _call(l,{max_length:c=null}={}){(0,r.validate_audio_inputs)(l,"WhisperFeatureExtractor");let d;const u=c??this.config.n_samples;return l.length>u?(l.length>this.config.n_samples&&console.warn("Attempting to extract features for audio longer than 30 seconds. If using a pipeline to extract transcript from a long audio clip, remember to specify `chunk_length_s` and/or `stride_length_s`."),d=l.slice(0,u)):(d=new Float32Array(u),d.set(l)),{input_features:(await this._extract_fbank_features(d)).unsqueeze_(0)}}}}),"./src/models/whisper/generation_whisper.js":((e,t,n)=>{n.r(t),n.d(t,{WhisperGenerationConfig:()=>s});var r=n("./src/generation/configuration_utils.js");class s extends r.GenerationConfig{return_timestamps=null;return_token_timestamps=null;num_frames=null;alignment_heads=null;task=null;language=null;no_timestamps_token_id=null;prompt_ids=null;is_multilingual=null;lang_to_id=null;task_to_id=null;max_initial_timestamp_index=1}}),"./src/models/whisper/processing_whisper.js":((e,t,n)=>{n.r(t),n.d(t,{WhisperProcessor:()=>o});var r=n("./src/models/auto/feature_extraction_auto.js"),s=n("./src/tokenizers.js"),i=n("./src/base/processing_utils.js");class o extends i.Processor{static tokenizer_class=s.AutoTokenizer;static feature_extractor_class=r.AutoFeatureExtractor;async _call(l){return await this.feature_extractor(l)}}}),"./src/models/yolos/image_processing_yolos.js":((e,t,n)=>{n.r(t),n.d(t,{YolosFeatureExtractor:()=>i,YolosImageProcessor:()=>s});var r=n("./src/base/image_processors_utils.js");class s extends r.ImageProcessor{post_process_object_detection(...a){return(0,r.post_process_object_detection)(...a)}}class i extends s{}}),"./src/ops/registry.js":((e,t,n)=>{n.r(t),n.d(t,{TensorOpRegistry:()=>o});var r=n("./src/backends/onnx.js"),s=n("./src/utils/tensor.js");const i=async(a,l,c)=>{const d=await(0,r.createInferenceSession)(new Uint8Array(a),l);return(async u=>{const p=(0,r.isONNXProxy)(),h=Object.fromEntries(Object.entries(u).map(([g,I])=>[g,(p?I.clone():I).ort_tensor])),m=await(0,r.runInferenceSession)(d,h);return Array.isArray(c)?c.map(g=>new s.Tensor(m[g])):new s.Tensor(m[c])})};class o{static session_options={};static get nearest_interpolate_4d(){return this._nearest_interpolate_4d||(this._nearest_interpolate_4d=i([8,10,18,0,58,129,1,10,41,10,1,120,10,0,10,0,10,1,115,18,1,121,34,6,82,101,115,105,122,101,42,18,10,4,109,111,100,101,34,7,110,101,97,114,101,115,116,160,1,3,18,1,114,90,31,10,1,120,18,26,10,24,8,1,18,20,10,3,18,1,98,10,3,18,1,99,10,3,18,1,104,10,3,18,1,119,90,15,10,1,115,18,10,10,8,8,7,18,4,10,2,8,4,98,31,10,1,121,18,26,10,24,8,1,18,20,10,3,18,1,98,10,3,18,1,99,10,3,18,1,104,10,3,18,1,119,66,2,16,21],this.session_options,"y")),this._nearest_interpolate_4d}static get bilinear_interpolate_4d(){return this._bilinear_interpolate_4d||(this._bilinear_interpolate_4d=i([8,9,18,0,58,128,1,10,40,10,1,120,10,0,10,0,10,1,115,18,1,121,34,6,82,101,115,105,122,101,42,17,10,4,109,111,100,101,34,6,108,105,110,101,97,114,160,1,3,18,1,114,90,31,10,1,120,18,26,10,24,8,1,18,20,10,3,18,1,98,10,3,18,1,99,10,3,18,1,104,10,3,18,1,119,90,15,10,1,115,18,10,10,8,8,7,18,4,10,2,8,4,98,31,10,1,121,18,26,10,24,8,1,18,20,10,3,18,1,98,10,3,18,1,99,10,3,18,1,104,10,3,18,1,119,66,2,16,20],this.session_options,"y")),this._bilinear_interpolate_4d}static get bicubic_interpolate_4d(){return this._bicubic_interpolate_4d||(this._bicubic_interpolate_4d=i([8,9,18,0,58,127,10,39,10,1,120,10,0,10,0,10,1,115,18,1,121,34,6,82,101,115,105,122,101,42,16,10,4,109,111,100,101,34,5,99,117,98,105,99,160,1,3,18,1,114,90,31,10,1,120,18,26,10,24,8,1,18,20,10,3,18,1,98,10,3,18,1,99,10,3,18,1,104,10,3,18,1,119,90,15,10,1,115,18,10,10,8,8,7,18,4,10,2,8,4,98,31,10,1,121,18,26,10,24,8,1,18,20,10,3,18,1,98,10,3,18,1,99,10,3,18,1,104,10,3,18,1,119,66,2,16,20],this.session_options,"y")),this._bicubic_interpolate_4d}static get matmul(){return this._matmul||(this._matmul=i([8,9,18,0,58,55,10,17,10,1,97,10,1,98,18,1,99,34,6,77,97,116,77,117,108,18,1,114,90,9,10,1,97,18,4,10,2,8,1,90,9,10,1,98,18,4,10,2,8,1,98,9,10,1,99,18,4,10,2,8,1,66,2,16,20],this.session_options,"c")),this._matmul}static get stft(){return this._stft||(this._stft=i([8,7,18,0,58,148,1,10,38,10,1,115,10,1,106,10,1,119,10,1,108,18,1,111,34,4,83,84,70,84,42,15,10,8,111,110,101,115,105,100,101,100,24,1,160,1,2,18,1,115,90,26,10,1,115,18,21,10,19,8,1,18,15,10,3,18,1,98,10,3,18,1,115,10,3,18,1,99,90,11,10,1,106,18,6,10,4,8,7,18,0,90,16,10,1,119,18,11,10,9,8,1,18,5,10,3,18,1,119,90,11,10,1,108,18,6,10,4,8,7,18,0,98,31,10,1,111,18,26,10,24,8,1,18,20,10,3,18,1,98,10,3,18,1,102,10,3,18,1,100,10,3,18,1,99,66,2,16,17],this.session_options,"o")),this._stft}static get rfft(){return this._rfft||(this._rfft=i([8,9,18,0,58,97,10,33,10,1,120,10,0,10,1,97,18,1,121,34,3,68,70,84,42,15,10,8,111,110,101,115,105,100,101,100,24,1,160,1,2,18,1,100,90,21,10,1,120,18,16,10,14,8,1,18,10,10,3,18,1,115,10,3,18,1,99,90,11,10,1,97,18,6,10,4,8,7,18,0,98,21,10,1,121,18,16,10,14,8,1,18,10,10,3,18,1,115,10,3,18,1,99,66,2,16,20],this.session_options,"y")),this._rfft}static get top_k(){return this._top_k||(this._top_k=i([8,10,18,0,58,73,10,18,10,1,120,10,1,107,18,1,118,18,1,105,34,4,84,111,112,75,18,1,116,90,9,10,1,120,18,4,10,2,8,1,90,15,10,1,107,18,10,10,8,8,7,18,4,10,2,8,1,98,9,10,1,118,18,4,10,2,8,1,98,9,10,1,105,18,4,10,2,8,7,66,2,16,21],this.session_options,["v","i"])),this._top_k}static get slice(){return this._slice||(this._slice=i([8,7,18,0,58,96,10,25,10,1,120,10,1,115,10,1,101,10,1,97,10,1,116,18,1,121,34,5,83,108,105,99,101,18,1,114,90,9,10,1,120,18,4,10,2,8,1,90,9,10,1,115,18,4,10,2,8,7,90,9,10,1,101,18,4,10,2,8,7,90,9,10,1,97,18,4,10,2,8,7,90,9,10,1,116,18,4,10,2,8,7,98,9,10,1,121,18,4,10,2,8,1,66,2,16,13],this.session_options,"y")),this._slice}}}),"./src/pipelines.js":((e,t,n)=>{n.r(t),n.d(t,{AudioClassificationPipeline:()=>j,AutomaticSpeechRecognitionPipeline:()=>K,BackgroundRemovalPipeline:()=>ne,DepthEstimationPipeline:()=>se,DocumentQuestionAnsweringPipeline:()=>X,FeatureExtractionPipeline:()=>L,FillMaskPipeline:()=>T,ImageClassificationPipeline:()=>Y,ImageFeatureExtractionPipeline:()=>G,ImageSegmentationPipeline:()=>te,ImageToImagePipeline:()=>z,ImageToTextPipeline:()=>U,ObjectDetectionPipeline:()=>N,Pipeline:()=>g,QuestionAnsweringPipeline:()=>_,SummarizationPipeline:()=>v,Text2TextGenerationPipeline:()=>M,TextClassificationPipeline:()=>I,TextGenerationPipeline:()=>k,TextToAudioPipeline:()=>D,TokenClassificationPipeline:()=>f,TranslationPipeline:()=>b,ZeroShotAudioClassificationPipeline:()=>R,ZeroShotClassificationPipeline:()=>F,ZeroShotImageClassificationPipeline:()=>le,ZeroShotObjectDetectionPipeline:()=>oe,pipeline:()=>ke});var r=n("./src/tokenizers.js"),s=n("./src/models.js"),i=n("./src/models/auto/processing_auto.js");n("./src/base/processing_utils.js");var o=n("./src/utils/generic.js"),a=n("./src/utils/core.js"),l=n("./src/utils/maths.js"),c=n("./src/utils/audio.js"),d=n("./src/utils/tensor.js"),u=n("./src/utils/image.js");async function p(Ce){return Array.isArray(Ce)||(Ce=[Ce]),await Promise.all(Ce.map(Z=>u.RawImage.read(Z)))}async function h(Ce,Z){return Array.isArray(Ce)||(Ce=[Ce]),await Promise.all(Ce.map(V=>typeof V=="string"||V instanceof URL?(0,c.read_audio)(V,Z):V instanceof Float64Array?new Float32Array(V):V))}function m(Ce,Z){Z&&(Ce=Ce.map(Ee=>Ee|0));const[V,fe,Te,Ie]=Ce;return{xmin:V,ymin:fe,xmax:Te,ymax:Ie}}class g extends o.Callable{constructor({task:Z,model:V,tokenizer:fe=null,processor:Te=null}){super(),this.task=Z,this.model=V,this.tokenizer=fe,this.processor=Te}async dispose(){await this.model.dispose()}}class I extends g{constructor(Z){super(Z)}async _call(Z,{top_k:V=1}={}){const fe=this.tokenizer(Z,{padding:!0,truncation:!0}),Te=await this.model(fe),Ie=this.model.config.problem_type==="multi_label_classification"?xe=>xe.sigmoid():xe=>new d.Tensor("float32",(0,l.softmax)(xe.data),xe.dims),Ee=this.model.config.id2label,De=[];for(const xe of Te.logits){const ze=Ie(xe),_e=await(0,d.topk)(ze,V),Le=_e[0].tolist(),Ne=_e[1].tolist().map((ot,Ve)=>({label:Ee?Ee[ot]:`LABEL_${ot}`,score:Le[Ve]}));V===1?De.push(...Ne):De.push(Ne)}return Array.isArray(Z)||V===1?De:De[0]}}class f extends g{constructor(Z){super(Z)}async _call(Z,{ignore_labels:V=["O"]}={}){const fe=Array.isArray(Z),Te=this.tokenizer(fe?Z:[Z],{padding:!0,truncation:!0}),Ee=(await this.model(Te)).logits,De=this.model.config.id2label,xe=[];for(let ze=0;ze<Ee.dims[0];++ze){const _e=Te.input_ids[ze],Le=Ee[ze],qe=[];for(let Ne=0;Ne<Le.dims[0];++Ne){const ot=Le[Ne],Ve=(0,l.max)(ot.data)[1],de=De?De[Ve]:`LABEL_${Ve}`;if(V.includes(de))continue;const ye=this.tokenizer.decode([_e[Ne].item()],{skip_special_tokens:!0});if(ye==="")continue;const Pe=(0,l.softmax)(ot.data);qe.push({entity:de,score:Pe[Ve],index:Ne,word:ye})}xe.push(qe)}return fe?xe:xe[0]}}class _ extends g{constructor(Z){super(Z)}async _call(Z,V,{top_k:fe=1}={}){const Te=this.tokenizer(Z,{text_pair:V,padding:!0,truncation:!0}),{start_logits:Ie,end_logits:Ee}=await this.model(Te),De=Te.input_ids.tolist(),xe=Te.attention_mask.tolist(),ze=this.tokenizer.all_special_ids,_e=[];for(let Le=0;Le<Ie.dims[0];++Le){const qe=De[Le],Ne=qe.findIndex(ve=>ve==this.tokenizer.sep_token_id);xe[Le].map((ve,Qe)=>ve==1&&(Qe===0||Qe>Ne&&ze.findIndex(ct=>ct==qe[Qe])===-1));const ot=Ie[Le].tolist(),Ve=Ee[Le].tolist();for(let ve=1;ve<ot.length;++ve)(xe[Le]==0||ve<=Ne||ze.findIndex(Qe=>Qe==qe[ve])!==-1)&&(ot[ve]=-1/0,Ve[ve]=-1/0);const de=(0,l.softmax)(ot).map((ve,Qe)=>[ve,Qe]),ye=(0,l.softmax)(Ve).map((ve,Qe)=>[ve,Qe]);de[0][0]=0,ye[0][0]=0;const Pe=(0,a.product)(de,ye).filter(ve=>ve[0][1]<=ve[1][1]).map(ve=>[ve[0][1],ve[1][1],ve[0][0]*ve[1][0]]).sort((ve,Qe)=>Qe[2]-ve[2]);for(let ve=0;ve<Math.min(Pe.length,fe);++ve){const[Qe,ct,zt]=Pe[ve],wt=qe.slice(Qe,ct+1),on=this.tokenizer.decode(wt,{skip_special_tokens:!0});_e.push({answer:on,score:zt})}}return fe===1?_e[0]:_e}}class T extends g{constructor(Z){super(Z)}async _call(Z,{top_k:V=5}={}){const fe=this.tokenizer(Z,{padding:!0,truncation:!0}),{logits:Te}=await this.model(fe),Ie=[],Ee=fe.input_ids.tolist();for(let De=0;De<Ee.length;++De){const xe=Ee[De],ze=xe.findIndex(ot=>ot==this.tokenizer.mask_token_id);if(ze===-1)throw Error(`Mask token (${this.tokenizer.mask_token}) not found in text.`);const _e=Te[De][ze],Le=await(0,d.topk)(new d.Tensor("float32",(0,l.softmax)(_e.data),_e.dims),V),qe=Le[0].tolist(),Ne=Le[1].tolist();Ie.push(Ne.map((ot,Ve)=>{const de=xe.slice();return de[ze]=ot,{score:qe[Ve],token:Number(ot),token_str:this.tokenizer.decode([ot]),sequence:this.tokenizer.decode(de,{skip_special_tokens:!0})}}))}return Array.isArray(Z)?Ie:Ie[0]}}class M extends g{_key="generated_text";constructor(Z){super(Z)}async _call(Z,V={}){Array.isArray(Z)||(Z=[Z]),this.model.config.prefix&&(Z=Z.map(xe=>this.model.config.prefix+xe));const fe=this.model.config.task_specific_params;fe&&fe[this.task]&&fe[this.task].prefix&&(Z=Z.map(xe=>fe[this.task].prefix+xe));const Te=this.tokenizer,Ie={padding:!0,truncation:!0};let Ee;this instanceof b&&"_build_translation_inputs"in Te?Ee=Te._build_translation_inputs(Z,Ie,V):Ee=Te(Z,Ie);const De=await this.model.generate({...Ee,...V});return Te.batch_decode(De,{skip_special_tokens:!0}).map(xe=>({[this._key]:xe}))}}class v extends M{_key="summary_text";constructor(Z){super(Z)}}class b extends M{_key="translation_text";constructor(Z){super(Z)}}function A(Ce){return Array.isArray(Ce)&&Ce.every(Z=>"role"in Z&&"content"in Z)}class k extends g{constructor(Z){super(Z)}async _call(Z,V={}){let fe=!1,Te=!1,Ie=V.add_special_tokens??(this.tokenizer.add_bos_token||this.tokenizer.add_eos_token)??!1,Ee;if(typeof Z=="string")Ee=Z=[Z];else if(Array.isArray(Z)&&Z.every(Ne=>typeof Ne=="string"))fe=!0,Ee=Z;else{if(A(Z))Z=[Z];else if(Array.isArray(Z)&&Z.every(A))fe=!0;else throw new Error("Input must be a string, an array of strings, a Chat, or an array of Chats");Te=!0,Ee=Z.map(Ne=>this.tokenizer.apply_chat_template(Ne,{tokenize:!1,add_generation_prompt:!0})),Ie=!1}const De=Te?!1:V.return_full_text??!0;this.tokenizer.padding_side="left";const xe=this.tokenizer(Ee,{add_special_tokens:Ie,padding:!0,truncation:!0}),ze=await this.model.generate({...xe,...V}),_e=this.tokenizer.batch_decode(ze,{skip_special_tokens:!0});let Le;!De&&xe.input_ids.dims.at(-1)>0&&(Le=this.tokenizer.batch_decode(xe.input_ids,{skip_special_tokens:!0}).map(Ne=>Ne.length));const qe=Array.from({length:Z.length},Ne=>[]);for(let Ne=0;Ne<_e.length;++Ne){const ot=Math.floor(Ne/ze.dims[0]*Z.length);Le&&(_e[Ne]=_e[Ne].slice(Le[ot])),qe[ot].push({generated_text:Te?[...Z[ot],{role:"assistant",content:_e[Ne]}]:_e[Ne]})}return!fe&&qe.length===1?qe[0]:qe}}class F extends g{constructor(Z){super(Z),this.label2id=Object.fromEntries(Object.entries(this.model.config.label2id).map(([V,fe])=>[V.toLowerCase(),fe])),this.entailment_id=this.label2id.entailment,this.entailment_id===void 0&&(console.warn("Could not find 'entailment' in label2id mapping. Using 2 as entailment_id."),this.entailment_id=2),this.contradiction_id=this.label2id.contradiction??this.label2id.not_entailment,this.contradiction_id===void 0&&(console.warn("Could not find 'contradiction' in label2id mapping. Using 0 as contradiction_id."),this.contradiction_id=0)}async _call(Z,V,{hypothesis_template:fe="This example is {}.",multi_label:Te=!1}={}){const Ie=Array.isArray(Z);Ie||(Z=[Z]),Array.isArray(V)||(V=[V]);const Ee=V.map(ze=>fe.replace("{}",ze)),De=Te||V.length===1,xe=[];for(const ze of Z){const _e=[];for(const Ne of Ee){const ot=this.tokenizer(ze,{text_pair:Ne,padding:!0,truncation:!0}),Ve=await this.model(ot);De?_e.push([Ve.logits.data[this.contradiction_id],Ve.logits.data[this.entailment_id]]):_e.push(Ve.logits.data[this.entailment_id])}const qe=(De?_e.map(Ne=>(0,l.softmax)(Ne)[1]):(0,l.softmax)(_e)).map((Ne,ot)=>[Ne,ot]).sort((Ne,ot)=>ot[0]-Ne[0]);xe.push({sequence:ze,labels:qe.map(Ne=>V[Ne[1]]),scores:qe.map(Ne=>Ne[0])})}return Ie?xe:xe[0]}}class L extends g{constructor(Z){super(Z)}async _call(Z,{pooling:V="none",normalize:fe=!1,quantize:Te=!1,precision:Ie="binary"}={}){const Ee=this.tokenizer(Z,{padding:!0,truncation:!0}),De=await this.model(Ee);let xe=De.last_hidden_state??De.logits??De.token_embeddings;switch(V){case"none":break;case"mean":xe=(0,d.mean_pooling)(xe,Ee.attention_mask);break;case"first_token":case"cls":xe=xe.slice(null,0);break;case"last_token":case"eos":xe=xe.slice(null,-1);break;default:throw Error(`Pooling method '${V}' not supported.`)}return fe&&(xe=xe.normalize(2,-1)),Te&&(xe=(0,d.quantize_embeddings)(xe,Ie)),xe}}class G extends g{constructor(Z){super(Z)}async _call(Z,{pool:V=null}={}){const fe=await p(Z),{pixel_values:Te}=await this.processor(fe),Ie=await this.model({pixel_values:Te});let Ee;if(V){if(!("pooler_output"in Ie))throw Error("No pooled output was returned. Make sure the model has a 'pooler' layer when using the 'pool' option.");Ee=Ie.pooler_output}else Ee=Ie.last_hidden_state??Ie.logits??Ie.image_embeds;return Ee}}class j extends g{constructor(Z){super(Z)}async _call(Z,{top_k:V=5}={}){const fe=this.processor.feature_extractor.config.sampling_rate,Te=await h(Z,fe),Ie=this.model.config.id2label,Ee=[];for(const De of Te){const xe=await this.processor(De),_e=(await this.model(xe)).logits[0],Le=await(0,d.topk)(new d.Tensor("float32",(0,l.softmax)(_e.data),_e.dims),V),qe=Le[0].tolist(),ot=Le[1].tolist().map((Ve,de)=>({label:Ie?Ie[Ve]:`LABEL_${Ve}`,score:qe[de]}));Ee.push(ot)}return Array.isArray(Z)?Ee:Ee[0]}}class R extends g{constructor(Z){super(Z)}async _call(Z,V,{hypothesis_template:fe="This is a sound of {}."}={}){const Te=!Array.isArray(Z);Te&&(Z=[Z]);const Ie=V.map(_e=>fe.replace("{}",_e)),Ee=this.tokenizer(Ie,{padding:!0,truncation:!0}),De=this.processor.feature_extractor.config.sampling_rate,xe=await h(Z,De),ze=[];for(const _e of xe){const Le=await this.processor(_e),qe=await this.model({...Ee,...Le}),Ne=(0,l.softmax)(qe.logits_per_audio.data);ze.push([...Ne].map((ot,Ve)=>({score:ot,label:V[Ve]})))}return Te?ze[0]:ze}}class K extends g{constructor(Z){super(Z)}async _call(Z,V={}){switch(this.model.config.model_type){case"whisper":case"lite-whisper":return this._call_whisper(Z,V);case"wav2vec2":case"wav2vec2-bert":case"unispeech":case"unispeech-sat":case"hubert":case"parakeet_ctc":return this._call_wav2vec2(Z,V);case"moonshine":return this._call_moonshine(Z,V);default:throw new Error(`AutomaticSpeechRecognitionPipeline does not support model type '${this.model.config.model_type}'.`)}}async _call_wav2vec2(Z,V){V.language&&console.warn('`language` parameter is not yet supported for `wav2vec2` models, defaulting to "English".'),V.task&&console.warn('`task` parameter is not yet supported for `wav2vec2` models, defaulting to "transcribe".');const fe=!Array.isArray(Z);fe&&(Z=[Z]);const Te=this.processor.feature_extractor.config.sampling_rate,Ie=await h(Z,Te),Ee=[];for(const De of Ie){const xe=await this.processor(De),_e=(await this.model(xe)).logits[0],Le=[];for(const Ne of _e)Le.push((0,l.max)(Ne.data)[1]);const qe=this.tokenizer.decode(Le,{skip_special_tokens:!0}).trim();Ee.push({text:qe})}return fe?Ee[0]:Ee}async _call_whisper(Z,V){const fe=V.return_timestamps??!1,Te=V.chunk_length_s??0,Ie=V.force_full_sequences??!1;let Ee=V.stride_length_s??null;const De={...V};fe==="word"&&(De.return_token_timestamps=!0,De.return_timestamps=!1);const xe=!Array.isArray(Z);xe&&(Z=[Z]);const ze=this.processor.feature_extractor.config.chunk_length/this.model.config.max_source_positions,_e=this.processor.feature_extractor.config.hop_length,Le=this.processor.feature_extractor.config.sampling_rate,qe=await h(Z,Le),Ne=[];for(const ot of qe){let Ve=[];if(Te>0){if(Ee===null)Ee=Te/6;else if(Te<=Ee)throw Error("`chunk_length_s` must be larger than `stride_length_s`.");const Pe=Le*Te,ve=Le*Ee,Qe=Pe-2*ve;let ct=0;for(;;){const zt=ct+Pe,wt=ot.subarray(ct,zt),on=await this.processor(wt),Tn=ct===0,kt=zt>=ot.length;if(Ve.push({stride:[wt.length,Tn?0:ve,kt?0:ve],input_features:on.input_features,is_last:kt}),kt)break;ct+=Qe}}else Ve=[{stride:[ot.length,0,0],input_features:(await this.processor(ot)).input_features,is_last:!0}];for(const Pe of Ve){De.num_frames=Math.floor(Pe.stride[0]/_e);const ve=await this.model.generate({inputs:Pe.input_features,...De});fe==="word"?(Pe.tokens=ve.sequences.tolist()[0],Pe.token_timestamps=ve.token_timestamps.tolist()[0].map(Qe=>(0,l.round)(Qe,2))):Pe.tokens=ve[0].tolist(),Pe.stride=Pe.stride.map(Qe=>Qe/Le)}const[de,ye]=this.tokenizer._decode_asr(Ve,{time_precision:ze,return_timestamps:fe,force_full_sequences:Ie});Ne.push({text:de,...ye})}return xe?Ne[0]:Ne}async _call_moonshine(Z,V){const fe=!Array.isArray(Z);fe&&(Z=[Z]);const Te=this.processor.feature_extractor.config.sampling_rate,Ie=await h(Z,Te),Ee=[];for(const De of Ie){const xe=await this.processor(De),ze=Math.floor(De.length/Te)*6,_e=await this.model.generate({max_new_tokens:ze,...V,...xe}),Le=this.processor.batch_decode(_e,{skip_special_tokens:!0})[0];Ee.push({text:Le})}return fe?Ee[0]:Ee}}class U extends g{constructor(Z){super(Z)}async _call(Z,V={}){const fe=Array.isArray(Z),Te=await p(Z),{pixel_values:Ie}=await this.processor(Te),Ee=[];for(const De of Ie){De.dims=[1,...De.dims];const xe=await this.model.generate({inputs:De,...V}),ze=this.tokenizer.batch_decode(xe,{skip_special_tokens:!0}).map(_e=>({generated_text:_e.trim()}));Ee.push(ze)}return fe?Ee:Ee[0]}}class Y extends g{constructor(Z){super(Z)}async _call(Z,{top_k:V=5}={}){const fe=await p(Z),{pixel_values:Te}=await this.processor(fe),Ie=await this.model({pixel_values:Te}),Ee=this.model.config.id2label,De=[];for(const xe of Ie.logits){const ze=await(0,d.topk)(new d.Tensor("float32",(0,l.softmax)(xe.data),xe.dims),V),_e=ze[0].tolist(),qe=ze[1].tolist().map((Ne,ot)=>({label:Ee?Ee[Ne]:`LABEL_${Ne}`,score:_e[ot]}));De.push(qe)}return Array.isArray(Z)?De:De[0]}}class te extends g{constructor(Z){super(Z),this.subtasks_mapping={panoptic:"post_process_panoptic_segmentation",instance:"post_process_instance_segmentation",semantic:"post_process_semantic_segmentation"}}async _call(Z,{threshold:V=.5,mask_threshold:fe=.5,overlap_mask_area_threshold:Te=.8,label_ids_to_fuse:Ie=null,target_sizes:Ee=null,subtask:De=null}={}){if(Array.isArray(Z)&&Z.length!==1)throw Error("Image segmentation pipeline currently only supports a batch size of 1.");const ze=await p(Z),_e=ze.map(Pe=>[Pe.height,Pe.width]),Le=await this.processor(ze),{inputNames:qe,outputNames:Ne}=this.model.sessions.model;if(!qe.includes("pixel_values")){if(qe.length!==1)throw Error(`Expected a single input name, but got ${qe.length} inputs: ${qe}.`);const Pe=qe[0];if(Pe in Le)throw Error(`Input name ${Pe} already exists in the inputs.`);Le[Pe]=Le.pixel_values}const ot=await this.model(Le);let Ve=null;if(De!==null)Ve=this.subtasks_mapping[De];else if(this.processor.image_processor){for(const[Pe,ve]of Object.entries(this.subtasks_mapping))if(ve in this.processor.image_processor){Ve=this.processor.image_processor[ve].bind(this.processor.image_processor),De=Pe;break}}const de=this.model.config.id2label,ye=[];if(De)if(De==="panoptic"||De==="instance"){const Pe=Ve(ot,V,fe,Te,Ie,Ee??_e)[0],ve=Pe.segmentation;for(const Qe of Pe.segments_info){const ct=new Uint8ClampedArray(ve.data.length);for(let wt=0;wt<ve.data.length;++wt)ve.data[wt]===Qe.id&&(ct[wt]=255);const zt=new u.RawImage(ct,ve.dims[1],ve.dims[0],1);ye.push({score:Qe.score,label:de[Qe.label_id],mask:zt})}}else if(De==="semantic"){const{segmentation:Pe,labels:ve}=Ve(ot,Ee??_e)[0];for(const Qe of ve){const ct=new Uint8ClampedArray(Pe.data.length);for(let wt=0;wt<Pe.data.length;++wt)Pe.data[wt]===Qe&&(ct[wt]=255);const zt=new u.RawImage(ct,Pe.dims[1],Pe.dims[0],1);ye.push({score:null,label:de[Qe],mask:zt})}}else throw Error(`Subtask ${De} not supported.`);else{const ve=ot[Ne[0]];for(let Qe=0;Qe<_e.length;++Qe){const ct=_e[Qe],zt=ve[Qe];zt.data.some(on=>on<-1e-5||on>1+1e-5)&&zt.sigmoid_();const wt=await u.RawImage.fromTensor(zt.mul_(255).to("uint8")).resize(ct[1],ct[0]);ye.push({label:null,score:null,mask:wt})}}return ye}}class ne extends te{constructor(Z){super(Z)}async _call(Z,V={}){if(Array.isArray(Z)&&Z.length!==1)throw Error("Background removal pipeline currently only supports a batch size of 1.");const Te=await p(Z),Ie=await super._call(Z,V);return Te.map((De,xe)=>{const ze=De.clone();return ze.putAlpha(Ie[xe].mask),ze})}}class le extends g{constructor(Z){super(Z)}async _call(Z,V,{hypothesis_template:fe="This is a photo of {}"}={}){const Te=Array.isArray(Z),Ie=await p(Z),Ee=V.map(qe=>fe.replace("{}",qe)),De=this.tokenizer(Ee,{padding:this.model.config.model_type==="siglip"?"max_length":!0,truncation:!0}),{pixel_values:xe}=await this.processor(Ie),ze=await this.model({...De,pixel_values:xe}),_e=this.model.config.model_type==="siglip"?qe=>qe.sigmoid().data:qe=>(0,l.softmax)(qe.data),Le=[];for(const qe of ze.logits_per_image){const ot=[..._e(qe)].map((Ve,de)=>({score:Ve,label:V[de]}));ot.sort((Ve,de)=>de.score-Ve.score),Le.push(ot)}return Te?Le:Le[0]}}class N extends g{constructor(Z){super(Z)}async _call(Z,{threshold:V=.9,percentage:fe=!1}={}){const Te=Array.isArray(Z);if(Te&&Z.length!==1)throw Error("Object detection pipeline currently only supports a batch size of 1.");const Ie=await p(Z),Ee=fe?null:Ie.map(Ne=>[Ne.height,Ne.width]),{pixel_values:De,pixel_mask:xe}=await this.processor(Ie),ze=await this.model({pixel_values:De,pixel_mask:xe}),_e=this.processor.image_processor.post_process_object_detection(ze,V,Ee),Le=this.model.config.id2label,qe=_e.map(Ne=>Ne.boxes.map((ot,Ve)=>({score:Ne.scores[Ve],label:Le[Ne.classes[Ve]],box:m(ot,!fe)})));return Te?qe:qe[0]}}class oe extends g{constructor(Z){super(Z)}async _call(Z,V,{threshold:fe=.1,top_k:Te=null,percentage:Ie=!1}={}){const Ee=Array.isArray(Z),De=await p(Z),xe=this.tokenizer(V,{padding:!0,truncation:!0}),ze=await this.processor(De),_e=[];for(let Le=0;Le<De.length;++Le){const qe=De[Le],Ne=Ie?null:[[qe.height,qe.width]],ot=ze.pixel_values[Le].unsqueeze_(0),Ve=await this.model({...xe,pixel_values:ot});let de;if("post_process_grounded_object_detection"in this.processor){const ye=this.processor.post_process_grounded_object_detection(Ve,xe.input_ids,{box_threshold:fe,text_threshold:fe,target_sizes:Ne})[0];de=ye.boxes.map((Pe,ve)=>({score:ye.scores[ve],label:ye.labels[ve],box:m(Pe,!Ie)}))}else{const ye=this.processor.image_processor.post_process_object_detection(Ve,fe,Ne,!0)[0];de=ye.boxes.map((Pe,ve)=>({score:ye.scores[ve],label:V[ye.classes[ve]],box:m(Pe,!Ie)}))}de.sort((ye,Pe)=>Pe.score-ye.score),Te!==null&&(de=de.slice(0,Te)),_e.push(de)}return Ee?_e:_e[0]}}class X extends g{constructor(Z){super(Z)}async _call(Z,V,fe={}){const Te=(await p(Z))[0],{pixel_values:Ie}=await this.processor(Te),Ee=`<s_docvqa><s_question>${V}</s_question><s_answer>`,De=this.tokenizer(Ee,{add_special_tokens:!1,padding:!0,truncation:!0}).input_ids,xe=await this.model.generate({inputs:Ie,max_length:this.model.config.decoder.max_position_embeddings,decoder_input_ids:De,...fe}),_e=this.tokenizer.batch_decode(xe)[0].match(/<s_answer>(.*?)<\/s_answer>/);let Le=null;return _e&&_e.length>=2&&(Le=_e[1].trim()),[{answer:Le}]}}class D extends g{DEFAULT_VOCODER_ID="Xenova/speecht5_hifigan";constructor(Z){super(Z),this.vocoder=Z.vocoder??null}async _prepare_speaker_embeddings(Z){if((typeof Z=="string"||Z instanceof URL)&&(Z=new Float32Array(await(await fetch(Z)).arrayBuffer())),Z instanceof Float32Array)Z=new d.Tensor("float32",Z,[Z.length]);else if(!(Z instanceof d.Tensor))throw new Error("Speaker embeddings must be a `Tensor`, `Float32Array`, `string`, or `URL`.");return Z}async _call(Z,{speaker_embeddings:V=null,num_inference_steps:fe,speed:Te}={}){return this.processor?this._call_text_to_spectrogram(Z,{speaker_embeddings:V}):this.model.config.model_type==="supertonic"?this._call_supertonic(Z,{speaker_embeddings:V,num_inference_steps:fe,speed:Te}):this._call_text_to_waveform(Z)}async _call_supertonic(Z,{speaker_embeddings:V,num_inference_steps:fe,speed:Te}){if(!V)throw new Error("Speaker embeddings must be provided for Supertonic models.");V=await this._prepare_speaker_embeddings(V);const{sampling_rate:Ie,style_dim:Ee}=this.model.config;V=V.view(1,-1,Ee);const De=this.tokenizer(Z,{padding:!0,truncation:!0}),{waveform:xe}=await this.model.generate_speech({...De,style:V,num_inference_steps:fe,speed:Te});return new c.RawAudio(xe.data,Ie)}async _call_text_to_waveform(Z){const V=this.tokenizer(Z,{padding:!0,truncation:!0}),{waveform:fe}=await this.model(V),Te=this.model.config.sampling_rate;return new c.RawAudio(fe.data,Te)}async _call_text_to_spectrogram(Z,{speaker_embeddings:V}){this.vocoder||(console.log("No vocoder specified, using default HifiGan vocoder."),this.vocoder=await s.AutoModel.from_pretrained(this.DEFAULT_VOCODER_ID,{dtype:"fp32"}));const{input_ids:fe}=this.tokenizer(Z,{padding:!0,truncation:!0});V=await this._prepare_speaker_embeddings(V),V=V.view(1,-1);const{waveform:Te}=await this.model.generate_speech(fe,V,{vocoder:this.vocoder}),Ie=this.processor.feature_extractor.config.sampling_rate;return new c.RawAudio(Te.data,Ie)}}class z extends g{constructor(Z){super(Z)}async _call(Z){const V=await p(Z),fe=await this.processor(V),Te=await this.model(fe),Ie=[];for(const Ee of Te.reconstruction){const De=Ee.squeeze().clamp_(0,1).mul_(255).round_().to("uint8");Ie.push(u.RawImage.fromTensor(De))}return Ie.length>1?Ie:Ie[0]}}class se extends g{constructor(Z){super(Z)}async _call(Z){const V=await p(Z),fe=await this.processor(V),{predicted_depth:Te}=await this.model(fe),Ie=[];for(let Ee=0;Ee<V.length;++Ee){const De=Te[Ee],[xe,ze]=De.dims.slice(-2),[_e,Le]=V[Ee].size,qe=(await(0,d.interpolate_4d)(De.view(1,1,xe,ze),{size:[Le,_e],mode:"bilinear"})).view(Le,_e),Ne=qe.min().item(),ot=qe.max().item(),Ve=qe.sub(Ne).div_(ot-Ne).mul_(255).to("uint8").unsqueeze(0),de=u.RawImage.fromTensor(Ve);Ie.push({predicted_depth:qe,depth:de})}return Ie.length>1?Ie:Ie[0]}}const me=Object.freeze({"text-classification":{tokenizer:r.AutoTokenizer,pipeline:I,model:s.AutoModelForSequenceClassification,default:{model:"Xenova/distilbert-base-uncased-finetuned-sst-2-english"},type:"text"},"token-classification":{tokenizer:r.AutoTokenizer,pipeline:f,model:s.AutoModelForTokenClassification,default:{model:"Xenova/bert-base-multilingual-cased-ner-hrl"},type:"text"},"question-answering":{tokenizer:r.AutoTokenizer,pipeline:_,model:s.AutoModelForQuestionAnswering,default:{model:"Xenova/distilbert-base-cased-distilled-squad"},type:"text"},"fill-mask":{tokenizer:r.AutoTokenizer,pipeline:T,model:s.AutoModelForMaskedLM,default:{model:"Xenova/bert-base-uncased"},type:"text"},summarization:{tokenizer:r.AutoTokenizer,pipeline:v,model:s.AutoModelForSeq2SeqLM,default:{model:"Xenova/distilbart-cnn-6-6"},type:"text"},translation:{tokenizer:r.AutoTokenizer,pipeline:b,model:s.AutoModelForSeq2SeqLM,default:{model:"Xenova/t5-small"},type:"text"},"text2text-generation":{tokenizer:r.AutoTokenizer,pipeline:M,model:s.AutoModelForSeq2SeqLM,default:{model:"Xenova/flan-t5-small"},type:"text"},"text-generation":{tokenizer:r.AutoTokenizer,pipeline:k,model:s.AutoModelForCausalLM,default:{model:"Xenova/gpt2"},type:"text"},"zero-shot-classification":{tokenizer:r.AutoTokenizer,pipeline:F,model:s.AutoModelForSequenceClassification,default:{model:"Xenova/distilbert-base-uncased-mnli"},type:"text"},"audio-classification":{pipeline:j,model:s.AutoModelForAudioClassification,processor:i.AutoProcessor,default:{model:"Xenova/wav2vec2-base-superb-ks"},type:"audio"},"zero-shot-audio-classification":{tokenizer:r.AutoTokenizer,pipeline:R,model:s.AutoModel,processor:i.AutoProcessor,default:{model:"Xenova/clap-htsat-unfused"},type:"multimodal"},"automatic-speech-recognition":{tokenizer:r.AutoTokenizer,pipeline:K,model:[s.AutoModelForSpeechSeq2Seq,s.AutoModelForCTC],processor:i.AutoProcessor,default:{model:"Xenova/whisper-tiny.en"},type:"multimodal"},"text-to-audio":{tokenizer:r.AutoTokenizer,pipeline:D,model:[s.AutoModelForTextToWaveform,s.AutoModelForTextToSpectrogram],processor:[i.AutoProcessor,null],default:{model:"Xenova/speecht5_tts"},type:"text"},"image-to-text":{tokenizer:r.AutoTokenizer,pipeline:U,model:s.AutoModelForVision2Seq,processor:i.AutoProcessor,default:{model:"Xenova/vit-gpt2-image-captioning"},type:"multimodal"},"image-classification":{pipeline:Y,model:s.AutoModelForImageClassification,processor:i.AutoProcessor,default:{model:"Xenova/vit-base-patch16-224"},type:"multimodal"},"image-segmentation":{pipeline:te,model:[s.AutoModelForImageSegmentation,s.AutoModelForSemanticSegmentation,s.AutoModelForUniversalSegmentation],processor:i.AutoProcessor,default:{model:"Xenova/detr-resnet-50-panoptic"},type:"multimodal"},"background-removal":{pipeline:ne,model:[s.AutoModelForImageSegmentation,s.AutoModelForSemanticSegmentation,s.AutoModelForUniversalSegmentation],processor:i.AutoProcessor,default:{model:"Xenova/modnet"},type:"image"},"zero-shot-image-classification":{tokenizer:r.AutoTokenizer,pipeline:le,model:s.AutoModel,processor:i.AutoProcessor,default:{model:"Xenova/clip-vit-base-patch32"},type:"multimodal"},"object-detection":{pipeline:N,model:s.AutoModelForObjectDetection,processor:i.AutoProcessor,default:{model:"Xenova/detr-resnet-50"},type:"multimodal"},"zero-shot-object-detection":{tokenizer:r.AutoTokenizer,pipeline:oe,model:s.AutoModelForZeroShotObjectDetection,processor:i.AutoProcessor,default:{model:"Xenova/owlvit-base-patch32"},type:"multimodal"},"document-question-answering":{tokenizer:r.AutoTokenizer,pipeline:X,model:s.AutoModelForDocumentQuestionAnswering,processor:i.AutoProcessor,default:{model:"Xenova/donut-base-finetuned-docvqa"},type:"multimodal"},"image-to-image":{pipeline:z,model:s.AutoModelForImageToImage,processor:i.AutoProcessor,default:{model:"Xenova/swin2SR-classical-sr-x2-64"},type:"image"},"depth-estimation":{pipeline:se,model:s.AutoModelForDepthEstimation,processor:i.AutoProcessor,default:{model:"Xenova/dpt-large"},type:"image"},"feature-extraction":{tokenizer:r.AutoTokenizer,pipeline:L,model:s.AutoModel,default:{model:"Xenova/all-MiniLM-L6-v2"},type:"text"},"image-feature-extraction":{processor:i.AutoProcessor,pipeline:G,model:[s.AutoModelForImageFeatureExtraction,s.AutoModel],default:{model:"Xenova/vit-base-patch16-224-in21k"},type:"image"}}),$e=Object.freeze({"sentiment-analysis":"text-classification",ner:"token-classification",asr:"automatic-speech-recognition","text-to-speech":"text-to-audio",embeddings:"feature-extraction"});async function ke(Ce,Z=null,{progress_callback:V=null,config:fe=null,cache_dir:Te=null,local_files_only:Ie=!1,revision:Ee="main",device:De=null,dtype:xe=null,subfolder:ze="onnx",use_external_data_format:_e=null,model_file_name:Le=null,session_options:qe={}}={}){Ce=$e[Ce]??Ce;const Ne=me[Ce.split("_",1)[0]];if(!Ne)throw Error(`Unsupported pipeline: ${Ce}. Must be one of [${Object.keys(me)}]`);Z||(Z=Ne.default.model,console.log(`No model specified. Using default model: "${Z}".`));const ot={progress_callback:V,config:fe,cache_dir:Te,local_files_only:Ie,revision:Ee,device:De,dtype:xe,subfolder:ze,use_external_data_format:_e,model_file_name:Le,session_options:qe},Ve=new Map([["tokenizer",Ne.tokenizer],["model",Ne.model],["processor",Ne.processor]]),de=await Be(Ve,Z,ot);de.task=Ce,(0,a.dispatchCallback)(V,{status:"ready",task:Ce,model:Z});const ye=Ne.pipeline;return new ye(de)}async function Be(Ce,Z,V){const fe=Object.create(null),Te=[];for(const[Ie,Ee]of Ce.entries()){if(!Ee)continue;let De;Array.isArray(Ee)?De=new Promise(async(xe,ze)=>{let _e;for(const Le of Ee){if(Le===null){xe(null);return}try{xe(await Le.from_pretrained(Z,V));return}catch(qe){if(qe.message?.includes("Unsupported model type"))_e=qe;else if(qe.message?.includes("Could not locate file"))_e=qe;else{ze(qe);return}}}ze(_e)}):De=Ee.from_pretrained(Z,V),fe[Ie]=De,Te.push(De)}await Promise.all(Te);for(const[Ie,Ee]of Object.entries(fe))fe[Ie]=await Ee;return fe}}),"./src/tokenizers.js":((e,t,n)=>{n.r(t),n.d(t,{AlbertTokenizer:()=>hs,AutoTokenizer:()=>Zi,BartTokenizer:()=>Ze,BertTokenizer:()=>rn,BlenderbotSmallTokenizer:()=>Ke,BlenderbotTokenizer:()=>Ue,BloomTokenizer:()=>Rt,CLIPTokenizer:()=>xs,CamembertTokenizer:()=>re,CodeGenTokenizer:()=>ys,CodeLlamaTokenizer:()=>bn,CohereTokenizer:()=>ws,ConvBertTokenizer:()=>ee,DebertaTokenizer:()=>It,DebertaV2Tokenizer:()=>fs,DistilBertTokenizer:()=>J,ElectraTokenizer:()=>Se,EsmTokenizer:()=>nr,FalconTokenizer:()=>tr,GPT2Tokenizer:()=>gt,GPTNeoXTokenizer:()=>_s,GemmaTokenizer:()=>Dr,Grok1Tokenizer:()=>On,HerbertTokenizer:()=>S,LlamaTokenizer:()=>Cn,M2M100Tokenizer:()=>$r,MBart50Tokenizer:()=>et,MBartTokenizer:()=>At,MPNetTokenizer:()=>gs,MarianTokenizer:()=>Fr,MgpstrTokenizer:()=>Fs,MobileBertTokenizer:()=>ms,NllbTokenizer:()=>yr,NougatTokenizer:()=>tn,PreTrainedTokenizer:()=>vt,Qwen2Tokenizer:()=>rr,RoFormerTokenizer:()=>W,RobertaTokenizer:()=>Lt,SiglipTokenizer:()=>bs,SpeechT5Tokenizer:()=>ut,SqueezeBertTokenizer:()=>_r,T5Tokenizer:()=>it,TokenizerModel:()=>G,VitsTokenizer:()=>$s,Wav2Vec2CTCTokenizer:()=>or,WhisperTokenizer:()=>Ds,XLMRobertaTokenizer:()=>Bn,XLMTokenizer:()=>pe,is_chinese_char:()=>T});var r=n("./src/utils/generic.js"),s=n("./src/utils/core.js"),i=n("./src/utils/hub.js"),o=n("./src/utils/maths.js"),a=n("./src/utils/tensor.js"),l=n("./src/utils/data-structures.js"),c=n("./node_modules/@huggingface/jinja/dist/index.js"),d=n("./src/models/whisper/common_whisper.js");async function u(he,$){const Q=await Promise.all([(0,i.getModelJSON)(he,"tokenizer.json",!0,$),(0,i.getModelJSON)(he,"tokenizer_config.json",!0,$)]);return $.legacy!==null&&(Q[1].legacy=$.legacy),Q}function p(he,$){const Q=[];let ie=0;for(const ae of he.matchAll($)){const we=ae[0];ie<ae.index&&Q.push(he.slice(ie,ae.index)),we.length>0&&Q.push(we),ie=ae.index+we.length}return ie<he.length&&Q.push(he.slice(ie)),Q}function h(he,$=!0){if(he.Regex!==void 0){let Q=he.Regex.replace(/\\([#&~])/g,"$1");for(const[ie,ae]of F)Q=Q.replaceAll(ie,ae);return new RegExp(Q,"gu")}else if(he.String!==void 0){const Q=(0,s.escapeRegExp)(he.String);return new RegExp($?Q:`(${Q})`,"gu")}else return console.warn("Unknown pattern type:",he),null}function m(he){return new Map(Object.entries(he))}function g(he){const $=he.dims;switch($.length){case 1:return he.tolist();case 2:if($[0]!==1)throw new Error("Unable to decode tensor with `batch size !== 1`. Use `tokenizer.batch_decode(...)` for batched inputs.");return he.tolist()[0];default:throw new Error(`Expected tensor to have 1-2 dimensions, got ${$.length}.`)}}function I(he){return he.replace(/ \./g,".").replace(/ \?/g,"?").replace(/ \!/g,"!").replace(/ ,/g,",").replace(/ \' /g,"'").replace(/ n\'t/g,"n't").replace(/ \'m/g,"'m").replace(/ \'s/g,"'s").replace(/ \'ve/g,"'ve").replace(/ \'re/g,"'re")}function f(he){return he.replace(new RegExp("\\p{M}","gu"),"")}function _(he){return f(he.toLowerCase())}function T(he){return he>=19968&&he<=40959||he>=13312&&he<=19903||he>=131072&&he<=173791||he>=173824&&he<=177983||he>=177984&&he<=178207||he>=178208&&he<=183983||he>=63744&&he<=64255||he>=194560&&he<=195103}function M(he,$,Q){const ie=[];let ae=0;for(;ae<he.length;){if(ie.push(he[ae]),($.get(he[ae])??Q)!==Q){++ae;continue}for(;++ae<he.length&&($.get(he[ae])??Q)===Q;)$.get(ie.at(-1))!==Q&&(ie[ie.length-1]+=he[ae])}return ie}function v(he){return he.match(/\S+/g)||[]}const b="\\p{P}\\u0021-\\u002F\\u003A-\\u0040\\u005B-\\u0060\\u007B-\\u007E",A=new RegExp(`^[${b}]+$`,"gu"),k=".,!?…。，、।۔،",F=new Map([["(?i:'s|'t|'re|'ve|'m|'ll|'d)","(?:'([sS]|[tT]|[rR][eE]|[vV][eE]|[mM]|[lL][lL]|[dD]))"],["(?i:[sdmt]|ll|ve|re)","(?:[sS]|[dD]|[mM]|[tT]|[lL][lL]|[vV][eE]|[rR][eE])"],["[^\\r\\n\\p{L}\\p{N}]?+","[^\\r\\n\\p{L}\\p{N}]?"],["[^\\s\\p{L}\\p{N}]++","[^\\s\\p{L}\\p{N}]+"],[` ?[^(\\s|[${k}])]+`,` ?[^\\s${k}]+`]]);class L{constructor($){this.content=$.content,this.id=$.id,this.single_word=$.single_word??!1,this.lstrip=$.lstrip??!1,this.rstrip=$.rstrip??!1,this.special=$.special??!1,this.normalized=$.normalized??null}}class G extends r.Callable{constructor($){super(),this.config=$,this.vocab=[],this.tokens_to_ids=new Map,this.unk_token_id=void 0,this.unk_token=void 0,this.end_of_word_suffix=void 0,this.fuse_unk=this.config.fuse_unk??!1}static fromConfig($,...Q){switch($.type){case"WordPiece":return new j($);case"Unigram":return new R($,...Q);case"BPE":return new Y($);default:if($.vocab)return Array.isArray($.vocab)?new R($,...Q):Object.hasOwn($,"continuing_subword_prefix")&&Object.hasOwn($,"unk_token")?Object.hasOwn($,"merges")?new Y($):new j($):new te($,...Q);throw new Error(`Unknown TokenizerModel type: ${$.type}`)}}_call($){return $=this.encode($),this.fuse_unk&&($=M($,this.tokens_to_ids,this.unk_token_id)),$}encode($){throw Error("encode should be implemented in subclass.")}convert_tokens_to_ids($){return $.map(Q=>this.tokens_to_ids.get(Q)??this.unk_token_id)}convert_ids_to_tokens($){return $.map(Q=>this.vocab[Q]??this.unk_token)}}class j extends G{constructor($){super($),this.tokens_to_ids=m($.vocab),this.unk_token_id=this.tokens_to_ids.get($.unk_token),this.unk_token=$.unk_token,this.max_input_chars_per_word=$.max_input_chars_per_word??100,this.vocab=new Array(this.tokens_to_ids.size);for(const[Q,ie]of this.tokens_to_ids)this.vocab[ie]=Q}encode($){const Q=[];for(const ie of $){const ae=[...ie];if(ae.length>this.max_input_chars_per_word){Q.push(this.unk_token);continue}let we=!1,je=0;const nt=[];for(;je<ae.length;){let rt=ae.length,st=null;for(;je<rt;){let Je=ae.slice(je,rt).join("");if(je>0&&(Je=this.config.continuing_subword_prefix+Je),this.tokens_to_ids.has(Je)){st=Je;break}--rt}if(st===null){we=!0;break}nt.push(st),je=rt}we?Q.push(this.unk_token):Q.push(...nt)}return Q}}class R extends G{constructor($,Q){super($);const ie=$.vocab.length;this.vocab=new Array(ie),this.scores=new Array(ie);for(let ae=0;ae<ie;++ae)[this.vocab[ae],this.scores[ae]]=$.vocab[ae];this.unk_token_id=$.unk_id,this.unk_token=this.vocab[$.unk_id],this.tokens_to_ids=new Map(this.vocab.map((ae,we)=>[ae,we])),this.bos_token=" ",this.bos_token_id=this.tokens_to_ids.get(this.bos_token),this.eos_token=Q.eos_token,this.eos_token_id=this.tokens_to_ids.get(this.eos_token),this.unk_token=this.vocab[this.unk_token_id],this.minScore=(0,o.min)(this.scores)[0],this.unk_score=this.minScore-10,this.scores[this.unk_token_id]=this.unk_score,this.trie=new l.CharTrie,this.trie.extend(this.vocab),this.fuse_unk=!0}populateNodes($){const Q=$.chars,ie=1;let ae=0;for(;ae<Q.length;){let we=!1;const je=Q.slice(ae).join(""),nt=this.trie.commonPrefixSearch(je);for(const rt of nt){const st=this.tokens_to_ids.get(rt),Je=this.scores[st],jt=(0,s.len)(rt);$.insert(ae,jt,Je,st),!we&&jt===ie&&(we=!0)}we||$.insert(ae,ie,this.unk_score,this.unk_token_id),ae+=ie}}tokenize($){const Q=new l.TokenLattice($,this.bos_token_id,this.eos_token_id);return this.populateNodes(Q),Q.tokens()}encode($){const Q=[];for(const ie of $){const ae=this.tokenize(ie);Q.push(...ae)}return Q}}const K=(()=>{const he=[...Array.from({length:94},(ae,we)=>we+33),...Array.from({length:12},(ae,we)=>we+161),...Array.from({length:82},(ae,we)=>we+174)],$=he.slice();let Q=0;for(let ae=0;ae<256;++ae)he.includes(ae)||(he.push(ae),$.push(256+Q),Q+=1);const ie=$.map(ae=>String.fromCharCode(ae));return Object.fromEntries(he.map((ae,we)=>[ae,ie[we]]))})(),U=(0,s.reverseDictionary)(K);class Y extends G{constructor($){super($),this.tokens_to_ids=m($.vocab),this.unk_token_id=this.tokens_to_ids.get($.unk_token),this.unk_token=$.unk_token,this.vocab=new Array(this.tokens_to_ids.size);for(const[ie,ae]of this.tokens_to_ids)this.vocab[ae]=ie;const Q=Array.isArray($.merges[0]);this.merges=Q?$.merges:$.merges.map(ie=>ie.split(" ",2)),this.bpe_ranks=new Map(this.merges.map((ie,ae)=>[JSON.stringify(ie),ae])),this.end_of_word_suffix=$.end_of_word_suffix,this.continuing_subword_suffix=$.continuing_subword_suffix??null,this.byte_fallback=this.config.byte_fallback??!1,this.byte_fallback&&(this.text_encoder=new TextEncoder),this.ignore_merges=this.config.ignore_merges??!1,this.max_length_to_cache=256,this.cache_capacity=1e4,this.cache=new l.LRUCache(this.cache_capacity)}clear_cache(){this.cache.clear()}bpe($){if($.length===0)return[];const Q=this.cache.get($);if(Q!==void 0)return Q;const ie=Array.from($);this.end_of_word_suffix&&(ie[ie.length-1]+=this.end_of_word_suffix);let ae=[];if(ie.length>1){const we=new l.PriorityQueue((rt,st)=>rt.score<st.score);let je={token:ie[0],bias:0,prev:null,next:null},nt=je;for(let rt=1;rt<ie.length;++rt){const st={bias:rt/ie.length,token:ie[rt],prev:nt,next:null};nt.next=st,this._add_node(we,nt),nt=st}for(;!we.isEmpty();){const rt=we.pop();if(rt.deleted||!rt.next||rt.next.deleted)continue;if(rt.deleted=!0,rt.next.deleted=!0,rt.prev){const Je={...rt.prev};rt.prev.deleted=!0,rt.prev=Je,Je.prev?Je.prev.next=Je:je=Je}const st={token:rt.token+rt.next.token,bias:rt.bias,prev:rt.prev,next:rt.next.next};st.prev?(st.prev.next=st,this._add_node(we,st.prev)):je=st,st.next&&(st.next.prev=st,this._add_node(we,st))}for(let rt=je;rt!==null;rt=rt.next)ae.push(rt.token)}else ae=ie;if(this.continuing_subword_suffix)for(let we=0;we<ae.length-1;++we)ae[we]+=this.continuing_subword_suffix;return $.length<this.max_length_to_cache&&this.cache.put($,ae),ae}_add_node($,Q){const ie=this.bpe_ranks.get(JSON.stringify([Q.token,Q.next.token]));ie!==void 0&&(Q.score=ie+Q.bias,$.push(Q))}encode($){const Q=[];for(const ie of $){if(this.ignore_merges&&this.tokens_to_ids.has(ie)){Q.push(ie);continue}const ae=this.bpe(ie);for(const we of ae)if(this.tokens_to_ids.has(we))Q.push(we);else if(this.byte_fallback){const je=Array.from(this.text_encoder.encode(we)).map(nt=>`<0x${nt.toString(16).toUpperCase().padStart(2,"0")}>`);je.every(nt=>this.tokens_to_ids.has(nt))?Q.push(...je):Q.push(this.unk_token)}else Q.push(this.unk_token)}return Q}}class te extends G{constructor($,Q){super($),this.tokens_to_ids=m(Q.target_lang?$.vocab[Q.target_lang]:$.vocab),this.bos_token=Q.bos_token,this.bos_token_id=this.tokens_to_ids.get(this.bos_token),this.eos_token=Q.eos_token,this.eos_token_id=this.tokens_to_ids.get(this.eos_token),this.pad_token=Q.pad_token,this.pad_token_id=this.tokens_to_ids.get(this.pad_token),this.unk_token=Q.unk_token,this.unk_token_id=this.tokens_to_ids.get(this.unk_token),this.vocab=new Array(this.tokens_to_ids.size);for(const[ie,ae]of this.tokens_to_ids)this.vocab[ae]=ie}encode($){return $}}class ne extends r.Callable{constructor($){super(),this.config=$}static fromConfig($){if($===null)return null;switch($.type){case"BertNormalizer":return new Ce($);case"Precompiled":return new kt($);case"Sequence":return new Be($);case"Replace":return new le($);case"NFC":return new oe($);case"NFD":return new X($);case"NFKC":return new D($);case"NFKD":return new z($);case"Strip":return new se($);case"StripAccents":return new me($);case"Lowercase":return new $e($);case"Prepend":return new ke($);default:throw new Error(`Unknown Normalizer type: ${$.type}`)}}normalize($){throw Error("normalize should be implemented in subclass.")}_call($){return this.normalize($)}}class le extends ne{normalize($){const Q=h(this.config.pattern);return Q===null?$:$.replaceAll(Q,this.config.content)}}class N extends ne{form=void 0;normalize($){return $=$.normalize(this.form),$}}class oe extends N{form="NFC"}class X extends N{form="NFD"}class D extends N{form="NFKC"}class z extends N{form="NFKD"}class se extends ne{normalize($){return this.config.strip_left&&this.config.strip_right?$=$.trim():(this.config.strip_left&&($=$.trimStart()),this.config.strip_right&&($=$.trimEnd())),$}}class me extends ne{normalize($){return $=f($),$}}class $e extends ne{normalize($){return $=$.toLowerCase(),$}}class ke extends ne{normalize($){return $=this.config.prepend+$,$}}class Be extends ne{constructor($){super($),this.normalizers=$.normalizers.map(Q=>ne.fromConfig(Q))}normalize($){return this.normalizers.reduce((Q,ie)=>ie.normalize(Q),$)}}class Ce extends ne{_tokenize_chinese_chars($){const Q=[];for(let ie=0;ie<$.length;++ie){const ae=$[ie],we=ae.charCodeAt(0);T(we)?(Q.push(" "),Q.push(ae),Q.push(" ")):Q.push(ae)}return Q.join("")}stripAccents($){return $.normalize("NFD").replace(new RegExp("\\p{Mn}","gu"),"")}_is_control($){switch($){case"	":case`
`:case"\r":return!1;default:return new RegExp("^\\p{Cc}|\\p{Cf}|\\p{Co}|\\p{Cs}$","u").test($)}}_clean_text($){const Q=[];for(const ie of $){const ae=ie.charCodeAt(0);ae===0||ae===65533||this._is_control(ie)||(/^\s$/.test(ie)?Q.push(" "):Q.push(ie))}return Q.join("")}normalize($){return this.config.clean_text&&($=this._clean_text($)),this.config.handle_chinese_chars&&($=this._tokenize_chinese_chars($)),this.config.lowercase?($=$.toLowerCase(),this.config.strip_accents!==!1&&($=this.stripAccents($))):this.config.strip_accents&&($=this.stripAccents($)),$}}class Z extends r.Callable{static fromConfig($){if($===null)return null;switch($.type){case"BertPreTokenizer":return new V($);case"Sequence":return new Pn($);case"Whitespace":return new Zn($);case"WhitespaceSplit":return new Hs($);case"Metaspace":return new on($);case"ByteLevel":return new fe($);case"Split":return new Te($);case"Punctuation":return new Ie($);case"Digits":return new Ee($);case"Replace":return new Ks($);case"FixedLength":return new xn($);default:throw new Error(`Unknown PreTokenizer type: ${$.type}`)}}pre_tokenize_text($,Q){throw Error("pre_tokenize_text should be implemented in subclass.")}pre_tokenize($,Q){return(Array.isArray($)?$.map(ie=>this.pre_tokenize_text(ie,Q)):this.pre_tokenize_text($,Q)).flat()}_call($,Q){return this.pre_tokenize($,Q)}}class V extends Z{constructor($){super(),this.pattern=new RegExp(`[^\\s${b}]+|[${b}]`,"gu")}pre_tokenize_text($,Q){return $.trim().match(this.pattern)||[]}}class fe extends Z{constructor($){super(),this.config=$,this.add_prefix_space=this.config.add_prefix_space,this.trim_offsets=this.config.trim_offsets,this.use_regex=this.config.use_regex??!0,this.pattern=new RegExp("'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+","gu"),this.byte_encoder=K,this.text_encoder=new TextEncoder}pre_tokenize_text($,Q){return this.add_prefix_space&&!$.startsWith(" ")&&($=" "+$),(this.use_regex?$.match(this.pattern)||[]:[$]).map(ae=>Array.from(this.text_encoder.encode(ae),we=>this.byte_encoder[we]).join(""))}}class Te extends Z{constructor($){super(),this.config=$,this.pattern=h(this.config.pattern,this.config.invert)}pre_tokenize_text($,Q){return this.pattern===null?[]:this.config.invert?$.match(this.pattern)||[]:this.config.behavior?.toLowerCase()==="removed"?$.split(this.pattern).filter(ie=>ie):p($,this.pattern)}}class Ie extends Z{constructor($){super(),this.config=$,this.pattern=new RegExp(`[^${b}]+|[${b}]+`,"gu")}pre_tokenize_text($,Q){return $.match(this.pattern)||[]}}class Ee extends Z{constructor($){super(),this.config=$;const Q=`[^\\d]+|\\d${this.config.individual_digits?"":"+"}`;this.pattern=new RegExp(Q,"gu")}pre_tokenize_text($,Q){return $.match(this.pattern)||[]}}class De extends r.Callable{constructor($){super(),this.config=$}static fromConfig($){if($===null)return null;switch($.type){case"TemplateProcessing":return new _e($);case"ByteLevel":return new Le($);case"RobertaProcessing":return new ze($);case"BertProcessing":return new xe($);case"Sequence":return new qe($);default:throw new Error(`Unknown PostProcessor type: ${$.type}`)}}post_process($,...Q){throw Error("post_process should be implemented in subclass.")}_call($,...Q){return this.post_process($,...Q)}}class xe extends De{constructor($){super($),this.cls=$.cls[0],this.sep=$.sep[0]}post_process($,Q=null,{add_special_tokens:ie=!0}={}){ie&&($=(0,s.mergeArrays)([this.cls],$,[this.sep]));let ae=new Array($.length).fill(0);if(Q!==null){const we=ie&&this instanceof ze?[this.sep]:[],je=ie?[this.sep]:[];$=(0,s.mergeArrays)($,we,Q,je),ae=(0,s.mergeArrays)(ae,new Array(Q.length+we.length+je.length).fill(1))}return{tokens:$,token_type_ids:ae}}}class ze extends xe{}class _e extends De{constructor($){super($),this.single=$.single,this.pair=$.pair}post_process($,Q=null,{add_special_tokens:ie=!0}={}){const ae=Q===null?this.single:this.pair;let we=[],je=[];for(const nt of ae)"SpecialToken"in nt?ie&&(we.push(nt.SpecialToken.id),je.push(nt.SpecialToken.type_id)):"Sequence"in nt&&(nt.Sequence.id==="A"?(we=(0,s.mergeArrays)(we,$),je=(0,s.mergeArrays)(je,new Array($.length).fill(nt.Sequence.type_id))):nt.Sequence.id==="B"&&(we=(0,s.mergeArrays)(we,Q),je=(0,s.mergeArrays)(je,new Array(Q.length).fill(nt.Sequence.type_id))));return{tokens:we,token_type_ids:je}}}class Le extends De{post_process($,Q=null){return Q&&($=(0,s.mergeArrays)($,Q)),{tokens:$}}}class qe extends De{constructor($){super($),this.processors=$.processors.map(Q=>De.fromConfig(Q))}post_process($,Q=null,ie={}){let ae;for(const we of this.processors)if(we instanceof Le)$=we.post_process($).tokens,Q&&(Q=we.post_process(Q).tokens);else{const je=we.post_process($,Q,ie);$=je.tokens,ae=je.token_type_ids}return{tokens:$,token_type_ids:ae}}}class Ne extends r.Callable{constructor($){super(),this.config=$,this.added_tokens=[],this.end_of_word_suffix=null,this.trim_offsets=$.trim_offsets}static fromConfig($){if($===null)return null;switch($.type){case"WordPiece":return new Pe($);case"Metaspace":return new Tn($);case"ByteLevel":return new ve($);case"Replace":return new ot($);case"ByteFallback":return new Ve($);case"Fuse":return new de($);case"Strip":return new ye($);case"Sequence":return new ct($);case"CTC":return new Qe($);case"BPEDecoder":return new zt($);default:throw new Error(`Unknown Decoder type: ${$.type}`)}}_call($){return this.decode($)}decode($){return this.decode_chain($).join("")}decode_chain($){throw Error("`decode_chain` should be implemented in subclass.")}}class ot extends Ne{decode_chain($){const Q=h(this.config.pattern);return Q===null?$:$.map(ie=>ie.replaceAll(Q,this.config.content))}}class Ve extends Ne{constructor($){super($),this.text_decoder=new TextDecoder}decode_chain($){const Q=[];let ie=[];for(const ae of $){let we=null;if(ae.length===6&&ae.startsWith("<0x")&&ae.endsWith(">")){const je=parseInt(ae.slice(3,5),16);isNaN(je)||(we=je)}if(we!==null)ie.push(we);else{if(ie.length>0){const je=this.text_decoder.decode(Uint8Array.from(ie));Q.push(je),ie=[]}Q.push(ae)}}if(ie.length>0){const ae=this.text_decoder.decode(Uint8Array.from(ie));Q.push(ae),ie=[]}return Q}}class de extends Ne{decode_chain($){return[$.join("")]}}class ye extends Ne{constructor($){super($),this.content=this.config.content,this.start=this.config.start,this.stop=this.config.stop}decode_chain($){return $.map(Q=>{let ie=0;for(let we=0;we<this.start&&Q[we]===this.content;++we){ie=we+1;continue}let ae=Q.length;for(let we=0;we<this.stop;++we){const je=Q.length-we-1;if(Q[je]===this.content){ae=je;continue}else break}return Q.slice(ie,ae)})}}class Pe extends Ne{constructor($){super($),this.cleanup=$.cleanup}decode_chain($){return $.map((Q,ie)=>(ie!==0&&(Q.startsWith(this.config.prefix)?Q=Q.replace(this.config.prefix,""):Q=" "+Q),this.cleanup&&(Q=I(Q)),Q))}}class ve extends Ne{constructor($){super($),this.byte_decoder=U,this.text_decoder=new TextDecoder("utf-8",{fatal:!1,ignoreBOM:!0}),this.end_of_word_suffix=null}convert_tokens_to_string($){const Q=$.join(""),ie=new Uint8Array([...Q].map(we=>this.byte_decoder[we]));return this.text_decoder.decode(ie)}decode_chain($){const Q=[];let ie=[];for(const ae of $)this.added_tokens.find(we=>we.content===ae)!==void 0?(ie.length>0&&(Q.push(this.convert_tokens_to_string(ie)),ie=[]),Q.push(ae)):ie.push(ae);return ie.length>0&&Q.push(this.convert_tokens_to_string(ie)),Q}}class Qe extends Ne{constructor($){super($),this.pad_token=this.config.pad_token,this.word_delimiter_token=this.config.word_delimiter_token,this.cleanup=this.config.cleanup}convert_tokens_to_string($){if($.length===0)return"";const Q=[$[0]];for(let we=1;we<$.length;++we)$[we]!==Q.at(-1)&&Q.push($[we]);let ae=Q.filter(we=>we!==this.pad_token).join("");return this.cleanup&&(ae=I(ae).replaceAll(this.word_delimiter_token," ").trim()),ae}decode_chain($){return[this.convert_tokens_to_string($)]}}class ct extends Ne{constructor($){super($),this.decoders=$.decoders.map(Q=>Ne.fromConfig(Q))}decode_chain($){return this.decoders.reduce((Q,ie)=>ie.decode_chain(Q),$)}}class zt extends Ne{constructor($){super($),this.suffix=this.config.suffix}decode_chain($){return $.map((Q,ie)=>Q.replaceAll(this.suffix,ie===$.length-1?"":" "))}}class wt extends Ne{decode_chain($){let Q="";for(let ie=1;ie<$.length;ie+=2)Q+=$[ie];return[Q]}}class on extends Z{constructor($){super(),this.replacement=$.replacement,this.strRep=$.str_rep||this.replacement,this.prepend_scheme=$.prepend_scheme??"always"}pre_tokenize_text($,{section_index:Q=void 0}={}){let ie=$.replaceAll(" ",this.strRep);return!ie.startsWith(this.replacement)&&(this.prepend_scheme==="always"||this.prepend_scheme==="first"&&Q===0)&&(ie=this.strRep+ie),[ie]}}class Tn extends Ne{constructor($){super($),this.replacement=$.replacement}decode_chain($){const Q=[];for(let ie=0;ie<$.length;++ie){let ae=$[ie].replaceAll(this.replacement," ");ie==0&&ae.startsWith(" ")&&(ae=ae.substring(1)),Q.push(ae)}return Q}}class kt extends ne{constructor($){super($),this.charsmap=$.precompiled_charsmap}normalize($){return $=$.replace(/[\u0001-\u0008\u000B\u000E-\u001F\u007F\u008F\u009F]/gm,""),$=$.replace(/[\u0009\u000A\u000C\u000D\u00A0\u1680\u2000-\u200F\u2028\u2029\u202F\u205F\u2581\u3000\uFEFF\uFFFD]/gm," "),$.includes("～")?$=$.split("～").map(ie=>ie.normalize("NFKC")).join("～"):$=$.normalize("NFKC"),$}}class Pn extends Z{constructor($){super(),this.tokenizers=$.pretokenizers.map(Q=>Z.fromConfig(Q))}pre_tokenize_text($,Q){return this.tokenizers.reduce((ie,ae)=>ae.pre_tokenize(ie,Q),[$])}}class Zn extends Z{constructor($){super()}pre_tokenize_text($,Q){return $.match(/\w+|[^\w\s]+/g)||[]}}class Hs extends Z{constructor($){super()}pre_tokenize_text($,Q){return v($)}}class Ks extends Z{constructor($){super(),this.config=$,this.pattern=h(this.config.pattern),this.content=this.config.content}pre_tokenize_text($,Q){return this.pattern===null?[$]:[$.replaceAll(this.pattern,this.config.content)]}}class xn extends Z{constructor($){super(),this._length=$.length}pre_tokenize_text($,Q){const ie=[];for(let ae=0;ae<$.length;ae+=this._length)ie.push($.slice(ae,ae+this._length));return ie}}const Kn=["bos_token","eos_token","unk_token","sep_token","pad_token","cls_token","mask_token"];function Ar(he,$,Q,ie){for(const ae of Object.keys(he)){const we=$-he[ae].length,je=Q(ae),nt=new Array(we).fill(je);he[ae]=ie==="right"?(0,s.mergeArrays)(he[ae],nt):(0,s.mergeArrays)(nt,he[ae])}}function Ls(he,$){for(const Q of Object.keys(he))he[Q].length=$}class vt extends r.Callable{return_token_type_ids=!1;padding_side="right";constructor($,Q){super(),this.config=Q,this.normalizer=ne.fromConfig($.normalizer),this.pre_tokenizer=Z.fromConfig($.pre_tokenizer),this.model=G.fromConfig($.model,Q),this.post_processor=De.fromConfig($.post_processor),this.decoder=Ne.fromConfig($.decoder),this.special_tokens=[],this.all_special_ids=[],this.added_tokens=[];for(const ie of $.added_tokens){const ae=new L(ie);this.added_tokens.push(ae),this.model.tokens_to_ids.set(ae.content,ae.id),this.model.vocab[ae.id]=ae.content,ae.special&&(this.special_tokens.push(ae.content),this.all_special_ids.push(ae.id))}if(this.additional_special_tokens=Q.additional_special_tokens??[],this.special_tokens.push(...this.additional_special_tokens),this.special_tokens=[...new Set(this.special_tokens)],this.decoder&&(this.decoder.added_tokens=this.added_tokens,this.decoder.end_of_word_suffix=this.model.end_of_word_suffix),this.added_tokens_splitter=new l.DictionarySplitter(this.added_tokens.map(ie=>ie.content)),this.added_tokens_map=new Map(this.added_tokens.map(ie=>[ie.content,ie])),this.mask_token=this.getToken("mask_token"),this.mask_token_id=this.model.tokens_to_ids.get(this.mask_token),this.pad_token=this.getToken("pad_token","eos_token"),this.pad_token_id=this.model.tokens_to_ids.get(this.pad_token),this.sep_token=this.getToken("sep_token"),this.sep_token_id=this.model.tokens_to_ids.get(this.sep_token),this.unk_token=this.getToken("unk_token"),this.unk_token_id=this.model.tokens_to_ids.get(this.unk_token),this.bos_token=this.getToken("bos_token"),this.bos_token_id=this.model.tokens_to_ids.get(this.bos_token),this.eos_token=this.getToken("eos_token"),this.eos_token_id=this.model.tokens_to_ids.get(this.eos_token),this.model_max_length=Q.model_max_length,this.remove_space=Q.remove_space,this.clean_up_tokenization_spaces=Q.clean_up_tokenization_spaces??!0,this.do_lowercase_and_remove_accent=Q.do_lowercase_and_remove_accent??!1,Q.padding_side&&(this.padding_side=Q.padding_side),this.add_bos_token=Q.add_bos_token,this.add_eos_token=Q.add_eos_token,this.legacy=!1,this.chat_template=Q.chat_template??null,Array.isArray(this.chat_template)){const ie=Object.create(null);for(const{name:ae,template:we}of this.chat_template){if(typeof ae!="string"||typeof we!="string")throw new Error('Chat template must be a list of objects with "name" and "template" properties');ie[ae]=we}this.chat_template=ie}this._compiled_template_cache=new Map}getToken(...$){for(const Q of $){const ie=this.config[Q];if(ie)if(typeof ie=="object"){if(ie.__type==="AddedToken")return ie.content;throw Error(`Unknown token: ${ie}`)}else return ie}return null}static async from_pretrained($,{progress_callback:Q=null,config:ie=null,cache_dir:ae=null,local_files_only:we=!1,revision:je="main",legacy:nt=null}={}){const rt=await u($,{progress_callback:Q,config:ie,cache_dir:ae,local_files_only:we,revision:je,legacy:nt});return new this(...rt)}_call($,{text_pair:Q=null,add_special_tokens:ie=!0,padding:ae=!1,truncation:we=null,max_length:je=null,return_tensor:nt=!0,return_token_type_ids:rt=null}={}){const st=Array.isArray($);let Je;if(st){if($.length===0)throw Error("text array must be non-empty");if(Q!==null){if(Array.isArray(Q)){if($.length!==Q.length)throw Error("text and text_pair must have the same length")}else throw Error("text_pair must also be an array");Je=$.map((Dt,fn)=>this._encode_plus(Dt,{text_pair:Q[fn],add_special_tokens:ie,return_token_type_ids:rt}))}else Je=$.map(Dt=>this._encode_plus(Dt,{add_special_tokens:ie,return_token_type_ids:rt}))}else{if($==null)throw Error("text may not be null or undefined");if(Array.isArray(Q))throw Error("When specifying `text_pair`, since `text` is a string, `text_pair` must also be a string (i.e., not an array).");Je=[this._encode_plus($,{text_pair:Q,add_special_tokens:ie,return_token_type_ids:rt})]}if(je===null?je=this.model_max_length:we===null&&(ae===!0?(console.warn("`max_length` is ignored when `padding: true` and there is no truncation strategy. To pad to max length, use `padding: 'max_length'`."),je=this.model_max_length):ae===!1&&(console.warn("Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation: true` to explicitly truncate examples to max length."),we=!0)),ae===!0&&(je=Math.min((0,o.max)(Je.map(Dt=>Dt.input_ids.length))[0],je??1/0)),je=Math.min(je,this.model_max_length??1/0),ae||we)for(let Dt=0;Dt<Je.length;++Dt)Je[Dt].input_ids.length!==je&&(Je[Dt].input_ids.length>je?we&&Ls(Je[Dt],je):ae&&Ar(Je[Dt],je,fn=>fn==="input_ids"?this.pad_token_id:0,this.padding_side));const jt={};if(nt){if(!(ae&&we)&&Je.some(fn=>{for(const jn of Object.keys(fn))if(fn[jn].length!==Je[0][jn]?.length)return!0;return!1}))throw Error("Unable to create tensor, you should probably activate truncation and/or padding with 'padding=true' and 'truncation=true' to have batched tensors with the same length.");const Dt=[Je.length,Je[0].input_ids.length];for(const fn of Object.keys(Je[0]))jt[fn]=new a.Tensor("int64",BigInt64Array.from(Je.flatMap(jn=>jn[fn]).map(BigInt)),Dt)}else{for(const Dt of Object.keys(Je[0]))jt[Dt]=Je.map(fn=>fn[Dt]);if(!st)for(const Dt of Object.keys(jt))jt[Dt]=jt[Dt][0]}return jt}_encode_text($){if($===null)return null;const Q=this.added_tokens_splitter.split($);for(let ae=0;ae<Q.length;++ae){const we=this.added_tokens_map.get(Q[ae]);we&&(we.lstrip&&ae>0&&(Q[ae-1]=Q[ae-1].trimEnd()),we.rstrip&&ae<Q.length-1&&(Q[ae+1]=Q[ae+1].trimStart()))}return Q.flatMap((ae,we)=>{if(ae.length===0)return[];if(this.added_tokens_map.has(ae))return[ae];if(this.remove_space===!0&&(ae=ae.trim().split(/\s+/).join(" ")),this.do_lowercase_and_remove_accent&&(ae=_(ae)),this.normalizer!==null&&(ae=this.normalizer(ae)),ae.length===0)return[];const je=this.pre_tokenizer!==null?this.pre_tokenizer(ae,{section_index:we}):[ae];return this.model(je)})}_encode_plus($,{text_pair:Q=null,add_special_tokens:ie=!0,return_token_type_ids:ae=null}={}){const{tokens:we,token_type_ids:je}=this._tokenize_helper($,{pair:Q,add_special_tokens:ie}),nt=this.model.convert_tokens_to_ids(we),rt={input_ids:nt,attention_mask:new Array(nt.length).fill(1)};return(ae??this.return_token_type_ids)&&je&&(rt.token_type_ids=je),rt}_tokenize_helper($,{pair:Q=null,add_special_tokens:ie=!1}={}){const ae=this._encode_text($),we=this._encode_text(Q);return this.post_processor?this.post_processor(ae,we,{add_special_tokens:ie}):{tokens:(0,s.mergeArrays)(ae??[],we??[])}}tokenize($,{pair:Q=null,add_special_tokens:ie=!1}={}){return this._tokenize_helper($,{pair:Q,add_special_tokens:ie}).tokens}encode($,{text_pair:Q=null,add_special_tokens:ie=!0,return_token_type_ids:ae=null}={}){return this._encode_plus($,{text_pair:Q,add_special_tokens:ie,return_token_type_ids:ae}).input_ids}batch_decode($,Q={}){return $ instanceof a.Tensor&&($=$.tolist()),$.map(ie=>this.decode(ie,Q))}decode($,Q={}){if($ instanceof a.Tensor&&($=g($)),!Array.isArray($)||$.length===0||!(0,s.isIntegralNumber)($[0]))throw Error("token_ids must be a non-empty array of integers.");return this.decode_single($,Q)}decode_single($,{skip_special_tokens:Q=!1,clean_up_tokenization_spaces:ie=null}){let ae=this.model.convert_ids_to_tokens($);Q&&(ae=ae.filter(je=>!this.special_tokens.includes(je)));let we=this.decoder?this.decoder(ae):ae.join(" ");return this.decoder&&this.decoder.end_of_word_suffix&&(we=we.replaceAll(this.decoder.end_of_word_suffix," "),Q&&(we=we.trim())),(ie??this.clean_up_tokenization_spaces)&&(we=I(we)),we}get_chat_template({chat_template:$=null,tools:Q=null}={}){if(this.chat_template&&typeof this.chat_template=="object"){const ie=this.chat_template;if($!==null&&Object.hasOwn(ie,$))$=ie[$];else if($===null)if(Q!==null&&"tool_use"in ie)$=ie.tool_use;else if("default"in ie)$=ie.default;else throw Error(`This model has multiple chat templates with no default specified! Please either pass a chat template or the name of the template you wish to use to the 'chat_template' argument. Available template names are ${Object.keys(ie).sort()}.`)}else if($===null)if(this.chat_template)$=this.chat_template;else throw Error("Cannot use apply_chat_template() because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating");return $}apply_chat_template($,{tools:Q=null,documents:ie=null,chat_template:ae=null,add_generation_prompt:we=!1,tokenize:je=!0,padding:nt=!1,truncation:rt=!1,max_length:st=null,return_tensor:Je=!0,return_dict:jt=!1,tokenizer_kwargs:Dt={},...fn}={}){if(ae=this.get_chat_template({chat_template:ae,tools:Q}),typeof ae!="string")throw Error(`chat_template must be a string, but got ${typeof ae}`);let jn=this._compiled_template_cache.get(ae);jn===void 0&&(jn=new c.Template(ae),this._compiled_template_cache.set(ae,jn));const an=Object.create(null);for(const nn of Kn){const xr=this.getToken(nn);xr&&(an[nn]=xr)}const gn=jn.render({messages:$,add_generation_prompt:we,tools:Q,documents:ie,...an,...fn});if(je){const nn=this._call(gn,{add_special_tokens:!1,padding:nt,truncation:rt,max_length:st,return_tensor:Je,...Dt});return jt?nn:nn.input_ids}return gn}}class rn extends vt{return_token_type_ids=!0}class hs extends vt{return_token_type_ids=!0}class ms extends vt{return_token_type_ids=!0}class _r extends vt{return_token_type_ids=!0}class It extends vt{return_token_type_ids=!0}class fs extends vt{return_token_type_ids=!0}class S extends vt{return_token_type_ids=!0}class ee extends vt{return_token_type_ids=!0}class W extends vt{return_token_type_ids=!0}class J extends vt{}class re extends vt{}class pe extends vt{return_token_type_ids=!0;constructor($,Q){super($,Q),console.warn('WARNING: `XLMTokenizer` is not yet supported by Hugging Face\'s "fast" tokenizers library. Therefore, you may experience slightly inaccurate results.')}}class Se extends vt{return_token_type_ids=!0}class it extends vt{}class gt extends vt{}class Ze extends vt{}class At extends vt{constructor($,Q){super($,Q),this.languageRegex=/^[a-z]{2}_[A-Z]{2}$/,this.language_codes=this.special_tokens.filter(ie=>this.languageRegex.test(ie)),this.lang_to_token=ie=>ie}_build_translation_inputs($,Q,ie){return wn(this,$,Q,ie)}}class et extends At{}class Lt extends vt{}class Rt extends vt{}const Fn="▁";class Cn extends vt{padding_side="left";constructor($,Q){super($,Q),this.legacy=Q.legacy??!0,this.legacy||(this.normalizer=null,this.pre_tokenizer=new on({replacement:Fn,prepend_scheme:"first"}))}_encode_text($){if($===null)return null;if(this.legacy||$.length===0)return super._encode_text($);let Q=super._encode_text(Fn+$.replaceAll(Fn," "));return Q.length>1&&Q[0]===Fn&&this.special_tokens.includes(Q[1])&&(Q=Q.slice(1)),Q}}class bn extends vt{}class Bn extends vt{}class gs extends vt{}class tr extends vt{}class _s extends vt{}class nr extends vt{}class rr extends vt{}class Dr extends vt{}class On extends vt{}function wn(he,$,Q,ie){if(!("language_codes"in he)||!Array.isArray(he.language_codes))throw new Error("Tokenizer must have `language_codes` attribute set and it should be an array of language ids.");if(!("languageRegex"in he)||!(he.languageRegex instanceof RegExp))throw new Error("Tokenizer must have `languageRegex` attribute set and it should be a regular expression.");if(!("lang_to_token"in he)||typeof he.lang_to_token!="function")throw new Error("Tokenizer must have `lang_to_token` attribute set and it should be a function.");const ae=ie.src_lang,we=ie.tgt_lang;if(!he.language_codes.includes(we))throw new Error(`Target language code "${we}" is not valid. Must be one of: {${he.language_codes.join(", ")}}`);if(ae!==void 0){if(!he.language_codes.includes(ae))throw new Error(`Source language code "${ae}" is not valid. Must be one of: {${he.language_codes.join(", ")}}`);for(const je of he.post_processor.config.single)if("SpecialToken"in je&&he.languageRegex.test(je.SpecialToken.id)){je.SpecialToken.id=he.lang_to_token(ae);break}}return ie.forced_bos_token_id=he.model.convert_tokens_to_ids([he.lang_to_token(we)])[0],he._call($,Q)}class yr extends vt{constructor($,Q){super($,Q),this.languageRegex=/^[a-z]{3}_[A-Z][a-z]{3}$/,this.language_codes=this.special_tokens.filter(ie=>this.languageRegex.test(ie)),this.lang_to_token=ie=>ie}_build_translation_inputs($,Q,ie){return wn(this,$,Q,ie)}}class $r extends vt{constructor($,Q){super($,Q),this.languageRegex=/^__[a-z]{2,3}__$/,this.language_codes=this.special_tokens.filter(ie=>this.languageRegex.test(ie)).map(ie=>ie.slice(2,-2)),this.lang_to_token=ie=>`__${ie}__`}_build_translation_inputs($,Q,ie){return wn(this,$,Q,ie)}}class Ds extends vt{get timestamp_begin(){return this.model.convert_tokens_to_ids(["<|notimestamps|>"])[0]+1}_decode_asr($,{return_timestamps:Q=!1,return_language:ie=!1,time_precision:ae=null,force_full_sequences:we=!0}={}){if(ae===null)throw Error("Must specify time_precision");let je=null;const nt=Q==="word";function rt(){return{language:je,timestamp:[null,null],text:""}}const st=[];let Je=rt(),jt=0;const Dt=this.timestamp_begin,jn=Dt+1500;let an=[],gn=[],nn=!1,xr=null;const Or=new Set(this.all_special_ids);for(const un of $){const Sn=un.tokens,qn=nt?un.token_timestamps:null;let Tr=null,Qr=Dt;if("stride"in un){const[Nn,dn,Ln]=un.stride;if(jt-=dn,xr=Nn-Ln,dn&&(Qr=dn/ae+Dt),Ln)for(let vn=Sn.length-1;vn>=0;--vn){const dr=Number(Sn[vn]);if(dr>=Dt){if(Tr!==null&&(dr-Dt)*ae<xr)break;Tr=dr}}}let Rn=[],jr=[];for(let Nn=0;Nn<Sn.length;++Nn){const dn=Number(Sn[Nn]);if(Or.has(dn)){const Ln=this.decode([dn]),vn=d.WHISPER_LANGUAGE_MAPPING.get(Ln.slice(2,-2));if(vn!==void 0){if(je!==null&&vn!==je&&!Q){an.push(Rn);const dr=this.findLongestCommonSequence(an)[0],qs=this.decode(dr);Je.text=qs,st.push(Je),an=[],Rn=[],Je=rt()}je=Je.language=vn}}else if(dn>=Dt&&dn<=jn){const Ln=(dn-Dt)*ae+jt,vn=(0,o.round)(Ln,2);if(Tr!==null&&dn>=Tr)nn=!0;else if(nn||an.length>0&&dn<Qr)nn=!1;else if(Je.timestamp[0]===null)Je.timestamp[0]=vn;else if(vn!==Je.timestamp[0]){Je.timestamp[1]=vn,an.push(Rn),nt&&gn.push(jr);const[dr,qs]=this.findLongestCommonSequence(an,gn),vs=this.decode(dr);Je.text=vs,nt&&(Je.words=this.collateWordTimestamps(dr,qs,je)),st.push(Je),an=[],Rn=[],gn=[],jr=[],Je=rt()}}else if(Rn.push(dn),nt){let Ln=(0,o.round)(qn[Nn]+jt,2),vn;if(Nn+1<qn.length){vn=(0,o.round)(qn[Nn+1]+jt,2);const dr=this.decode([dn]);A.test(dr)&&(vn=(0,o.round)(Math.min(Ln+ae,vn),2))}else vn=null;jr.push([Ln,vn])}}if("stride"in un){const[Nn,dn,Ln]=un.stride;jt+=Nn-Ln}Rn.length>0?(an.push(Rn),nt&&gn.push(jr)):an.every(Nn=>Nn.length===0)&&(Je=rt(),an=[],Rn=[],gn=[],jr=[])}if(an.length>0){if(we&&Q)throw new Error("Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.");const[un,Sn]=this.findLongestCommonSequence(an,gn),qn=this.decode(un);Je.text=qn,nt&&(Je.words=this.collateWordTimestamps(un,Sn,je)),st.push(Je)}let Gn=Object.create(null);const qr=st.map(un=>un.text).join("");if(Q||ie){for(let un=0;un<st.length;++un){const Sn=st[un];Q||delete Sn.timestamp,ie||delete Sn.language}if(nt){const un=[];for(const Sn of st)for(const qn of Sn.words)un.push(qn);Gn={chunks:un}}else Gn={chunks:st}}return[qr,Gn]}findLongestCommonSequence($,Q=null){let ie=$[0],ae=ie.length,we=[];const je=Array.isArray(Q)&&Q.length>0;let nt=je?[]:null,rt=je?Q[0]:null;for(let st=1;st<$.length;++st){const Je=$[st];let jt=0,Dt=[ae,ae,0,0];const fn=Je.length;for(let Gn=1;Gn<ae+fn;++Gn){const qr=Math.max(0,ae-Gn),un=Math.min(ae,ae+fn-Gn),Sn=ie.slice(qr,un),qn=Math.max(0,Gn-ae),Tr=Math.min(fn,Gn),Qr=Je.slice(qn,Tr);if(Sn.length!==Qr.length)throw new Error("There is a bug within whisper `decode_asr` function, please report it. Dropping to prevent bad inference.");let Rn;je?Rn=Sn.filter((dn,Ln)=>dn===Qr[Ln]&&rt[qr+Ln]<=Q[st][qn+Ln]).length:Rn=Sn.filter((dn,Ln)=>dn===Qr[Ln]).length;const jr=Gn/1e4,Nn=Rn/Gn+jr;Rn>1&&Nn>jt&&(jt=Nn,Dt=[qr,un,qn,Tr])}const[jn,an,gn,nn]=Dt,xr=Math.floor((an+jn)/2),Or=Math.floor((nn+gn)/2);we.push(...ie.slice(0,xr)),ie=Je.slice(Or),ae=ie.length,je&&(nt.push(...rt.slice(0,xr)),rt=Q[st].slice(Or))}return we.push(...ie),je?(nt.push(...rt),[we,nt]):[we,[]]}collateWordTimestamps($,Q,ie){const[ae,we,je]=this.combineTokensIntoWords($,ie),nt=[];for(let rt=0;rt<ae.length;++rt){const st=je[rt];nt.push({text:ae[rt],timestamp:[Q[st.at(0)][0],Q[st.at(-1)][1]]})}return nt}combineTokensIntoWords($,Q,ie=`"'“¡¿([{-`,ae=`"'.。,，!！?？:：”)]}、`){Q=Q??"english";let we,je,nt;return["chinese","japanese","thai","lao","myanmar"].includes(Q)?[we,je,nt]=this.splitTokensOnUnicode($):[we,je,nt]=this.splitTokensOnSpaces($),this.mergePunctuations(we,je,nt,ie,ae)}decode($,Q){let ie;return Q?.decode_with_timestamps?($ instanceof a.Tensor&&($=g($)),ie=this.decodeWithTimestamps($,Q)):ie=super.decode($,Q),ie}decodeWithTimestamps($,Q){const ie=Q?.time_precision??.02,ae=Array.from(this.all_special_ids).at(-1)+1;let we=[[]];for(let je of $)if(je=Number(je),je>=ae){const nt=((je-ae)*ie).toFixed(2);we.push(`<|${nt}|>`),we.push([])}else we[we.length-1].push(je);return we=we.map(je=>typeof je=="string"?je:super.decode(je,Q)),we.join("")}splitTokensOnUnicode($){const Q=this.decode($,{decode_with_timestamps:!0}),ie="�",ae=[],we=[],je=[];let nt=[],rt=[],st=0;for(let Je=0;Je<$.length;++Je){const jt=$[Je];nt.push(jt),rt.push(Je);const Dt=this.decode(nt,{decode_with_timestamps:!0});(!Dt.includes(ie)||Q[st+Dt.indexOf(ie)]===ie)&&(ae.push(Dt),we.push(nt),je.push(rt),nt=[],rt=[],st+=Dt.length)}return[ae,we,je]}splitTokensOnSpaces($){const[Q,ie,ae]=this.splitTokensOnUnicode($),we=[],je=[],nt=[],rt=new RegExp(`^[${b}]$`,"gu");for(let st=0;st<Q.length;++st){const Je=Q[st],jt=ie[st],Dt=ae[st],fn=jt[0]>=this.model.tokens_to_ids.get("<|endoftext|>"),jn=Je.startsWith(" "),an=Je.trim(),gn=rt.test(an);if(fn||jn||gn||we.length===0)we.push(Je),je.push(jt),nt.push(Dt);else{const nn=we.length-1;we[nn]+=Je,je[nn].push(...jt),nt[nn].push(...Dt)}}return[we,je,nt]}mergePunctuations($,Q,ie,ae,we){const je=structuredClone($),nt=structuredClone(Q),rt=structuredClone(ie);let st=je.length-2,Je=je.length-1;for(;st>=0;)je[st].startsWith(" ")&&ae.includes(je[st].trim())?(je[Je]=je[st]+je[Je],nt[Je]=(0,s.mergeArrays)(nt[st],nt[Je]),rt[Je]=(0,s.mergeArrays)(rt[st],rt[Je]),je[st]="",nt[st]=[],rt[st]=[]):Je=st,--st;for(st=0,Je=1;Je<je.length;)!je[st].endsWith(" ")&&we.includes(je[Je])?(je[st]+=je[Je],nt[st]=(0,s.mergeArrays)(nt[st],nt[Je]),rt[st]=(0,s.mergeArrays)(rt[st],rt[Je]),je[Je]="",nt[Je]=[],rt[Je]=[]):st=Je,++Je;return[je.filter(jt=>jt),nt.filter(jt=>jt.length>0),rt.filter(jt=>jt.length>0)]}}class ys extends vt{}class xs extends vt{}class bs extends vt{}class Fr extends vt{constructor($,Q){super($,Q),this.languageRegex=/^(>>\w+<<)\s*/g,this.supported_language_codes=this.model.vocab.filter(ie=>this.languageRegex.test(ie)),console.warn('WARNING: `MarianTokenizer` is not yet supported by Hugging Face\'s "fast" tokenizers library. Therefore, you may experience slightly inaccurate results.')}_encode_text($){if($===null)return null;const[Q,...ie]=$.trim().split(this.languageRegex);if(ie.length===0)return super._encode_text(Q);if(ie.length===2){const[ae,we]=ie;return this.supported_language_codes.includes(ae)||console.warn(`Unsupported language code "${ae}" detected, which may lead to unexpected behavior. Should be one of: ${JSON.stringify(this.supported_language_codes)}`),(0,s.mergeArrays)([ae],super._encode_text(we))}}}class or extends vt{}class Ue extends vt{}class Ke extends vt{}class ut extends vt{}class tn extends vt{}class $s extends vt{constructor($,Q){super($,Q),this.decoder=new wt({})}}class ws extends vt{}class Fs extends vt{}class Zi{static TOKENIZER_CLASS_MAPPING={T5Tokenizer:it,DistilBertTokenizer:J,CamembertTokenizer:re,DebertaTokenizer:It,DebertaV2Tokenizer:fs,BertTokenizer:rn,HerbertTokenizer:S,ConvBertTokenizer:ee,RoFormerTokenizer:W,XLMTokenizer:pe,ElectraTokenizer:Se,MobileBertTokenizer:ms,SqueezeBertTokenizer:_r,AlbertTokenizer:hs,GPT2Tokenizer:gt,BartTokenizer:Ze,MBartTokenizer:At,MBart50Tokenizer:et,RobertaTokenizer:Lt,WhisperTokenizer:Ds,CodeGenTokenizer:ys,CLIPTokenizer:xs,SiglipTokenizer:bs,MarianTokenizer:Fr,BloomTokenizer:Rt,NllbTokenizer:yr,M2M100Tokenizer:$r,LlamaTokenizer:Cn,CodeLlamaTokenizer:bn,XLMRobertaTokenizer:Bn,MPNetTokenizer:gs,FalconTokenizer:tr,GPTNeoXTokenizer:_s,EsmTokenizer:nr,Wav2Vec2CTCTokenizer:or,BlenderbotTokenizer:Ue,BlenderbotSmallTokenizer:Ke,SpeechT5Tokenizer:ut,NougatTokenizer:tn,VitsTokenizer:$s,Qwen2Tokenizer:rr,GemmaTokenizer:Dr,Grok1Tokenizer:On,CohereTokenizer:ws,MgpstrTokenizer:Fs,PreTrainedTokenizer:vt};static async from_pretrained($,{progress_callback:Q=null,config:ie=null,cache_dir:ae=null,local_files_only:we=!1,revision:je="main",legacy:nt=null}={}){const[rt,st]=await u($,{progress_callback:Q,config:ie,cache_dir:ae,local_files_only:we,revision:je,legacy:nt}),Je=st.tokenizer_class?.replace(/Fast$/,"")??"PreTrainedTokenizer";let jt=this.TOKENIZER_CLASS_MAPPING[Je];return jt||(console.warn(`Unknown tokenizer class "${Je}", attempting to construct from base class.`),jt=vt),new jt(rt,st)}}}),"./src/utils/audio.js":((e,t,n)=>{n.r(t),n.d(t,{RawAudio:()=>j,hamming:()=>p,hanning:()=>u,mel_filter_bank:()=>T,read_audio:()=>c,spectrogram:()=>k,window_function:()=>F});var r=n("./src/utils/hub.js"),s=n("./src/utils/maths.js"),i=n("./src/utils/core.js"),o=n("./src/env.js"),a=n("./src/utils/tensor.js"),l=n("?7992");async function c(R,K){if(typeof AudioContext>"u")throw Error("Unable to load audio from path/URL since `AudioContext` is not available in your environment. Instead, audio data should be passed directly to the pipeline/processor. For more information and some example code, see https://huggingface.co/docs/transformers.js/guides/node-audio-processing.");const U=await(await(0,r.getFile)(R)).arrayBuffer(),Y=new AudioContext({sampleRate:K});typeof K>"u"&&console.warn(`No sampling rate provided, using default of ${Y.sampleRate}Hz.`);const te=await Y.decodeAudioData(U);let ne;if(te.numberOfChannels===2){const le=Math.sqrt(2),N=te.getChannelData(0),oe=te.getChannelData(1);ne=new Float32Array(N.length);for(let X=0;X<te.length;++X)ne[X]=le*(N[X]+oe[X])/2}else ne=te.getChannelData(0);return ne}function d(R,K){if(R<1)return new Float64Array;if(R===1)return new Float64Array([1]);const U=1-K,Y=2*Math.PI/(R-1),te=new Float64Array(R);for(let ne=0;ne<R;++ne)te[ne]=K-U*Math.cos(ne*Y);return te}function u(R){return d(R,.5)}function p(R){return d(R,.54)}const h={htk:R=>2595*Math.log10(1+R/700),kaldi:R=>1127*Math.log(1+R/700),slaney:(R,K=1e3,U=15,Y=27/Math.log(6.4))=>R>=K?U+Math.log(R/K)*Y:3*R/200};function m(R,K="htk"){const U=h[K];if(!U)throw new Error('mel_scale should be one of "htk", "slaney" or "kaldi".');return typeof R=="number"?U(R):R.map(Y=>U(Y))}const g={htk:R=>700*(10**(R/2595)-1),kaldi:R=>700*(Math.exp(R/1127)-1),slaney:(R,K=1e3,U=15,Y=Math.log(6.4)/27)=>R>=U?K*Math.exp(Y*(R-U)):200*R/3};function I(R,K="htk"){const U=g[K];if(!U)throw new Error('mel_scale should be one of "htk", "slaney" or "kaldi".');return typeof R=="number"?U(R):R.map(Y=>U(Y))}function f(R,K){const U=Float64Array.from({length:K.length-1},(le,N)=>K[N+1]-K[N]),Y=Array.from({length:R.length},()=>new Array(K.length));for(let le=0;le<R.length;++le){const N=Y[le];for(let oe=0;oe<K.length;++oe)N[oe]=K[oe]-R[le]}const te=K.length-2,ne=Array.from({length:te},()=>new Array(R.length));for(let le=0;le<R.length;++le){const N=Y[le];for(let oe=0;oe<te;++oe){const X=-N[oe]/U[oe],D=N[oe+2]/U[oe+1];ne[oe][le]=Math.max(0,Math.min(X,D))}}return ne}function _(R,K,U){const Y=(K-R)/(U-1);return Float64Array.from({length:U},(te,ne)=>R+Y*ne)}function T(R,K,U,Y,te,ne=null,le="htk",N=!1){if(ne!==null&&ne!=="slaney")throw new Error('norm must be one of null or "slaney"');if(R<2)throw new Error(`Require num_frequency_bins: ${R} >= 2`);if(U>Y)throw new Error(`Require min_frequency: ${U} <= max_frequency: ${Y}`);const oe=m(U,le),X=m(Y,le),D=_(oe,X,K+2);let z=I(D,le),se;if(N){const $e=te/((R-1)*2);se=m(Float64Array.from({length:R},(ke,Be)=>Be*$e),le),z=D}else se=_(0,Math.floor(te/2),R);const me=f(se,z);if(ne!==null&&ne==="slaney")for(let $e=0;$e<K;++$e){const ke=me[$e],Be=2/(z[$e+2]-z[$e]);for(let Ce=0;Ce<R;++Ce)ke[Ce]*=Be}return me}function M(R,K,U){const Y=new R.constructor(R.length+K+U),te=R.length-1;for(let ne=0;ne<R.length;++ne)Y[K+ne]=R[ne];for(let ne=1;ne<=K;++ne)Y[K-ne]=R[(0,i.calculateReflectOffset)(ne,te)];for(let ne=1;ne<=U;++ne)Y[te+K+ne]=R[(0,i.calculateReflectOffset)(te-ne,te)];return Y}function v(R,K,U,Y,te){if(U<=0)throw new Error("reference must be greater than zero");if(Y<=0)throw new Error("min_value must be greater than zero");U=Math.max(Y,U);const ne=Math.log10(U);for(let le=0;le<R.length;++le)R[le]=K*Math.log10(Math.max(Y,R[le])-ne);if(te!==null){if(te<=0)throw new Error("db_range must be greater than zero");const le=(0,s.max)(R)[0]-te;for(let N=0;N<R.length;++N)R[N]=Math.max(R[N],le)}return R}function b(R,K=1,U=1e-5,Y=null){return v(R,20,K,U,Y)}function A(R,K=1,U=1e-10,Y=null){return v(R,10,K,U,Y)}async function k(R,K,U,Y,{fft_length:te=null,power:ne=1,center:le=!0,pad_mode:N="reflect",onesided:oe=!0,preemphasis:X=null,preemphasis_htk_flavor:D=!0,mel_filters:z=null,mel_floor:se=1e-10,log_mel:me=null,reference:$e=1,min_value:ke=1e-10,db_range:Be=null,remove_dc_offset:Ce=null,min_num_frames:Z=null,max_num_frames:V=null,do_pad:fe=!0,transpose:Te=!1,mel_offset:Ie=0}={}){const Ee=K.length;if(te===null&&(te=U),U>te)throw Error(`frame_length (${U}) may not be larger than fft_length (${te})`);if(Ee!==U)throw new Error(`Length of the window (${Ee}) must equal frame_length (${U})`);if(Y<=0)throw new Error("hop_length must be greater than zero");if(ne===null&&z!==null)throw new Error("You have provided `mel_filters` but `power` is `None`. Mel spectrogram computation is not yet supported for complex-valued spectrogram. Specify `power` to fix this issue.");if(!D)throw new Error("`preemphasis_htk_flavor=false` is not currently supported.");if(le)switch(N){case"reflect":{const Pe=Math.floor((te-1)/2)+1;R=M(R,Pe,Pe);break}case"constant":{const Pe=Math.floor(te/2),ve=new R.constructor(R.length+2*Pe);ve.set(R,Pe),R=ve;break}default:throw new Error(`pad_mode="${N}" not implemented yet.`)}let De=Math.floor(1+Math.floor((R.length-U)/Y));Z!==null&&De<Z&&(De=Z);const xe=oe?Math.floor(te/2)+1:te;let ze=De,_e=De;V!==null&&(V>De?fe&&(_e=V):_e=ze=V);const Le=new s.FFT(te),qe=new Float64Array(te),Ne=new Float64Array(Le.outputBufferSize),ot=new Float32Array(xe*_e);for(let Pe=0;Pe<ze;++Pe){const ve=Pe*Y,Qe=Math.min(R.length-ve,U);Qe!==U&&qe.fill(0,0,U);for(let ct=0;ct<Qe;++ct)qe[ct]=R[ve+ct];if(Ce){let ct=0;for(let wt=0;wt<Qe;++wt)ct+=qe[wt];const zt=ct/Qe;for(let wt=0;wt<Qe;++wt)qe[wt]-=zt}if(X!==null){for(let ct=Qe-1;ct>=1;--ct)qe[ct]-=X*qe[ct-1];qe[0]*=1-X}for(let ct=0;ct<K.length;++ct)qe[ct]*=K[ct];Le.realTransform(Ne,qe);for(let ct=0;ct<xe;++ct){const zt=ct<<1;ot[ct*_e+Pe]=Ne[zt]**2+Ne[zt+1]**2}}if(ne!==null&&ne!==2){const Pe=ne/2;for(let ve=0;ve<ot.length;++ve)ot[ve]**=Pe}const Ve=z.length;let de=await(0,a.matmul)(new a.Tensor("float32",z.flat(),[Ve,xe]),new a.Tensor("float32",ot,[xe,_e]));Te&&(de=de.transpose(1,0));const ye=de.data;for(let Pe=0;Pe<ye.length;++Pe)ye[Pe]=Ie+Math.max(se,ye[Pe]);if(ne!==null&&me!==null){const Pe=Math.min(ye.length,ze*Ve);switch(me){case"log":for(let ve=0;ve<Pe;++ve)ye[ve]=Math.log(ye[ve]);break;case"log10":for(let ve=0;ve<Pe;++ve)ye[ve]=Math.log10(ye[ve]);break;case"dB":if(ne===1)b(ye,$e,ke,Be);else if(ne===2)A(ye,$e,ke,Be);else throw new Error(`Cannot use log_mel option '${me}' with power ${ne}`);break;default:throw new Error(`log_mel must be one of null, 'log', 'log10' or 'dB'. Got '${me}'`)}}return de}function F(R,K,{periodic:U=!0,frame_length:Y=null,center:te=!0}={}){const ne=U?R+1:R;let le;switch(K){case"boxcar":le=new Float64Array(ne).fill(1);break;case"hann":case"hann_window":le=u(ne);break;case"hamming":le=p(ne);break;case"povey":le=u(ne).map(N=>Math.pow(N,.85));break;default:throw new Error(`Unknown window type ${K}.`)}if(U&&(le=le.subarray(0,R)),Y===null)return le;if(R>Y)throw new Error(`Length of the window (${R}) may not be larger than frame_length (${Y})`);return le}function L(R,K){let U=44;const Y=new ArrayBuffer(U+R.length*4),te=new DataView(Y);G(te,0,"RIFF"),te.setUint32(4,36+R.length*4,!0),G(te,8,"WAVE"),G(te,12,"fmt "),te.setUint32(16,16,!0),te.setUint16(20,3,!0),te.setUint16(22,1,!0),te.setUint32(24,K,!0),te.setUint32(28,K*4,!0),te.setUint16(32,4,!0),te.setUint16(34,32,!0),G(te,36,"data"),te.setUint32(40,R.length*4,!0);for(let ne=0;ne<R.length;++ne,U+=4)te.setFloat32(U,R[ne],!0);return Y}function G(R,K,U){for(let Y=0;Y<U.length;++Y)R.setUint8(K+Y,U.charCodeAt(Y))}class j{constructor(K,U){this.audio=K,this.sampling_rate=U}toWav(){return L(this.audio,this.sampling_rate)}toBlob(){const K=this.toWav();return new Blob([K],{type:"audio/wav"})}async save(K){let U;if(o.apis.IS_BROWSER_ENV){if(o.apis.IS_WEBWORKER_ENV)throw new Error("Unable to save a file from a Web Worker.");U=i.saveBlob}else if(o.apis.IS_FS_AVAILABLE)U=async(Y,te)=>{let ne=await te.arrayBuffer();l.writeFileSync(Y,Buffer.from(ne))};else throw new Error("Unable to save because filesystem is disabled in this environment.");await U(K,this.toBlob())}}}),"./src/utils/constants.js":((e,t,n)=>{n.r(t),n.d(t,{CHAT_TEMPLATE_NAME:()=>l,CONFIG_NAME:()=>s,FEATURE_EXTRACTOR_NAME:()=>i,GENERATION_CONFIG_NAME:()=>c,GITHUB_ISSUE_URL:()=>r,IMAGE_PROCESSOR_NAME:()=>o,PROCESSOR_NAME:()=>a});const r="https://github.com/huggingface/transformers.js/issues/new/choose",s="config.json",i="preprocessor_config.json",o=i,a="processor_config.json",l="chat_template.jinja",c="generation_config.json"}),"./src/utils/core.js":((e,t,n)=>{n.r(t),n.d(t,{calculateDimensions:()=>c,calculateReflectOffset:()=>h,count:()=>f,dispatchCallback:()=>r,escapeRegExp:()=>i,isIntegralNumber:()=>a,isNullishDimension:()=>l,isTypedArray:()=>o,len:()=>I,mergeArrays:()=>u,pick:()=>g,pop:()=>d,product:()=>p,reverseDictionary:()=>s,saveBlob:()=>m});function r(_,T){_&&_(T)}function s(_){return Object.fromEntries(Object.entries(_).map(([T,M])=>[M,T]))}function i(_){return _.replace(/[.*+?^${}()|[\]\\]/g,"\\$&")}function o(_){return _?.prototype?.__proto__?.constructor?.name==="TypedArray"}function a(_){return Number.isInteger(_)||typeof _=="bigint"}function l(_){return _==null||_===-1}function c(_){const T=[];let M=_;for(;Array.isArray(M);)T.push(M.length),M=M[0];return T}function d(_,T,M=void 0){const v=_[T];if(v!==void 0)return delete _[T],v;if(M===void 0)throw Error(`Key ${T} does not exist in object.`);return M}function u(..._){return Array.prototype.concat.apply([],_)}function p(..._){return _.reduce((T,M)=>T.flatMap(v=>M.map(b=>[v,b])))}function h(_,T){return Math.abs((_+T)%(2*T)-T)}function m(_,T){const M=URL.createObjectURL(T),v=document.createElement("a");v.href=M,v.download=_,v.click(),v.remove(),URL.revokeObjectURL(M)}function g(_,T){return Object.assign({},...T.map(M=>{if(_[M]!==void 0)return{[M]:_[M]}}))}function I(_){let T=0;for(const M of _)++T;return T}function f(_,T){let M=0;for(const v of _)v===T&&++M;return M}}),"./src/utils/data-structures.js":((e,t,n)=>{n.r(t),n.d(t,{CharTrie:()=>s,DictionarySplitter:()=>l,LRUCache:()=>c,PriorityQueue:()=>r,TokenLattice:()=>o});class r{constructor(u=(h,m)=>h>m,p=1/0){this._heap=[],this._comparator=u,this._maxSize=p}get size(){return this._heap.length}isEmpty(){return this.size===0}peek(){return this._heap[0]}push(...u){return this.extend(u)}extend(u){for(const p of u)if(this.size<this._maxSize)this._heap.push(p),this._siftUp();else{const h=this._smallest();this._comparator(p,this._heap[h])&&(this._heap[h]=p,this._siftUpFrom(h))}return this.size}pop(){const u=this.peek(),p=this.size-1;return p>0&&this._swap(0,p),this._heap.pop(),this._siftDown(),u}replace(u){const p=this.peek();return this._heap[0]=u,this._siftDown(),p}_parent(u){return(u+1>>>1)-1}_left(u){return(u<<1)+1}_right(u){return u+1<<1}_greater(u,p){return this._comparator(this._heap[u],this._heap[p])}_swap(u,p){const h=this._heap[u];this._heap[u]=this._heap[p],this._heap[p]=h}_siftUp(){this._siftUpFrom(this.size-1)}_siftUpFrom(u){for(;u>0&&this._greater(u,this._parent(u));)this._swap(u,this._parent(u)),u=this._parent(u)}_siftDown(){let u=0;for(;this._left(u)<this.size&&this._greater(this._left(u),u)||this._right(u)<this.size&&this._greater(this._right(u),u);){const p=this._right(u)<this.size&&this._greater(this._right(u),this._left(u))?this._right(u):this._left(u);this._swap(u,p),u=p}}_smallest(){return 2**Math.floor(Math.log2(this.size))-1}}class s{constructor(){this.root=i.default()}extend(u){for(const p of u)this.push(p)}push(u){let p=this.root;for(const h of u){let m=p.children.get(h);m===void 0&&(m=i.default(),p.children.set(h,m)),p=m}p.isLeaf=!0}*commonPrefixSearch(u){let p=this.root;if(p===void 0)return;let h="";for(const m of u){if(h+=m,p=p.children.get(m),p===void 0)return;p.isLeaf&&(yield h)}}}class i{constructor(u,p){this.isLeaf=u,this.children=p}static default(){return new i(!1,new Map)}}class o{constructor(u,p,h){this.chars=Array.from(u),this.len=this.chars.length,this.bosTokenId=p,this.eosTokenId=h,this.nodes=[],this.beginNodes=Array.from({length:this.len+1},()=>[]),this.endNodes=Array.from({length:this.len+1},()=>[]);const m=new a(this.bosTokenId,0,0,0,0),g=new a(this.eosTokenId,1,this.len,0,0);this.nodes.push(m.clone()),this.nodes.push(g.clone()),this.beginNodes[this.len].push(g),this.endNodes[0].push(m)}insert(u,p,h,m){const g=this.nodes.length,I=new a(m,g,u,p,h);this.beginNodes[u].push(I),this.endNodes[u+p].push(I),this.nodes.push(I)}viterbi(){const u=this.len;let p=0;for(;p<=u;){if(this.beginNodes[p].length==0)return[];for(let f of this.beginNodes[p]){f.prev=null;let _=0,T=null;for(let M of this.endNodes[p]){const v=M.backtraceScore+f.score;(T===null||v>_)&&(T=M.clone(),_=v)}if(T!==null)f.prev=T,f.backtraceScore=_;else return[]}++p}const h=[],g=this.beginNodes[u][0].prev;if(g===null)return[];let I=g.clone();for(;I.prev!==null;)h.push(I.clone()),I=I.clone().prev.clone();return h.reverse(),h}piece(u){return this.chars.slice(u.pos,u.pos+u.length).join("")}tokens(){return this.viterbi().map(p=>this.piece(p))}tokenIds(){return this.viterbi().map(p=>p.tokenId)}}class a{constructor(u,p,h,m,g){this.tokenId=u,this.nodeId=p,this.pos=h,this.length=m,this.score=g,this.prev=null,this.backtraceScore=0}clone(){const u=new a(this.tokenId,this.nodeId,this.pos,this.length,this.score);return u.prev=this.prev,u.backtraceScore=this.backtraceScore,u}}class l{constructor(u){this.trie=this._buildTrie(u)}_buildTrie(u){const p=Object.create(null);for(const h of u){let m=p;for(let g=0;g<h.length;++g)m=m[h[g]]??=Object.create(null);m.end=h}return p}split(u){const p=[],h=u.length;let m=0,g=0;for(;g<h;){let I=this.trie,f=null,_=g;for(;_<h&&(I=I[u[_]]);)I.end&&(f=I.end),++_;f?(g>m&&p.push(u.slice(m,g)),p.push(f),g+=f.length,m=g):++g}return m<h&&p.push(u.slice(m)),p}}class c{constructor(u){this.capacity=u,this.cache=new Map}get(u){if(!this.cache.has(u))return;const p=this.cache.get(u);return this.cache.delete(u),this.cache.set(u,p),p}put(u,p){this.cache.has(u)&&this.cache.delete(u),this.cache.set(u,p),this.cache.size>this.capacity&&this.cache.delete(this.cache.keys().next().value)}clear(){this.cache.clear()}}}),"./src/utils/devices.js":((e,t,n)=>{n.r(t),n.d(t,{DEVICE_TYPES:()=>r});const r=Object.freeze({auto:"auto",gpu:"gpu",cpu:"cpu",wasm:"wasm",webgpu:"webgpu",cuda:"cuda",dml:"dml",webnn:"webnn","webnn-npu":"webnn-npu","webnn-gpu":"webnn-gpu","webnn-cpu":"webnn-cpu"})}),"./src/utils/dtypes.js":((e,t,n)=>{n.r(t),n.d(t,{DATA_TYPES:()=>o,DEFAULT_DEVICE_DTYPE_MAPPING:()=>a,DEFAULT_DTYPE_SUFFIX_MAPPING:()=>l,isWebGpuFp16Supported:()=>i});var r=n("./src/env.js"),s=n("./src/utils/devices.js");const i=(function(){let c;return async function(){if(c===void 0)if(!r.apis.IS_WEBGPU_AVAILABLE)c=!1;else try{c=(await navigator.gpu.requestAdapter()).features.has("shader-f16")}catch{c=!1}return c}})(),o=Object.freeze({auto:"auto",fp32:"fp32",fp16:"fp16",q8:"q8",int8:"int8",uint8:"uint8",q4:"q4",bnb4:"bnb4",q4f16:"q4f16"}),a=Object.freeze({[s.DEVICE_TYPES.wasm]:o.q8}),l=Object.freeze({[o.fp32]:"",[o.fp16]:"_fp16",[o.int8]:"_int8",[o.uint8]:"_uint8",[o.q8]:"_quantized",[o.q4]:"_q4",[o.q4f16]:"_q4f16",[o.bnb4]:"_bnb4"})}),"./src/utils/generic.js":((e,t,n)=>{n.r(t),n.d(t,{Callable:()=>r});const r=class{constructor(){let s=function(...i){return s._call(...i)};return Object.setPrototypeOf(s,new.target.prototype)}_call(...s){throw Error("Must implement _call method in subclass")}}}),"./src/utils/hub.js":((e,t,n)=>{n.r(t),n.d(t,{MAX_EXTERNAL_DATA_CHUNKS:()=>a,getFile:()=>h,getModelFile:()=>_,getModelJSON:()=>M,getModelText:()=>T});var r=n("?7992"),s=n("?5af5"),i=n("./src/env.js"),o=n("./src/utils/core.js");const a=100,l={txt:"text/plain",html:"text/html",css:"text/css",js:"text/javascript",json:"application/json",png:"image/png",jpg:"image/jpeg",jpeg:"image/jpeg",gif:"image/gif"};class c{constructor(k){if(this.filePath=k,this.headers=new Headers,this.exists=r.existsSync(k),this.exists){this.status=200,this.statusText="OK";let F=r.statSync(k);this.headers.set("content-length",F.size.toString()),this.updateContentType();const L=r.createReadStream(k);this.body=new ReadableStream({start(G){L.on("data",j=>G.enqueue(j)),L.on("end",()=>G.close()),L.on("error",j=>G.error(j))},cancel(){L.destroy()}})}else this.status=404,this.statusText="Not Found",this.body=null}updateContentType(){const k=this.filePath.toString().split(".").pop().toLowerCase();this.headers.set("content-type",l[k]??"application/octet-stream")}clone(){let k=new c(this.filePath);return k.exists=this.exists,k.status=this.status,k.statusText=this.statusText,k.headers=new Headers(this.headers),k}async arrayBuffer(){return(await r.promises.readFile(this.filePath)).buffer}async blob(){const k=await r.promises.readFile(this.filePath);return new Blob([k],{type:this.headers.get("content-type")})}async text(){return await r.promises.readFile(this.filePath,"utf8")}async json(){return JSON.parse(await this.text())}}function d(A,k=null,F=null){let L;try{L=new URL(A)}catch{return!1}return!(k&&!k.includes(L.protocol)||F&&!F.includes(L.hostname))}const u=/^(\b[\w\-.]+\b\/)?\b[\w\-.]{1,96}\b$/;function p(A){return!(!u.test(A)||A.includes("..")||A.includes("--")||A.endsWith(".git")||A.endsWith(".ipynb"))}async function h(A){if(i.env.useFS&&!d(A,["http:","https:","blob:"]))return new c(A instanceof URL?A.protocol==="file:"?A.pathname:A.toString():A);if(typeof process<"u"&&process?.release?.name==="node"){const k=!!sm?.TESTING_REMOTELY,F=i.env.version,L=new Headers;if(L.set("User-Agent",`transformers.js/${F}; is_ci/${k};`),d(A,["http:","https:"],["huggingface.co","hf.co"])){const j=sm?.HF_TOKEN??sm?.HF_ACCESS_TOKEN;j&&L.set("Authorization",`Bearer ${j}`)}return fetch(A,{headers:L})}else return fetch(A)}const m={400:"Bad request error occurred while trying to load file",401:"Unauthorized access to file",403:"Forbidden access to file",404:"Could not locate file",408:"Request timeout error occurred while trying to load file",500:"Internal server error error occurred while trying to load file",502:"Bad gateway error occurred while trying to load file",503:"Service unavailable error occurred while trying to load file",504:"Gateway timeout error occurred while trying to load file"};function g(A,k,F){if(!F)return null;const L=m[A]??`Error (${A}) occurred while trying to load file`;throw Error(`${L}: "${k}".`)}class I{constructor(k){this.path=k}async match(k){let F=s.join(this.path,k),L=new c(F);if(L.exists)return L}async put(k,F,L=void 0){let G=s.join(this.path,k);try{const j=F.headers.get("Content-Length"),R=parseInt(j??"0");let K=0;await r.promises.mkdir(s.dirname(G),{recursive:!0});const U=r.createWriteStream(G),Y=F.body.getReader();for(;;){const{done:te,value:ne}=await Y.read();if(te)break;await new Promise((N,oe)=>{U.write(ne,X=>{if(X){oe(X);return}N()})}),K+=ne.length;const le=R?K/R*100:0;L?.({progress:le,loaded:K,total:R})}U.close()}catch(j){try{await r.promises.unlink(G)}catch{}throw j}}}async function f(A,...k){for(let F of k)try{let L=await A.match(F);if(L)return L}catch{continue}}async function _(A,k,F=!0,L={},G=!1){if(!i.env.allowLocalModels){if(L.local_files_only)throw Error("Invalid configuration detected: local models are disabled (`env.allowLocalModels=false`) but you have requested to only use local models (`local_files_only=true`).");if(!i.env.allowRemoteModels)throw Error("Invalid configuration detected: both local and remote models are disabled. Fix by setting `env.allowLocalModels` or `env.allowRemoteModels` to `true`.")}(0,o.dispatchCallback)(L.progress_callback,{status:"initiate",name:A,file:k});let j;if(!j&&i.env.useCustomCache){if(!i.env.customCache)throw Error("`env.useCustomCache=true`, but `env.customCache` is not defined.");if(!i.env.customCache.match||!i.env.customCache.put)throw new Error("`env.customCache` must be an object which implements the `match` and `put` functions of the Web Cache API. For more information, see https://developer.mozilla.org/en-US/docs/Web/API/Cache");j=i.env.customCache}if(!j&&i.env.useBrowserCache){if(typeof caches>"u")throw Error("Browser cache is not available in this environment.");try{j=await caches.open("transformers-cache")}catch(se){console.warn("An error occurred while opening the browser cache:",se)}}if(!j&&i.env.useFSCache){if(!i.apis.IS_FS_AVAILABLE)throw Error("File System Cache is not available in this environment.");j=new I(L.cache_dir??i.env.cacheDir)}const R=L.revision??"main",K=b(A,k),U=p(A),Y=U?b(i.env.localModelPath,K):K,te=b(i.env.remoteHost,i.env.remotePathTemplate.replaceAll("{model}",A).replaceAll("{revision}",encodeURIComponent(R)),k);let ne;const le=j instanceof I?R==="main"?K:b(A,R,k):te;let N=!1,oe;j&&(oe=await f(j,Y,le));const X=oe!==void 0;if(oe===void 0){if(i.env.allowLocalModels)if(d(K,["http:","https:"])){if(L.local_files_only)throw new Error(`\`local_files_only=true\`, but attempted to load a remote file from: ${K}.`);if(!i.env.allowRemoteModels)throw new Error(`\`env.allowRemoteModels=false\`, but attempted to load a remote file from: ${K}.`)}else try{oe=await h(Y),ne=Y}catch(me){console.warn(`Unable to load from local path "${Y}": "${me}"`)}if(oe===void 0||oe.status===404){if(L.local_files_only||!i.env.allowRemoteModels){if(F)throw Error(`\`local_files_only=true\` or \`env.allowRemoteModels=false\` and file was not found locally at "${Y}".`);return null}if(!U)throw Error(`Local file missing at "${Y}" and download aborted due to invalid model ID "${A}".`);if(oe=await h(te),oe.status!==200)return g(oe.status,te,F);ne=le}N=j&&typeof Response<"u"&&oe instanceof Response&&oe.status===200}(0,o.dispatchCallback)(L.progress_callback,{status:"download",name:A,file:k});let D;if(!(i.apis.IS_NODE_ENV&&G)){let se;L.progress_callback?X&&typeof navigator<"u"&&/firefox/i.test(navigator.userAgent)?(se=new Uint8Array(await oe.arrayBuffer()),(0,o.dispatchCallback)(L.progress_callback,{status:"progress",name:A,file:k,progress:100,loaded:se.length,total:se.length})):se=await v(oe,me=>{(0,o.dispatchCallback)(L.progress_callback,{status:"progress",name:A,file:k,...me})}):se=new Uint8Array(await oe.arrayBuffer()),D=se}if(N&&ne&&await j.match(ne)===void 0)if(D)await j.put(ne,new Response(D,{headers:oe.headers})).catch(se=>{console.warn(`Unable to add response to browser cache: ${se}.`)});else{const se=L.progress_callback?me=>(0,o.dispatchCallback)(L.progress_callback,{status:"progress",name:A,file:k,...me}):void 0;await j.put(ne,oe,se)}if((0,o.dispatchCallback)(L.progress_callback,{status:"done",name:A,file:k}),D){if(!i.apis.IS_NODE_ENV&&G)throw new Error("Cannot return path in a browser environment.");return D}if(oe instanceof c)return oe.filePath;const z=await j?.match(ne);if(z instanceof c)return z.filePath;if(z instanceof Response)return new Uint8Array(await z.arrayBuffer());if(typeof z=="string")return z;throw new Error("Unable to get model file path or buffer.")}async function T(A,k,F=!0,L={}){const G=await _(A,k,F,L,!1);return G===null?null:new TextDecoder("utf-8").decode(G)}async function M(A,k,F=!0,L={}){const G=await T(A,k,F,L);return G===null?{}:JSON.parse(G)}async function v(A,k){const F=A.headers.get("Content-Length");F===null&&console.warn("Unable to determine content-length from response headers. Will expand buffer when needed.");let L=parseInt(F??"0"),G=new Uint8Array(L),j=0;const R=A.body.getReader();async function K(){const{done:U,value:Y}=await R.read();if(U)return;const te=j+Y.length;if(te>L){L=te;const le=new Uint8Array(L);le.set(G),G=le}G.set(Y,j),j=te;const ne=j/L*100;return k({progress:ne,loaded:j,total:L}),K()}return await K(),G}function b(...A){return A=A.map((k,F)=>(F&&(k=k.replace(new RegExp("^/"),"")),F!==A.length-1&&(k=k.replace(new RegExp("/$"),"")),k)),A.join("/")}}),"./src/utils/image.js":((e,t,n)=>{n.r(t),n.d(t,{RawImage:()=>m,load_image:()=>g});var r=n("./src/utils/core.js"),s=n("./src/utils/hub.js"),i=n("./src/env.js"),o=n("./src/utils/tensor.js"),a=n("?2b25");let l,c,d;const u=i.apis.IS_BROWSER_ENV||i.apis.IS_WEBWORKER_ENV;if(u)l=(I,f)=>{if(!self.OffscreenCanvas)throw new Error("OffscreenCanvas not supported by this browser.");return new self.OffscreenCanvas(I,f)},d=self.createImageBitmap,c=self.ImageData;else if(a)d=async I=>{const _=(await I.metadata()).channels,{data:T,info:M}=await I.rotate().raw().toBuffer({resolveWithObject:!0}),v=new m(new Uint8ClampedArray(T),M.width,M.height,M.channels);return _!==void 0&&_!==M.channels&&v.convert(_),v};else throw new Error("Unable to load image processing library.");const p={0:"nearest",1:"lanczos",2:"bilinear",3:"bicubic",4:"box",5:"hamming"},h=new Map([["png","image/png"],["jpg","image/jpeg"],["jpeg","image/jpeg"],["gif","image/gif"]]);class m{constructor(f,_,T,M){this.data=f,this.width=_,this.height=T,this.channels=M}get size(){return[this.width,this.height]}static async read(f){if(f instanceof m)return f;if(typeof f=="string"||f instanceof URL)return await this.fromURL(f);if(f instanceof Blob)return await this.fromBlob(f);if(typeof HTMLCanvasElement<"u"&&f instanceof HTMLCanvasElement||typeof OffscreenCanvas<"u"&&f instanceof OffscreenCanvas)return this.fromCanvas(f);throw new Error(`Unsupported input type: ${typeof f}`)}static fromCanvas(f){if(!u)throw new Error("fromCanvas() is only supported in browser environments.");const T=f.getContext("2d").getImageData(0,0,f.width,f.height).data;return new m(T,f.width,f.height,4)}static async fromURL(f){const _=await(0,s.getFile)(f);if(_.status!==200)throw new Error(`Unable to read image from "${f}" (${_.status} ${_.statusText})`);const T=await _.blob();return this.fromBlob(T)}static async fromBlob(f){if(u){const _=await d(f),T=l(_.width,_.height).getContext("2d");return T.drawImage(_,0,0),new this(T.getImageData(0,0,_.width,_.height).data,_.width,_.height,4)}else{const _=a(await f.arrayBuffer());return await d(_)}}static fromTensor(f,_="CHW"){if(f.dims.length!==3)throw new Error(`Tensor should have 3 dimensions, but has ${f.dims.length} dimensions.`);if(_==="CHW")f=f.transpose(1,2,0);else if(_!=="HWC")throw new Error(`Unsupported channel format: ${_}`);if(!(f.data instanceof Uint8ClampedArray||f.data instanceof Uint8Array))throw new Error(`Unsupported tensor type: ${f.type}`);switch(f.dims[2]){case 1:case 2:case 3:case 4:return new m(f.data,f.dims[1],f.dims[0],f.dims[2]);default:throw new Error(`Unsupported number of channels: ${f.dims[2]}`)}}grayscale(){if(this.channels===1)return this;const f=new Uint8ClampedArray(this.width*this.height*1);switch(this.channels){case 3:case 4:for(let _=0,T=0;_<this.data.length;_+=this.channels){const M=this.data[_],v=this.data[_+1],b=this.data[_+2];f[T++]=Math.round(.2989*M+.587*v+.114*b)}break;default:throw new Error(`Conversion failed due to unsupported number of channels: ${this.channels}`)}return this._update(f,this.width,this.height,1)}rgb(){if(this.channels===3)return this;const f=new Uint8ClampedArray(this.width*this.height*3);switch(this.channels){case 1:for(let _=0,T=0;_<this.data.length;++_)f[T++]=this.data[_],f[T++]=this.data[_],f[T++]=this.data[_];break;case 4:for(let _=0,T=0;_<this.data.length;_+=4)f[T++]=this.data[_],f[T++]=this.data[_+1],f[T++]=this.data[_+2];break;default:throw new Error(`Conversion failed due to unsupported number of channels: ${this.channels}`)}return this._update(f,this.width,this.height,3)}rgba(){if(this.channels===4)return this;const f=new Uint8ClampedArray(this.width*this.height*4);switch(this.channels){case 1:for(let _=0,T=0;_<this.data.length;++_)f[T++]=this.data[_],f[T++]=this.data[_],f[T++]=this.data[_],f[T++]=255;break;case 3:for(let _=0,T=0;_<this.data.length;_+=3)f[T++]=this.data[_],f[T++]=this.data[_+1],f[T++]=this.data[_+2],f[T++]=255;break;default:throw new Error(`Conversion failed due to unsupported number of channels: ${this.channels}`)}return this._update(f,this.width,this.height,4)}putAlpha(f){if(f.width!==this.width||f.height!==this.height)throw new Error(`Expected mask size to be ${this.width}x${this.height}, but got ${f.width}x${f.height}`);if(f.channels!==1)throw new Error(`Expected mask to have 1 channel, but got ${f.channels}`);const _=this.data,T=f.data,M=this.width*this.height;if(this.channels===3){const v=new Uint8ClampedArray(M*4);for(let b=0,A=0,k=0;b<M;++b)v[k++]=_[A++],v[k++]=_[A++],v[k++]=_[A++],v[k++]=T[b];return this._update(v,this.width,this.height,4)}else if(this.channels===4){for(let v=0;v<M;++v)_[4*v+3]=T[v];return this}throw new Error(`Expected image to have 3 or 4 channels, but got ${this.channels}`)}async resize(f,_,{resample:T=2}={}){if(this.width===f&&this.height===_)return this;let M=p[T]??T;const v=(0,r.isNullishDimension)(f),b=(0,r.isNullishDimension)(_);if(v&&b)return this;if(v?f=_/this.height*this.width:b&&(_=f/this.width*this.height),u){const A=this.channels,k=this.toCanvas(),F=l(f,_).getContext("2d");return F.drawImage(k,0,0,f,_),new m(F.getImageData(0,0,f,_).data,f,_,4).convert(A)}else{let A=this.toSharp();switch(M){case"box":case"hamming":(M==="box"||M==="hamming")&&(console.warn(`Resampling method ${M} is not yet supported. Using bilinear instead.`),M="bilinear");case"nearest":case"bilinear":case"bicubic":A=A.affine([f/this.width,0,0,_/this.height],{interpolator:M});break;case"lanczos":A=A.resize({width:f,height:_,fit:"fill",kernel:"lanczos3"});break;default:throw new Error(`Resampling method ${M} is not supported.`)}return await d(A)}}async pad([f,_,T,M]){if(f=Math.max(f,0),_=Math.max(_,0),T=Math.max(T,0),M=Math.max(M,0),f===0&&_===0&&T===0&&M===0)return this;if(u){const v=this.channels,b=this.toCanvas(),A=this.width+f+_,k=this.height+T+M,F=l(A,k).getContext("2d");return F.drawImage(b,0,0,this.width,this.height,f,T,this.width,this.height),new m(F.getImageData(0,0,A,k).data,A,k,4).convert(v)}else{const v=this.toSharp().extend({left:f,right:_,top:T,bottom:M});return await d(v)}}async crop([f,_,T,M]){if(f=Math.max(f,0),_=Math.max(_,0),T=Math.min(T,this.width-1),M=Math.min(M,this.height-1),f===0&&_===0&&T===this.width-1&&M===this.height-1)return this;const v=T-f+1,b=M-_+1;if(u){const A=this.channels,k=this.toCanvas(),F=l(v,b).getContext("2d");return F.drawImage(k,f,_,v,b,0,0,v,b),new m(F.getImageData(0,0,v,b).data,v,b,4).convert(A)}else{const A=this.toSharp().extract({left:f,top:_,width:v,height:b});return await d(A)}}async center_crop(f,_){if(this.width===f&&this.height===_)return this;const T=(this.width-f)/2,M=(this.height-_)/2;if(u){const v=this.channels,b=this.toCanvas(),A=l(f,_).getContext("2d");let k=0,F=0,L=0,G=0;return T>=0?k=T:L=-T,M>=0?F=M:G=-M,A.drawImage(b,k,F,f,_,L,G,f,_),new m(A.getImageData(0,0,f,_).data,f,_,4).convert(v)}else{let v=this.toSharp();if(T>=0&&M>=0)v=v.extract({left:Math.floor(T),top:Math.floor(M),width:f,height:_});else if(T<=0&&M<=0){const b=Math.floor(-M),A=Math.floor(-T);v=v.extend({top:b,left:A,right:f-this.width-A,bottom:_-this.height-b})}else{let b=[0,0],A=0;M<0?(b[0]=Math.floor(-M),b[1]=_-this.height-b[0]):A=Math.floor(M);let k=[0,0],F=0;T<0?(k[0]=Math.floor(-T),k[1]=f-this.width-k[0]):F=Math.floor(T),v=v.extend({top:b[0],bottom:b[1],left:k[0],right:k[1]}).extract({left:F,top:A,width:f,height:_})}return await d(v)}}async toBlob(f="image/png",_=1){if(!u)throw new Error("toBlob() is only supported in browser environments.");return await this.toCanvas().convertToBlob({type:f,quality:_})}toTensor(f="CHW"){let _=new o.Tensor("uint8",new Uint8Array(this.data),[this.height,this.width,this.channels]);if(f!=="HWC")if(f==="CHW")_=_.permute(2,0,1);else throw new Error(`Unsupported channel format: ${f}`);return _}toCanvas(){if(!u)throw new Error("toCanvas() is only supported in browser environments.");const f=this.clone().rgba(),_=l(f.width,f.height),T=new c(f.data,f.width,f.height);return _.getContext("2d").putImageData(T,0,0),_}split(){const{data:f,width:_,height:T,channels:M}=this,v=f.constructor,b=f.length/M,A=Array.from({length:M},()=>new v(b));for(let k=0;k<b;++k){const F=M*k;for(let L=0;L<M;++L)A[L][k]=f[F+L]}return A.map(k=>new m(k,_,T,1))}_update(f,_,T,M=null){return this.data=f,this.width=_,this.height=T,M!==null&&(this.channels=M),this}clone(){return new m(this.data.slice(),this.width,this.height,this.channels)}convert(f){if(this.channels===f)return this;switch(f){case 1:this.grayscale();break;case 3:this.rgb();break;case 4:this.rgba();break;default:throw new Error(`Conversion failed due to unsupported number of channels: ${this.channels}`)}return this}async save(f){if(u){if(i.apis.IS_WEBWORKER_ENV)throw new Error("Unable to save an image from a Web Worker.");const _=f.split(".").pop().toLowerCase(),T=h.get(_)??"image/png",M=await this.toBlob(T);(0,r.saveBlob)(f,M)}else{if(i.apis.IS_FS_AVAILABLE)return await this.toSharp().toFile(f);throw new Error("Unable to save the image because filesystem is disabled in this environment.")}}toSharp(){if(u)throw new Error("toSharp() is only supported in server-side environments.");return a(this.data,{raw:{width:this.width,height:this.height,channels:this.channels}})}}const g=m.read.bind(m)}),"./src/utils/maths.js":((e,t,n)=>{n.r(t),n.d(t,{FFT:()=>g,bankers_round:()=>_,cos_sim:()=>l,dot:()=>a,dynamic_time_warping:()=>T,interpolate_data:()=>r,log_softmax:()=>o,magnitude:()=>c,max:()=>u,medianFilter:()=>I,min:()=>d,permute_data:()=>s,round:()=>f,softmax:()=>i});function r(M,[v,b,A],[k,F],L="bilinear",G=!1){const j=F/A,R=k/b,K=new M.constructor(k*F*v),U=b*A,Y=k*F;for(let te=0;te<k;++te)for(let ne=0;ne<F;++ne){const le=te*F+ne,N=(ne+.5)/j-.5,oe=(te+.5)/R-.5;let X=Math.floor(N),D=Math.floor(oe);const z=Math.min(X+1,A-1),se=Math.min(D+1,b-1);X=Math.max(X,0),D=Math.max(D,0);const me=N-X,$e=oe-D,ke=(1-me)*(1-$e),Be=me*(1-$e),Ce=(1-me)*$e,Z=me*$e,V=D*A,fe=se*A,Te=V+X,Ie=V+z,Ee=fe+X,De=fe+z;for(let xe=0;xe<v;++xe){const ze=xe*U;K[xe*Y+le]=ke*M[ze+Te]+Be*M[ze+Ie]+Ce*M[ze+Ee]+Z*M[ze+De]}}return K}function s(M,v,b){const A=new Array(b.length),k=new Array(b.length);for(let G=b.length-1,j=1;G>=0;--G)k[G]=j,A[G]=v[b[G]],j*=A[G];const F=b.map((G,j)=>k[b.indexOf(j)]),L=new M.constructor(M.length);for(let G=0;G<M.length;++G){let j=0;for(let R=v.length-1,K=G;R>=0;--R)j+=K%v[R]*F[R],K=Math.floor(K/v[R]);L[j]=M[G]}return[L,A]}function i(M){const v=u(M)[0],b=M.map(F=>Math.exp(F-v)),A=b.reduce((F,L)=>F+L,0);return b.map(F=>F/A)}function o(M){const v=u(M)[0];let b=0;for(let F=0;F<M.length;++F)b+=Math.exp(M[F]-v);const A=Math.log(b);return M.map(F=>F-v-A)}function a(M,v){let b=0;for(let A=0;A<M.length;++A)b+=M[A]*v[A];return b}function l(M,v){const b=a(M,v),A=c(M),k=c(v);return b/(A*k)}function c(M){return Math.sqrt(M.reduce((v,b)=>v+b*b,0))}function d(M){if(M.length===0)throw Error("Array must not be empty");let v=M[0],b=0;for(let A=1;A<M.length;++A)M[A]<v&&(v=M[A],b=A);return[v,b]}function u(M){if(M.length===0)throw Error("Array must not be empty");let v=M[0],b=0;for(let A=1;A<M.length;++A)M[A]>v&&(v=M[A],b=A);return[v,b]}function p(M){return M>0&&(M&M-1)===0}class h{constructor(v){if(this.size=v|0,this.size<=1||!p(this.size))throw new Error("FFT size must be a power of two larger than 1");this._csize=v<<1,this.table=new Float64Array(this.size*2);for(let A=0;A<this.table.length;A+=2){const k=Math.PI*A/this.size;this.table[A]=Math.cos(k),this.table[A+1]=-Math.sin(k)}let b=0;for(let A=1;this.size>A;A<<=1)++b;this._width=b%2===0?b-1:b,this._bitrev=new Int32Array(1<<this._width);for(let A=0;A<this._bitrev.length;++A){this._bitrev[A]=0;for(let k=0;k<this._width;k+=2){const F=this._width-k-2;this._bitrev[A]|=(A>>>k&3)<<F}}}createComplexArray(){return new Float64Array(this._csize)}fromComplexArray(v,b){const A=b||new Array(v.length>>>1);for(let k=0;k<v.length;k+=2)A[k>>>1]=v[k];return A}toComplexArray(v,b){const A=b||this.createComplexArray();for(let k=0;k<A.length;k+=2)A[k]=v[k>>>1],A[k+1]=0;return A}transform(v,b){if(v===b)throw new Error("Input and output buffers must be different");this._transform4(v,b,1)}realTransform(v,b){if(v===b)throw new Error("Input and output buffers must be different");this._realTransform4(v,b,1)}inverseTransform(v,b){if(v===b)throw new Error("Input and output buffers must be different");this._transform4(v,b,-1);for(let A=0;A<v.length;++A)v[A]/=this.size}_transform4(v,b,A){const k=this._csize;let L=1<<this._width,G=k/L<<1,j,R;const K=this._bitrev;if(G===4)for(j=0,R=0;j<k;j+=G,++R){const Y=K[R];this._singleTransform2(b,v,j,Y,L)}else for(j=0,R=0;j<k;j+=G,++R){const Y=K[R];this._singleTransform4(b,v,j,Y,L,A)}const U=this.table;for(L>>=2;L>=2;L>>=2){G=k/L<<1;const Y=G>>>2;for(j=0;j<k;j+=G){const te=j+Y-1;for(let ne=j,le=0;ne<te;ne+=2,le+=L){const N=ne,oe=N+Y,X=oe+Y,D=X+Y,z=v[N],se=v[N+1],me=v[oe],$e=v[oe+1],ke=v[X],Be=v[X+1],Ce=v[D],Z=v[D+1],V=U[le],fe=A*U[le+1],Te=me*V-$e*fe,Ie=me*fe+$e*V,Ee=U[2*le],De=A*U[2*le+1],xe=ke*Ee-Be*De,ze=ke*De+Be*Ee,_e=U[3*le],Le=A*U[3*le+1],qe=Ce*_e-Z*Le,Ne=Ce*Le+Z*_e,ot=z+xe,Ve=se+ze,de=z-xe,ye=se-ze,Pe=Te+qe,ve=Ie+Ne,Qe=A*(Te-qe),ct=A*(Ie-Ne);v[N]=ot+Pe,v[N+1]=Ve+ve,v[oe]=de+ct,v[oe+1]=ye-Qe,v[X]=ot-Pe,v[X+1]=Ve-ve,v[D]=de-ct,v[D+1]=ye+Qe}}}}_singleTransform2(v,b,A,k,F){const L=v[k],G=v[k+1],j=v[k+F],R=v[k+F+1];b[A]=L+j,b[A+1]=G+R,b[A+2]=L-j,b[A+3]=G-R}_singleTransform4(v,b,A,k,F,L){const G=F*2,j=F*3,R=v[k],K=v[k+1],U=v[k+F],Y=v[k+F+1],te=v[k+G],ne=v[k+G+1],le=v[k+j],N=v[k+j+1],oe=R+te,X=K+ne,D=R-te,z=K-ne,se=U+le,me=Y+N,$e=L*(U-le),ke=L*(Y-N);b[A]=oe+se,b[A+1]=X+me,b[A+2]=D+ke,b[A+3]=z-$e,b[A+4]=oe-se,b[A+5]=X-me,b[A+6]=D-ke,b[A+7]=z+$e}_realTransform4(v,b,A){const k=this._csize;let L=1<<this._width,G=k/L<<1,j,R;const K=this._bitrev;if(G===4)for(j=0,R=0;j<k;j+=G,++R){const te=K[R];this._singleRealTransform2(b,v,j,te>>>1,L>>>1)}else for(j=0,R=0;j<k;j+=G,++R){const te=K[R];this._singleRealTransform4(b,v,j,te>>>1,L>>>1,A)}const U=this.table;for(L>>=2;L>=2;L>>=2){G=k/L<<1;const te=G>>>1,ne=te>>>1,le=ne>>>1;for(j=0;j<k;j+=G)for(let N=0,oe=0;N<=le;N+=2,oe+=L){const X=j+N,D=X+ne,z=D+ne,se=z+ne,me=v[X],$e=v[X+1],ke=v[D],Be=v[D+1],Ce=v[z],Z=v[z+1],V=v[se],fe=v[se+1],Te=me,Ie=$e,Ee=U[oe],De=A*U[oe+1],xe=ke*Ee-Be*De,ze=ke*De+Be*Ee,_e=U[2*oe],Le=A*U[2*oe+1],qe=Ce*_e-Z*Le,Ne=Ce*Le+Z*_e,ot=U[3*oe],Ve=A*U[3*oe+1],de=V*ot-fe*Ve,ye=V*Ve+fe*ot,Pe=Te+qe,ve=Ie+Ne,Qe=Te-qe,ct=Ie-Ne,zt=xe+de,wt=ze+ye,on=A*(xe-de),Tn=A*(ze-ye);if(v[X]=Pe+zt,v[X+1]=ve+wt,v[D]=Qe+Tn,v[D+1]=ct-on,N===0){v[z]=Pe-zt,v[z+1]=ve-wt;continue}if(N===le)continue;const kt=j+ne-N,Pn=j+te-N;v[kt]=Qe-A*Tn,v[kt+1]=-ct-A*on,v[Pn]=Pe-A*zt,v[Pn+1]=-ve+A*wt}}const Y=k>>>1;for(let te=2;te<Y;te+=2)v[k-te]=v[te],v[k-te+1]=-v[te+1]}_singleRealTransform2(v,b,A,k,F){const L=v[k],G=v[k+F];b[A]=L+G,b[A+1]=0,b[A+2]=L-G,b[A+3]=0}_singleRealTransform4(v,b,A,k,F,L){const G=F*2,j=F*3,R=v[k],K=v[k+F],U=v[k+G],Y=v[k+j],te=R+U,ne=R-U,le=K+Y,N=L*(K-Y);b[A]=te+le,b[A+1]=0,b[A+2]=ne,b[A+3]=-N,b[A+4]=te-le,b[A+5]=0,b[A+6]=ne,b[A+7]=N}}class m{constructor(v){const b=2*(v-1),A=2*(2*v-1),k=2**Math.ceil(Math.log2(A));this.bufferSize=k,this._a=b;const F=new Float64Array(A),L=new Float64Array(k);this._chirpBuffer=new Float64Array(k),this._buffer1=new Float64Array(k),this._buffer2=new Float64Array(k),this._outBuffer1=new Float64Array(k),this._outBuffer2=new Float64Array(k);const G=-2*Math.PI/v,j=Math.cos(G),R=Math.sin(G);for(let K=0;K<A>>1;++K){const U=(K+1-v)**2/2,Y=Math.sqrt(j**2+R**2)**U,te=U*Math.atan2(R,j),ne=2*K;F[ne]=Y*Math.cos(te),F[ne+1]=Y*Math.sin(te),L[ne]=F[ne],L[ne+1]=-F[ne+1]}this._slicedChirpBuffer=F.subarray(b,A),this._f=new h(k>>1),this._f.transform(this._chirpBuffer,L)}_transform(v,b,A){const k=this._buffer1,F=this._buffer2,L=this._outBuffer1,G=this._outBuffer2,j=this._chirpBuffer,R=this._slicedChirpBuffer,K=this._a;if(A)for(let U=0;U<R.length;U+=2){const Y=U+1,te=U>>1,ne=b[te];k[U]=ne*R[U],k[Y]=ne*R[Y]}else for(let U=0;U<R.length;U+=2){const Y=U+1;k[U]=b[U]*R[U]-b[Y]*R[Y],k[Y]=b[U]*R[Y]+b[Y]*R[U]}this._f.transform(L,k);for(let U=0;U<j.length;U+=2){const Y=U+1;F[U]=L[U]*j[U]-L[Y]*j[Y],F[Y]=L[U]*j[Y]+L[Y]*j[U]}this._f.inverseTransform(G,F);for(let U=0;U<G.length;U+=2){const Y=G[U+K],te=G[U+K+1],ne=R[U],le=R[U+1];v[U]=Y*ne-te*le,v[U+1]=Y*le+te*ne}}transform(v,b){this._transform(v,b,!1)}realTransform(v,b){this._transform(v,b,!0)}}class g{constructor(v){this.fft_length=v,this.isPowerOfTwo=p(v),this.isPowerOfTwo?(this.fft=new h(v),this.outputBufferSize=2*v):(this.fft=new m(v),this.outputBufferSize=this.fft.bufferSize)}realTransform(v,b){this.fft.realTransform(v,b)}transform(v,b){this.fft.transform(v,b)}}function I(M,v){if(v%2===0||v<=0)throw new Error("Window size must be a positive odd number");const b=new M.constructor(M.length),A=new M.constructor(v),k=Math.floor(v/2);for(let F=0;F<M.length;++F){let L=0;for(let G=-k;G<=k;++G){let j=F+G;j<0?j=Math.abs(j):j>=M.length&&(j=2*(M.length-1)-j),A[L++]=M[j]}A.sort(),b[F]=A[k]}return b}function f(M,v){const b=Math.pow(10,v);return Math.round(M*b)/b}function _(M){const v=Math.round(M);return Math.abs(M)%1===.5?v%2===0?v:v-1:v}function T(M){const v=M.length,b=M[0].length,A=[v+1,b+1],k=Array.from({length:A[0]},()=>Array(A[1]).fill(1/0));k[0][0]=0;const F=Array.from({length:A[0]},()=>Array(A[1]).fill(-1));for(let K=1;K<A[1];++K)for(let U=1;U<A[0];++U){const Y=k[U-1][K-1],te=k[U-1][K],ne=k[U][K-1];let le,N;Y<te&&Y<ne?(le=Y,N=0):te<Y&&te<ne?(le=te,N=1):(le=ne,N=2),k[U][K]=M[U-1][K-1]+le,F[U][K]=N}for(let K=0;K<A[1];++K)F[0][K]=2;for(let K=0;K<A[0];++K)F[K][0]=1;let L=v,G=b,j=[],R=[];for(;L>0||G>0;)switch(j.push(L-1),R.push(G-1),F[L][G]){case 0:--L,--G;break;case 1:--L;break;case 2:--G;break;default:throw new Error(`Internal error in dynamic time warping. Unexpected trace[${L}, ${G}]. Please file a bug report.`)}return j.reverse(),R.reverse(),[j,R]}}),"./src/utils/tensor.js":((e,t,n)=>{n.r(t),n.d(t,{DataTypeMap:()=>o,Tensor:()=>a,cat:()=>b,full:()=>R,full_like:()=>K,interpolate:()=>d,interpolate_4d:()=>u,layer_norm:()=>_,matmul:()=>p,mean:()=>L,mean_pooling:()=>f,ones:()=>U,ones_like:()=>Y,permute:()=>c,quantize_embeddings:()=>oe,rand:()=>le,randn:()=>N,rfft:()=>h,slice:()=>I,stack:()=>A,std_mean:()=>F,topk:()=>m,zeros:()=>te,zeros_like:()=>ne});var r=n("./src/utils/maths.js"),s=n("./src/backends/onnx.js"),i=n("./src/ops/registry.js");const o=Object.freeze({float32:Float32Array,float16:typeof Float16Array<"u"?Float16Array:Uint16Array,float64:Float64Array,string:Array,int8:Int8Array,uint8:Uint8Array,int16:Int16Array,uint16:Uint16Array,int32:Int32Array,uint32:Uint32Array,int64:BigInt64Array,uint64:BigUint64Array,bool:Uint8Array,uint4:Uint8Array,int4:Int8Array});class a{get dims(){return this.ort_tensor.dims}set dims(D){this.ort_tensor.dims=D}get type(){return this.ort_tensor.type}get data(){return this.ort_tensor.data}get size(){return this.ort_tensor.size}get location(){return this.ort_tensor.location}ort_tensor;constructor(...D){return(0,s.isONNXTensor)(D[0])?this.ort_tensor=D[0]:this.ort_tensor=new s.Tensor(D[0],D[1],D[2]),new Proxy(this,{get:(z,se)=>{if(typeof se=="string"){let me=Number(se);if(Number.isInteger(me))return z._getitem(me)}return z[se]},set:(z,se,me)=>z[se]=me})}dispose(){this.ort_tensor.dispose()}*[Symbol.iterator](){const[D,...z]=this.dims;if(z.length>0){const se=z.reduce((me,$e)=>me*$e);for(let me=0;me<D;++me)yield this._subarray(me,se,z)}else yield*this.data}_getitem(D){const[z,...se]=this.dims;if(D=v(D,z),se.length>0){const me=se.reduce(($e,ke)=>$e*ke);return this._subarray(D,me,se)}else return new a(this.type,[this.data[D]],se)}indexOf(D){const z=this.data;for(let se=0;se<z.length;++se)if(z[se]==D)return se;return-1}_subarray(D,z,se){const me=D*z,$e=(D+1)*z,ke="subarray"in this.data?this.data.subarray(me,$e):this.data.slice(me,$e);return new a(this.type,ke,se)}item(){const D=this.data;if(D.length!==1)throw new Error(`a Tensor with ${D.length} elements cannot be converted to Scalar`);return D[0]}tolist(){return l(this.data,this.dims)}sigmoid(){return this.clone().sigmoid_()}sigmoid_(){const D=this.data;for(let z=0;z<D.length;++z)D[z]=1/(1+Math.exp(-D[z]));return this}map(D){return this.clone().map_(D)}map_(D){const z=this.data;for(let se=0;se<z.length;++se)z[se]=D(z[se],se,z);return this}mul(D){return this.clone().mul_(D)}mul_(D){const z=this.data;for(let se=0;se<z.length;++se)z[se]*=D;return this}div(D){return this.clone().div_(D)}div_(D){const z=this.data;for(let se=0;se<z.length;++se)z[se]/=D;return this}add(D){return this.clone().add_(D)}add_(D){const z=this.data;for(let se=0;se<z.length;++se)z[se]+=D;return this}sub(D){return this.clone().sub_(D)}sub_(D){const z=this.data;for(let se=0;se<z.length;++se)z[se]-=D;return this}clone(){return new a(this.type,this.data.slice(),this.dims.slice())}slice(...D){const z=[],se=[];for(let V=0;V<this.dims.length;++V){let fe=D[V];if(fe==null)se.push([0,this.dims[V]]),z.push(this.dims[V]);else if(typeof fe=="number")fe=v(fe,this.dims[V],V),se.push([fe,fe+1]);else if(Array.isArray(fe)&&fe.length===2){let[Te,Ie]=fe;if(Te=Te===null?0:v(Te,this.dims[V],V,!1),Ie=Ie===null?this.dims[V]:v(Ie,this.dims[V],V,!1),Te>Ie)throw new Error(`Invalid slice: ${fe}`);const Ee=[Math.max(Te,0),Math.min(Ie,this.dims[V])];se.push(Ee),z.push(Ee[1]-Ee[0])}else throw new Error(`Invalid slice: ${fe}`)}const me=se.map(([V,fe])=>fe-V),$e=me.reduce((V,fe)=>V*fe),ke=this.data,Be=new ke.constructor($e),Ce=this.stride();let Z=!0;for(let V=1;V<me.length;++V)if(se[V][0]!==0||se[V][1]!==this.dims[V]){Z=!1;break}if(Z){const V=se[0][0]*Ce[0],fe=se[0][1]*Ce[0];if(ArrayBuffer.isView(ke))Be.set(ke.subarray(V,fe));else if(Array.isArray(ke)){const Te=ke.slice(V,fe);for(let Ie=0;Ie<Te.length;++Ie)Be[Ie]=Te[Ie]}else throw new Error("Unsupported data type for slicing")}else for(let V=0;V<$e;++V){let fe=0;for(let Te=me.length-1,Ie=V;Te>=0;--Te){const Ee=me[Te];fe+=(Ie%Ee+se[Te][0])*Ce[Te],Ie=Math.floor(Ie/Ee)}Be[V]=ke[fe]}return new a(this.type,Be,z)}permute(...D){return c(this,D)}transpose(...D){return this.permute(...D)}sum(D=null,z=!1){return this.norm(1,D,z)}norm(D="fro",z=null,se=!1){if(D==="fro")D=2;else if(typeof D=="string")throw Error(`Unsupported norm: ${D}`);const me=this.data,$e=(Z,V)=>Z+V**D;if(z===null){const Z=me.reduce($e,0)**(1/D);return new a(this.type,[Z],[])}const[ke,Be,Ce]=k($e,this,z,se);if(D!==1)for(let Z=0;Z<Be.length;++Z)Be[Z]=Be[Z]**(1/D);return new a(ke,Be,Ce)}normalize_(D=2,z=1){z=v(z,this.dims.length);const se=this.norm(D,z,!0),me=this.data,$e=se.data;for(let ke=0;ke<me.length;++ke){let Be=0;for(let Ce=this.dims.length-1,Z=ke,V=1;Ce>=0;--Ce){const fe=this.dims[Ce];if(Ce!==z){const Te=Z%fe;Be+=Te*V,V*=this.dims[Ce]}Z=Math.floor(Z/fe)}me[ke]/=$e[Be]}return this}normalize(D=2,z=1){return this.clone().normalize_(D,z)}stride(){return G(this.dims)}squeeze(D=null){return new a(this.type,this.data,T(this.dims,D))}squeeze_(D=null){return this.dims=T(this.dims,D),this}unsqueeze(D=null){return new a(this.type,this.data,M(this.dims,D))}unsqueeze_(D=null){return this.dims=M(this.dims,D),this}flatten_(D=0,z=-1){z=(z+this.dims.length)%this.dims.length;let se=this.dims.slice(0,D),me=this.dims.slice(D,z+1),$e=this.dims.slice(z+1);return this.dims=[...se,me.reduce((ke,Be)=>ke*Be,1),...$e],this}flatten(D=0,z=-1){return this.clone().flatten_(D,z)}view(...D){let z=-1;for(let me=0;me<D.length;++me)if(D[me]===-1){if(z!==-1)throw new Error("Only one dimension can be inferred");z=me}const se=this.data;if(z!==-1){const me=D.reduce(($e,ke,Be)=>Be!==z?$e*ke:$e,1);D[z]=se.length/me}return new a(this.type,se,D)}neg_(){const D=this.data;for(let z=0;z<D.length;++z)D[z]=-D[z];return this}neg(){return this.clone().neg_()}gt(D){const z=new Uint8Array(this.data.length),se=this.data;for(let me=0;me<se.length;++me)z[me]=se[me]>D?1:0;return new a("bool",z,this.dims)}lt(D){const z=new Uint8Array(this.data.length),se=this.data;for(let me=0;me<se.length;++me)z[me]=se[me]<D?1:0;return new a("bool",z,this.dims)}clamp_(D,z){const se=this.data;for(let me=0;me<se.length;++me)se[me]=Math.min(Math.max(se[me],D),z);return this}clamp(D,z){return this.clone().clamp_(D,z)}round_(){const D=this.data;for(let z=0;z<D.length;++z)D[z]=Math.round(D[z]);return this}round(){return this.clone().round_()}mean(D=null,z=!1){return L(this,D,z)}min(D=null,z=!1){if(D===null){const ke=(0,r.min)(this.data)[0];return new a(this.type,[ke],[])}const[se,me,$e]=k((ke,Be)=>Math.min(ke,Be),this,D,z,1/0);return new a(se,me,$e)}max(D=null,z=!1){if(D===null){const ke=(0,r.max)(this.data)[0];return new a(this.type,[ke],[])}const[se,me,$e]=k((ke,Be)=>Math.max(ke,Be),this,D,z,-1/0);return new a(se,me,$e)}argmin(D=null,z=!1){if(D!==null)throw new Error("`dim !== null` not yet implemented.");const se=(0,r.min)(this.data)[1];return new a("int64",[BigInt(se)],[])}argmax(D=null,z=!1){if(D!==null)throw new Error("`dim !== null` not yet implemented.");const se=(0,r.max)(this.data)[1];return new a("int64",[BigInt(se)],[])}to(D){if(this.type===D)return this;if(!o.hasOwnProperty(D))throw new Error(`Unsupported type: ${D}`);let z;const se=["int64","uint64"].includes(this.type),me=["int64","uint64"].includes(D);return se&&!me?z=Number:!se&&me&&(["float16","float32","float64"].includes(this.type)?z=$e=>BigInt(Math.floor($e)):z=BigInt),new a(D,o[D].from(this.data,z),this.dims)}}function l(X,D){const z=X.length,se=D.reduce(($e,ke)=>$e*ke);if(z!==se)throw Error(`cannot reshape array of size ${z} into shape (${D})`);let me=X;for(let $e=D.length-1;$e>=0;$e--)me=me.reduce((ke,Be)=>{let Ce=ke[ke.length-1];return Ce.length<D[$e]?Ce.push(Be):ke.push([Be]),ke},[[]]);return me[0]}function c(X,D){const[z,se]=(0,r.permute_data)(X.data,X.dims,D);return new a(X.type,z,se)}function d(X,[D,z],se="bilinear",me=!1){const $e=X.dims.at(-3)??1,ke=X.dims.at(-2),Be=X.dims.at(-1);let Ce=(0,r.interpolate_data)(X.data,[$e,ke,Be],[D,z],se,me);return new a(X.type,Ce,[$e,D,z])}async function u(X,{size:D=null,mode:z="bilinear"}={}){if(X.dims.length!==4)throw new Error("`interpolate_4d` currently only supports 4D input.");if(!D)throw new Error("`interpolate_4d` requires a `size` argument.");let se;if(D.length===2)se=[...X.dims.slice(0,2),...D];else if(D.length===3)se=[X.dims[0],...D];else if(D.length===4)se=D;else throw new Error("`size` must be of length 2, 3, or 4.");let me;if(z==="nearest")me=await i.TensorOpRegistry.nearest_interpolate_4d;else if(z==="bilinear")me=await i.TensorOpRegistry.bilinear_interpolate_4d;else if(z==="bicubic")me=await i.TensorOpRegistry.bicubic_interpolate_4d;else throw new Error(`Unsupported mode: ${z}`);const $e=new a("int64",new BigInt64Array(se.map(BigInt)),[se.length]);return await me({x:X,s:$e})}async function p(X,D){return await(await i.TensorOpRegistry.matmul)({a:X,b:D})}async function h(X,D){return await(await i.TensorOpRegistry.rfft)({x:X,a:D})}async function m(X,D){const z=await i.TensorOpRegistry.top_k;return D==null?D=X.dims.at(-1):D=Math.min(D,X.dims.at(-1)),await z({x:X,k:new a("int64",[BigInt(D)],[1])})}const g=X=>new a("int64",X,[X.length]);async function I(X,D,z,se,me){return await(await i.TensorOpRegistry.slice)({x:X,s:g(D),e:g(z),a:g(se),t:g(me??new Array(se.length).fill(1))})}function f(X,D){const z=X.data,se=D.data,me=[X.dims[0],X.dims[2]],$e=new z.constructor(me[0]*me[1]),[ke,Be,Ce]=X.dims;let Z=0;for(let V=0;V<ke;++V){const fe=V*Ce*Be;for(let Te=0;Te<Ce;++Te){let Ie=0,Ee=0;const De=V*Be,xe=fe+Te;for(let _e=0;_e<Be;++_e){const Le=Number(se[De+_e]);Ee+=Le,Ie+=z[xe+_e*Ce]*Le}const ze=Ie/Ee;$e[Z++]=ze}}return new a(X.type,$e,me)}function _(X,D,{eps:z=1e-5}={}){if(X.dims.length!==2)throw new Error("`layer_norm` currently only supports 2D input.");const[se,me]=X.dims;if(D.length!==1&&D[0]!==me)throw new Error("`normalized_shape` must be a 1D array with shape `[input.dims[1]]`.");const[$e,ke]=F(X,1,0,!0),Be=$e.data,Ce=ke.data,Z=X.data,V=new Z.constructor(Z.length);for(let fe=0;fe<se;++fe){const Te=fe*me;for(let Ie=0;Ie<me;++Ie){const Ee=Te+Ie;V[Ee]=(Z[Ee]-Ce[fe])/(Be[fe]+z)}}return new a(X.type,V,X.dims)}function T(X,D){return X=X.slice(),D===null?X=X.filter(z=>z!==1):typeof D=="number"?X[D]===1&&X.splice(D,1):Array.isArray(D)&&(X=X.filter((z,se)=>z!==1||!D.includes(se))),X}function M(X,D){return D=v(D,X.length+1),X=X.slice(),X.splice(D,0,1),X}function v(X,D,z=null,se=!0){if(X<-D||X>=D){if(se)throw new Error(`IndexError: index ${X} is out of bounds for dimension${z===null?"":" "+z} with size ${D}`);return X<-D?0:D}return X<0&&(X=(X%D+D)%D),X}function b(X,D=0){D=v(D,X[0].dims.length);const z=X[0].dims.slice();z[D]=X.reduce((ke,Be)=>ke+Be.dims[D],0);const se=z.reduce((ke,Be)=>ke*Be,1),me=new X[0].data.constructor(se),$e=X[0].type;if(D===0){let ke=0;for(const Be of X){const Ce=Be.data;me.set(Ce,ke),ke+=Ce.length}}else{let ke=0;for(let Be=0;Be<X.length;++Be){const{data:Ce,dims:Z}=X[Be];for(let V=0;V<Ce.length;++V){let fe=0;for(let Te=Z.length-1,Ie=V,Ee=1;Te>=0;--Te){const De=Z[Te];let xe=Ie%De;Te===D&&(xe+=ke),fe+=xe*Ee,Ee*=z[Te],Ie=Math.floor(Ie/De)}me[fe]=Ce[V]}ke+=Z[D]}}return new a($e,me,z)}function A(X,D=0){return b(X.map(z=>z.unsqueeze(D)),D)}function k(X,D,z=null,se=!1,me=null){const $e=D.data,ke=D.dims;z=v(z,ke.length);const Be=ke.slice();Be[z]=1;const Ce=new $e.constructor($e.length/ke[z]);me!==null&&Ce.fill(me);for(let Z=0;Z<$e.length;++Z){let V=0;for(let fe=ke.length-1,Te=Z,Ie=1;fe>=0;--fe){const Ee=ke[fe];if(fe!==z){const De=Te%Ee;V+=De*Ie,Ie*=Be[fe]}Te=Math.floor(Te/Ee)}Ce[V]=X(Ce[V],$e[Z],Z,V)}return se||Be.splice(z,1),[D.type,Ce,Be]}function F(X,D=null,z=1,se=!1){const me=X.data,$e=X.dims;if(D===null){const Ie=me.reduce((ze,_e)=>ze+_e,0)/me.length,Ee=Math.sqrt(me.reduce((ze,_e)=>ze+(_e-Ie)**2,0)/(me.length-z)),De=new a(X.type,[Ie],[]);return[new a(X.type,[Ee],[]),De]}D=v(D,$e.length);const ke=L(X,D,se),Be=ke.data,[Ce,Z,V]=k((Te,Ie,Ee,De)=>Te+(Ie-Be[De])**2,X,D,se);for(let Te=0;Te<Z.length;++Te)Z[Te]=Math.sqrt(Z[Te]/($e[D]-z));return[new a(Ce,Z,V),ke]}function L(X,D=null,z=!1){const se=X.dims,me=X.data;if(D===null){const Ce=me.reduce((Z,V)=>Z+V,0);return new a(X.type,[Ce/me.length],[])}D=v(D,se.length);const[$e,ke,Be]=k((Ce,Z)=>Ce+Z,X,D,z);if(se[D]!==1)for(let Ce=0;Ce<ke.length;++Ce)ke[Ce]/=se[D];return new a($e,ke,Be)}function G(X){const D=new Array(X.length);for(let z=X.length-1,se=1;z>=0;--z)D[z]=se,se*=X[z];return D}function j(X,D,z,se){const me=X.reduce(($e,ke)=>$e*ke,1);return new a(z,new se(me).fill(D),X)}function R(X,D){let z,se;if(typeof D=="number")z="float32",se=Float32Array;else if(typeof D=="bigint")z="int64",se=BigInt64Array;else if(typeof D=="boolean")z="bool",se=Uint8Array;else throw new Error(`Unsupported data type: ${typeof D}`);return j(X,D,z,se)}function K(X,D){return R(X.dims,D)}function U(X){return j(X,1n,"int64",BigInt64Array)}function Y(X){return U(X.dims)}function te(X){return j(X,0n,"int64",BigInt64Array)}function ne(X){return te(X.dims)}function le(X){const D=X.reduce((z,se)=>z*se,1);return new a("float32",Float32Array.from({length:D},()=>Math.random()),X)}function N(X){const D=X.reduce((se,me)=>se*me,1);function z(){const se=1-Math.random(),me=1-Math.random();return Math.sqrt(-2*Math.log(se))*Math.cos(2*Math.PI*me)}return new a("float32",Float32Array.from({length:D},()=>z()),X)}function oe(X,D){if(X.dims.length!==2)throw new Error("The tensor must have 2 dimensions");if(X.dims.at(-1)%8!==0)throw new Error("The last dimension of the tensor must be a multiple of 8");if(!["binary","ubinary"].includes(D))throw new Error("The precision must be either 'binary' or 'ubinary'");const z=D==="binary",se=z?"int8":"uint8",me=z?Int8Array:Uint8Array,$e=X.data,ke=new me($e.length/8);for(let Be=0;Be<$e.length;++Be){const Ce=$e[Be]>0?1:0,Z=Math.floor(Be/8),V=Be%8;ke[Z]|=Ce<<7-V,z&&V===0&&(ke[Z]-=128)}return new a(se,ke,[X.dims[0],X.dims[1]/8])}}),"./src/utils/video.js":((e,t,n)=>{n.r(t),n.d(t,{RawVideo:()=>o,RawVideoFrame:()=>i,load_video:()=>a});var r=n("./src/utils/image.js"),s=n("./src/env.js");class i{constructor(c,d){this.image=c,this.timestamp=d}}class o{constructor(c,d){c.length>0&&c[0]instanceof r.RawImage&&(c=c.map((u,p)=>new i(u,(p+1)/(c.length+1)*d))),this.frames=c,this.duration=d}get width(){return this.frames[0].image.width}get height(){return this.frames[0].image.height}get fps(){return this.frames.length/this.duration}}async function a(l,{num_frames:c=null,fps:d=null}={}){if(!s.apis.IS_BROWSER_ENV)throw new Error("`load_video` is currently only supported in browser environments.");if(c==null&&d==null)throw new Error("Either num_frames or fps must be provided.");const u=[],p=document.createElement("video");if(p.crossOrigin="anonymous",p.muted=!0,typeof l=="string")p.src=l;else if(l instanceof Blob)p.src=URL.createObjectURL(l);else if(l instanceof HTMLVideoElement)p.src=l.src;else throw new Error("Invalid URL or video element provided.");if(await new Promise(T=>p.onloadedmetadata=T),p.seekable.start(0)===p.seekable.end(0)){const M=await(await fetch(p.src)).blob();p.src=URL.createObjectURL(M),await new Promise(v=>p.onloadedmetadata=v)}const h=p.duration;let m,g;c!=null?(m=c,g=c===1?0:h/(c-1)):(g=1/d,m=Math.floor(h/g));let I=[];for(let T=0;T<m;++T)I.push(c===1?h/2:T*g);const f=document.createElement("canvas");f.width=p.videoWidth,f.height=p.videoHeight;const _=f.getContext("2d",{willReadFrequently:!0});for(const T of I){p.currentTime=T,await new Promise(A=>{p.onseeked=A}),_.drawImage(p,0,0,f.width,f.height);const M=_.getImageData(0,0,f.width,f.height),v=new r.RawImage(M.data,f.width,f.height,4),b=new i(v,T);u.push(b)}return p.remove(),new o(u,h)}})},YT={};function Qt(e){var t=YT[e];if(t!==void 0)return t.exports;var n=YT[e]={exports:{}};return CV[e](n,n.exports,Qt),n.exports}(()=>{var e=Object.getPrototypeOf?n=>Object.getPrototypeOf(n):n=>n.__proto__,t;Qt.t=function(n,r){if(r&1&&(n=this(n)),r&8||typeof n=="object"&&n&&(r&4&&n.__esModule||r&16&&typeof n.then=="function"))return n;var s=Object.create(null);Qt.r(s);var i={};t=t||[null,e({}),e([]),e(e)];for(var o=r&2&&n;typeof o=="object"&&!~t.indexOf(o);o=e(o))Object.getOwnPropertyNames(o).forEach(a=>i[a]=()=>n[a]);return i.default=()=>n,Qt.d(s,i),s}})();Qt.d=(e,t)=>{for(var n in t)Qt.o(t,n)&&!Qt.o(e,n)&&Object.defineProperty(e,n,{enumerable:!0,get:t[n]})};Qt.o=(e,t)=>Object.prototype.hasOwnProperty.call(e,t);Qt.r=e=>{typeof Symbol<"u"&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(e,"__esModule",{value:!0})};var y={};(()=>{Qt.r(y),Qt.d(y,{ASTFeatureExtractor:()=>u.ASTFeatureExtractor,ASTForAudioClassification:()=>n.ASTForAudioClassification,ASTModel:()=>n.ASTModel,ASTPreTrainedModel:()=>n.ASTPreTrainedModel,AlbertForMaskedLM:()=>n.AlbertForMaskedLM,AlbertForQuestionAnswering:()=>n.AlbertForQuestionAnswering,AlbertForSequenceClassification:()=>n.AlbertForSequenceClassification,AlbertModel:()=>n.AlbertModel,AlbertPreTrainedModel:()=>n.AlbertPreTrainedModel,AlbertTokenizer:()=>r.AlbertTokenizer,ArceeForCausalLM:()=>n.ArceeForCausalLM,ArceeModel:()=>n.ArceeModel,ArceePreTrainedModel:()=>n.ArceePreTrainedModel,AudioClassificationPipeline:()=>t.AudioClassificationPipeline,AutoConfig:()=>s.AutoConfig,AutoFeatureExtractor:()=>p.AutoFeatureExtractor,AutoImageProcessor:()=>g.AutoImageProcessor,AutoModel:()=>n.AutoModel,AutoModelForAudioClassification:()=>n.AutoModelForAudioClassification,AutoModelForAudioFrameClassification:()=>n.AutoModelForAudioFrameClassification,AutoModelForAudioTextToText:()=>n.AutoModelForAudioTextToText,AutoModelForCTC:()=>n.AutoModelForCTC,AutoModelForCausalLM:()=>n.AutoModelForCausalLM,AutoModelForDepthEstimation:()=>n.AutoModelForDepthEstimation,AutoModelForDocumentQuestionAnswering:()=>n.AutoModelForDocumentQuestionAnswering,AutoModelForImageClassification:()=>n.AutoModelForImageClassification,AutoModelForImageFeatureExtraction:()=>n.AutoModelForImageFeatureExtraction,AutoModelForImageMatting:()=>n.AutoModelForImageMatting,AutoModelForImageSegmentation:()=>n.AutoModelForImageSegmentation,AutoModelForImageTextToText:()=>n.AutoModelForImageTextToText,AutoModelForImageToImage:()=>n.AutoModelForImageToImage,AutoModelForMaskGeneration:()=>n.AutoModelForMaskGeneration,AutoModelForMaskedLM:()=>n.AutoModelForMaskedLM,AutoModelForNormalEstimation:()=>n.AutoModelForNormalEstimation,AutoModelForObjectDetection:()=>n.AutoModelForObjectDetection,AutoModelForPoseEstimation:()=>n.AutoModelForPoseEstimation,AutoModelForQuestionAnswering:()=>n.AutoModelForQuestionAnswering,AutoModelForSemanticSegmentation:()=>n.AutoModelForSemanticSegmentation,AutoModelForSeq2SeqLM:()=>n.AutoModelForSeq2SeqLM,AutoModelForSequenceClassification:()=>n.AutoModelForSequenceClassification,AutoModelForSpeechSeq2Seq:()=>n.AutoModelForSpeechSeq2Seq,AutoModelForTextToSpectrogram:()=>n.AutoModelForTextToSpectrogram,AutoModelForTextToWaveform:()=>n.AutoModelForTextToWaveform,AutoModelForTokenClassification:()=>n.AutoModelForTokenClassification,AutoModelForUniversalSegmentation:()=>n.AutoModelForUniversalSegmentation,AutoModelForVision2Seq:()=>n.AutoModelForVision2Seq,AutoModelForXVector:()=>n.AutoModelForXVector,AutoModelForZeroShotObjectDetection:()=>n.AutoModelForZeroShotObjectDetection,AutoProcessor:()=>_.AutoProcessor,AutoTokenizer:()=>r.AutoTokenizer,AutomaticSpeechRecognitionPipeline:()=>t.AutomaticSpeechRecognitionPipeline,BackgroundRemovalPipeline:()=>t.BackgroundRemovalPipeline,BartForConditionalGeneration:()=>n.BartForConditionalGeneration,BartForSequenceClassification:()=>n.BartForSequenceClassification,BartModel:()=>n.BartModel,BartPretrainedModel:()=>n.BartPretrainedModel,BartTokenizer:()=>r.BartTokenizer,BaseModelOutput:()=>n.BaseModelOutput,BaseStreamer:()=>T.BaseStreamer,BeitFeatureExtractor:()=>m.BeitFeatureExtractor,BeitForImageClassification:()=>n.BeitForImageClassification,BeitModel:()=>n.BeitModel,BeitPreTrainedModel:()=>n.BeitPreTrainedModel,BertForMaskedLM:()=>n.BertForMaskedLM,BertForQuestionAnswering:()=>n.BertForQuestionAnswering,BertForSequenceClassification:()=>n.BertForSequenceClassification,BertForTokenClassification:()=>n.BertForTokenClassification,BertModel:()=>n.BertModel,BertPreTrainedModel:()=>n.BertPreTrainedModel,BertTokenizer:()=>r.BertTokenizer,BitImageProcessor:()=>m.BitImageProcessor,BlenderbotForConditionalGeneration:()=>n.BlenderbotForConditionalGeneration,BlenderbotModel:()=>n.BlenderbotModel,BlenderbotPreTrainedModel:()=>n.BlenderbotPreTrainedModel,BlenderbotSmallForConditionalGeneration:()=>n.BlenderbotSmallForConditionalGeneration,BlenderbotSmallModel:()=>n.BlenderbotSmallModel,BlenderbotSmallPreTrainedModel:()=>n.BlenderbotSmallPreTrainedModel,BlenderbotSmallTokenizer:()=>r.BlenderbotSmallTokenizer,BlenderbotTokenizer:()=>r.BlenderbotTokenizer,BloomForCausalLM:()=>n.BloomForCausalLM,BloomModel:()=>n.BloomModel,BloomPreTrainedModel:()=>n.BloomPreTrainedModel,BloomTokenizer:()=>r.BloomTokenizer,CLIPFeatureExtractor:()=>m.CLIPFeatureExtractor,CLIPImageProcessor:()=>m.CLIPImageProcessor,CLIPModel:()=>n.CLIPModel,CLIPPreTrainedModel:()=>n.CLIPPreTrainedModel,CLIPSegForImageSegmentation:()=>n.CLIPSegForImageSegmentation,CLIPSegModel:()=>n.CLIPSegModel,CLIPSegPreTrainedModel:()=>n.CLIPSegPreTrainedModel,CLIPTextModel:()=>n.CLIPTextModel,CLIPTextModelWithProjection:()=>n.CLIPTextModelWithProjection,CLIPTokenizer:()=>r.CLIPTokenizer,CLIPVisionModel:()=>n.CLIPVisionModel,CLIPVisionModelWithProjection:()=>n.CLIPVisionModelWithProjection,CamembertForMaskedLM:()=>n.CamembertForMaskedLM,CamembertForQuestionAnswering:()=>n.CamembertForQuestionAnswering,CamembertForSequenceClassification:()=>n.CamembertForSequenceClassification,CamembertForTokenClassification:()=>n.CamembertForTokenClassification,CamembertModel:()=>n.CamembertModel,CamembertPreTrainedModel:()=>n.CamembertPreTrainedModel,CamembertTokenizer:()=>r.CamembertTokenizer,CausalLMOutput:()=>n.CausalLMOutput,CausalLMOutputWithPast:()=>n.CausalLMOutputWithPast,ChineseCLIPFeatureExtractor:()=>m.ChineseCLIPFeatureExtractor,ChineseCLIPModel:()=>n.ChineseCLIPModel,ChineseCLIPPreTrainedModel:()=>n.ChineseCLIPPreTrainedModel,ClapAudioModelWithProjection:()=>n.ClapAudioModelWithProjection,ClapFeatureExtractor:()=>u.ClapFeatureExtractor,ClapModel:()=>n.ClapModel,ClapPreTrainedModel:()=>n.ClapPreTrainedModel,ClapTextModelWithProjection:()=>n.ClapTextModelWithProjection,ClassifierFreeGuidanceLogitsProcessor:()=>v.ClassifierFreeGuidanceLogitsProcessor,CodeGenForCausalLM:()=>n.CodeGenForCausalLM,CodeGenModel:()=>n.CodeGenModel,CodeGenPreTrainedModel:()=>n.CodeGenPreTrainedModel,CodeGenTokenizer:()=>r.CodeGenTokenizer,CodeLlamaTokenizer:()=>r.CodeLlamaTokenizer,CohereForCausalLM:()=>n.CohereForCausalLM,CohereModel:()=>n.CohereModel,CoherePreTrainedModel:()=>n.CoherePreTrainedModel,CohereTokenizer:()=>r.CohereTokenizer,ConvBertForMaskedLM:()=>n.ConvBertForMaskedLM,ConvBertForQuestionAnswering:()=>n.ConvBertForQuestionAnswering,ConvBertForSequenceClassification:()=>n.ConvBertForSequenceClassification,ConvBertForTokenClassification:()=>n.ConvBertForTokenClassification,ConvBertModel:()=>n.ConvBertModel,ConvBertPreTrainedModel:()=>n.ConvBertPreTrainedModel,ConvBertTokenizer:()=>r.ConvBertTokenizer,ConvNextFeatureExtractor:()=>m.ConvNextFeatureExtractor,ConvNextForImageClassification:()=>n.ConvNextForImageClassification,ConvNextImageProcessor:()=>m.ConvNextImageProcessor,ConvNextModel:()=>n.ConvNextModel,ConvNextPreTrainedModel:()=>n.ConvNextPreTrainedModel,ConvNextV2ForImageClassification:()=>n.ConvNextV2ForImageClassification,ConvNextV2Model:()=>n.ConvNextV2Model,ConvNextV2PreTrainedModel:()=>n.ConvNextV2PreTrainedModel,DFineForObjectDetection:()=>n.DFineForObjectDetection,DFineModel:()=>n.DFineModel,DFinePreTrainedModel:()=>n.DFinePreTrainedModel,DINOv3ConvNextModel:()=>n.DINOv3ConvNextModel,DINOv3ConvNextPreTrainedModel:()=>n.DINOv3ConvNextPreTrainedModel,DINOv3ViTImageProcessor:()=>m.DINOv3ViTImageProcessor,DINOv3ViTModel:()=>n.DINOv3ViTModel,DINOv3ViTPreTrainedModel:()=>n.DINOv3ViTPreTrainedModel,DPTFeatureExtractor:()=>m.DPTFeatureExtractor,DPTForDepthEstimation:()=>n.DPTForDepthEstimation,DPTImageProcessor:()=>m.DPTImageProcessor,DPTModel:()=>n.DPTModel,DPTPreTrainedModel:()=>n.DPTPreTrainedModel,DacDecoderModel:()=>n.DacDecoderModel,DacDecoderOutput:()=>n.DacDecoderOutput,DacEncoderModel:()=>n.DacEncoderModel,DacEncoderOutput:()=>n.DacEncoderOutput,DacFeatureExtractor:()=>u.DacFeatureExtractor,DacModel:()=>n.DacModel,DacPreTrainedModel:()=>n.DacPreTrainedModel,DataTypeMap:()=>l.DataTypeMap,DebertaForMaskedLM:()=>n.DebertaForMaskedLM,DebertaForQuestionAnswering:()=>n.DebertaForQuestionAnswering,DebertaForSequenceClassification:()=>n.DebertaForSequenceClassification,DebertaForTokenClassification:()=>n.DebertaForTokenClassification,DebertaModel:()=>n.DebertaModel,DebertaPreTrainedModel:()=>n.DebertaPreTrainedModel,DebertaTokenizer:()=>r.DebertaTokenizer,DebertaV2ForMaskedLM:()=>n.DebertaV2ForMaskedLM,DebertaV2ForQuestionAnswering:()=>n.DebertaV2ForQuestionAnswering,DebertaV2ForSequenceClassification:()=>n.DebertaV2ForSequenceClassification,DebertaV2ForTokenClassification:()=>n.DebertaV2ForTokenClassification,DebertaV2Model:()=>n.DebertaV2Model,DebertaV2PreTrainedModel:()=>n.DebertaV2PreTrainedModel,DebertaV2Tokenizer:()=>r.DebertaV2Tokenizer,DecisionTransformerModel:()=>n.DecisionTransformerModel,DecisionTransformerPreTrainedModel:()=>n.DecisionTransformerPreTrainedModel,DeiTFeatureExtractor:()=>m.DeiTFeatureExtractor,DeiTForImageClassification:()=>n.DeiTForImageClassification,DeiTImageProcessor:()=>m.DeiTImageProcessor,DeiTModel:()=>n.DeiTModel,DeiTPreTrainedModel:()=>n.DeiTPreTrainedModel,DepthAnythingForDepthEstimation:()=>n.DepthAnythingForDepthEstimation,DepthAnythingPreTrainedModel:()=>n.DepthAnythingPreTrainedModel,DepthEstimationPipeline:()=>t.DepthEstimationPipeline,DepthProForDepthEstimation:()=>n.DepthProForDepthEstimation,DepthProPreTrainedModel:()=>n.DepthProPreTrainedModel,DetrFeatureExtractor:()=>m.DetrFeatureExtractor,DetrForObjectDetection:()=>n.DetrForObjectDetection,DetrForSegmentation:()=>n.DetrForSegmentation,DetrImageProcessor:()=>m.DetrImageProcessor,DetrModel:()=>n.DetrModel,DetrObjectDetectionOutput:()=>n.DetrObjectDetectionOutput,DetrPreTrainedModel:()=>n.DetrPreTrainedModel,DetrSegmentationOutput:()=>n.DetrSegmentationOutput,Dinov2ForImageClassification:()=>n.Dinov2ForImageClassification,Dinov2Model:()=>n.Dinov2Model,Dinov2PreTrainedModel:()=>n.Dinov2PreTrainedModel,Dinov2WithRegistersForImageClassification:()=>n.Dinov2WithRegistersForImageClassification,Dinov2WithRegistersModel:()=>n.Dinov2WithRegistersModel,Dinov2WithRegistersPreTrainedModel:()=>n.Dinov2WithRegistersPreTrainedModel,DistilBertForMaskedLM:()=>n.DistilBertForMaskedLM,DistilBertForQuestionAnswering:()=>n.DistilBertForQuestionAnswering,DistilBertForSequenceClassification:()=>n.DistilBertForSequenceClassification,DistilBertForTokenClassification:()=>n.DistilBertForTokenClassification,DistilBertModel:()=>n.DistilBertModel,DistilBertPreTrainedModel:()=>n.DistilBertPreTrainedModel,DistilBertTokenizer:()=>r.DistilBertTokenizer,DocumentQuestionAnsweringPipeline:()=>t.DocumentQuestionAnsweringPipeline,DonutFeatureExtractor:()=>m.DonutFeatureExtractor,DonutImageProcessor:()=>m.DonutImageProcessor,DonutSwinModel:()=>n.DonutSwinModel,DonutSwinPreTrainedModel:()=>n.DonutSwinPreTrainedModel,EdgeTamModel:()=>n.EdgeTamModel,EfficientNetForImageClassification:()=>n.EfficientNetForImageClassification,EfficientNetImageProcessor:()=>m.EfficientNetImageProcessor,EfficientNetModel:()=>n.EfficientNetModel,EfficientNetPreTrainedModel:()=>n.EfficientNetPreTrainedModel,ElectraForMaskedLM:()=>n.ElectraForMaskedLM,ElectraForQuestionAnswering:()=>n.ElectraForQuestionAnswering,ElectraForSequenceClassification:()=>n.ElectraForSequenceClassification,ElectraForTokenClassification:()=>n.ElectraForTokenClassification,ElectraModel:()=>n.ElectraModel,ElectraPreTrainedModel:()=>n.ElectraPreTrainedModel,ElectraTokenizer:()=>r.ElectraTokenizer,EncodecFeatureExtractor:()=>u.EncodecFeatureExtractor,EosTokenCriteria:()=>M.EosTokenCriteria,Ernie4_5ForCausalLM:()=>n.Ernie4_5ForCausalLM,Ernie4_5Model:()=>n.Ernie4_5Model,Ernie4_5PreTrainedModel:()=>n.Ernie4_5PreTrainedModel,EsmForMaskedLM:()=>n.EsmForMaskedLM,EsmForSequenceClassification:()=>n.EsmForSequenceClassification,EsmForTokenClassification:()=>n.EsmForTokenClassification,EsmModel:()=>n.EsmModel,EsmPreTrainedModel:()=>n.EsmPreTrainedModel,EsmTokenizer:()=>r.EsmTokenizer,ExaoneForCausalLM:()=>n.ExaoneForCausalLM,ExaoneModel:()=>n.ExaoneModel,ExaonePreTrainedModel:()=>n.ExaonePreTrainedModel,FFT:()=>c.FFT,FalconForCausalLM:()=>n.FalconForCausalLM,FalconModel:()=>n.FalconModel,FalconPreTrainedModel:()=>n.FalconPreTrainedModel,FalconTokenizer:()=>r.FalconTokenizer,FastViTForImageClassification:()=>n.FastViTForImageClassification,FastViTModel:()=>n.FastViTModel,FastViTPreTrainedModel:()=>n.FastViTPreTrainedModel,FeatureExtractionPipeline:()=>t.FeatureExtractionPipeline,FeatureExtractor:()=>d.FeatureExtractor,FillMaskPipeline:()=>t.FillMaskPipeline,Florence2ForConditionalGeneration:()=>n.Florence2ForConditionalGeneration,Florence2PreTrainedModel:()=>n.Florence2PreTrainedModel,Florence2Processor:()=>f.Florence2Processor,ForcedBOSTokenLogitsProcessor:()=>v.ForcedBOSTokenLogitsProcessor,ForcedEOSTokenLogitsProcessor:()=>v.ForcedEOSTokenLogitsProcessor,GLPNFeatureExtractor:()=>m.GLPNFeatureExtractor,GLPNForDepthEstimation:()=>n.GLPNForDepthEstimation,GLPNModel:()=>n.GLPNModel,GLPNPreTrainedModel:()=>n.GLPNPreTrainedModel,GPT2LMHeadModel:()=>n.GPT2LMHeadModel,GPT2Model:()=>n.GPT2Model,GPT2PreTrainedModel:()=>n.GPT2PreTrainedModel,GPT2Tokenizer:()=>r.GPT2Tokenizer,GPTBigCodeForCausalLM:()=>n.GPTBigCodeForCausalLM,GPTBigCodeModel:()=>n.GPTBigCodeModel,GPTBigCodePreTrainedModel:()=>n.GPTBigCodePreTrainedModel,GPTJForCausalLM:()=>n.GPTJForCausalLM,GPTJModel:()=>n.GPTJModel,GPTJPreTrainedModel:()=>n.GPTJPreTrainedModel,GPTNeoForCausalLM:()=>n.GPTNeoForCausalLM,GPTNeoModel:()=>n.GPTNeoModel,GPTNeoPreTrainedModel:()=>n.GPTNeoPreTrainedModel,GPTNeoXForCausalLM:()=>n.GPTNeoXForCausalLM,GPTNeoXModel:()=>n.GPTNeoXModel,GPTNeoXPreTrainedModel:()=>n.GPTNeoXPreTrainedModel,GPTNeoXTokenizer:()=>r.GPTNeoXTokenizer,Gemma2ForCausalLM:()=>n.Gemma2ForCausalLM,Gemma2Model:()=>n.Gemma2Model,Gemma2PreTrainedModel:()=>n.Gemma2PreTrainedModel,Gemma3ForCausalLM:()=>n.Gemma3ForCausalLM,Gemma3Model:()=>n.Gemma3Model,Gemma3PreTrainedModel:()=>n.Gemma3PreTrainedModel,Gemma3nAudioFeatureExtractor:()=>u.Gemma3nAudioFeatureExtractor,Gemma3nForConditionalGeneration:()=>n.Gemma3nForConditionalGeneration,Gemma3nPreTrainedModel:()=>n.Gemma3nPreTrainedModel,Gemma3nProcessor:()=>f.Gemma3nProcessor,GemmaForCausalLM:()=>n.GemmaForCausalLM,GemmaModel:()=>n.GemmaModel,GemmaPreTrainedModel:()=>n.GemmaPreTrainedModel,GemmaTokenizer:()=>r.GemmaTokenizer,GlmForCausalLM:()=>n.GlmForCausalLM,GlmModel:()=>n.GlmModel,GlmPreTrainedModel:()=>n.GlmPreTrainedModel,GraniteForCausalLM:()=>n.GraniteForCausalLM,GraniteModel:()=>n.GraniteModel,GraniteMoeHybridForCausalLM:()=>n.GraniteMoeHybridForCausalLM,GraniteMoeHybridModel:()=>n.GraniteMoeHybridModel,GraniteMoeHybridPreTrainedModel:()=>n.GraniteMoeHybridPreTrainedModel,GranitePreTrainedModel:()=>n.GranitePreTrainedModel,Grok1Tokenizer:()=>r.Grok1Tokenizer,GroundingDinoForObjectDetection:()=>n.GroundingDinoForObjectDetection,GroundingDinoImageProcessor:()=>m.GroundingDinoImageProcessor,GroundingDinoPreTrainedModel:()=>n.GroundingDinoPreTrainedModel,GroundingDinoProcessor:()=>f.GroundingDinoProcessor,GroupViTModel:()=>n.GroupViTModel,GroupViTPreTrainedModel:()=>n.GroupViTPreTrainedModel,HeliumForCausalLM:()=>n.HeliumForCausalLM,HeliumModel:()=>n.HeliumModel,HeliumPreTrainedModel:()=>n.HeliumPreTrainedModel,HerbertTokenizer:()=>r.HerbertTokenizer,HieraForImageClassification:()=>n.HieraForImageClassification,HieraModel:()=>n.HieraModel,HieraPreTrainedModel:()=>n.HieraPreTrainedModel,HubertForCTC:()=>n.HubertForCTC,HubertForSequenceClassification:()=>n.HubertForSequenceClassification,HubertModel:()=>n.HubertModel,HubertPreTrainedModel:()=>n.HubertPreTrainedModel,IJepaForImageClassification:()=>n.IJepaForImageClassification,IJepaModel:()=>n.IJepaModel,IJepaPreTrainedModel:()=>n.IJepaPreTrainedModel,Idefics3ForConditionalGeneration:()=>n.Idefics3ForConditionalGeneration,Idefics3ImageProcessor:()=>m.Idefics3ImageProcessor,Idefics3PreTrainedModel:()=>n.Idefics3PreTrainedModel,Idefics3Processor:()=>f.Idefics3Processor,ImageClassificationPipeline:()=>t.ImageClassificationPipeline,ImageFeatureExtractionPipeline:()=>t.ImageFeatureExtractionPipeline,ImageFeatureExtractor:()=>u.ImageFeatureExtractor,ImageMattingOutput:()=>n.ImageMattingOutput,ImageProcessor:()=>h.ImageProcessor,ImageSegmentationPipeline:()=>t.ImageSegmentationPipeline,ImageToImagePipeline:()=>t.ImageToImagePipeline,ImageToTextPipeline:()=>t.ImageToTextPipeline,InterruptableStoppingCriteria:()=>M.InterruptableStoppingCriteria,JAISLMHeadModel:()=>n.JAISLMHeadModel,JAISModel:()=>n.JAISModel,JAISPreTrainedModel:()=>n.JAISPreTrainedModel,JinaCLIPImageProcessor:()=>m.JinaCLIPImageProcessor,JinaCLIPModel:()=>n.JinaCLIPModel,JinaCLIPPreTrainedModel:()=>n.JinaCLIPPreTrainedModel,JinaCLIPProcessor:()=>f.JinaCLIPProcessor,JinaCLIPTextModel:()=>n.JinaCLIPTextModel,JinaCLIPVisionModel:()=>n.JinaCLIPVisionModel,Lfm2ForCausalLM:()=>n.Lfm2ForCausalLM,Lfm2Model:()=>n.Lfm2Model,Lfm2PreTrainedModel:()=>n.Lfm2PreTrainedModel,LiteWhisperForConditionalGeneration:()=>n.LiteWhisperForConditionalGeneration,Llama4ForCausalLM:()=>n.Llama4ForCausalLM,Llama4PreTrainedModel:()=>n.Llama4PreTrainedModel,LlamaForCausalLM:()=>n.LlamaForCausalLM,LlamaModel:()=>n.LlamaModel,LlamaPreTrainedModel:()=>n.LlamaPreTrainedModel,LlamaTokenizer:()=>r.LlamaTokenizer,LlavaForConditionalGeneration:()=>n.LlavaForConditionalGeneration,LlavaOnevisionForConditionalGeneration:()=>n.LlavaOnevisionForConditionalGeneration,LlavaOnevisionImageProcessor:()=>m.LlavaOnevisionImageProcessor,LlavaPreTrainedModel:()=>n.LlavaPreTrainedModel,LlavaProcessor:()=>f.LlavaProcessor,LlavaQwen2ForCausalLM:()=>n.LlavaQwen2ForCausalLM,LogitsProcessor:()=>v.LogitsProcessor,LogitsProcessorList:()=>v.LogitsProcessorList,LogitsWarper:()=>v.LogitsWarper,LongT5ForConditionalGeneration:()=>n.LongT5ForConditionalGeneration,LongT5Model:()=>n.LongT5Model,LongT5PreTrainedModel:()=>n.LongT5PreTrainedModel,M2M100ForConditionalGeneration:()=>n.M2M100ForConditionalGeneration,M2M100Model:()=>n.M2M100Model,M2M100PreTrainedModel:()=>n.M2M100PreTrainedModel,M2M100Tokenizer:()=>r.M2M100Tokenizer,MBart50Tokenizer:()=>r.MBart50Tokenizer,MBartForCausalLM:()=>n.MBartForCausalLM,MBartForConditionalGeneration:()=>n.MBartForConditionalGeneration,MBartForSequenceClassification:()=>n.MBartForSequenceClassification,MBartModel:()=>n.MBartModel,MBartPreTrainedModel:()=>n.MBartPreTrainedModel,MBartTokenizer:()=>r.MBartTokenizer,MPNetForMaskedLM:()=>n.MPNetForMaskedLM,MPNetForQuestionAnswering:()=>n.MPNetForQuestionAnswering,MPNetForSequenceClassification:()=>n.MPNetForSequenceClassification,MPNetForTokenClassification:()=>n.MPNetForTokenClassification,MPNetModel:()=>n.MPNetModel,MPNetPreTrainedModel:()=>n.MPNetPreTrainedModel,MPNetTokenizer:()=>r.MPNetTokenizer,MT5ForConditionalGeneration:()=>n.MT5ForConditionalGeneration,MT5Model:()=>n.MT5Model,MT5PreTrainedModel:()=>n.MT5PreTrainedModel,MarianMTModel:()=>n.MarianMTModel,MarianModel:()=>n.MarianModel,MarianPreTrainedModel:()=>n.MarianPreTrainedModel,MarianTokenizer:()=>r.MarianTokenizer,Mask2FormerImageProcessor:()=>m.Mask2FormerImageProcessor,MaskFormerFeatureExtractor:()=>m.MaskFormerFeatureExtractor,MaskFormerForInstanceSegmentation:()=>n.MaskFormerForInstanceSegmentation,MaskFormerImageProcessor:()=>m.MaskFormerImageProcessor,MaskFormerModel:()=>n.MaskFormerModel,MaskFormerPreTrainedModel:()=>n.MaskFormerPreTrainedModel,MaskedLMOutput:()=>n.MaskedLMOutput,MaxLengthCriteria:()=>M.MaxLengthCriteria,Metric3DForDepthEstimation:()=>n.Metric3DForDepthEstimation,Metric3DPreTrainedModel:()=>n.Metric3DPreTrainedModel,Metric3Dv2ForDepthEstimation:()=>n.Metric3Dv2ForDepthEstimation,Metric3Dv2PreTrainedModel:()=>n.Metric3Dv2PreTrainedModel,MgpstrForSceneTextRecognition:()=>n.MgpstrForSceneTextRecognition,MgpstrModelOutput:()=>n.MgpstrModelOutput,MgpstrPreTrainedModel:()=>n.MgpstrPreTrainedModel,MgpstrProcessor:()=>f.MgpstrProcessor,MgpstrTokenizer:()=>r.MgpstrTokenizer,MimiDecoderModel:()=>n.MimiDecoderModel,MimiDecoderOutput:()=>n.MimiDecoderOutput,MimiEncoderModel:()=>n.MimiEncoderModel,MimiEncoderOutput:()=>n.MimiEncoderOutput,MimiModel:()=>n.MimiModel,MimiPreTrainedModel:()=>n.MimiPreTrainedModel,MinLengthLogitsProcessor:()=>v.MinLengthLogitsProcessor,MinNewTokensLengthLogitsProcessor:()=>v.MinNewTokensLengthLogitsProcessor,Ministral3ForCausalLM:()=>n.Ministral3ForCausalLM,Ministral3Model:()=>n.Ministral3Model,Ministral3PreTrainedModel:()=>n.Ministral3PreTrainedModel,MinistralForCausalLM:()=>n.MinistralForCausalLM,MinistralModel:()=>n.MinistralModel,MinistralPreTrainedModel:()=>n.MinistralPreTrainedModel,Mistral3ForConditionalGeneration:()=>n.Mistral3ForConditionalGeneration,MistralForCausalLM:()=>n.MistralForCausalLM,MistralModel:()=>n.MistralModel,MistralPreTrainedModel:()=>n.MistralPreTrainedModel,MobileBertForMaskedLM:()=>n.MobileBertForMaskedLM,MobileBertForQuestionAnswering:()=>n.MobileBertForQuestionAnswering,MobileBertForSequenceClassification:()=>n.MobileBertForSequenceClassification,MobileBertModel:()=>n.MobileBertModel,MobileBertPreTrainedModel:()=>n.MobileBertPreTrainedModel,MobileBertTokenizer:()=>r.MobileBertTokenizer,MobileLLMForCausalLM:()=>n.MobileLLMForCausalLM,MobileLLMModel:()=>n.MobileLLMModel,MobileLLMPreTrainedModel:()=>n.MobileLLMPreTrainedModel,MobileNetV1FeatureExtractor:()=>m.MobileNetV1FeatureExtractor,MobileNetV1ForImageClassification:()=>n.MobileNetV1ForImageClassification,MobileNetV1ForSemanticSegmentation:()=>n.MobileNetV1ForSemanticSegmentation,MobileNetV1ImageProcessor:()=>m.MobileNetV1ImageProcessor,MobileNetV1Model:()=>n.MobileNetV1Model,MobileNetV1PreTrainedModel:()=>n.MobileNetV1PreTrainedModel,MobileNetV2FeatureExtractor:()=>m.MobileNetV2FeatureExtractor,MobileNetV2ForImageClassification:()=>n.MobileNetV2ForImageClassification,MobileNetV2ForSemanticSegmentation:()=>n.MobileNetV2ForSemanticSegmentation,MobileNetV2ImageProcessor:()=>m.MobileNetV2ImageProcessor,MobileNetV2Model:()=>n.MobileNetV2Model,MobileNetV2PreTrainedModel:()=>n.MobileNetV2PreTrainedModel,MobileNetV3FeatureExtractor:()=>m.MobileNetV3FeatureExtractor,MobileNetV3ForImageClassification:()=>n.MobileNetV3ForImageClassification,MobileNetV3ForSemanticSegmentation:()=>n.MobileNetV3ForSemanticSegmentation,MobileNetV3ImageProcessor:()=>m.MobileNetV3ImageProcessor,MobileNetV3Model:()=>n.MobileNetV3Model,MobileNetV3PreTrainedModel:()=>n.MobileNetV3PreTrainedModel,MobileNetV4FeatureExtractor:()=>m.MobileNetV4FeatureExtractor,MobileNetV4ForImageClassification:()=>n.MobileNetV4ForImageClassification,MobileNetV4ForSemanticSegmentation:()=>n.MobileNetV4ForSemanticSegmentation,MobileNetV4ImageProcessor:()=>m.MobileNetV4ImageProcessor,MobileNetV4Model:()=>n.MobileNetV4Model,MobileNetV4PreTrainedModel:()=>n.MobileNetV4PreTrainedModel,MobileViTFeatureExtractor:()=>m.MobileViTFeatureExtractor,MobileViTForImageClassification:()=>n.MobileViTForImageClassification,MobileViTImageProcessor:()=>m.MobileViTImageProcessor,MobileViTModel:()=>n.MobileViTModel,MobileViTPreTrainedModel:()=>n.MobileViTPreTrainedModel,MobileViTV2ForImageClassification:()=>n.MobileViTV2ForImageClassification,MobileViTV2Model:()=>n.MobileViTV2Model,MobileViTV2PreTrainedModel:()=>n.MobileViTV2PreTrainedModel,ModelOutput:()=>n.ModelOutput,ModernBertDecoderForCausalLM:()=>n.ModernBertDecoderForCausalLM,ModernBertDecoderModel:()=>n.ModernBertDecoderModel,ModernBertDecoderPreTrainedModel:()=>n.ModernBertDecoderPreTrainedModel,ModernBertForMaskedLM:()=>n.ModernBertForMaskedLM,ModernBertForSequenceClassification:()=>n.ModernBertForSequenceClassification,ModernBertForTokenClassification:()=>n.ModernBertForTokenClassification,ModernBertModel:()=>n.ModernBertModel,ModernBertPreTrainedModel:()=>n.ModernBertPreTrainedModel,Moondream1ForConditionalGeneration:()=>n.Moondream1ForConditionalGeneration,MoonshineFeatureExtractor:()=>u.MoonshineFeatureExtractor,MoonshineForConditionalGeneration:()=>n.MoonshineForConditionalGeneration,MoonshineModel:()=>n.MoonshineModel,MoonshinePreTrainedModel:()=>n.MoonshinePreTrainedModel,MoonshineProcessor:()=>f.MoonshineProcessor,MptForCausalLM:()=>n.MptForCausalLM,MptModel:()=>n.MptModel,MptPreTrainedModel:()=>n.MptPreTrainedModel,MultiModalityCausalLM:()=>n.MultiModalityCausalLM,MultiModalityPreTrainedModel:()=>n.MultiModalityPreTrainedModel,MusicgenForCausalLM:()=>n.MusicgenForCausalLM,MusicgenForConditionalGeneration:()=>n.MusicgenForConditionalGeneration,MusicgenModel:()=>n.MusicgenModel,MusicgenPreTrainedModel:()=>n.MusicgenPreTrainedModel,NanoChatForCausalLM:()=>n.NanoChatForCausalLM,NanoChatModel:()=>n.NanoChatModel,NanoChatPreTrainedModel:()=>n.NanoChatPreTrainedModel,NeoBertForMaskedLM:()=>n.NeoBertForMaskedLM,NeoBertForQuestionAnswering:()=>n.NeoBertForQuestionAnswering,NeoBertForSequenceClassification:()=>n.NeoBertForSequenceClassification,NeoBertForTokenClassification:()=>n.NeoBertForTokenClassification,NeoBertModel:()=>n.NeoBertModel,NeoBertPreTrainedModel:()=>n.NeoBertPreTrainedModel,NllbTokenizer:()=>r.NllbTokenizer,NoBadWordsLogitsProcessor:()=>v.NoBadWordsLogitsProcessor,NoRepeatNGramLogitsProcessor:()=>v.NoRepeatNGramLogitsProcessor,NomicBertModel:()=>n.NomicBertModel,NomicBertPreTrainedModel:()=>n.NomicBertPreTrainedModel,NougatImageProcessor:()=>m.NougatImageProcessor,NougatTokenizer:()=>r.NougatTokenizer,OPTForCausalLM:()=>n.OPTForCausalLM,OPTModel:()=>n.OPTModel,OPTPreTrainedModel:()=>n.OPTPreTrainedModel,ObjectDetectionPipeline:()=>t.ObjectDetectionPipeline,Olmo2ForCausalLM:()=>n.Olmo2ForCausalLM,Olmo2Model:()=>n.Olmo2Model,Olmo2PreTrainedModel:()=>n.Olmo2PreTrainedModel,OlmoForCausalLM:()=>n.OlmoForCausalLM,OlmoModel:()=>n.OlmoModel,OlmoPreTrainedModel:()=>n.OlmoPreTrainedModel,OpenELMForCausalLM:()=>n.OpenELMForCausalLM,OpenELMModel:()=>n.OpenELMModel,OpenELMPreTrainedModel:()=>n.OpenELMPreTrainedModel,OwlViTFeatureExtractor:()=>m.OwlViTFeatureExtractor,OwlViTForObjectDetection:()=>n.OwlViTForObjectDetection,OwlViTImageProcessor:()=>m.OwlViTImageProcessor,OwlViTModel:()=>n.OwlViTModel,OwlViTPreTrainedModel:()=>n.OwlViTPreTrainedModel,OwlViTProcessor:()=>f.OwlViTProcessor,Owlv2ForObjectDetection:()=>n.Owlv2ForObjectDetection,Owlv2ImageProcessor:()=>m.Owlv2ImageProcessor,Owlv2Model:()=>n.Owlv2Model,Owlv2PreTrainedModel:()=>n.Owlv2PreTrainedModel,PaliGemmaForConditionalGeneration:()=>n.PaliGemmaForConditionalGeneration,PaliGemmaPreTrainedModel:()=>n.PaliGemmaPreTrainedModel,PaliGemmaProcessor:()=>f.PaliGemmaProcessor,ParakeetFeatureExtractor:()=>u.ParakeetFeatureExtractor,ParakeetForCTC:()=>n.ParakeetForCTC,ParakeetPreTrainedModel:()=>n.ParakeetPreTrainedModel,PatchTSMixerForPrediction:()=>n.PatchTSMixerForPrediction,PatchTSMixerModel:()=>n.PatchTSMixerModel,PatchTSMixerPreTrainedModel:()=>n.PatchTSMixerPreTrainedModel,PatchTSTForPrediction:()=>n.PatchTSTForPrediction,PatchTSTModel:()=>n.PatchTSTModel,PatchTSTPreTrainedModel:()=>n.PatchTSTPreTrainedModel,Phi3ForCausalLM:()=>n.Phi3ForCausalLM,Phi3Model:()=>n.Phi3Model,Phi3PreTrainedModel:()=>n.Phi3PreTrainedModel,Phi3VForCausalLM:()=>n.Phi3VForCausalLM,Phi3VImageProcessor:()=>m.Phi3VImageProcessor,Phi3VPreTrainedModel:()=>n.Phi3VPreTrainedModel,Phi3VProcessor:()=>f.Phi3VProcessor,PhiForCausalLM:()=>n.PhiForCausalLM,PhiModel:()=>n.PhiModel,PhiPreTrainedModel:()=>n.PhiPreTrainedModel,Pipeline:()=>t.Pipeline,PixtralImageProcessor:()=>m.PixtralImageProcessor,PixtralProcessor:()=>f.PixtralProcessor,PreTrainedModel:()=>n.PreTrainedModel,PreTrainedTokenizer:()=>r.PreTrainedTokenizer,PretrainedConfig:()=>s.PretrainedConfig,PretrainedMixin:()=>n.PretrainedMixin,Processor:()=>I.Processor,PvtForImageClassification:()=>n.PvtForImageClassification,PvtImageProcessor:()=>m.PvtImageProcessor,PvtModel:()=>n.PvtModel,PvtPreTrainedModel:()=>n.PvtPreTrainedModel,PyAnnoteFeatureExtractor:()=>u.PyAnnoteFeatureExtractor,PyAnnoteForAudioFrameClassification:()=>n.PyAnnoteForAudioFrameClassification,PyAnnoteModel:()=>n.PyAnnoteModel,PyAnnotePreTrainedModel:()=>n.PyAnnotePreTrainedModel,PyAnnoteProcessor:()=>f.PyAnnoteProcessor,QuestionAnsweringModelOutput:()=>n.QuestionAnsweringModelOutput,QuestionAnsweringPipeline:()=>t.QuestionAnsweringPipeline,Qwen2ForCausalLM:()=>n.Qwen2ForCausalLM,Qwen2Model:()=>n.Qwen2Model,Qwen2PreTrainedModel:()=>n.Qwen2PreTrainedModel,Qwen2Tokenizer:()=>r.Qwen2Tokenizer,Qwen2VLForConditionalGeneration:()=>n.Qwen2VLForConditionalGeneration,Qwen2VLImageProcessor:()=>m.Qwen2VLImageProcessor,Qwen2VLPreTrainedModel:()=>n.Qwen2VLPreTrainedModel,Qwen2VLProcessor:()=>f.Qwen2VLProcessor,Qwen3ForCausalLM:()=>n.Qwen3ForCausalLM,Qwen3Model:()=>n.Qwen3Model,Qwen3PreTrainedModel:()=>n.Qwen3PreTrainedModel,RFDetrForObjectDetection:()=>n.RFDetrForObjectDetection,RFDetrModel:()=>n.RFDetrModel,RFDetrObjectDetectionOutput:()=>n.RFDetrObjectDetectionOutput,RFDetrPreTrainedModel:()=>n.RFDetrPreTrainedModel,RTDetrForObjectDetection:()=>n.RTDetrForObjectDetection,RTDetrImageProcessor:()=>m.RTDetrImageProcessor,RTDetrModel:()=>n.RTDetrModel,RTDetrObjectDetectionOutput:()=>n.RTDetrObjectDetectionOutput,RTDetrPreTrainedModel:()=>n.RTDetrPreTrainedModel,RTDetrV2ForObjectDetection:()=>n.RTDetrV2ForObjectDetection,RTDetrV2Model:()=>n.RTDetrV2Model,RTDetrV2ObjectDetectionOutput:()=>n.RTDetrV2ObjectDetectionOutput,RTDetrV2PreTrainedModel:()=>n.RTDetrV2PreTrainedModel,RawAudio:()=>i.RawAudio,RawImage:()=>o.RawImage,RawVideo:()=>a.RawVideo,RawVideoFrame:()=>a.RawVideoFrame,RepetitionPenaltyLogitsProcessor:()=>v.RepetitionPenaltyLogitsProcessor,ResNetForImageClassification:()=>n.ResNetForImageClassification,ResNetModel:()=>n.ResNetModel,ResNetPreTrainedModel:()=>n.ResNetPreTrainedModel,RoFormerForMaskedLM:()=>n.RoFormerForMaskedLM,RoFormerForQuestionAnswering:()=>n.RoFormerForQuestionAnswering,RoFormerForSequenceClassification:()=>n.RoFormerForSequenceClassification,RoFormerForTokenClassification:()=>n.RoFormerForTokenClassification,RoFormerModel:()=>n.RoFormerModel,RoFormerPreTrainedModel:()=>n.RoFormerPreTrainedModel,RoFormerTokenizer:()=>r.RoFormerTokenizer,RobertaForMaskedLM:()=>n.RobertaForMaskedLM,RobertaForQuestionAnswering:()=>n.RobertaForQuestionAnswering,RobertaForSequenceClassification:()=>n.RobertaForSequenceClassification,RobertaForTokenClassification:()=>n.RobertaForTokenClassification,RobertaModel:()=>n.RobertaModel,RobertaPreTrainedModel:()=>n.RobertaPreTrainedModel,RobertaTokenizer:()=>r.RobertaTokenizer,Sam2ImageProcessor:()=>m.Sam2ImageProcessor,Sam2ImageSegmentationOutput:()=>n.Sam2ImageSegmentationOutput,Sam2Model:()=>n.Sam2Model,Sam2PreTrainedModel:()=>n.Sam2PreTrainedModel,Sam2Processor:()=>f.Sam2Processor,Sam2VideoProcessor:()=>f.Sam2VideoProcessor,Sam3ImageProcessor:()=>m.Sam3ImageProcessor,Sam3TrackerModel:()=>n.Sam3TrackerModel,SamImageProcessor:()=>m.SamImageProcessor,SamImageSegmentationOutput:()=>n.SamImageSegmentationOutput,SamModel:()=>n.SamModel,SamPreTrainedModel:()=>n.SamPreTrainedModel,SamProcessor:()=>f.SamProcessor,SapiensForDepthEstimation:()=>n.SapiensForDepthEstimation,SapiensForNormalEstimation:()=>n.SapiensForNormalEstimation,SapiensForSemanticSegmentation:()=>n.SapiensForSemanticSegmentation,SapiensPreTrainedModel:()=>n.SapiensPreTrainedModel,SeamlessM4TFeatureExtractor:()=>u.SeamlessM4TFeatureExtractor,SegformerFeatureExtractor:()=>m.SegformerFeatureExtractor,SegformerForImageClassification:()=>n.SegformerForImageClassification,SegformerForSemanticSegmentation:()=>n.SegformerForSemanticSegmentation,SegformerImageProcessor:()=>m.SegformerImageProcessor,SegformerModel:()=>n.SegformerModel,SegformerPreTrainedModel:()=>n.SegformerPreTrainedModel,Seq2SeqLMOutput:()=>n.Seq2SeqLMOutput,SequenceClassifierOutput:()=>n.SequenceClassifierOutput,SiglipImageProcessor:()=>m.SiglipImageProcessor,SiglipModel:()=>n.SiglipModel,SiglipPreTrainedModel:()=>n.SiglipPreTrainedModel,SiglipTextModel:()=>n.SiglipTextModel,SiglipTokenizer:()=>r.SiglipTokenizer,SiglipVisionModel:()=>n.SiglipVisionModel,SmolLM3ForCausalLM:()=>n.SmolLM3ForCausalLM,SmolLM3Model:()=>n.SmolLM3Model,SmolLM3PreTrainedModel:()=>n.SmolLM3PreTrainedModel,SmolVLMForConditionalGeneration:()=>n.SmolVLMForConditionalGeneration,SmolVLMImageProcessor:()=>m.SmolVLMImageProcessor,SmolVLMProcessor:()=>f.SmolVLMProcessor,SnacDecoderModel:()=>n.SnacDecoderModel,SnacEncoderModel:()=>n.SnacEncoderModel,SnacFeatureExtractor:()=>u.SnacFeatureExtractor,SnacModel:()=>n.SnacModel,SnacPreTrainedModel:()=>n.SnacPreTrainedModel,SpeechT5FeatureExtractor:()=>u.SpeechT5FeatureExtractor,SpeechT5ForSpeechToText:()=>n.SpeechT5ForSpeechToText,SpeechT5ForTextToSpeech:()=>n.SpeechT5ForTextToSpeech,SpeechT5HifiGan:()=>n.SpeechT5HifiGan,SpeechT5Model:()=>n.SpeechT5Model,SpeechT5PreTrainedModel:()=>n.SpeechT5PreTrainedModel,SpeechT5Processor:()=>f.SpeechT5Processor,SpeechT5Tokenizer:()=>r.SpeechT5Tokenizer,SqueezeBertForMaskedLM:()=>n.SqueezeBertForMaskedLM,SqueezeBertForQuestionAnswering:()=>n.SqueezeBertForQuestionAnswering,SqueezeBertForSequenceClassification:()=>n.SqueezeBertForSequenceClassification,SqueezeBertModel:()=>n.SqueezeBertModel,SqueezeBertPreTrainedModel:()=>n.SqueezeBertPreTrainedModel,SqueezeBertTokenizer:()=>r.SqueezeBertTokenizer,StableLmForCausalLM:()=>n.StableLmForCausalLM,StableLmModel:()=>n.StableLmModel,StableLmPreTrainedModel:()=>n.StableLmPreTrainedModel,Starcoder2ForCausalLM:()=>n.Starcoder2ForCausalLM,Starcoder2Model:()=>n.Starcoder2Model,Starcoder2PreTrainedModel:()=>n.Starcoder2PreTrainedModel,StoppingCriteria:()=>M.StoppingCriteria,StoppingCriteriaList:()=>M.StoppingCriteriaList,StyleTextToSpeech2Model:()=>n.StyleTextToSpeech2Model,StyleTextToSpeech2PreTrainedModel:()=>n.StyleTextToSpeech2PreTrainedModel,SummarizationPipeline:()=>t.SummarizationPipeline,SupertonicForConditionalGeneration:()=>n.SupertonicForConditionalGeneration,SupertonicPreTrainedModel:()=>n.SupertonicPreTrainedModel,SuppressTokensAtBeginLogitsProcessor:()=>v.SuppressTokensAtBeginLogitsProcessor,Swin2SRForImageSuperResolution:()=>n.Swin2SRForImageSuperResolution,Swin2SRImageProcessor:()=>m.Swin2SRImageProcessor,Swin2SRModel:()=>n.Swin2SRModel,Swin2SRPreTrainedModel:()=>n.Swin2SRPreTrainedModel,SwinForImageClassification:()=>n.SwinForImageClassification,SwinForSemanticSegmentation:()=>n.SwinForSemanticSegmentation,SwinModel:()=>n.SwinModel,SwinPreTrainedModel:()=>n.SwinPreTrainedModel,T5ForConditionalGeneration:()=>n.T5ForConditionalGeneration,T5Model:()=>n.T5Model,T5PreTrainedModel:()=>n.T5PreTrainedModel,T5Tokenizer:()=>r.T5Tokenizer,TableTransformerForObjectDetection:()=>n.TableTransformerForObjectDetection,TableTransformerModel:()=>n.TableTransformerModel,TableTransformerObjectDetectionOutput:()=>n.TableTransformerObjectDetectionOutput,TableTransformerPreTrainedModel:()=>n.TableTransformerPreTrainedModel,TemperatureLogitsWarper:()=>v.TemperatureLogitsWarper,Tensor:()=>l.Tensor,Text2TextGenerationPipeline:()=>t.Text2TextGenerationPipeline,TextClassificationPipeline:()=>t.TextClassificationPipeline,TextGenerationPipeline:()=>t.TextGenerationPipeline,TextStreamer:()=>T.TextStreamer,TextToAudioPipeline:()=>t.TextToAudioPipeline,TokenClassificationPipeline:()=>t.TokenClassificationPipeline,TokenClassifierOutput:()=>n.TokenClassifierOutput,TokenizerModel:()=>r.TokenizerModel,TopKLogitsWarper:()=>v.TopKLogitsWarper,TopPLogitsWarper:()=>v.TopPLogitsWarper,TrOCRForCausalLM:()=>n.TrOCRForCausalLM,TrOCRPreTrainedModel:()=>n.TrOCRPreTrainedModel,TranslationPipeline:()=>t.TranslationPipeline,UltravoxModel:()=>n.UltravoxModel,UltravoxPreTrainedModel:()=>n.UltravoxPreTrainedModel,UltravoxProcessor:()=>f.UltravoxProcessor,UniSpeechForCTC:()=>n.UniSpeechForCTC,UniSpeechForSequenceClassification:()=>n.UniSpeechForSequenceClassification,UniSpeechModel:()=>n.UniSpeechModel,UniSpeechPreTrainedModel:()=>n.UniSpeechPreTrainedModel,UniSpeechSatForAudioFrameClassification:()=>n.UniSpeechSatForAudioFrameClassification,UniSpeechSatForCTC:()=>n.UniSpeechSatForCTC,UniSpeechSatForSequenceClassification:()=>n.UniSpeechSatForSequenceClassification,UniSpeechSatModel:()=>n.UniSpeechSatModel,UniSpeechSatPreTrainedModel:()=>n.UniSpeechSatPreTrainedModel,VLChatProcessor:()=>f.VLChatProcessor,VLMImageProcessor:()=>m.VLMImageProcessor,VaultGemmaForCausalLM:()=>n.VaultGemmaForCausalLM,VaultGemmaModel:()=>n.VaultGemmaModel,VaultGemmaPreTrainedModel:()=>n.VaultGemmaPreTrainedModel,ViTFeatureExtractor:()=>m.ViTFeatureExtractor,ViTForImageClassification:()=>n.ViTForImageClassification,ViTImageProcessor:()=>m.ViTImageProcessor,ViTMAEModel:()=>n.ViTMAEModel,ViTMAEPreTrainedModel:()=>n.ViTMAEPreTrainedModel,ViTMSNForImageClassification:()=>n.ViTMSNForImageClassification,ViTMSNModel:()=>n.ViTMSNModel,ViTMSNPreTrainedModel:()=>n.ViTMSNPreTrainedModel,ViTModel:()=>n.ViTModel,ViTPreTrainedModel:()=>n.ViTPreTrainedModel,VisionEncoderDecoderModel:()=>n.VisionEncoderDecoderModel,VitMatteForImageMatting:()=>n.VitMatteForImageMatting,VitMatteImageProcessor:()=>m.VitMatteImageProcessor,VitMattePreTrainedModel:()=>n.VitMattePreTrainedModel,VitPoseForPoseEstimation:()=>n.VitPoseForPoseEstimation,VitPoseImageProcessor:()=>m.VitPoseImageProcessor,VitPosePreTrainedModel:()=>n.VitPosePreTrainedModel,VitsModel:()=>n.VitsModel,VitsModelOutput:()=>n.VitsModelOutput,VitsPreTrainedModel:()=>n.VitsPreTrainedModel,VitsTokenizer:()=>r.VitsTokenizer,VoxtralForConditionalGeneration:()=>n.VoxtralForConditionalGeneration,VoxtralProcessor:()=>f.VoxtralProcessor,Wav2Vec2BertForCTC:()=>n.Wav2Vec2BertForCTC,Wav2Vec2BertForSequenceClassification:()=>n.Wav2Vec2BertForSequenceClassification,Wav2Vec2BertModel:()=>n.Wav2Vec2BertModel,Wav2Vec2BertPreTrainedModel:()=>n.Wav2Vec2BertPreTrainedModel,Wav2Vec2CTCTokenizer:()=>r.Wav2Vec2CTCTokenizer,Wav2Vec2FeatureExtractor:()=>u.Wav2Vec2FeatureExtractor,Wav2Vec2ForAudioFrameClassification:()=>n.Wav2Vec2ForAudioFrameClassification,Wav2Vec2ForCTC:()=>n.Wav2Vec2ForCTC,Wav2Vec2ForSequenceClassification:()=>n.Wav2Vec2ForSequenceClassification,Wav2Vec2Model:()=>n.Wav2Vec2Model,Wav2Vec2PreTrainedModel:()=>n.Wav2Vec2PreTrainedModel,Wav2Vec2Processor:()=>f.Wav2Vec2Processor,Wav2Vec2ProcessorWithLM:()=>f.Wav2Vec2ProcessorWithLM,WavLMForAudioFrameClassification:()=>n.WavLMForAudioFrameClassification,WavLMForCTC:()=>n.WavLMForCTC,WavLMForSequenceClassification:()=>n.WavLMForSequenceClassification,WavLMForXVector:()=>n.WavLMForXVector,WavLMModel:()=>n.WavLMModel,WavLMPreTrainedModel:()=>n.WavLMPreTrainedModel,WeSpeakerFeatureExtractor:()=>u.WeSpeakerFeatureExtractor,WeSpeakerResNetModel:()=>n.WeSpeakerResNetModel,WeSpeakerResNetPreTrainedModel:()=>n.WeSpeakerResNetPreTrainedModel,WhisperFeatureExtractor:()=>u.WhisperFeatureExtractor,WhisperForConditionalGeneration:()=>n.WhisperForConditionalGeneration,WhisperModel:()=>n.WhisperModel,WhisperPreTrainedModel:()=>n.WhisperPreTrainedModel,WhisperProcessor:()=>f.WhisperProcessor,WhisperTextStreamer:()=>T.WhisperTextStreamer,WhisperTimeStampLogitsProcessor:()=>v.WhisperTimeStampLogitsProcessor,WhisperTokenizer:()=>r.WhisperTokenizer,XLMForQuestionAnswering:()=>n.XLMForQuestionAnswering,XLMForSequenceClassification:()=>n.XLMForSequenceClassification,XLMForTokenClassification:()=>n.XLMForTokenClassification,XLMModel:()=>n.XLMModel,XLMPreTrainedModel:()=>n.XLMPreTrainedModel,XLMRobertaForMaskedLM:()=>n.XLMRobertaForMaskedLM,XLMRobertaForQuestionAnswering:()=>n.XLMRobertaForQuestionAnswering,XLMRobertaForSequenceClassification:()=>n.XLMRobertaForSequenceClassification,XLMRobertaForTokenClassification:()=>n.XLMRobertaForTokenClassification,XLMRobertaModel:()=>n.XLMRobertaModel,XLMRobertaPreTrainedModel:()=>n.XLMRobertaPreTrainedModel,XLMRobertaTokenizer:()=>r.XLMRobertaTokenizer,XLMTokenizer:()=>r.XLMTokenizer,XLMWithLMHeadModel:()=>n.XLMWithLMHeadModel,XVectorOutput:()=>n.XVectorOutput,YolosFeatureExtractor:()=>m.YolosFeatureExtractor,YolosForObjectDetection:()=>n.YolosForObjectDetection,YolosImageProcessor:()=>m.YolosImageProcessor,YolosModel:()=>n.YolosModel,YolosObjectDetectionOutput:()=>n.YolosObjectDetectionOutput,YolosPreTrainedModel:()=>n.YolosPreTrainedModel,ZeroShotAudioClassificationPipeline:()=>t.ZeroShotAudioClassificationPipeline,ZeroShotClassificationPipeline:()=>t.ZeroShotClassificationPipeline,ZeroShotImageClassificationPipeline:()=>t.ZeroShotImageClassificationPipeline,ZeroShotObjectDetectionPipeline:()=>t.ZeroShotObjectDetectionPipeline,bankers_round:()=>c.bankers_round,cat:()=>l.cat,cos_sim:()=>c.cos_sim,dot:()=>c.dot,dynamic_time_warping:()=>c.dynamic_time_warping,env:()=>e.env,full:()=>l.full,full_like:()=>l.full_like,getCacheShapes:()=>s.getCacheShapes,hamming:()=>i.hamming,hanning:()=>i.hanning,interpolate:()=>l.interpolate,interpolate_4d:()=>l.interpolate_4d,interpolate_data:()=>c.interpolate_data,is_chinese_char:()=>r.is_chinese_char,layer_norm:()=>l.layer_norm,load_image:()=>o.load_image,load_video:()=>a.load_video,log_softmax:()=>c.log_softmax,magnitude:()=>c.magnitude,matmul:()=>l.matmul,max:()=>c.max,mean:()=>l.mean,mean_pooling:()=>l.mean_pooling,medianFilter:()=>c.medianFilter,mel_filter_bank:()=>i.mel_filter_bank,min:()=>c.min,ones:()=>l.ones,ones_like:()=>l.ones_like,permute:()=>l.permute,permute_data:()=>c.permute_data,pipeline:()=>t.pipeline,quantize_embeddings:()=>l.quantize_embeddings,rand:()=>l.rand,randn:()=>l.randn,read_audio:()=>i.read_audio,rfft:()=>l.rfft,round:()=>c.round,slice:()=>l.slice,softmax:()=>c.softmax,spectrogram:()=>i.spectrogram,stack:()=>l.stack,std_mean:()=>l.std_mean,topk:()=>l.topk,window_function:()=>i.window_function,zeros:()=>l.zeros,zeros_like:()=>l.zeros_like});var e=Qt("./src/env.js"),t=Qt("./src/pipelines.js"),n=Qt("./src/models.js"),r=Qt("./src/tokenizers.js"),s=Qt("./src/configs.js"),i=Qt("./src/utils/audio.js"),o=Qt("./src/utils/image.js"),a=Qt("./src/utils/video.js"),l=Qt("./src/utils/tensor.js"),c=Qt("./src/utils/maths.js"),d=Qt("./src/base/feature_extraction_utils.js"),u=Qt("./src/models/feature_extractors.js"),p=Qt("./src/models/auto/feature_extraction_auto.js"),h=Qt("./src/base/image_processors_utils.js"),m=Qt("./src/models/image_processors.js"),g=Qt("./src/models/auto/image_processing_auto.js"),I=Qt("./src/base/processing_utils.js"),f=Qt("./src/models/processors.js"),_=Qt("./src/models/auto/processing_auto.js"),T=Qt("./src/generation/streamers.js"),M=Qt("./src/generation/stopping_criteria.js"),v=Qt("./src/generation/logits_process.js")})();y.ASTFeatureExtractor;y.ASTForAudioClassification;y.ASTModel;y.ASTPreTrainedModel;y.AlbertForMaskedLM;y.AlbertForQuestionAnswering;y.AlbertForSequenceClassification;y.AlbertModel;y.AlbertPreTrainedModel;y.AlbertTokenizer;y.ArceeForCausalLM;y.ArceeModel;y.ArceePreTrainedModel;y.AudioClassificationPipeline;y.AutoConfig;y.AutoFeatureExtractor;y.AutoImageProcessor;y.AutoModel;y.AutoModelForAudioClassification;y.AutoModelForAudioFrameClassification;y.AutoModelForAudioTextToText;y.AutoModelForCTC;y.AutoModelForCausalLM;y.AutoModelForDepthEstimation;y.AutoModelForDocumentQuestionAnswering;y.AutoModelForImageClassification;y.AutoModelForImageFeatureExtraction;y.AutoModelForImageMatting;y.AutoModelForImageSegmentation;y.AutoModelForImageTextToText;y.AutoModelForImageToImage;y.AutoModelForMaskGeneration;y.AutoModelForMaskedLM;y.AutoModelForNormalEstimation;y.AutoModelForObjectDetection;y.AutoModelForPoseEstimation;y.AutoModelForQuestionAnswering;y.AutoModelForSemanticSegmentation;y.AutoModelForSeq2SeqLM;y.AutoModelForSequenceClassification;y.AutoModelForSpeechSeq2Seq;y.AutoModelForTextToSpectrogram;y.AutoModelForTextToWaveform;y.AutoModelForTokenClassification;y.AutoModelForUniversalSegmentation;y.AutoModelForVision2Seq;y.AutoModelForXVector;y.AutoModelForZeroShotObjectDetection;y.AutoProcessor;var IV=y.AutoTokenizer;y.AutomaticSpeechRecognitionPipeline;y.BackgroundRemovalPipeline;y.BartForConditionalGeneration;y.BartForSequenceClassification;y.BartModel;y.BartPretrainedModel;y.BartTokenizer;y.BaseModelOutput;y.BaseStreamer;y.BeitFeatureExtractor;y.BeitForImageClassification;y.BeitModel;y.BeitPreTrainedModel;y.BertForMaskedLM;y.BertForQuestionAnswering;y.BertForSequenceClassification;y.BertForTokenClassification;y.BertModel;y.BertPreTrainedModel;y.BertTokenizer;y.BitImageProcessor;y.BlenderbotForConditionalGeneration;y.BlenderbotModel;y.BlenderbotPreTrainedModel;y.BlenderbotSmallForConditionalGeneration;y.BlenderbotSmallModel;y.BlenderbotSmallPreTrainedModel;y.BlenderbotSmallTokenizer;y.BlenderbotTokenizer;y.BloomForCausalLM;y.BloomModel;y.BloomPreTrainedModel;y.BloomTokenizer;y.CLIPFeatureExtractor;y.CLIPImageProcessor;y.CLIPModel;y.CLIPPreTrainedModel;y.CLIPSegForImageSegmentation;y.CLIPSegModel;y.CLIPSegPreTrainedModel;y.CLIPTextModel;y.CLIPTextModelWithProjection;y.CLIPTokenizer;y.CLIPVisionModel;y.CLIPVisionModelWithProjection;y.CamembertForMaskedLM;y.CamembertForQuestionAnswering;y.CamembertForSequenceClassification;y.CamembertForTokenClassification;y.CamembertModel;y.CamembertPreTrainedModel;y.CamembertTokenizer;y.CausalLMOutput;y.CausalLMOutputWithPast;y.ChineseCLIPFeatureExtractor;y.ChineseCLIPModel;y.ChineseCLIPPreTrainedModel;y.ClapAudioModelWithProjection;y.ClapFeatureExtractor;y.ClapModel;y.ClapPreTrainedModel;y.ClapTextModelWithProjection;y.ClassifierFreeGuidanceLogitsProcessor;y.CodeGenForCausalLM;y.CodeGenModel;y.CodeGenPreTrainedModel;y.CodeGenTokenizer;y.CodeLlamaTokenizer;y.CohereForCausalLM;y.CohereModel;y.CoherePreTrainedModel;y.CohereTokenizer;y.ConvBertForMaskedLM;y.ConvBertForQuestionAnswering;y.ConvBertForSequenceClassification;y.ConvBertForTokenClassification;y.ConvBertModel;y.ConvBertPreTrainedModel;y.ConvBertTokenizer;y.ConvNextFeatureExtractor;y.ConvNextForImageClassification;y.ConvNextImageProcessor;y.ConvNextModel;y.ConvNextPreTrainedModel;y.ConvNextV2ForImageClassification;y.ConvNextV2Model;y.ConvNextV2PreTrainedModel;y.DFineForObjectDetection;y.DFineModel;y.DFinePreTrainedModel;y.DINOv3ConvNextModel;y.DINOv3ConvNextPreTrainedModel;y.DINOv3ViTImageProcessor;y.DINOv3ViTModel;y.DINOv3ViTPreTrainedModel;y.DPTFeatureExtractor;y.DPTForDepthEstimation;y.DPTImageProcessor;y.DPTModel;y.DPTPreTrainedModel;y.DacDecoderModel;y.DacDecoderOutput;y.DacEncoderModel;y.DacEncoderOutput;y.DacFeatureExtractor;y.DacModel;y.DacPreTrainedModel;y.DataTypeMap;y.DebertaForMaskedLM;y.DebertaForQuestionAnswering;y.DebertaForSequenceClassification;y.DebertaForTokenClassification;y.DebertaModel;y.DebertaPreTrainedModel;y.DebertaTokenizer;y.DebertaV2ForMaskedLM;y.DebertaV2ForQuestionAnswering;y.DebertaV2ForSequenceClassification;y.DebertaV2ForTokenClassification;y.DebertaV2Model;y.DebertaV2PreTrainedModel;y.DebertaV2Tokenizer;y.DecisionTransformerModel;y.DecisionTransformerPreTrainedModel;y.DeiTFeatureExtractor;y.DeiTForImageClassification;y.DeiTImageProcessor;y.DeiTModel;y.DeiTPreTrainedModel;y.DepthAnythingForDepthEstimation;y.DepthAnythingPreTrainedModel;y.DepthEstimationPipeline;y.DepthProForDepthEstimation;y.DepthProPreTrainedModel;y.DetrFeatureExtractor;y.DetrForObjectDetection;y.DetrForSegmentation;y.DetrImageProcessor;y.DetrModel;y.DetrObjectDetectionOutput;y.DetrPreTrainedModel;y.DetrSegmentationOutput;y.Dinov2ForImageClassification;y.Dinov2Model;y.Dinov2PreTrainedModel;y.Dinov2WithRegistersForImageClassification;y.Dinov2WithRegistersModel;y.Dinov2WithRegistersPreTrainedModel;y.DistilBertForMaskedLM;y.DistilBertForQuestionAnswering;y.DistilBertForSequenceClassification;y.DistilBertForTokenClassification;y.DistilBertModel;y.DistilBertPreTrainedModel;y.DistilBertTokenizer;y.DocumentQuestionAnsweringPipeline;y.DonutFeatureExtractor;y.DonutImageProcessor;y.DonutSwinModel;y.DonutSwinPreTrainedModel;y.EdgeTamModel;y.EfficientNetForImageClassification;y.EfficientNetImageProcessor;y.EfficientNetModel;y.EfficientNetPreTrainedModel;y.ElectraForMaskedLM;y.ElectraForQuestionAnswering;y.ElectraForSequenceClassification;y.ElectraForTokenClassification;y.ElectraModel;y.ElectraPreTrainedModel;y.ElectraTokenizer;y.EncodecFeatureExtractor;y.EosTokenCriteria;y.Ernie4_5ForCausalLM;y.Ernie4_5Model;y.Ernie4_5PreTrainedModel;y.EsmForMaskedLM;y.EsmForSequenceClassification;y.EsmForTokenClassification;y.EsmModel;y.EsmPreTrainedModel;y.EsmTokenizer;y.ExaoneForCausalLM;y.ExaoneModel;y.ExaonePreTrainedModel;y.FFT;y.FalconForCausalLM;y.FalconModel;y.FalconPreTrainedModel;y.FalconTokenizer;y.FastViTForImageClassification;y.FastViTModel;y.FastViTPreTrainedModel;y.FeatureExtractionPipeline;y.FeatureExtractor;y.FillMaskPipeline;y.Florence2ForConditionalGeneration;y.Florence2PreTrainedModel;y.Florence2Processor;y.ForcedBOSTokenLogitsProcessor;y.ForcedEOSTokenLogitsProcessor;y.GLPNFeatureExtractor;y.GLPNForDepthEstimation;y.GLPNModel;y.GLPNPreTrainedModel;y.GPT2LMHeadModel;y.GPT2Model;y.GPT2PreTrainedModel;y.GPT2Tokenizer;y.GPTBigCodeForCausalLM;y.GPTBigCodeModel;y.GPTBigCodePreTrainedModel;y.GPTJForCausalLM;y.GPTJModel;y.GPTJPreTrainedModel;y.GPTNeoForCausalLM;y.GPTNeoModel;y.GPTNeoPreTrainedModel;y.GPTNeoXForCausalLM;y.GPTNeoXModel;y.GPTNeoXPreTrainedModel;y.GPTNeoXTokenizer;y.Gemma2ForCausalLM;y.Gemma2Model;y.Gemma2PreTrainedModel;y.Gemma3ForCausalLM;y.Gemma3Model;y.Gemma3PreTrainedModel;y.Gemma3nAudioFeatureExtractor;y.Gemma3nForConditionalGeneration;y.Gemma3nPreTrainedModel;y.Gemma3nProcessor;y.GemmaForCausalLM;y.GemmaModel;y.GemmaPreTrainedModel;y.GemmaTokenizer;y.GlmForCausalLM;y.GlmModel;y.GlmPreTrainedModel;y.GraniteForCausalLM;y.GraniteModel;y.GraniteMoeHybridForCausalLM;y.GraniteMoeHybridModel;y.GraniteMoeHybridPreTrainedModel;y.GranitePreTrainedModel;y.Grok1Tokenizer;y.GroundingDinoForObjectDetection;y.GroundingDinoImageProcessor;y.GroundingDinoPreTrainedModel;y.GroundingDinoProcessor;y.GroupViTModel;y.GroupViTPreTrainedModel;y.HeliumForCausalLM;y.HeliumModel;y.HeliumPreTrainedModel;y.HerbertTokenizer;y.HieraForImageClassification;y.HieraModel;y.HieraPreTrainedModel;y.HubertForCTC;y.HubertForSequenceClassification;y.HubertModel;y.HubertPreTrainedModel;y.IJepaForImageClassification;y.IJepaModel;y.IJepaPreTrainedModel;y.Idefics3ForConditionalGeneration;y.Idefics3ImageProcessor;y.Idefics3PreTrainedModel;y.Idefics3Processor;y.ImageClassificationPipeline;y.ImageFeatureExtractionPipeline;y.ImageFeatureExtractor;y.ImageMattingOutput;y.ImageProcessor;y.ImageSegmentationPipeline;y.ImageToImagePipeline;y.ImageToTextPipeline;y.InterruptableStoppingCriteria;y.JAISLMHeadModel;y.JAISModel;y.JAISPreTrainedModel;y.JinaCLIPImageProcessor;y.JinaCLIPModel;y.JinaCLIPPreTrainedModel;y.JinaCLIPProcessor;y.JinaCLIPTextModel;y.JinaCLIPVisionModel;y.Lfm2ForCausalLM;y.Lfm2Model;y.Lfm2PreTrainedModel;y.LiteWhisperForConditionalGeneration;y.Llama4ForCausalLM;y.Llama4PreTrainedModel;y.LlamaForCausalLM;y.LlamaModel;y.LlamaPreTrainedModel;y.LlamaTokenizer;y.LlavaForConditionalGeneration;y.LlavaOnevisionForConditionalGeneration;y.LlavaOnevisionImageProcessor;y.LlavaPreTrainedModel;y.LlavaProcessor;y.LlavaQwen2ForCausalLM;y.LogitsProcessor;y.LogitsProcessorList;y.LogitsWarper;y.LongT5ForConditionalGeneration;y.LongT5Model;y.LongT5PreTrainedModel;y.M2M100ForConditionalGeneration;y.M2M100Model;y.M2M100PreTrainedModel;y.M2M100Tokenizer;y.MBart50Tokenizer;y.MBartForCausalLM;y.MBartForConditionalGeneration;y.MBartForSequenceClassification;y.MBartModel;y.MBartPreTrainedModel;y.MBartTokenizer;y.MPNetForMaskedLM;y.MPNetForQuestionAnswering;y.MPNetForSequenceClassification;y.MPNetForTokenClassification;y.MPNetModel;y.MPNetPreTrainedModel;y.MPNetTokenizer;y.MT5ForConditionalGeneration;y.MT5Model;y.MT5PreTrainedModel;y.MarianMTModel;y.MarianModel;y.MarianPreTrainedModel;y.MarianTokenizer;y.Mask2FormerImageProcessor;y.MaskFormerFeatureExtractor;y.MaskFormerForInstanceSegmentation;y.MaskFormerImageProcessor;y.MaskFormerModel;y.MaskFormerPreTrainedModel;y.MaskedLMOutput;y.MaxLengthCriteria;y.Metric3DForDepthEstimation;y.Metric3DPreTrainedModel;y.Metric3Dv2ForDepthEstimation;y.Metric3Dv2PreTrainedModel;y.MgpstrForSceneTextRecognition;y.MgpstrModelOutput;y.MgpstrPreTrainedModel;y.MgpstrProcessor;y.MgpstrTokenizer;y.MimiDecoderModel;y.MimiDecoderOutput;y.MimiEncoderModel;y.MimiEncoderOutput;y.MimiModel;y.MimiPreTrainedModel;y.MinLengthLogitsProcessor;y.MinNewTokensLengthLogitsProcessor;y.Ministral3ForCausalLM;y.Ministral3Model;y.Ministral3PreTrainedModel;y.MinistralForCausalLM;y.MinistralModel;y.MinistralPreTrainedModel;y.Mistral3ForConditionalGeneration;y.MistralForCausalLM;y.MistralModel;y.MistralPreTrainedModel;y.MobileBertForMaskedLM;y.MobileBertForQuestionAnswering;y.MobileBertForSequenceClassification;y.MobileBertModel;y.MobileBertPreTrainedModel;y.MobileBertTokenizer;y.MobileLLMForCausalLM;y.MobileLLMModel;y.MobileLLMPreTrainedModel;y.MobileNetV1FeatureExtractor;y.MobileNetV1ForImageClassification;y.MobileNetV1ForSemanticSegmentation;y.MobileNetV1ImageProcessor;y.MobileNetV1Model;y.MobileNetV1PreTrainedModel;y.MobileNetV2FeatureExtractor;y.MobileNetV2ForImageClassification;y.MobileNetV2ForSemanticSegmentation;y.MobileNetV2ImageProcessor;y.MobileNetV2Model;y.MobileNetV2PreTrainedModel;y.MobileNetV3FeatureExtractor;y.MobileNetV3ForImageClassification;y.MobileNetV3ForSemanticSegmentation;y.MobileNetV3ImageProcessor;y.MobileNetV3Model;y.MobileNetV3PreTrainedModel;y.MobileNetV4FeatureExtractor;y.MobileNetV4ForImageClassification;y.MobileNetV4ForSemanticSegmentation;y.MobileNetV4ImageProcessor;y.MobileNetV4Model;y.MobileNetV4PreTrainedModel;y.MobileViTFeatureExtractor;y.MobileViTForImageClassification;y.MobileViTImageProcessor;y.MobileViTModel;y.MobileViTPreTrainedModel;y.MobileViTV2ForImageClassification;y.MobileViTV2Model;y.MobileViTV2PreTrainedModel;y.ModelOutput;y.ModernBertDecoderForCausalLM;y.ModernBertDecoderModel;y.ModernBertDecoderPreTrainedModel;y.ModernBertForMaskedLM;y.ModernBertForSequenceClassification;y.ModernBertForTokenClassification;y.ModernBertModel;y.ModernBertPreTrainedModel;y.Moondream1ForConditionalGeneration;y.MoonshineFeatureExtractor;y.MoonshineForConditionalGeneration;y.MoonshineModel;y.MoonshinePreTrainedModel;y.MoonshineProcessor;y.MptForCausalLM;y.MptModel;y.MptPreTrainedModel;y.MultiModalityCausalLM;y.MultiModalityPreTrainedModel;y.MusicgenForCausalLM;y.MusicgenForConditionalGeneration;y.MusicgenModel;y.MusicgenPreTrainedModel;y.NanoChatForCausalLM;y.NanoChatModel;y.NanoChatPreTrainedModel;y.NeoBertForMaskedLM;y.NeoBertForQuestionAnswering;y.NeoBertForSequenceClassification;y.NeoBertForTokenClassification;y.NeoBertModel;y.NeoBertPreTrainedModel;y.NllbTokenizer;y.NoBadWordsLogitsProcessor;y.NoRepeatNGramLogitsProcessor;y.NomicBertModel;y.NomicBertPreTrainedModel;y.NougatImageProcessor;y.NougatTokenizer;y.OPTForCausalLM;y.OPTModel;y.OPTPreTrainedModel;y.ObjectDetectionPipeline;y.Olmo2ForCausalLM;y.Olmo2Model;y.Olmo2PreTrainedModel;y.OlmoForCausalLM;y.OlmoModel;y.OlmoPreTrainedModel;y.OpenELMForCausalLM;y.OpenELMModel;y.OpenELMPreTrainedModel;y.OwlViTFeatureExtractor;y.OwlViTForObjectDetection;y.OwlViTImageProcessor;y.OwlViTModel;y.OwlViTPreTrainedModel;y.OwlViTProcessor;y.Owlv2ForObjectDetection;y.Owlv2ImageProcessor;y.Owlv2Model;y.Owlv2PreTrainedModel;y.PaliGemmaForConditionalGeneration;y.PaliGemmaPreTrainedModel;y.PaliGemmaProcessor;y.ParakeetFeatureExtractor;y.ParakeetForCTC;y.ParakeetPreTrainedModel;y.PatchTSMixerForPrediction;y.PatchTSMixerModel;y.PatchTSMixerPreTrainedModel;y.PatchTSTForPrediction;y.PatchTSTModel;y.PatchTSTPreTrainedModel;y.Phi3ForCausalLM;y.Phi3Model;y.Phi3PreTrainedModel;y.Phi3VForCausalLM;y.Phi3VImageProcessor;y.Phi3VPreTrainedModel;y.Phi3VProcessor;y.PhiForCausalLM;y.PhiModel;y.PhiPreTrainedModel;y.Pipeline;y.PixtralImageProcessor;y.PixtralProcessor;y.PreTrainedModel;y.PreTrainedTokenizer;y.PretrainedConfig;y.PretrainedMixin;y.Processor;y.PvtForImageClassification;y.PvtImageProcessor;y.PvtModel;y.PvtPreTrainedModel;y.PyAnnoteFeatureExtractor;y.PyAnnoteForAudioFrameClassification;y.PyAnnoteModel;y.PyAnnotePreTrainedModel;y.PyAnnoteProcessor;y.QuestionAnsweringModelOutput;y.QuestionAnsweringPipeline;y.Qwen2ForCausalLM;y.Qwen2Model;y.Qwen2PreTrainedModel;y.Qwen2Tokenizer;y.Qwen2VLForConditionalGeneration;y.Qwen2VLImageProcessor;y.Qwen2VLPreTrainedModel;y.Qwen2VLProcessor;y.Qwen3ForCausalLM;y.Qwen3Model;y.Qwen3PreTrainedModel;y.RFDetrForObjectDetection;y.RFDetrModel;y.RFDetrObjectDetectionOutput;y.RFDetrPreTrainedModel;y.RTDetrForObjectDetection;y.RTDetrImageProcessor;y.RTDetrModel;y.RTDetrObjectDetectionOutput;y.RTDetrPreTrainedModel;y.RTDetrV2ForObjectDetection;y.RTDetrV2Model;y.RTDetrV2ObjectDetectionOutput;y.RTDetrV2PreTrainedModel;y.RawAudio;y.RawImage;y.RawVideo;y.RawVideoFrame;y.RepetitionPenaltyLogitsProcessor;y.ResNetForImageClassification;y.ResNetModel;y.ResNetPreTrainedModel;y.RoFormerForMaskedLM;y.RoFormerForQuestionAnswering;y.RoFormerForSequenceClassification;y.RoFormerForTokenClassification;y.RoFormerModel;y.RoFormerPreTrainedModel;y.RoFormerTokenizer;y.RobertaForMaskedLM;y.RobertaForQuestionAnswering;y.RobertaForSequenceClassification;y.RobertaForTokenClassification;y.RobertaModel;y.RobertaPreTrainedModel;y.RobertaTokenizer;y.Sam2ImageProcessor;y.Sam2ImageSegmentationOutput;y.Sam2Model;y.Sam2PreTrainedModel;y.Sam2Processor;y.Sam2VideoProcessor;y.Sam3ImageProcessor;y.Sam3TrackerModel;y.SamImageProcessor;y.SamImageSegmentationOutput;y.SamModel;y.SamPreTrainedModel;y.SamProcessor;y.SapiensForDepthEstimation;y.SapiensForNormalEstimation;y.SapiensForSemanticSegmentation;y.SapiensPreTrainedModel;y.SeamlessM4TFeatureExtractor;y.SegformerFeatureExtractor;y.SegformerForImageClassification;y.SegformerForSemanticSegmentation;y.SegformerImageProcessor;y.SegformerModel;y.SegformerPreTrainedModel;y.Seq2SeqLMOutput;y.SequenceClassifierOutput;y.SiglipImageProcessor;y.SiglipModel;y.SiglipPreTrainedModel;y.SiglipTextModel;y.SiglipTokenizer;y.SiglipVisionModel;y.SmolLM3ForCausalLM;y.SmolLM3Model;y.SmolLM3PreTrainedModel;y.SmolVLMForConditionalGeneration;y.SmolVLMImageProcessor;y.SmolVLMProcessor;y.SnacDecoderModel;y.SnacEncoderModel;y.SnacFeatureExtractor;y.SnacModel;y.SnacPreTrainedModel;y.SpeechT5FeatureExtractor;y.SpeechT5ForSpeechToText;y.SpeechT5ForTextToSpeech;y.SpeechT5HifiGan;y.SpeechT5Model;y.SpeechT5PreTrainedModel;y.SpeechT5Processor;y.SpeechT5Tokenizer;y.SqueezeBertForMaskedLM;y.SqueezeBertForQuestionAnswering;y.SqueezeBertForSequenceClassification;y.SqueezeBertModel;y.SqueezeBertPreTrainedModel;y.SqueezeBertTokenizer;y.StableLmForCausalLM;y.StableLmModel;y.StableLmPreTrainedModel;y.Starcoder2ForCausalLM;y.Starcoder2Model;y.Starcoder2PreTrainedModel;y.StoppingCriteria;y.StoppingCriteriaList;y.StyleTextToSpeech2Model;y.StyleTextToSpeech2PreTrainedModel;y.SummarizationPipeline;y.SupertonicForConditionalGeneration;y.SupertonicPreTrainedModel;y.SuppressTokensAtBeginLogitsProcessor;y.Swin2SRForImageSuperResolution;y.Swin2SRImageProcessor;y.Swin2SRModel;y.Swin2SRPreTrainedModel;y.SwinForImageClassification;y.SwinForSemanticSegmentation;y.SwinModel;y.SwinPreTrainedModel;y.T5ForConditionalGeneration;y.T5Model;y.T5PreTrainedModel;y.T5Tokenizer;y.TableTransformerForObjectDetection;y.TableTransformerModel;y.TableTransformerObjectDetectionOutput;y.TableTransformerPreTrainedModel;y.TemperatureLogitsWarper;y.Tensor;y.Text2TextGenerationPipeline;y.TextClassificationPipeline;y.TextGenerationPipeline;y.TextStreamer;y.TextToAudioPipeline;y.TokenClassificationPipeline;y.TokenClassifierOutput;y.TokenizerModel;y.TopKLogitsWarper;y.TopPLogitsWarper;y.TrOCRForCausalLM;y.TrOCRPreTrainedModel;y.TranslationPipeline;y.UltravoxModel;y.UltravoxPreTrainedModel;y.UltravoxProcessor;y.UniSpeechForCTC;y.UniSpeechForSequenceClassification;y.UniSpeechModel;y.UniSpeechPreTrainedModel;y.UniSpeechSatForAudioFrameClassification;y.UniSpeechSatForCTC;y.UniSpeechSatForSequenceClassification;y.UniSpeechSatModel;y.UniSpeechSatPreTrainedModel;y.VLChatProcessor;y.VLMImageProcessor;y.VaultGemmaForCausalLM;y.VaultGemmaModel;y.VaultGemmaPreTrainedModel;y.ViTFeatureExtractor;y.ViTForImageClassification;y.ViTImageProcessor;y.ViTMAEModel;y.ViTMAEPreTrainedModel;y.ViTMSNForImageClassification;y.ViTMSNModel;y.ViTMSNPreTrainedModel;y.ViTModel;y.ViTPreTrainedModel;y.VisionEncoderDecoderModel;y.VitMatteForImageMatting;y.VitMatteImageProcessor;y.VitMattePreTrainedModel;y.VitPoseForPoseEstimation;y.VitPoseImageProcessor;y.VitPosePreTrainedModel;y.VitsModel;y.VitsModelOutput;y.VitsPreTrainedModel;y.VitsTokenizer;y.VoxtralForConditionalGeneration;y.VoxtralProcessor;y.Wav2Vec2BertForCTC;y.Wav2Vec2BertForSequenceClassification;y.Wav2Vec2BertModel;y.Wav2Vec2BertPreTrainedModel;y.Wav2Vec2CTCTokenizer;y.Wav2Vec2FeatureExtractor;y.Wav2Vec2ForAudioFrameClassification;y.Wav2Vec2ForCTC;y.Wav2Vec2ForSequenceClassification;y.Wav2Vec2Model;y.Wav2Vec2PreTrainedModel;y.Wav2Vec2Processor;y.Wav2Vec2ProcessorWithLM;y.WavLMForAudioFrameClassification;y.WavLMForCTC;y.WavLMForSequenceClassification;y.WavLMForXVector;y.WavLMModel;y.WavLMPreTrainedModel;y.WeSpeakerFeatureExtractor;y.WeSpeakerResNetModel;y.WeSpeakerResNetPreTrainedModel;y.WhisperFeatureExtractor;y.WhisperForConditionalGeneration;y.WhisperModel;y.WhisperPreTrainedModel;y.WhisperProcessor;y.WhisperTextStreamer;y.WhisperTimeStampLogitsProcessor;y.WhisperTokenizer;y.XLMForQuestionAnswering;y.XLMForSequenceClassification;y.XLMForTokenClassification;y.XLMModel;y.XLMPreTrainedModel;y.XLMRobertaForMaskedLM;y.XLMRobertaForQuestionAnswering;y.XLMRobertaForSequenceClassification;y.XLMRobertaForTokenClassification;y.XLMRobertaModel;y.XLMRobertaPreTrainedModel;y.XLMRobertaTokenizer;y.XLMTokenizer;y.XLMWithLMHeadModel;y.XVectorOutput;y.YolosFeatureExtractor;y.YolosForObjectDetection;y.YolosImageProcessor;y.YolosModel;y.YolosObjectDetectionOutput;y.YolosPreTrainedModel;y.ZeroShotAudioClassificationPipeline;y.ZeroShotClassificationPipeline;y.ZeroShotImageClassificationPipeline;y.ZeroShotObjectDetectionPipeline;y.bankers_round;y.cat;y.cos_sim;y.dot;y.dynamic_time_warping;var e3=y.env;y.full;y.full_like;y.getCacheShapes;y.hamming;y.hanning;y.interpolate;y.interpolate_4d;y.interpolate_data;y.is_chinese_char;y.layer_norm;y.load_image;y.load_video;y.log_softmax;y.magnitude;y.matmul;y.max;y.mean;y.mean_pooling;y.medianFilter;y.mel_filter_bank;y.min;y.ones;y.ones_like;y.permute;y.permute_data;var kV=y.pipeline;y.quantize_embeddings;y.rand;y.randn;y.read_audio;y.rfft;y.round;y.slice;y.softmax;y.spectrogram;y.stack;y.std_mean;y.topk;y.window_function;y.zeros;y.zeros_like;e3.allowLocalModels=!1;e3.useBrowserCache=!0;const EV="onnx-community/gpt2";class Fo{static instance=null;generator=null;isLoading=!1;loadingState={status:"idle",progress:0,statusText:""};constructor(){}static getInstance(){return Fo.instance||(Fo.instance=new Fo),Fo.instance}getLoadingState(){return{...this.loadingState}}isModelLoaded(){return this.generator!==null}isModelLoading(){return this.isLoading}getGenerator(){return this.generator}async loadModel(t){if(!this.generator&&!this.isLoading){this.isLoading=!0,this.updateState({status:"loading",progress:0,statusText:"Initializing..."},t);try{this.updateState({status:"loading",progress:10,statusText:"Loading tokenizer and model..."},t),this.generator=await kV("text-generation",EV,{progress_callback:n=>{if(n.progress!==void 0){const r=Math.round(n.progress),s=n.file?`Downloading ${n.file}...`:`Loading... ${r}%`;this.updateState({status:"loading",progress:10+r*.85,statusText:s},t)}}}),this.updateState({status:"ready",progress:100,statusText:"Model ready!"},t)}catch(n){const r=n instanceof Error?n.message:"Unknown error";throw this.updateState({status:"error",progress:0,statusText:"Failed to load model",error:r},t),n}finally{this.isLoading=!1}}}updateState(t,n){this.loadingState={...this.loadingState,...t},n?.(this.getLoadingState())}async generate(t,n={}){if(!this.generator)throw new Error("Model not loaded. Call loadModel() first.");const{maxNewTokens:r=1,temperature:s=1,topK:i=50,topP:o=.9,doSample:a=!0}=n,l=await this.generator(t,{max_new_tokens:r,temperature:s,top_k:i,top_p:o,do_sample:a,return_full_text:!0});return Array.isArray(l)&&l.length>0?l[0].generated_text:t}dispose(){this.generator=null,this.loadingState={status:"idle",progress:0,statusText:""}}}const SV="Xenova/gpt2";class Oo{static instance=null;tokenizer=null;isLoading=!1;constructor(){}static getInstance(){return Oo.instance||(Oo.instance=new Oo),Oo.instance}isLoaded(){return this.tokenizer!==null}async load(){if(!this.tokenizer&&!this.isLoading){this.isLoading=!0;try{this.tokenizer=await IV.from_pretrained(SV)}finally{this.isLoading=!1}}}tokenize(t){if(!this.tokenizer)throw new Error("Tokenizer not loaded. Call load() first.");const n=this.tokenizer.encode(t);return Array.from(n).map((i,o)=>{const a=this.tokenizer.decode([i],{skip_special_tokens:!1});return{id:o,text:a,tokenId:i}})}encode(t){if(!this.tokenizer)throw new Error("Tokenizer not loaded. Call load() first.");return Array.from(this.tokenizer.encode(t))}decode(t){if(!this.tokenizer)throw new Error("Tokenizer not loaded. Call load() first.");return this.tokenizer.decode(t,{skip_special_tokens:!1})}decodeToken(t){if(!this.tokenizer)throw new Error("Tokenizer not loaded. Call load() first.");return this.tokenizer.decode([t],{skip_special_tokens:!1})}getVocabSize(){if(!this.tokenizer)throw new Error("Tokenizer not loaded. Call load() first.");return 50257}getTopKPredictions(t,n=10,r=1){if(!this.tokenizer)throw new Error("Tokenizer not loaded. Call load() first.");const s=t.map(u=>u/r),i=Math.max(...s),o=s.map(u=>Math.exp(u-i)),a=o.reduce((u,p)=>u+p,0),c=o.map(u=>u/a).map((u,p)=>({prob:u,index:p,logit:t[p]}));return c.sort((u,p)=>p.prob-u.prob),c.slice(0,n).map((u,p)=>({token:this.decodeToken(u.index),tokenId:u.index,probability:u.prob,logit:u.logit,rank:p+1}))}dispose(){this.tokenizer=null}}const Ni={vocabSize:50257,nLayer:12,nHead:12,nEmbd:768},LV={temperature:1,topK:10,topP:.9,selectedLayer:0,selectedHead:0,showAllHeads:!1,animationSpeed:"normal"};class ai{static instance=null;modelLoader;tokenizer;inferenceState={isRunning:!1,currentStage:"idle",result:null};constructor(){this.modelLoader=Fo.getInstance(),this.tokenizer=Oo.getInstance()}static getInstance(){return ai.instance||(ai.instance=new ai),ai.instance}async loadModel(t){await Promise.all([this.modelLoader.loadModel(t),this.tokenizer.load()])}isModelLoaded(){return this.modelLoader.isModelLoaded()&&this.tokenizer.isLoaded()}isModelLoading(){return this.modelLoader.isModelLoading()}getModelLoadingState(){return this.modelLoader.getLoadingState()}getInferenceState(){return{...this.inferenceState}}async runInference(t,n,r){if(!this.isModelLoaded())throw new Error("Model not loaded. Call loadModel() first.");if(this.inferenceState.isRunning)throw new Error("Inference already in progress.");this.inferenceState={isRunning:!0,currentStage:"idle",result:null};try{this.updateStage("tokenizing",r),await this.delay(100);const s=this.tokenizer.tokenize(t);this.updateStage("embedding",r),await this.delay(100);const i=this.generateEmbeddingData(s);this.updateStage("attention",r),await this.delay(100);const o=this.generateAttentionData(s.length);this.updateStage("mlp",r),await this.delay(100);const a=this.generateLayerOutputs(s.length);this.updateStage("output",r);const c=(await this.modelLoader.generate(t,{maxNewTokens:1,temperature:n.temperature,topK:n.topK,topP:n.topP,doSample:n.temperature>0})).slice(t.length),d=this.generatePredictions(c,n),u={inputText:t,tokens:s,embeddings:i,attention:o,layerOutputs:a,logits:d.map(p=>p.logit),predictions:d,generatedToken:c};return this.updateStage("complete",r),this.inferenceState.result=u,u}catch(s){throw this.inferenceState.error=s instanceof Error?s.message:"Unknown error",this.updateStage("idle",r),s}finally{this.inferenceState.isRunning=!1}}updateStage(t,n){this.inferenceState.currentStage=t,n?.(this.getInferenceState())}delay(t){return new Promise(n=>setTimeout(n,t))}generateEmbeddingData(t){const{nEmbd:n}=Ni;return t.map((r,s)=>{const i=this.generateEmbeddingVector(r.tokenId,n),o=this.generatePositionEmbedding(s,n),a=i.map((l,c)=>l+o[c]);return{tokenId:r.tokenId,tokenEmbedding:i,positionEmbedding:o,combined:a}})}generateEmbeddingVector(t,n){const r=[];let s=t;for(let i=0;i<n;i++)s=s*1103515245+12345&2147483647,r.push(s/2147483647*2-1);return r}generatePositionEmbedding(t,n){const r=[];for(let s=0;s<n;s++){const i=t/Math.pow(1e4,2*Math.floor(s/2)/n);r.push(s%2===0?Math.sin(i):Math.cos(i))}return r}generateAttentionData(t){const{nLayer:n,nHead:r}=Ni;return{layers:Array.from({length:n},(i,o)=>Array.from({length:r},(a,l)=>{const c=this.generateCausalAttention(t,o,l);return{layer:o,head:l,weights:c}}))}}generateCausalAttention(t,n,r){const s=[];for(let i=0;i<t;i++){const o=[];let a=0;for(let l=0;l<t;l++)if(l<=i){const c=(n*12+r)*1e3+i*100+l,d=Math.abs(Math.sin(c)*Math.cos(c*.7));o.push(d),a+=d}else o.push(0);s.push(o.map(l=>l/a))}return s}generateLayerOutputs(t){const{nLayer:n,nEmbd:r}=Ni;return Array.from({length:n},(s,i)=>({layer:i,attentionOutput:this.generateRandomVector(r*t),mlpInput:this.generateRandomVector(r*t),mlpOutput:this.generateRandomVector(r*t),residualOutput:this.generateRandomVector(r*t)}))}generateRandomVector(t){return Array.from({length:Math.min(t,100)},()=>Math.random()*2-1)}generatePredictions(t,n){const r=[],s=this.tokenizer.encode(t)[0]||0;r.push({token:t||" ",tokenId:s,probability:.3+Math.random()*.4,logit:5+Math.random()*3,rank:1});const i=[" the"," a"," to"," and"," of"," is"," in"," that"," it"];let o=1-r[0].probability;for(let a=0;a<Math.min(n.topK-1,i.length);a++){const l=o*(.5*Math.pow(.6,a));r.push({token:i[a],tokenId:100+a,probability:l,logit:r[0].logit-(a+1)*.5,rank:a+2}),o-=l}return r}dispose(){this.modelLoader.dispose(),this.tokenizer.dispose(),this.inferenceState={isRunning:!1,currentStage:"idle",result:null}}}function DV(){const e=Me.useRef(null),[t,n]=Me.useState({status:"idle",progress:0,statusText:""});Me.useEffect(()=>(e.current=ai.getInstance(),e.current.isModelLoaded()&&n({status:"ready",progress:100,statusText:"Model ready!"}),()=>{}),[]);const r=Me.useCallback(async()=>{if(e.current||(e.current=ai.getInstance()),e.current.isModelLoaded()){n({status:"ready",progress:100,statusText:"Model ready!"});return}if(!e.current.isModelLoading())try{await e.current.loadModel(i=>{n(i)})}catch(i){n({status:"error",progress:0,statusText:"Failed to load model",error:i instanceof Error?i.message:"Unknown error"})}},[]),s=Me.useCallback(()=>{e.current&&e.current.dispose(),n({status:"idle",progress:0,statusText:""})},[]);return{loadingState:t,isLoaded:t.status==="ready",isLoading:t.status==="loading",error:t.error||null,loadModel:r,reset:s}}function $V(){const e=Me.useRef(null),t=DV(),[n,r]=Me.useState(!1),[s,i]=Me.useState("idle"),[o,a]=Me.useState(null),[l,c]=Me.useState(null),[d,u]=Me.useState(LV),p=Me.useCallback(()=>(e.current||(e.current=ai.getInstance()),e.current),[]),h=Me.useCallback(async(I,f)=>{const _=p();if(!_.isModelLoaded())return c("Model not loaded. Please load the model first."),null;if(n)return null;const T={...d,...f};r(!0),c(null),i("idle");try{const M=await _.runInference(I,T,v=>{i(v.currentStage),v.error&&c(v.error)});return a(M),M}catch(M){const v=M instanceof Error?M.message:"Inference failed";return c(v),null}finally{r(!1)}},[p,n,d]),m=Me.useCallback(I=>{u(f=>({...f,...I}))},[]),g=Me.useCallback(()=>{r(!1),i("idle"),a(null),c(null),t.reset()},[t]);return{loadingState:t.loadingState,isLoaded:t.isLoaded,isLoading:t.isLoading,loadModel:t.loadModel,isInferring:n,currentStage:s,result:o,error:l,runInference:h,reset:g,settings:d,updateSettings:m}}const JT=["bg-blue-500/20 border-blue-500/50 text-blue-300","bg-purple-500/20 border-purple-500/50 text-purple-300","bg-green-500/20 border-green-500/50 text-green-300","bg-orange-500/20 border-orange-500/50 text-orange-300","bg-pink-500/20 border-pink-500/50 text-pink-300","bg-cyan-500/20 border-cyan-500/50 text-cyan-300","bg-yellow-500/20 border-yellow-500/50 text-yellow-300","bg-red-500/20 border-red-500/50 text-red-300","bg-indigo-500/20 border-indigo-500/50 text-indigo-300","bg-teal-500/20 border-teal-500/50 text-teal-300"];function FV(e){return JT[e%JT.length]}function OV(e){let t=e.replace(/Ġ/g,"␣");return t=t.replace(/Ċ/g,"↵"),t===" "&&(t="␣"),t}const jV=({tokens:e,inputText:t,isActive:n,i18n:r})=>{const[s,i]=TP.useState(null);return w.jsxs("div",{className:`
        bg-gray-800/50 rounded-xl border p-4 transition-all duration-300
        ${n?"border-blue-500/50 shadow-lg shadow-blue-500/10":"border-gray-700/50"}
      `,children:[w.jsxs("div",{className:"flex items-center gap-2 mb-4",children:[w.jsx("div",{className:`
            w-8 h-8 rounded-full flex items-center justify-center text-sm font-bold
            ${n?"bg-blue-500 text-white":"bg-gray-700 text-gray-400"}
          `,children:"1"}),w.jsxs("div",{children:[w.jsx("h3",{className:"text-white font-semibold",children:r.stage1}),w.jsx("p",{className:"text-gray-400 text-sm",children:r.stage1Desc})]})]}),w.jsxs("div",{className:"mb-4",children:[w.jsx("div",{className:"text-xs text-gray-500 mb-1",children:"Input:"}),w.jsxs("div",{className:"bg-gray-900/50 rounded-lg p-3 font-mono text-sm text-gray-300",children:['"',t||"...",'"']})]}),w.jsx("div",{className:"flex justify-center my-2",children:w.jsx("svg",{className:`w-6 h-6 ${n?"text-blue-500":"text-gray-600"}`,fill:"none",viewBox:"0 0 24 24",stroke:"currentColor",children:w.jsx("path",{strokeLinecap:"round",strokeLinejoin:"round",strokeWidth:2,d:"M19 14l-7 7m0 0l-7-7m7 7V3"})})}),w.jsxs("div",{className:"mb-4",children:[w.jsxs("div",{className:"text-xs text-gray-500 mb-2",children:["Tokens (",e.length,"):"]}),w.jsxs("div",{className:"flex flex-wrap gap-2",children:[w.jsx(vr,{mode:"popLayout",children:e.map((o,a)=>w.jsxs(lt.div,{initial:{opacity:0,scale:.8,y:10},animate:{opacity:1,scale:1,y:0},exit:{opacity:0,scale:.8},transition:{duration:.3,delay:a*.05,type:"spring",stiffness:200},className:`
                  relative px-3 py-1.5 rounded-lg border font-mono text-sm
                  cursor-pointer transition-all duration-200
                  ${FV(o.tokenId)}
                  ${s===a?"scale-110 z-10":""}
                `,onMouseEnter:()=>i(a),onMouseLeave:()=>i(null),children:[w.jsx("span",{className:"relative z-10",children:OV(o.text)}),s===a&&w.jsxs(lt.div,{initial:{opacity:0,y:5},animate:{opacity:1,y:0},className:`absolute left-1/2 -translate-x-1/2 bottom-full mb-2
                               bg-gray-900 border border-gray-700 rounded-lg p-2
                               shadow-xl min-w-[120px] z-20`,children:[w.jsxs("div",{className:"text-xs space-y-1",children:[w.jsxs("div",{className:"flex justify-between",children:[w.jsx("span",{className:"text-gray-500",children:"Position:"}),w.jsx("span",{className:"text-white",children:o.id})]}),w.jsxs("div",{className:"flex justify-between",children:[w.jsx("span",{className:"text-gray-500",children:"Token ID:"}),w.jsx("span",{className:"text-white",children:o.tokenId})]}),w.jsxs("div",{className:"flex justify-between",children:[w.jsx("span",{className:"text-gray-500",children:"Text:"}),w.jsxs("span",{className:"text-white font-mono",children:['"',o.text,'"']})]})]}),w.jsx("div",{className:`absolute left-1/2 -translate-x-1/2 top-full
                                  border-4 border-transparent border-t-gray-700`})]})]},`${o.id}-${o.tokenId}`))}),e.length===0&&w.jsx("div",{className:"text-gray-500 text-sm italic",children:"Enter text to see tokenization"})]})]}),e.length>0&&w.jsxs("div",{className:"mt-4 pt-4 border-t border-gray-700/50",children:[w.jsx("div",{className:"text-xs text-gray-500 mb-2",children:"Token IDs:"}),w.jsxs("div",{className:"bg-gray-900/50 rounded-lg p-2 font-mono text-xs text-gray-400 overflow-x-auto",children:["[",e.map(o=>o.tokenId).join(", "),"]"]})]})]})},RV=({predictions:e,generatedToken:t,isActive:n,i18n:r})=>{const s=e.length>0?Math.max(...e.map(i=>i.probability)):1;return w.jsxs("div",{className:`
        bg-gray-800/50 rounded-xl border p-4 transition-all duration-300
        ${n?"border-green-500/50 shadow-lg shadow-green-500/10":"border-gray-700/50"}
      `,children:[w.jsxs("div",{className:"flex items-center gap-2 mb-4",children:[w.jsx("div",{className:`
            w-8 h-8 rounded-full flex items-center justify-center text-sm font-bold
            ${n?"bg-green-500 text-white":"bg-gray-700 text-gray-400"}
          `,children:"5"}),w.jsxs("div",{children:[w.jsx("h3",{className:"text-white font-semibold",children:r.stage5}),w.jsx("p",{className:"text-gray-400 text-sm",children:r.stage5Desc})]})]}),t&&w.jsxs("div",{className:"mb-4 p-3 bg-green-500/10 border border-green-500/30 rounded-lg",children:[w.jsxs("div",{className:"text-xs text-green-400 mb-1",children:[r.nextTokenPrediction,":"]}),w.jsxs("div",{className:"flex items-center gap-2",children:[w.jsxs("span",{className:"text-2xl font-bold text-green-400 font-mono",children:['"',t,'"']}),e[0]&&w.jsxs("span",{className:"text-sm text-gray-400",children:["(",(e[0].probability*100).toFixed(1),"%)"]})]})]}),w.jsxs("div",{className:"space-y-2",children:[w.jsx("div",{className:"text-xs text-gray-500 mb-2",children:"Top-K Predictions:"}),e.length===0?w.jsx("div",{className:"text-gray-500 text-sm italic",children:"Run inference to see predictions"}):e.slice(0,10).map((i,o)=>w.jsxs(lt.div,{initial:{opacity:0,x:-20},animate:{opacity:1,x:0},transition:{delay:o*.05},className:"flex items-center gap-2",children:[w.jsxs("div",{className:"w-6 text-xs text-gray-500 text-right",children:["#",i.rank]}),w.jsxs("div",{className:`
                  w-24 px-2 py-1 rounded font-mono text-sm truncate
                  ${o===0?"bg-green-500/20 text-green-300":"bg-gray-700/50 text-gray-300"}
                `,children:['"',i.token.replace(/ /g,"␣"),'"']}),w.jsx("div",{className:"flex-1 h-6 bg-gray-900/50 rounded overflow-hidden",children:w.jsx(lt.div,{initial:{width:0},animate:{width:`${i.probability/s*100}%`},transition:{duration:.5,delay:o*.05},className:`
                    h-full flex items-center px-2
                    ${o===0?"bg-green-500/50":"bg-blue-500/30"}
                  `,children:w.jsxs("span",{className:"text-xs text-white whitespace-nowrap",children:[(i.probability*100).toFixed(1),"%"]})})}),w.jsx("div",{className:"w-16 text-xs text-gray-500 text-right",children:i.logit.toFixed(2)})]},`${i.tokenId}-${o}`))]}),e.length>0&&w.jsxs("div",{className:"mt-4 pt-4 border-t border-gray-700/50 flex justify-between text-xs text-gray-500",children:[w.jsx("span",{children:r.probability}),w.jsx("span",{children:r.logit})]})]})},NV=Math.sqrt(50),zV=Math.sqrt(10),BV=Math.sqrt(2);function Tu(e,t,n){const r=(t-e)/Math.max(0,n),s=Math.floor(Math.log10(r)),i=r/Math.pow(10,s),o=i>=NV?10:i>=zV?5:i>=BV?2:1;let a,l,c;return s<0?(c=Math.pow(10,-s)/o,a=Math.round(e*c),l=Math.round(t*c),a/c<e&&++a,l/c>t&&--l,c=-c):(c=Math.pow(10,s)*o,a=Math.round(e/c),l=Math.round(t/c),a*c<e&&++a,l*c>t&&--l),l<a&&.5<=n&&n<2?Tu(e,t,n*2):[a,l,c]}function GV(e,t,n){if(t=+t,e=+e,n=+n,!(n>0))return[];if(e===t)return[e];const r=t<e,[s,i,o]=r?Tu(t,e,n):Tu(e,t,n);if(!(i>=s))return[];const a=i-s+1,l=new Array(a);if(r)if(o<0)for(let c=0;c<a;++c)l[c]=(i-c)/-o;else for(let c=0;c<a;++c)l[c]=(i-c)*o;else if(o<0)for(let c=0;c<a;++c)l[c]=(s+c)/-o;else for(let c=0;c<a;++c)l[c]=(s+c)*o;return l}function df(e,t,n){return t=+t,e=+e,n=+n,Tu(e,t,n)[2]}function VV(e,t,n){t=+t,e=+e,n=+n;const r=t<e,s=r?df(t,e,n):df(e,t,n);return(r?-1:1)*(s<0?1/-s:s)}var UV={value:()=>{}};function t3(){for(var e=0,t=arguments.length,n={},r;e<t;++e){if(!(r=arguments[e]+"")||r in n||/[\s.]/.test(r))throw new Error("illegal type: "+r);n[r]=[]}return new su(n)}function su(e){this._=e}function WV(e,t){return e.trim().split(/^|\s+/).map(function(n){var r="",s=n.indexOf(".");if(s>=0&&(r=n.slice(s+1),n=n.slice(0,s)),n&&!t.hasOwnProperty(n))throw new Error("unknown type: "+n);return{type:n,name:r}})}su.prototype=t3.prototype={constructor:su,on:function(e,t){var n=this._,r=WV(e+"",n),s,i=-1,o=r.length;if(arguments.length<2){for(;++i<o;)if((s=(e=r[i]).type)&&(s=HV(n[s],e.name)))return s;return}if(t!=null&&typeof t!="function")throw new Error("invalid callback: "+t);for(;++i<o;)if(s=(e=r[i]).type)n[s]=ZT(n[s],e.name,t);else if(t==null)for(s in n)n[s]=ZT(n[s],e.name,null);return this},copy:function(){var e={},t=this._;for(var n in t)e[n]=t[n].slice();return new su(e)},call:function(e,t){if((s=arguments.length-2)>0)for(var n=new Array(s),r=0,s,i;r<s;++r)n[r]=arguments[r+2];if(!this._.hasOwnProperty(e))throw new Error("unknown type: "+e);for(i=this._[e],r=0,s=i.length;r<s;++r)i[r].value.apply(t,n)},apply:function(e,t,n){if(!this._.hasOwnProperty(e))throw new Error("unknown type: "+e);for(var r=this._[e],s=0,i=r.length;s<i;++s)r[s].value.apply(t,n)}};function HV(e,t){for(var n=0,r=e.length,s;n<r;++n)if((s=e[n]).name===t)return s.value}function ZT(e,t,n){for(var r=0,s=e.length;r<s;++r)if(e[r].name===t){e[r]=UV,e=e.slice(0,r).concat(e.slice(r+1));break}return n!=null&&e.push({name:t,value:n}),e}var pf="http://www.w3.org/1999/xhtml";const eP={svg:"http://www.w3.org/2000/svg",xhtml:pf,xlink:"http://www.w3.org/1999/xlink",xml:"http://www.w3.org/XML/1998/namespace",xmlns:"http://www.w3.org/2000/xmlns/"};function qu(e){var t=e+="",n=t.indexOf(":");return n>=0&&(t=e.slice(0,n))!=="xmlns"&&(e=e.slice(n+1)),eP.hasOwnProperty(t)?{space:eP[t],local:e}:e}function KV(e){return function(){var t=this.ownerDocument,n=this.namespaceURI;return n===pf&&t.documentElement.namespaceURI===pf?t.createElement(e):t.createElementNS(n,e)}}function qV(e){return function(){return this.ownerDocument.createElementNS(e.space,e.local)}}function n3(e){var t=qu(e);return(t.local?qV:KV)(t)}function QV(){}function Wg(e){return e==null?QV:function(){return this.querySelector(e)}}function XV(e){typeof e!="function"&&(e=Wg(e));for(var t=this._groups,n=t.length,r=new Array(n),s=0;s<n;++s)for(var i=t[s],o=i.length,a=r[s]=new Array(o),l,c,d=0;d<o;++d)(l=i[d])&&(c=e.call(l,l.__data__,d,i))&&("__data__"in l&&(c.__data__=l.__data__),a[d]=c);return new Lr(r,this._parents)}function YV(e){return e==null?[]:Array.isArray(e)?e:Array.from(e)}function JV(){return[]}function r3(e){return e==null?JV:function(){return this.querySelectorAll(e)}}function ZV(e){return function(){return YV(e.apply(this,arguments))}}function eU(e){typeof e=="function"?e=ZV(e):e=r3(e);for(var t=this._groups,n=t.length,r=[],s=[],i=0;i<n;++i)for(var o=t[i],a=o.length,l,c=0;c<a;++c)(l=o[c])&&(r.push(e.call(l,l.__data__,c,o)),s.push(l));return new Lr(r,s)}function s3(e){return function(){return this.matches(e)}}function i3(e){return function(t){return t.matches(e)}}var tU=Array.prototype.find;function nU(e){return function(){return tU.call(this.children,e)}}function rU(){return this.firstElementChild}function sU(e){return this.select(e==null?rU:nU(typeof e=="function"?e:i3(e)))}var iU=Array.prototype.filter;function oU(){return Array.from(this.children)}function aU(e){return function(){return iU.call(this.children,e)}}function lU(e){return this.selectAll(e==null?oU:aU(typeof e=="function"?e:i3(e)))}function cU(e){typeof e!="function"&&(e=s3(e));for(var t=this._groups,n=t.length,r=new Array(n),s=0;s<n;++s)for(var i=t[s],o=i.length,a=r[s]=[],l,c=0;c<o;++c)(l=i[c])&&e.call(l,l.__data__,c,i)&&a.push(l);return new Lr(r,this._parents)}function o3(e){return new Array(e.length)}function uU(){return new Lr(this._enter||this._groups.map(o3),this._parents)}function Pu(e,t){this.ownerDocument=e.ownerDocument,this.namespaceURI=e.namespaceURI,this._next=null,this._parent=e,this.__data__=t}Pu.prototype={constructor:Pu,appendChild:function(e){return this._parent.insertBefore(e,this._next)},insertBefore:function(e,t){return this._parent.insertBefore(e,t)},querySelector:function(e){return this._parent.querySelector(e)},querySelectorAll:function(e){return this._parent.querySelectorAll(e)}};function dU(e){return function(){return e}}function pU(e,t,n,r,s,i){for(var o=0,a,l=t.length,c=i.length;o<c;++o)(a=t[o])?(a.__data__=i[o],r[o]=a):n[o]=new Pu(e,i[o]);for(;o<l;++o)(a=t[o])&&(s[o]=a)}function hU(e,t,n,r,s,i,o){var a,l,c=new Map,d=t.length,u=i.length,p=new Array(d),h;for(a=0;a<d;++a)(l=t[a])&&(p[a]=h=o.call(l,l.__data__,a,t)+"",c.has(h)?s[a]=l:c.set(h,l));for(a=0;a<u;++a)h=o.call(e,i[a],a,i)+"",(l=c.get(h))?(r[a]=l,l.__data__=i[a],c.delete(h)):n[a]=new Pu(e,i[a]);for(a=0;a<d;++a)(l=t[a])&&c.get(p[a])===l&&(s[a]=l)}function mU(e){return e.__data__}function fU(e,t){if(!arguments.length)return Array.from(this,mU);var n=t?hU:pU,r=this._parents,s=this._groups;typeof e!="function"&&(e=dU(e));for(var i=s.length,o=new Array(i),a=new Array(i),l=new Array(i),c=0;c<i;++c){var d=r[c],u=s[c],p=u.length,h=gU(e.call(d,d&&d.__data__,c,r)),m=h.length,g=a[c]=new Array(m),I=o[c]=new Array(m),f=l[c]=new Array(p);n(d,u,g,I,f,h,t);for(var _=0,T=0,M,v;_<m;++_)if(M=g[_]){for(_>=T&&(T=_+1);!(v=I[T])&&++T<m;);M._next=v||null}}return o=new Lr(o,r),o._enter=a,o._exit=l,o}function gU(e){return typeof e=="object"&&"length"in e?e:Array.from(e)}function _U(){return new Lr(this._exit||this._groups.map(o3),this._parents)}function yU(e,t,n){var r=this.enter(),s=this,i=this.exit();return typeof e=="function"?(r=e(r),r&&(r=r.selection())):r=r.append(e+""),t!=null&&(s=t(s),s&&(s=s.selection())),n==null?i.remove():n(i),r&&s?r.merge(s).order():s}function xU(e){for(var t=e.selection?e.selection():e,n=this._groups,r=t._groups,s=n.length,i=r.length,o=Math.min(s,i),a=new Array(s),l=0;l<o;++l)for(var c=n[l],d=r[l],u=c.length,p=a[l]=new Array(u),h,m=0;m<u;++m)(h=c[m]||d[m])&&(p[m]=h);for(;l<s;++l)a[l]=n[l];return new Lr(a,this._parents)}function bU(){for(var e=this._groups,t=-1,n=e.length;++t<n;)for(var r=e[t],s=r.length-1,i=r[s],o;--s>=0;)(o=r[s])&&(i&&o.compareDocumentPosition(i)^4&&i.parentNode.insertBefore(o,i),i=o);return this}function wU(e){e||(e=vU);function t(u,p){return u&&p?e(u.__data__,p.__data__):!u-!p}for(var n=this._groups,r=n.length,s=new Array(r),i=0;i<r;++i){for(var o=n[i],a=o.length,l=s[i]=new Array(a),c,d=0;d<a;++d)(c=o[d])&&(l[d]=c);l.sort(t)}return new Lr(s,this._parents).order()}function vU(e,t){return e<t?-1:e>t?1:e>=t?0:NaN}function MU(){var e=arguments[0];return arguments[0]=this,e.apply(null,arguments),this}function AU(){return Array.from(this)}function TU(){for(var e=this._groups,t=0,n=e.length;t<n;++t)for(var r=e[t],s=0,i=r.length;s<i;++s){var o=r[s];if(o)return o}return null}function PU(){let e=0;for(const t of this)++e;return e}function CU(){return!this.node()}function IU(e){for(var t=this._groups,n=0,r=t.length;n<r;++n)for(var s=t[n],i=0,o=s.length,a;i<o;++i)(a=s[i])&&e.call(a,a.__data__,i,s);return this}function kU(e){return function(){this.removeAttribute(e)}}function EU(e){return function(){this.removeAttributeNS(e.space,e.local)}}function SU(e,t){return function(){this.setAttribute(e,t)}}function LU(e,t){return function(){this.setAttributeNS(e.space,e.local,t)}}function DU(e,t){return function(){var n=t.apply(this,arguments);n==null?this.removeAttribute(e):this.setAttribute(e,n)}}function $U(e,t){return function(){var n=t.apply(this,arguments);n==null?this.removeAttributeNS(e.space,e.local):this.setAttributeNS(e.space,e.local,n)}}function FU(e,t){var n=qu(e);if(arguments.length<2){var r=this.node();return n.local?r.getAttributeNS(n.space,n.local):r.getAttribute(n)}return this.each((t==null?n.local?EU:kU:typeof t=="function"?n.local?$U:DU:n.local?LU:SU)(n,t))}function a3(e){return e.ownerDocument&&e.ownerDocument.defaultView||e.document&&e||e.defaultView}function OU(e){return function(){this.style.removeProperty(e)}}function jU(e,t,n){return function(){this.style.setProperty(e,t,n)}}function RU(e,t,n){return function(){var r=t.apply(this,arguments);r==null?this.style.removeProperty(e):this.style.setProperty(e,r,n)}}function NU(e,t,n){return arguments.length>1?this.each((t==null?OU:typeof t=="function"?RU:jU)(e,t,n??"")):Wo(this.node(),e)}function Wo(e,t){return e.style.getPropertyValue(t)||a3(e).getComputedStyle(e,null).getPropertyValue(t)}function zU(e){return function(){delete this[e]}}function BU(e,t){return function(){this[e]=t}}function GU(e,t){return function(){var n=t.apply(this,arguments);n==null?delete this[e]:this[e]=n}}function VU(e,t){return arguments.length>1?this.each((t==null?zU:typeof t=="function"?GU:BU)(e,t)):this.node()[e]}function l3(e){return e.trim().split(/^|\s+/)}function Hg(e){return e.classList||new c3(e)}function c3(e){this._node=e,this._names=l3(e.getAttribute("class")||"")}c3.prototype={add:function(e){var t=this._names.indexOf(e);t<0&&(this._names.push(e),this._node.setAttribute("class",this._names.join(" ")))},remove:function(e){var t=this._names.indexOf(e);t>=0&&(this._names.splice(t,1),this._node.setAttribute("class",this._names.join(" ")))},contains:function(e){return this._names.indexOf(e)>=0}};function u3(e,t){for(var n=Hg(e),r=-1,s=t.length;++r<s;)n.add(t[r])}function d3(e,t){for(var n=Hg(e),r=-1,s=t.length;++r<s;)n.remove(t[r])}function UU(e){return function(){u3(this,e)}}function WU(e){return function(){d3(this,e)}}function HU(e,t){return function(){(t.apply(this,arguments)?u3:d3)(this,e)}}function KU(e,t){var n=l3(e+"");if(arguments.length<2){for(var r=Hg(this.node()),s=-1,i=n.length;++s<i;)if(!r.contains(n[s]))return!1;return!0}return this.each((typeof t=="function"?HU:t?UU:WU)(n,t))}function qU(){this.textContent=""}function QU(e){return function(){this.textContent=e}}function XU(e){return function(){var t=e.apply(this,arguments);this.textContent=t??""}}function YU(e){return arguments.length?this.each(e==null?qU:(typeof e=="function"?XU:QU)(e)):this.node().textContent}function JU(){this.innerHTML=""}function ZU(e){return function(){this.innerHTML=e}}function eW(e){return function(){var t=e.apply(this,arguments);this.innerHTML=t??""}}function tW(e){return arguments.length?this.each(e==null?JU:(typeof e=="function"?eW:ZU)(e)):this.node().innerHTML}function nW(){this.nextSibling&&this.parentNode.appendChild(this)}function rW(){return this.each(nW)}function sW(){this.previousSibling&&this.parentNode.insertBefore(this,this.parentNode.firstChild)}function iW(){return this.each(sW)}function oW(e){var t=typeof e=="function"?e:n3(e);return this.select(function(){return this.appendChild(t.apply(this,arguments))})}function aW(){return null}function lW(e,t){var n=typeof e=="function"?e:n3(e),r=t==null?aW:typeof t=="function"?t:Wg(t);return this.select(function(){return this.insertBefore(n.apply(this,arguments),r.apply(this,arguments)||null)})}function cW(){var e=this.parentNode;e&&e.removeChild(this)}function uW(){return this.each(cW)}function dW(){var e=this.cloneNode(!1),t=this.parentNode;return t?t.insertBefore(e,this.nextSibling):e}function pW(){var e=this.cloneNode(!0),t=this.parentNode;return t?t.insertBefore(e,this.nextSibling):e}function hW(e){return this.select(e?pW:dW)}function mW(e){return arguments.length?this.property("__data__",e):this.node().__data__}function fW(e){return function(t){e.call(this,t,this.__data__)}}function gW(e){return e.trim().split(/^|\s+/).map(function(t){var n="",r=t.indexOf(".");return r>=0&&(n=t.slice(r+1),t=t.slice(0,r)),{type:t,name:n}})}function _W(e){return function(){var t=this.__on;if(t){for(var n=0,r=-1,s=t.length,i;n<s;++n)i=t[n],(!e.type||i.type===e.type)&&i.name===e.name?this.removeEventListener(i.type,i.listener,i.options):t[++r]=i;++r?t.length=r:delete this.__on}}}function yW(e,t,n){return function(){var r=this.__on,s,i=fW(t);if(r){for(var o=0,a=r.length;o<a;++o)if((s=r[o]).type===e.type&&s.name===e.name){this.removeEventListener(s.type,s.listener,s.options),this.addEventListener(s.type,s.listener=i,s.options=n),s.value=t;return}}this.addEventListener(e.type,i,n),s={type:e.type,name:e.name,value:t,listener:i,options:n},r?r.push(s):this.__on=[s]}}function xW(e,t,n){var r=gW(e+""),s,i=r.length,o;if(arguments.length<2){var a=this.node().__on;if(a){for(var l=0,c=a.length,d;l<c;++l)for(s=0,d=a[l];s<i;++s)if((o=r[s]).type===d.type&&o.name===d.name)return d.value}return}for(a=t?yW:_W,s=0;s<i;++s)this.each(a(r[s],t,n));return this}function p3(e,t,n){var r=a3(e),s=r.CustomEvent;typeof s=="function"?s=new s(t,n):(s=r.document.createEvent("Event"),n?(s.initEvent(t,n.bubbles,n.cancelable),s.detail=n.detail):s.initEvent(t,!1,!1)),e.dispatchEvent(s)}function bW(e,t){return function(){return p3(this,e,t)}}function wW(e,t){return function(){return p3(this,e,t.apply(this,arguments))}}function vW(e,t){return this.each((typeof t=="function"?wW:bW)(e,t))}function*MW(){for(var e=this._groups,t=0,n=e.length;t<n;++t)for(var r=e[t],s=0,i=r.length,o;s<i;++s)(o=r[s])&&(yield o)}var h3=[null];function Lr(e,t){this._groups=e,this._parents=t}function Cl(){return new Lr([[document.documentElement]],h3)}function AW(){return this}Lr.prototype=Cl.prototype={constructor:Lr,select:XV,selectAll:eU,selectChild:sU,selectChildren:lU,filter:cU,data:fU,enter:uU,exit:_U,join:yU,merge:xU,selection:AW,order:bU,sort:wU,call:MU,nodes:AU,node:TU,size:PU,empty:CU,each:IU,attr:FU,style:NU,property:VU,classed:KU,text:YU,html:tW,raise:rW,lower:iW,append:oW,insert:lW,remove:uW,clone:hW,datum:mW,on:xW,dispatch:vW,[Symbol.iterator]:MW};function im(e){return typeof e=="string"?new Lr([[document.querySelector(e)]],[document.documentElement]):new Lr([[e]],h3)}function Kg(e,t,n){e.prototype=t.prototype=n,n.constructor=e}function m3(e,t){var n=Object.create(e.prototype);for(var r in t)n[r]=t[r];return n}function Il(){}var _l=.7,Cu=1/_l,zo="\\s*([+-]?\\d+)\\s*",yl="\\s*([+-]?(?:\\d*\\.)?\\d+(?:[eE][+-]?\\d+)?)\\s*",Is="\\s*([+-]?(?:\\d*\\.)?\\d+(?:[eE][+-]?\\d+)?)%\\s*",TW=/^#([0-9a-f]{3,8})$/,PW=new RegExp(`^rgb\\(${zo},${zo},${zo}\\)$`),CW=new RegExp(`^rgb\\(${Is},${Is},${Is}\\)$`),IW=new RegExp(`^rgba\\(${zo},${zo},${zo},${yl}\\)$`),kW=new RegExp(`^rgba\\(${Is},${Is},${Is},${yl}\\)$`),EW=new RegExp(`^hsl\\(${yl},${Is},${Is}\\)$`),SW=new RegExp(`^hsla\\(${yl},${Is},${Is},${yl}\\)$`),tP={aliceblue:15792383,antiquewhite:16444375,aqua:65535,aquamarine:8388564,azure:15794175,beige:16119260,bisque:16770244,black:0,blanchedalmond:16772045,blue:255,blueviolet:9055202,brown:10824234,burlywood:14596231,cadetblue:6266528,chartreuse:8388352,chocolate:13789470,coral:16744272,cornflowerblue:6591981,cornsilk:16775388,crimson:14423100,cyan:65535,darkblue:139,darkcyan:35723,darkgoldenrod:12092939,darkgray:11119017,darkgreen:25600,darkgrey:11119017,darkkhaki:12433259,darkmagenta:9109643,darkolivegreen:5597999,darkorange:16747520,darkorchid:10040012,darkred:9109504,darksalmon:15308410,darkseagreen:9419919,darkslateblue:4734347,darkslategray:3100495,darkslategrey:3100495,darkturquoise:52945,darkviolet:9699539,deeppink:16716947,deepskyblue:49151,dimgray:6908265,dimgrey:6908265,dodgerblue:2003199,firebrick:11674146,floralwhite:16775920,forestgreen:2263842,fuchsia:16711935,gainsboro:14474460,ghostwhite:16316671,gold:16766720,goldenrod:14329120,gray:8421504,green:32768,greenyellow:11403055,grey:8421504,honeydew:15794160,hotpink:16738740,indianred:13458524,indigo:4915330,ivory:16777200,khaki:15787660,lavender:15132410,lavenderblush:16773365,lawngreen:8190976,lemonchiffon:16775885,lightblue:11393254,lightcoral:15761536,lightcyan:14745599,lightgoldenrodyellow:16448210,lightgray:13882323,lightgreen:9498256,lightgrey:13882323,lightpink:16758465,lightsalmon:16752762,lightseagreen:2142890,lightskyblue:8900346,lightslategray:7833753,lightslategrey:7833753,lightsteelblue:11584734,lightyellow:16777184,lime:65280,limegreen:3329330,linen:16445670,magenta:16711935,maroon:8388608,mediumaquamarine:6737322,mediumblue:205,mediumorchid:12211667,mediumpurple:9662683,mediumseagreen:3978097,mediumslateblue:8087790,mediumspringgreen:64154,mediumturquoise:4772300,mediumvioletred:13047173,midnightblue:1644912,mintcream:16121850,mistyrose:16770273,moccasin:16770229,navajowhite:16768685,navy:128,oldlace:16643558,olive:8421376,olivedrab:7048739,orange:16753920,orangered:16729344,orchid:14315734,palegoldenrod:15657130,palegreen:10025880,paleturquoise:11529966,palevioletred:14381203,papayawhip:16773077,peachpuff:16767673,peru:13468991,pink:16761035,plum:14524637,powderblue:11591910,purple:8388736,rebeccapurple:6697881,red:16711680,rosybrown:12357519,royalblue:4286945,saddlebrown:9127187,salmon:16416882,sandybrown:16032864,seagreen:3050327,seashell:16774638,sienna:10506797,silver:12632256,skyblue:8900331,slateblue:6970061,slategray:7372944,slategrey:7372944,snow:16775930,springgreen:65407,steelblue:4620980,tan:13808780,teal:32896,thistle:14204888,tomato:16737095,turquoise:4251856,violet:15631086,wheat:16113331,white:16777215,whitesmoke:16119285,yellow:16776960,yellowgreen:10145074};Kg(Il,qi,{copy(e){return Object.assign(new this.constructor,this,e)},displayable(){return this.rgb().displayable()},hex:nP,formatHex:nP,formatHex8:LW,formatHsl:DW,formatRgb:rP,toString:rP});function nP(){return this.rgb().formatHex()}function LW(){return this.rgb().formatHex8()}function DW(){return f3(this).formatHsl()}function rP(){return this.rgb().formatRgb()}function qi(e){var t,n;return e=(e+"").trim().toLowerCase(),(t=TW.exec(e))?(n=t[1].length,t=parseInt(t[1],16),n===6?sP(t):n===3?new wr(t>>8&15|t>>4&240,t>>4&15|t&240,(t&15)<<4|t&15,1):n===8?Gc(t>>24&255,t>>16&255,t>>8&255,(t&255)/255):n===4?Gc(t>>12&15|t>>8&240,t>>8&15|t>>4&240,t>>4&15|t&240,((t&15)<<4|t&15)/255):null):(t=PW.exec(e))?new wr(t[1],t[2],t[3],1):(t=CW.exec(e))?new wr(t[1]*255/100,t[2]*255/100,t[3]*255/100,1):(t=IW.exec(e))?Gc(t[1],t[2],t[3],t[4]):(t=kW.exec(e))?Gc(t[1]*255/100,t[2]*255/100,t[3]*255/100,t[4]):(t=EW.exec(e))?aP(t[1],t[2]/100,t[3]/100,1):(t=SW.exec(e))?aP(t[1],t[2]/100,t[3]/100,t[4]):tP.hasOwnProperty(e)?sP(tP[e]):e==="transparent"?new wr(NaN,NaN,NaN,0):null}function sP(e){return new wr(e>>16&255,e>>8&255,e&255,1)}function Gc(e,t,n,r){return r<=0&&(e=t=n=NaN),new wr(e,t,n,r)}function $W(e){return e instanceof Il||(e=qi(e)),e?(e=e.rgb(),new wr(e.r,e.g,e.b,e.opacity)):new wr}function Iu(e,t,n,r){return arguments.length===1?$W(e):new wr(e,t,n,r??1)}function wr(e,t,n,r){this.r=+e,this.g=+t,this.b=+n,this.opacity=+r}Kg(wr,Iu,m3(Il,{brighter(e){return e=e==null?Cu:Math.pow(Cu,e),new wr(this.r*e,this.g*e,this.b*e,this.opacity)},darker(e){return e=e==null?_l:Math.pow(_l,e),new wr(this.r*e,this.g*e,this.b*e,this.opacity)},rgb(){return this},clamp(){return new wr(Gi(this.r),Gi(this.g),Gi(this.b),ku(this.opacity))},displayable(){return-.5<=this.r&&this.r<255.5&&-.5<=this.g&&this.g<255.5&&-.5<=this.b&&this.b<255.5&&0<=this.opacity&&this.opacity<=1},hex:iP,formatHex:iP,formatHex8:FW,formatRgb:oP,toString:oP}));function iP(){return`#${zi(this.r)}${zi(this.g)}${zi(this.b)}`}function FW(){return`#${zi(this.r)}${zi(this.g)}${zi(this.b)}${zi((isNaN(this.opacity)?1:this.opacity)*255)}`}function oP(){const e=ku(this.opacity);return`${e===1?"rgb(":"rgba("}${Gi(this.r)}, ${Gi(this.g)}, ${Gi(this.b)}${e===1?")":`, ${e})`}`}function ku(e){return isNaN(e)?1:Math.max(0,Math.min(1,e))}function Gi(e){return Math.max(0,Math.min(255,Math.round(e)||0))}function zi(e){return e=Gi(e),(e<16?"0":"")+e.toString(16)}function aP(e,t,n,r){return r<=0?e=t=n=NaN:n<=0||n>=1?e=t=NaN:t<=0&&(e=NaN),new as(e,t,n,r)}function f3(e){if(e instanceof as)return new as(e.h,e.s,e.l,e.opacity);if(e instanceof Il||(e=qi(e)),!e)return new as;if(e instanceof as)return e;e=e.rgb();var t=e.r/255,n=e.g/255,r=e.b/255,s=Math.min(t,n,r),i=Math.max(t,n,r),o=NaN,a=i-s,l=(i+s)/2;return a?(t===i?o=(n-r)/a+(n<r)*6:n===i?o=(r-t)/a+2:o=(t-n)/a+4,a/=l<.5?i+s:2-i-s,o*=60):a=l>0&&l<1?0:o,new as(o,a,l,e.opacity)}function OW(e,t,n,r){return arguments.length===1?f3(e):new as(e,t,n,r??1)}function as(e,t,n,r){this.h=+e,this.s=+t,this.l=+n,this.opacity=+r}Kg(as,OW,m3(Il,{brighter(e){return e=e==null?Cu:Math.pow(Cu,e),new as(this.h,this.s,this.l*e,this.opacity)},darker(e){return e=e==null?_l:Math.pow(_l,e),new as(this.h,this.s,this.l*e,this.opacity)},rgb(){var e=this.h%360+(this.h<0)*360,t=isNaN(e)||isNaN(this.s)?0:this.s,n=this.l,r=n+(n<.5?n:1-n)*t,s=2*n-r;return new wr(om(e>=240?e-240:e+120,s,r),om(e,s,r),om(e<120?e+240:e-120,s,r),this.opacity)},clamp(){return new as(lP(this.h),Vc(this.s),Vc(this.l),ku(this.opacity))},displayable(){return(0<=this.s&&this.s<=1||isNaN(this.s))&&0<=this.l&&this.l<=1&&0<=this.opacity&&this.opacity<=1},formatHsl(){const e=ku(this.opacity);return`${e===1?"hsl(":"hsla("}${lP(this.h)}, ${Vc(this.s)*100}%, ${Vc(this.l)*100}%${e===1?")":`, ${e})`}`}}));function lP(e){return e=(e||0)%360,e<0?e+360:e}function Vc(e){return Math.max(0,Math.min(1,e||0))}function om(e,t,n){return(e<60?t+(n-t)*e/60:e<180?n:e<240?t+(n-t)*(240-e)/60:t)*255}function jW(e,t,n,r,s){var i=e*e,o=i*e;return((1-3*e+3*i-o)*t+(4-6*i+3*o)*n+(1+3*e+3*i-3*o)*r+o*s)/6}function RW(e){var t=e.length-1;return function(n){var r=n<=0?n=0:n>=1?(n=1,t-1):Math.floor(n*t),s=e[r],i=e[r+1],o=r>0?e[r-1]:2*s-i,a=r<t-1?e[r+2]:2*i-s;return jW((n-r/t)*t,o,s,i,a)}}const qg=e=>()=>e;function NW(e,t){return function(n){return e+n*t}}function zW(e,t,n){return e=Math.pow(e,n),t=Math.pow(t,n)-e,n=1/n,function(r){return Math.pow(e+r*t,n)}}function BW(e){return(e=+e)==1?g3:function(t,n){return n-t?zW(t,n,e):qg(isNaN(t)?n:t)}}function g3(e,t){var n=t-e;return n?NW(e,n):qg(isNaN(e)?t:e)}const Eu=(function e(t){var n=BW(t);function r(s,i){var o=n((s=Iu(s)).r,(i=Iu(i)).r),a=n(s.g,i.g),l=n(s.b,i.b),c=g3(s.opacity,i.opacity);return function(d){return s.r=o(d),s.g=a(d),s.b=l(d),s.opacity=c(d),s+""}}return r.gamma=e,r})(1);function GW(e){return function(t){var n=t.length,r=new Array(n),s=new Array(n),i=new Array(n),o,a;for(o=0;o<n;++o)a=Iu(t[o]),r[o]=a.r||0,s[o]=a.g||0,i[o]=a.b||0;return r=e(r),s=e(s),i=e(i),a.opacity=1,function(l){return a.r=r(l),a.g=s(l),a.b=i(l),a+""}}}var VW=GW(RW);function UW(e,t){t||(t=[]);var n=e?Math.min(t.length,e.length):0,r=t.slice(),s;return function(i){for(s=0;s<n;++s)r[s]=e[s]*(1-i)+t[s]*i;return r}}function WW(e){return ArrayBuffer.isView(e)&&!(e instanceof DataView)}function HW(e,t){var n=t?t.length:0,r=e?Math.min(n,e.length):0,s=new Array(r),i=new Array(n),o;for(o=0;o<r;++o)s[o]=Qg(e[o],t[o]);for(;o<n;++o)i[o]=t[o];return function(a){for(o=0;o<r;++o)i[o]=s[o](a);return i}}function KW(e,t){var n=new Date;return e=+e,t=+t,function(r){return n.setTime(e*(1-r)+t*r),n}}function Ps(e,t){return e=+e,t=+t,function(n){return e*(1-n)+t*n}}function qW(e,t){var n={},r={},s;(e===null||typeof e!="object")&&(e={}),(t===null||typeof t!="object")&&(t={});for(s in t)s in e?n[s]=Qg(e[s],t[s]):r[s]=t[s];return function(i){for(s in n)r[s]=n[s](i);return r}}var hf=/[-+]?(?:\d+\.?\d*|\.?\d+)(?:[eE][-+]?\d+)?/g,am=new RegExp(hf.source,"g");function QW(e){return function(){return e}}function XW(e){return function(t){return e(t)+""}}function _3(e,t){var n=hf.lastIndex=am.lastIndex=0,r,s,i,o=-1,a=[],l=[];for(e=e+"",t=t+"";(r=hf.exec(e))&&(s=am.exec(t));)(i=s.index)>n&&(i=t.slice(n,i),a[o]?a[o]+=i:a[++o]=i),(r=r[0])===(s=s[0])?a[o]?a[o]+=s:a[++o]=s:(a[++o]=null,l.push({i:o,x:Ps(r,s)})),n=am.lastIndex;return n<t.length&&(i=t.slice(n),a[o]?a[o]+=i:a[++o]=i),a.length<2?l[0]?XW(l[0].x):QW(t):(t=l.length,function(c){for(var d=0,u;d<t;++d)a[(u=l[d]).i]=u.x(c);return a.join("")})}function Qg(e,t){var n=typeof t,r;return t==null||n==="boolean"?qg(t):(n==="number"?Ps:n==="string"?(r=qi(t))?(t=r,Eu):_3:t instanceof qi?Eu:t instanceof Date?KW:WW(t)?UW:Array.isArray(t)?HW:typeof t.valueOf!="function"&&typeof t.toString!="function"||isNaN(t)?qW:Ps)(e,t)}function YW(e,t){return e=+e,t=+t,function(n){return Math.round(e*(1-n)+t*n)}}var cP=180/Math.PI,mf={translateX:0,translateY:0,rotate:0,skewX:0,scaleX:1,scaleY:1};function y3(e,t,n,r,s,i){var o,a,l;return(o=Math.sqrt(e*e+t*t))&&(e/=o,t/=o),(l=e*n+t*r)&&(n-=e*l,r-=t*l),(a=Math.sqrt(n*n+r*r))&&(n/=a,r/=a,l/=a),e*r<t*n&&(e=-e,t=-t,l=-l,o=-o),{translateX:s,translateY:i,rotate:Math.atan2(t,e)*cP,skewX:Math.atan(l)*cP,scaleX:o,scaleY:a}}var Uc;function JW(e){const t=new(typeof DOMMatrix=="function"?DOMMatrix:WebKitCSSMatrix)(e+"");return t.isIdentity?mf:y3(t.a,t.b,t.c,t.d,t.e,t.f)}function ZW(e){return e==null||(Uc||(Uc=document.createElementNS("http://www.w3.org/2000/svg","g")),Uc.setAttribute("transform",e),!(e=Uc.transform.baseVal.consolidate()))?mf:(e=e.matrix,y3(e.a,e.b,e.c,e.d,e.e,e.f))}function x3(e,t,n,r){function s(c){return c.length?c.pop()+" ":""}function i(c,d,u,p,h,m){if(c!==u||d!==p){var g=h.push("translate(",null,t,null,n);m.push({i:g-4,x:Ps(c,u)},{i:g-2,x:Ps(d,p)})}else(u||p)&&h.push("translate("+u+t+p+n)}function o(c,d,u,p){c!==d?(c-d>180?d+=360:d-c>180&&(c+=360),p.push({i:u.push(s(u)+"rotate(",null,r)-2,x:Ps(c,d)})):d&&u.push(s(u)+"rotate("+d+r)}function a(c,d,u,p){c!==d?p.push({i:u.push(s(u)+"skewX(",null,r)-2,x:Ps(c,d)}):d&&u.push(s(u)+"skewX("+d+r)}function l(c,d,u,p,h,m){if(c!==u||d!==p){var g=h.push(s(h)+"scale(",null,",",null,")");m.push({i:g-4,x:Ps(c,u)},{i:g-2,x:Ps(d,p)})}else(u!==1||p!==1)&&h.push(s(h)+"scale("+u+","+p+")")}return function(c,d){var u=[],p=[];return c=e(c),d=e(d),i(c.translateX,c.translateY,d.translateX,d.translateY,u,p),o(c.rotate,d.rotate,u,p),a(c.skewX,d.skewX,u,p),l(c.scaleX,c.scaleY,d.scaleX,d.scaleY,u,p),c=d=null,function(h){for(var m=-1,g=p.length,I;++m<g;)u[(I=p[m]).i]=I.x(h);return u.join("")}}}var e6=x3(JW,"px, ","px)","deg)"),t6=x3(ZW,", ",")",")"),Ho=0,Xa=0,Na=0,b3=1e3,Su,Ya,Lu=0,Qi=0,Qu=0,xl=typeof performance=="object"&&performance.now?performance:Date,w3=typeof window=="object"&&window.requestAnimationFrame?window.requestAnimationFrame.bind(window):function(e){setTimeout(e,17)};function Xg(){return Qi||(w3(n6),Qi=xl.now()+Qu)}function n6(){Qi=0}function Du(){this._call=this._time=this._next=null}Du.prototype=v3.prototype={constructor:Du,restart:function(e,t,n){if(typeof e!="function")throw new TypeError("callback is not a function");n=(n==null?Xg():+n)+(t==null?0:+t),!this._next&&Ya!==this&&(Ya?Ya._next=this:Su=this,Ya=this),this._call=e,this._time=n,ff()},stop:function(){this._call&&(this._call=null,this._time=1/0,ff())}};function v3(e,t,n){var r=new Du;return r.restart(e,t,n),r}function r6(){Xg(),++Ho;for(var e=Su,t;e;)(t=Qi-e._time)>=0&&e._call.call(void 0,t),e=e._next;--Ho}function uP(){Qi=(Lu=xl.now())+Qu,Ho=Xa=0;try{r6()}finally{Ho=0,i6(),Qi=0}}function s6(){var e=xl.now(),t=e-Lu;t>b3&&(Qu-=t,Lu=e)}function i6(){for(var e,t=Su,n,r=1/0;t;)t._call?(r>t._time&&(r=t._time),e=t,t=t._next):(n=t._next,t._next=null,t=e?e._next=n:Su=n);Ya=e,ff(r)}function ff(e){if(!Ho){Xa&&(Xa=clearTimeout(Xa));var t=e-Qi;t>24?(e<1/0&&(Xa=setTimeout(uP,e-xl.now()-Qu)),Na&&(Na=clearInterval(Na))):(Na||(Lu=xl.now(),Na=setInterval(s6,b3)),Ho=1,w3(uP))}}function dP(e,t,n){var r=new Du;return t=t==null?0:+t,r.restart(s=>{r.stop(),e(s+t)},t,n),r}var o6=t3("start","end","cancel","interrupt"),a6=[],M3=0,pP=1,gf=2,iu=3,hP=4,_f=5,ou=6;function Xu(e,t,n,r,s,i){var o=e.__transition;if(!o)e.__transition={};else if(n in o)return;l6(e,n,{name:t,index:r,group:s,on:o6,tween:a6,time:i.time,delay:i.delay,duration:i.duration,ease:i.ease,timer:null,state:M3})}function Yg(e,t){var n=ps(e,t);if(n.state>M3)throw new Error("too late; already scheduled");return n}function Ss(e,t){var n=ps(e,t);if(n.state>iu)throw new Error("too late; already running");return n}function ps(e,t){var n=e.__transition;if(!n||!(n=n[t]))throw new Error("transition not found");return n}function l6(e,t,n){var r=e.__transition,s;r[t]=n,n.timer=v3(i,0,n.time);function i(c){n.state=pP,n.timer.restart(o,n.delay,n.time),n.delay<=c&&o(c-n.delay)}function o(c){var d,u,p,h;if(n.state!==pP)return l();for(d in r)if(h=r[d],h.name===n.name){if(h.state===iu)return dP(o);h.state===hP?(h.state=ou,h.timer.stop(),h.on.call("interrupt",e,e.__data__,h.index,h.group),delete r[d]):+d<t&&(h.state=ou,h.timer.stop(),h.on.call("cancel",e,e.__data__,h.index,h.group),delete r[d])}if(dP(function(){n.state===iu&&(n.state=hP,n.timer.restart(a,n.delay,n.time),a(c))}),n.state=gf,n.on.call("start",e,e.__data__,n.index,n.group),n.state===gf){for(n.state=iu,s=new Array(p=n.tween.length),d=0,u=-1;d<p;++d)(h=n.tween[d].value.call(e,e.__data__,n.index,n.group))&&(s[++u]=h);s.length=u+1}}function a(c){for(var d=c<n.duration?n.ease.call(null,c/n.duration):(n.timer.restart(l),n.state=_f,1),u=-1,p=s.length;++u<p;)s[u].call(e,d);n.state===_f&&(n.on.call("end",e,e.__data__,n.index,n.group),l())}function l(){n.state=ou,n.timer.stop(),delete r[t];for(var c in r)return;delete e.__transition}}function c6(e,t){var n=e.__transition,r,s,i=!0,o;if(n){t=t==null?null:t+"";for(o in n){if((r=n[o]).name!==t){i=!1;continue}s=r.state>gf&&r.state<_f,r.state=ou,r.timer.stop(),r.on.call(s?"interrupt":"cancel",e,e.__data__,r.index,r.group),delete n[o]}i&&delete e.__transition}}function u6(e){return this.each(function(){c6(this,e)})}function d6(e,t){var n,r;return function(){var s=Ss(this,e),i=s.tween;if(i!==n){r=n=i;for(var o=0,a=r.length;o<a;++o)if(r[o].name===t){r=r.slice(),r.splice(o,1);break}}s.tween=r}}function p6(e,t,n){var r,s;if(typeof n!="function")throw new Error;return function(){var i=Ss(this,e),o=i.tween;if(o!==r){s=(r=o).slice();for(var a={name:t,value:n},l=0,c=s.length;l<c;++l)if(s[l].name===t){s[l]=a;break}l===c&&s.push(a)}i.tween=s}}function h6(e,t){var n=this._id;if(e+="",arguments.length<2){for(var r=ps(this.node(),n).tween,s=0,i=r.length,o;s<i;++s)if((o=r[s]).name===e)return o.value;return null}return this.each((t==null?d6:p6)(n,e,t))}function Jg(e,t,n){var r=e._id;return e.each(function(){var s=Ss(this,r);(s.value||(s.value={}))[t]=n.apply(this,arguments)}),function(s){return ps(s,r).value[t]}}function A3(e,t){var n;return(typeof t=="number"?Ps:t instanceof qi?Eu:(n=qi(t))?(t=n,Eu):_3)(e,t)}function m6(e){return function(){this.removeAttribute(e)}}function f6(e){return function(){this.removeAttributeNS(e.space,e.local)}}function g6(e,t,n){var r,s=n+"",i;return function(){var o=this.getAttribute(e);return o===s?null:o===r?i:i=t(r=o,n)}}function _6(e,t,n){var r,s=n+"",i;return function(){var o=this.getAttributeNS(e.space,e.local);return o===s?null:o===r?i:i=t(r=o,n)}}function y6(e,t,n){var r,s,i;return function(){var o,a=n(this),l;return a==null?void this.removeAttribute(e):(o=this.getAttribute(e),l=a+"",o===l?null:o===r&&l===s?i:(s=l,i=t(r=o,a)))}}function x6(e,t,n){var r,s,i;return function(){var o,a=n(this),l;return a==null?void this.removeAttributeNS(e.space,e.local):(o=this.getAttributeNS(e.space,e.local),l=a+"",o===l?null:o===r&&l===s?i:(s=l,i=t(r=o,a)))}}function b6(e,t){var n=qu(e),r=n==="transform"?t6:A3;return this.attrTween(e,typeof t=="function"?(n.local?x6:y6)(n,r,Jg(this,"attr."+e,t)):t==null?(n.local?f6:m6)(n):(n.local?_6:g6)(n,r,t))}function w6(e,t){return function(n){this.setAttribute(e,t.call(this,n))}}function v6(e,t){return function(n){this.setAttributeNS(e.space,e.local,t.call(this,n))}}function M6(e,t){var n,r;function s(){var i=t.apply(this,arguments);return i!==r&&(n=(r=i)&&v6(e,i)),n}return s._value=t,s}function A6(e,t){var n,r;function s(){var i=t.apply(this,arguments);return i!==r&&(n=(r=i)&&w6(e,i)),n}return s._value=t,s}function T6(e,t){var n="attr."+e;if(arguments.length<2)return(n=this.tween(n))&&n._value;if(t==null)return this.tween(n,null);if(typeof t!="function")throw new Error;var r=qu(e);return this.tween(n,(r.local?M6:A6)(r,t))}function P6(e,t){return function(){Yg(this,e).delay=+t.apply(this,arguments)}}function C6(e,t){return t=+t,function(){Yg(this,e).delay=t}}function I6(e){var t=this._id;return arguments.length?this.each((typeof e=="function"?P6:C6)(t,e)):ps(this.node(),t).delay}function k6(e,t){return function(){Ss(this,e).duration=+t.apply(this,arguments)}}function E6(e,t){return t=+t,function(){Ss(this,e).duration=t}}function S6(e){var t=this._id;return arguments.length?this.each((typeof e=="function"?k6:E6)(t,e)):ps(this.node(),t).duration}function L6(e,t){if(typeof t!="function")throw new Error;return function(){Ss(this,e).ease=t}}function D6(e){var t=this._id;return arguments.length?this.each(L6(t,e)):ps(this.node(),t).ease}function $6(e,t){return function(){var n=t.apply(this,arguments);if(typeof n!="function")throw new Error;Ss(this,e).ease=n}}function F6(e){if(typeof e!="function")throw new Error;return this.each($6(this._id,e))}function O6(e){typeof e!="function"&&(e=s3(e));for(var t=this._groups,n=t.length,r=new Array(n),s=0;s<n;++s)for(var i=t[s],o=i.length,a=r[s]=[],l,c=0;c<o;++c)(l=i[c])&&e.call(l,l.__data__,c,i)&&a.push(l);return new Vs(r,this._parents,this._name,this._id)}function j6(e){if(e._id!==this._id)throw new Error;for(var t=this._groups,n=e._groups,r=t.length,s=n.length,i=Math.min(r,s),o=new Array(r),a=0;a<i;++a)for(var l=t[a],c=n[a],d=l.length,u=o[a]=new Array(d),p,h=0;h<d;++h)(p=l[h]||c[h])&&(u[h]=p);for(;a<r;++a)o[a]=t[a];return new Vs(o,this._parents,this._name,this._id)}function R6(e){return(e+"").trim().split(/^|\s+/).every(function(t){var n=t.indexOf(".");return n>=0&&(t=t.slice(0,n)),!t||t==="start"})}function N6(e,t,n){var r,s,i=R6(t)?Yg:Ss;return function(){var o=i(this,e),a=o.on;a!==r&&(s=(r=a).copy()).on(t,n),o.on=s}}function z6(e,t){var n=this._id;return arguments.length<2?ps(this.node(),n).on.on(e):this.each(N6(n,e,t))}function B6(e){return function(){var t=this.parentNode;for(var n in this.__transition)if(+n!==e)return;t&&t.removeChild(this)}}function G6(){return this.on("end.remove",B6(this._id))}function V6(e){var t=this._name,n=this._id;typeof e!="function"&&(e=Wg(e));for(var r=this._groups,s=r.length,i=new Array(s),o=0;o<s;++o)for(var a=r[o],l=a.length,c=i[o]=new Array(l),d,u,p=0;p<l;++p)(d=a[p])&&(u=e.call(d,d.__data__,p,a))&&("__data__"in d&&(u.__data__=d.__data__),c[p]=u,Xu(c[p],t,n,p,c,ps(d,n)));return new Vs(i,this._parents,t,n)}function U6(e){var t=this._name,n=this._id;typeof e!="function"&&(e=r3(e));for(var r=this._groups,s=r.length,i=[],o=[],a=0;a<s;++a)for(var l=r[a],c=l.length,d,u=0;u<c;++u)if(d=l[u]){for(var p=e.call(d,d.__data__,u,l),h,m=ps(d,n),g=0,I=p.length;g<I;++g)(h=p[g])&&Xu(h,t,n,g,p,m);i.push(p),o.push(d)}return new Vs(i,o,t,n)}var W6=Cl.prototype.constructor;function H6(){return new W6(this._groups,this._parents)}function K6(e,t){var n,r,s;return function(){var i=Wo(this,e),o=(this.style.removeProperty(e),Wo(this,e));return i===o?null:i===n&&o===r?s:s=t(n=i,r=o)}}function T3(e){return function(){this.style.removeProperty(e)}}function q6(e,t,n){var r,s=n+"",i;return function(){var o=Wo(this,e);return o===s?null:o===r?i:i=t(r=o,n)}}function Q6(e,t,n){var r,s,i;return function(){var o=Wo(this,e),a=n(this),l=a+"";return a==null&&(l=a=(this.style.removeProperty(e),Wo(this,e))),o===l?null:o===r&&l===s?i:(s=l,i=t(r=o,a))}}function X6(e,t){var n,r,s,i="style."+t,o="end."+i,a;return function(){var l=Ss(this,e),c=l.on,d=l.value[i]==null?a||(a=T3(t)):void 0;(c!==n||s!==d)&&(r=(n=c).copy()).on(o,s=d),l.on=r}}function Y6(e,t,n){var r=(e+="")=="transform"?e6:A3;return t==null?this.styleTween(e,K6(e,r)).on("end.style."+e,T3(e)):typeof t=="function"?this.styleTween(e,Q6(e,r,Jg(this,"style."+e,t))).each(X6(this._id,e)):this.styleTween(e,q6(e,r,t),n).on("end.style."+e,null)}function J6(e,t,n){return function(r){this.style.setProperty(e,t.call(this,r),n)}}function Z6(e,t,n){var r,s;function i(){var o=t.apply(this,arguments);return o!==s&&(r=(s=o)&&J6(e,o,n)),r}return i._value=t,i}function e8(e,t,n){var r="style."+(e+="");if(arguments.length<2)return(r=this.tween(r))&&r._value;if(t==null)return this.tween(r,null);if(typeof t!="function")throw new Error;return this.tween(r,Z6(e,t,n??""))}function t8(e){return function(){this.textContent=e}}function n8(e){return function(){var t=e(this);this.textContent=t??""}}function r8(e){return this.tween("text",typeof e=="function"?n8(Jg(this,"text",e)):t8(e==null?"":e+""))}function s8(e){return function(t){this.textContent=e.call(this,t)}}function i8(e){var t,n;function r(){var s=e.apply(this,arguments);return s!==n&&(t=(n=s)&&s8(s)),t}return r._value=e,r}function o8(e){var t="text";if(arguments.length<1)return(t=this.tween(t))&&t._value;if(e==null)return this.tween(t,null);if(typeof e!="function")throw new Error;return this.tween(t,i8(e))}function a8(){for(var e=this._name,t=this._id,n=P3(),r=this._groups,s=r.length,i=0;i<s;++i)for(var o=r[i],a=o.length,l,c=0;c<a;++c)if(l=o[c]){var d=ps(l,t);Xu(l,e,n,c,o,{time:d.time+d.delay+d.duration,delay:0,duration:d.duration,ease:d.ease})}return new Vs(r,this._parents,e,n)}function l8(){var e,t,n=this,r=n._id,s=n.size();return new Promise(function(i,o){var a={value:o},l={value:function(){--s===0&&i()}};n.each(function(){var c=Ss(this,r),d=c.on;d!==e&&(t=(e=d).copy(),t._.cancel.push(a),t._.interrupt.push(a),t._.end.push(l)),c.on=t}),s===0&&i()})}var c8=0;function Vs(e,t,n,r){this._groups=e,this._parents=t,this._name=n,this._id=r}function P3(){return++c8}var Ns=Cl.prototype;Vs.prototype={constructor:Vs,select:V6,selectAll:U6,selectChild:Ns.selectChild,selectChildren:Ns.selectChildren,filter:O6,merge:j6,selection:H6,transition:a8,call:Ns.call,nodes:Ns.nodes,node:Ns.node,size:Ns.size,empty:Ns.empty,each:Ns.each,on:z6,attr:b6,attrTween:T6,style:Y6,styleTween:e8,text:r8,textTween:o8,remove:G6,tween:h6,delay:I6,duration:S6,ease:D6,easeVarying:F6,end:l8,[Symbol.iterator]:Ns[Symbol.iterator]};function u8(e){return((e*=2)<=1?e*e*e:(e-=2)*e*e+2)/2}var d8={time:null,delay:0,duration:250,ease:u8};function p8(e,t){for(var n;!(n=e.__transition)||!(n=n[t]);)if(!(e=e.parentNode))throw new Error(`transition ${t} not found`);return n}function h8(e){var t,n;e instanceof Vs?(t=e._id,e=e._name):(t=P3(),(n=d8).time=Xg(),e=e==null?null:e+"");for(var r=this._groups,s=r.length,i=0;i<s;++i)for(var o=r[i],a=o.length,l,c=0;c<a;++c)(l=o[c])&&Xu(l,e,t,c,o,n||p8(l,t));return new Vs(r,this._parents,e,t)}Cl.prototype.interrupt=u6;Cl.prototype.transition=h8;function m8(e){return Math.abs(e=Math.round(e))>=1e21?e.toLocaleString("en").replace(/,/g,""):e.toString(10)}function $u(e,t){if(!isFinite(e)||e===0)return null;var n=(e=t?e.toExponential(t-1):e.toExponential()).indexOf("e"),r=e.slice(0,n);return[r.length>1?r[0]+r.slice(2):r,+e.slice(n+1)]}function Ko(e){return e=$u(Math.abs(e)),e?e[1]:NaN}function f8(e,t){return function(n,r){for(var s=n.length,i=[],o=0,a=e[0],l=0;s>0&&a>0&&(l+a+1>r&&(a=Math.max(1,r-l)),i.push(n.substring(s-=a,s+a)),!((l+=a+1)>r));)a=e[o=(o+1)%e.length];return i.reverse().join(t)}}function g8(e){return function(t){return t.replace(/[0-9]/g,function(n){return e[+n]})}}var _8=/^(?:(.)?([<>=^]))?([+\-( ])?([$#])?(0)?(\d+)?(,)?(\.\d+)?(~)?([a-z%])?$/i;function Fu(e){if(!(t=_8.exec(e)))throw new Error("invalid format: "+e);var t;return new Zg({fill:t[1],align:t[2],sign:t[3],symbol:t[4],zero:t[5],width:t[6],comma:t[7],precision:t[8]&&t[8].slice(1),trim:t[9],type:t[10]})}Fu.prototype=Zg.prototype;function Zg(e){this.fill=e.fill===void 0?" ":e.fill+"",this.align=e.align===void 0?">":e.align+"",this.sign=e.sign===void 0?"-":e.sign+"",this.symbol=e.symbol===void 0?"":e.symbol+"",this.zero=!!e.zero,this.width=e.width===void 0?void 0:+e.width,this.comma=!!e.comma,this.precision=e.precision===void 0?void 0:+e.precision,this.trim=!!e.trim,this.type=e.type===void 0?"":e.type+""}Zg.prototype.toString=function(){return this.fill+this.align+this.sign+this.symbol+(this.zero?"0":"")+(this.width===void 0?"":Math.max(1,this.width|0))+(this.comma?",":"")+(this.precision===void 0?"":"."+Math.max(0,this.precision|0))+(this.trim?"~":"")+this.type};function y8(e){e:for(var t=e.length,n=1,r=-1,s;n<t;++n)switch(e[n]){case".":r=s=n;break;case"0":r===0&&(r=n),s=n;break;default:if(!+e[n])break e;r>0&&(r=0);break}return r>0?e.slice(0,r)+e.slice(s+1):e}var Ou;function x8(e,t){var n=$u(e,t);if(!n)return Ou=void 0,e.toPrecision(t);var r=n[0],s=n[1],i=s-(Ou=Math.max(-8,Math.min(8,Math.floor(s/3)))*3)+1,o=r.length;return i===o?r:i>o?r+new Array(i-o+1).join("0"):i>0?r.slice(0,i)+"."+r.slice(i):"0."+new Array(1-i).join("0")+$u(e,Math.max(0,t+i-1))[0]}function mP(e,t){var n=$u(e,t);if(!n)return e+"";var r=n[0],s=n[1];return s<0?"0."+new Array(-s).join("0")+r:r.length>s+1?r.slice(0,s+1)+"."+r.slice(s+1):r+new Array(s-r.length+2).join("0")}const fP={"%":(e,t)=>(e*100).toFixed(t),b:e=>Math.round(e).toString(2),c:e=>e+"",d:m8,e:(e,t)=>e.toExponential(t),f:(e,t)=>e.toFixed(t),g:(e,t)=>e.toPrecision(t),o:e=>Math.round(e).toString(8),p:(e,t)=>mP(e*100,t),r:mP,s:x8,X:e=>Math.round(e).toString(16).toUpperCase(),x:e=>Math.round(e).toString(16)};function gP(e){return e}var _P=Array.prototype.map,yP=["y","z","a","f","p","n","µ","m","","k","M","G","T","P","E","Z","Y"];function b8(e){var t=e.grouping===void 0||e.thousands===void 0?gP:f8(_P.call(e.grouping,Number),e.thousands+""),n=e.currency===void 0?"":e.currency[0]+"",r=e.currency===void 0?"":e.currency[1]+"",s=e.decimal===void 0?".":e.decimal+"",i=e.numerals===void 0?gP:g8(_P.call(e.numerals,String)),o=e.percent===void 0?"%":e.percent+"",a=e.minus===void 0?"−":e.minus+"",l=e.nan===void 0?"NaN":e.nan+"";function c(u,p){u=Fu(u);var h=u.fill,m=u.align,g=u.sign,I=u.symbol,f=u.zero,_=u.width,T=u.comma,M=u.precision,v=u.trim,b=u.type;b==="n"?(T=!0,b="g"):fP[b]||(M===void 0&&(M=12),v=!0,b="g"),(f||h==="0"&&m==="=")&&(f=!0,h="0",m="=");var A=(p?.prefix!==void 0?p.prefix:"")+(I==="$"?n:I==="#"&&/[boxX]/.test(b)?"0"+b.toLowerCase():""),k=(I==="$"?r:/[%p]/.test(b)?o:"")+(p?.suffix!==void 0?p.suffix:""),F=fP[b],L=/[defgprs%]/.test(b);M=M===void 0?6:/[gprs]/.test(b)?Math.max(1,Math.min(21,M)):Math.max(0,Math.min(20,M));function G(j){var R=A,K=k,U,Y,te;if(b==="c")K=F(j)+K,j="";else{j=+j;var ne=j<0||1/j<0;if(j=isNaN(j)?l:F(Math.abs(j),M),v&&(j=y8(j)),ne&&+j==0&&g!=="+"&&(ne=!1),R=(ne?g==="("?g:a:g==="-"||g==="("?"":g)+R,K=(b==="s"&&!isNaN(j)&&Ou!==void 0?yP[8+Ou/3]:"")+K+(ne&&g==="("?")":""),L){for(U=-1,Y=j.length;++U<Y;)if(te=j.charCodeAt(U),48>te||te>57){K=(te===46?s+j.slice(U+1):j.slice(U))+K,j=j.slice(0,U);break}}}T&&!f&&(j=t(j,1/0));var le=R.length+j.length+K.length,N=le<_?new Array(_-le+1).join(h):"";switch(T&&f&&(j=t(N+j,N.length?_-K.length:1/0),N=""),m){case"<":j=R+j+K+N;break;case"=":j=R+N+j+K;break;case"^":j=N.slice(0,le=N.length>>1)+R+j+K+N.slice(le);break;default:j=N+R+j+K;break}return i(j)}return G.toString=function(){return u+""},G}function d(u,p){var h=Math.max(-8,Math.min(8,Math.floor(Ko(p)/3)))*3,m=Math.pow(10,-h),g=c((u=Fu(u),u.type="f",u),{suffix:yP[8+h/3]});return function(I){return g(m*I)}}return{format:c,formatPrefix:d}}var Wc,C3,I3;w8({thousands:",",grouping:[3],currency:["$",""]});function w8(e){return Wc=b8(e),C3=Wc.format,I3=Wc.formatPrefix,Wc}function v8(e){return Math.max(0,-Ko(Math.abs(e)))}function M8(e,t){return Math.max(0,Math.max(-8,Math.min(8,Math.floor(Ko(t)/3)))*3-Ko(Math.abs(e)))}function A8(e,t){return e=Math.abs(e),t=Math.abs(t)-e,Math.max(0,Ko(t)-Ko(e))+1}function T8(e,t){switch(arguments.length){case 0:break;case 1:{typeof e=="function"?this.interpolator(e):this.range(e);break}default:{this.domain(e),typeof t=="function"?this.interpolator(t):this.range(t);break}}return this}function k3(e){return e}function P8(e,t,n,r){var s=VV(e,t,n),i;switch(r=Fu(r??",f"),r.type){case"s":{var o=Math.max(Math.abs(e),Math.abs(t));return r.precision==null&&!isNaN(i=M8(s,o))&&(r.precision=i),I3(r,o)}case"":case"e":case"g":case"p":case"r":{r.precision==null&&!isNaN(i=A8(s,Math.max(Math.abs(e),Math.abs(t))))&&(r.precision=i-(r.type==="e"));break}case"f":case"%":{r.precision==null&&!isNaN(i=v8(s))&&(r.precision=i-(r.type==="%")*2);break}}return C3(r)}function C8(e){var t=e.domain;return e.ticks=function(n){var r=t();return GV(r[0],r[r.length-1],n??10)},e.tickFormat=function(n,r){var s=t();return P8(s[0],s[s.length-1],n??10,r)},e.nice=function(n){n==null&&(n=10);var r=t(),s=0,i=r.length-1,o=r[s],a=r[i],l,c,d=10;for(a<o&&(c=o,o=a,a=c,c=s,s=i,i=c);d-- >0;){if(c=df(o,a,n),c===l)return r[s]=o,r[i]=a,t(r);if(c>0)o=Math.floor(o/c)*c,a=Math.ceil(a/c)*c;else if(c<0)o=Math.ceil(o*c)/c,a=Math.floor(a*c)/c;else break;l=c}return e},e}function I8(){var e=0,t=1,n,r,s,i,o=k3,a=!1,l;function c(u){return u==null||isNaN(u=+u)?l:o(s===0?.5:(u=(i(u)-n)*s,a?Math.max(0,Math.min(1,u)):u))}c.domain=function(u){return arguments.length?([e,t]=u,n=i(e=+e),r=i(t=+t),s=n===r?0:1/(r-n),c):[e,t]},c.clamp=function(u){return arguments.length?(a=!!u,c):a},c.interpolator=function(u){return arguments.length?(o=u,c):o};function d(u){return function(p){var h,m;return arguments.length?([h,m]=p,o=u(h,m),c):[o(0),o(1)]}}return c.range=d(Qg),c.rangeRound=d(YW),c.unknown=function(u){return arguments.length?(l=u,c):l},function(u){return i=u,n=u(e),r=u(t),s=n===r?0:1/(r-n),c}}function k8(e,t){return t.domain(e.domain()).interpolator(e.interpolator()).clamp(e.clamp()).unknown(e.unknown())}function E3(){var e=C8(I8()(k3));return e.copy=function(){return k8(e,E3())},T8.apply(e,arguments)}function E8(e){for(var t=e.length/6|0,n=new Array(t),r=0;r<t;)n[r]="#"+e.slice(r*6,++r*6);return n}const S8=e=>VW(e[e.length-1]);var L8=new Array(3).concat("deebf79ecae13182bd","eff3ffbdd7e76baed62171b5","eff3ffbdd7e76baed63182bd08519c","eff3ffc6dbef9ecae16baed63182bd08519c","eff3ffc6dbef9ecae16baed64292c62171b5084594","f7fbffdeebf7c6dbef9ecae16baed64292c62171b5084594","f7fbffdeebf7c6dbef9ecae16baed64292c62171b508519c08306b").map(E8);const D8=S8(L8);function Ja(e,t,n){this.k=e,this.x=t,this.y=n}Ja.prototype={constructor:Ja,scale:function(e){return e===1?this:new Ja(this.k*e,this.x,this.y)},translate:function(e,t){return e===0&t===0?this:new Ja(this.k,this.x+this.k*e,this.y+this.k*t)},apply:function(e){return[e[0]*this.k+this.x,e[1]*this.k+this.y]},applyX:function(e){return e*this.k+this.x},applyY:function(e){return e*this.k+this.y},invert:function(e){return[(e[0]-this.x)/this.k,(e[1]-this.y)/this.k]},invertX:function(e){return(e-this.x)/this.k},invertY:function(e){return(e-this.y)/this.k},rescaleX:function(e){return e.copy().domain(e.range().map(this.invertX,this).map(e.invert,e))},rescaleY:function(e){return e.copy().domain(e.range().map(this.invertY,this).map(e.invert,e))},toString:function(){return"translate("+this.x+","+this.y+") scale("+this.k+")"}};Ja.prototype;const $8=12,F8=12,O8=({attentionHead:e,tokens:t,selectedLayer:n,selectedHead:r,onLayerChange:s,onHeadChange:i,isActive:o,i18n:a})=>{const l=Me.useRef(null),[c,d]=Me.useState(null);return Me.useEffect(()=>{if(!l.current||!e||t.length===0)return;const u=im(l.current);u.selectAll("*").remove();const p={top:40,right:20,bottom:60,left:60},h=l.current.clientWidth||300,m=l.current.clientHeight||300,g=h-p.left-p.right,I=m-p.top-p.bottom,f=t.length,_=Math.min(g/f,I/f,40),T=_*f,M=_*f,v=u.append("g").attr("transform",`translate(${p.left+(g-T)/2}, ${p.top})`),b=E3(D8).domain([0,1]),A=e.weights;for(let F=0;F<f;F++)for(let L=0;L<f;L++){const G=A[F]?.[L]??0;v.append("rect").attr("x",L*_).attr("y",F*_).attr("width",_-1).attr("height",_-1).attr("fill",b(G)).attr("rx",2).attr("class","cursor-pointer").on("mouseenter",function(){im(this).attr("stroke","#fff").attr("stroke-width",2),d({row:F,col:L})}).on("mouseleave",function(){im(this).attr("stroke","none"),d(null)})}const k=F=>{const L=F.replace(/Ġ/g,"").trim()||"␣";return L.length>4?L.slice(0,4)+"…":L};v.selectAll(".x-label").data(t).enter().append("text").attr("class","x-label").attr("x",(F,L)=>L*_+_/2).attr("y",-8).attr("text-anchor","middle").attr("fill","#9ca3af").attr("font-size",Math.min(_*.6,12)).attr("font-family","monospace").text(F=>k(F.text)),v.selectAll(".y-label").data(t).enter().append("text").attr("class","y-label").attr("x",-8).attr("y",(F,L)=>L*_+_/2).attr("text-anchor","end").attr("dominant-baseline","middle").attr("fill","#9ca3af").attr("font-size",Math.min(_*.6,12)).attr("font-family","monospace").text(F=>k(F.text)),u.append("text").attr("x",p.left+g/2).attr("y",p.top+M+35).attr("text-anchor","middle").attr("fill","#6b7280").attr("font-size",11).text("Key (attending to)"),u.append("text").attr("transform","rotate(-90)").attr("x",-(p.top+M/2)).attr("y",15).attr("text-anchor","middle").attr("fill","#6b7280").attr("font-size",11).text("Query (from)")},[e,t]),w.jsxs("div",{className:`
        bg-gray-800/50 rounded-xl border p-4 transition-all duration-300
        ${o?"border-purple-500/50 shadow-lg shadow-purple-500/10":"border-gray-700/50"}
      `,children:[w.jsxs("div",{className:"flex items-center gap-2 mb-4",children:[w.jsx("div",{className:`
            w-8 h-8 rounded-full flex items-center justify-center text-sm font-bold
            ${o?"bg-purple-500 text-white":"bg-gray-700 text-gray-400"}
          `,children:"3"}),w.jsxs("div",{children:[w.jsx("h3",{className:"text-white font-semibold",children:a.stage3}),w.jsx("p",{className:"text-gray-400 text-sm",children:a.stage3Desc})]})]}),w.jsxs("div",{className:"flex gap-4 mb-4",children:[w.jsxs("div",{className:"flex-1",children:[w.jsx("label",{className:"text-xs text-gray-500 mb-1 block",children:a.selectLayer}),w.jsx("div",{className:"flex flex-wrap gap-1",children:Array.from({length:$8},(u,p)=>w.jsx("button",{onClick:()=>s(p),className:`
                  w-7 h-7 text-xs rounded transition-all
                  ${n===p?"bg-purple-500 text-white":"bg-gray-700 text-gray-400 hover:bg-gray-600"}
                `,children:p},`layer-${p}`))})]}),w.jsxs("div",{className:"flex-1",children:[w.jsx("label",{className:"text-xs text-gray-500 mb-1 block",children:a.selectHead}),w.jsx("div",{className:"flex flex-wrap gap-1",children:Array.from({length:F8},(u,p)=>w.jsx("button",{onClick:()=>i(p),className:`
                  w-7 h-7 text-xs rounded transition-all
                  ${r===p?"bg-purple-500 text-white":"bg-gray-700 text-gray-400 hover:bg-gray-600"}
                `,children:p},`head-${p}`))})]})]}),w.jsxs("div",{className:"text-sm text-gray-400 mb-2",children:[a.layer," ",n," • ",a.head," ",r]}),w.jsx("div",{className:"relative bg-gray-900/50 rounded-lg p-2",style:{minHeight:280},children:t.length===0?w.jsx("div",{className:"flex items-center justify-center h-64 text-gray-500 text-sm",children:"Enter text to see attention pattern"}):w.jsxs(w.Fragment,{children:[w.jsx("svg",{ref:l,className:"w-full h-64"}),c&&e&&w.jsxs(lt.div,{initial:{opacity:0},animate:{opacity:1},className:"absolute top-2 right-2 bg-gray-900 border border-gray-700 rounded-lg p-2 text-xs",children:[w.jsxs("div",{className:"text-gray-500",children:['"',t[c.row]?.text,'" → "',t[c.col]?.text,'"']}),w.jsxs("div",{className:"text-white font-mono",children:[((e.weights[c.row]?.[c.col]??0)*100).toFixed(2),"%"]})]})]})}),w.jsxs("div",{className:"mt-3 flex items-center gap-2 text-xs text-gray-500",children:[w.jsx("span",{children:"0%"}),w.jsx("div",{className:"flex-1 h-2 rounded bg-gradient-to-r from-gray-900 via-blue-700 to-blue-400"}),w.jsx("span",{children:"100%"})]})]})},Hc=e=>{const t=["#60a5fa","#a78bfa","#34d399","#fbbf24","#f87171","#2dd4bf","#fb923c","#e879f9","#38bdf8","#4ade80"];return t[e%t.length]},j8=[{id:"input",label:"Input",labelZh:"输入",labelJa:"入力"},{id:"tokenize",label:"Tokenize",labelZh:"分词",labelJa:"トークン化"},{id:"embed",label:"Embed",labelZh:"嵌入",labelJa:"埋め込み"},{id:"attention",label:"Attention",labelZh:"注意力",labelJa:"アテンション"},{id:"mlp",label:"MLP",labelZh:"MLP",labelJa:"MLP"},{id:"output",label:"Output",labelZh:"输出",labelJa:"出力"}],R8=({tokens:e,predictions:t,attentionHead:n,currentStage:r,selectedLayer:s,selectedHead:i,onLayerChange:o,onHeadChange:a,isActive:l,i18n:c})=>{const[d,u]=Me.useState(null),[p,h]=Me.useState(null),m=Me.useMemo(()=>{switch(r){case"tokenizing":return 1;case"embedding":return 2;case"attention":return 3;case"mlp":return 4;case"output":case"complete":return 5;default:return 0}},[r]),I=p!==null?(f=>!n||f<0?[]:n.weights[f]||[])(p):[];return w.jsxs("div",{className:`
      bg-gray-900 rounded-2xl border-2 p-6 transition-all duration-300 overflow-x-auto
      ${l?"border-blue-500/50 shadow-xl shadow-blue-500/10":"border-gray-700/50"}
    `,children:[w.jsx("div",{className:"flex justify-between mb-4 min-w-[800px]",children:j8.map((f,_)=>w.jsx("div",{className:`
              flex-1 text-center text-sm font-medium transition-all
              ${_<=m?"text-blue-400":"text-gray-600"}
            `,children:f.labelZh},f.id))}),w.jsxs("div",{className:"relative min-w-[800px] h-[400px]",children:[w.jsxs("svg",{className:"absolute inset-0 w-full h-full pointer-events-none",children:[w.jsx("defs",{children:w.jsxs("linearGradient",{id:"lineGradient",x1:"0%",y1:"0%",x2:"100%",y2:"0%",children:[w.jsx("stop",{offset:"0%",stopColor:"#3b82f6",stopOpacity:"0.3"}),w.jsx("stop",{offset:"100%",stopColor:"#8b5cf6",stopOpacity:"0.3"})]})}),e.length>0&&w.jsxs(w.Fragment,{children:[[0,1,2,3,4].map(f=>w.jsx("line",{x1:`${(f+.5)*(100/6)+8}%`,y1:"50%",x2:`${(f+1.5)*(100/6)-8}%`,y2:"50%",stroke:"url(#lineGradient)",strokeWidth:"2",strokeDasharray:f<m?"0":"5,5"},`connector-${f}`)),p!==null&&I.length>0&&w.jsx(w.Fragment,{children:I.map((f,_)=>{if(f<.05||_>p)return null;const T=80+p*40,M=80+_*40,v=Math.min(f*2,1);return w.jsx(lt.path,{initial:{pathLength:0,opacity:0},animate:{pathLength:1,opacity:v},d:`M ${50*(100/6)}% ${T} Q ${55*(100/6)}% ${(T+M)/2} ${50*(100/6)}% ${M}`,fill:"none",stroke:Hc(_),strokeWidth:Math.max(1,f*8),strokeOpacity:v},`attn-${_}`)})})]})]}),w.jsxs("div",{className:"relative flex h-full",children:[w.jsx("div",{className:"flex-1 flex flex-col items-center justify-center p-2",children:w.jsxs("div",{className:"bg-gray-800/50 rounded-xl p-4 border border-gray-700 w-full max-w-[120px]",children:[w.jsx("div",{className:"text-xs text-gray-500 mb-2 text-center",children:"Text"}),w.jsx("div",{className:"text-white text-sm font-mono text-center break-all",children:e.length>0?e.map(f=>f.text).join("").slice(0,20)+(e.map(f=>f.text).join("").length>20?"...":""):"..."})]})}),w.jsxs("div",{className:"flex-1 flex flex-col items-center justify-start p-2 pt-8 overflow-y-auto max-h-full",children:[w.jsx(vr,{children:e.map((f,_)=>w.jsx(lt.div,{initial:{opacity:0,x:-20},animate:{opacity:1,x:0},transition:{delay:_*.05},className:`
                    px-3 py-1.5 mb-2 rounded-lg text-sm font-mono cursor-pointer
                    transition-all duration-200 border
                    ${p===_?"ring-2 ring-white scale-110 z-10":d===_?"scale-105":""}
                  `,style:{backgroundColor:`${Hc(_)}20`,borderColor:`${Hc(_)}50`,color:Hc(_)},onMouseEnter:()=>u(_),onMouseLeave:()=>u(null),onClick:()=>h(p===_?null:_),children:f.text.replace(/ /g,"␣")||"␣"},`token-${_}`))}),e.length===0&&w.jsx("div",{className:"text-gray-600 text-sm",children:"..."})]}),w.jsxs("div",{className:"flex-1 flex flex-col items-center justify-start p-2 pt-8",children:[e.map((f,_)=>w.jsx(lt.div,{initial:{opacity:0},animate:{opacity:1},transition:{delay:_*.05+.2},className:"mb-2 flex gap-0.5",children:Array.from({length:8}).map((T,M)=>{const v=Math.sin((_+1)*(M+1)*.5);return w.jsx("div",{className:"w-2 h-6 rounded-sm",style:{backgroundColor:v>0?`rgba(96, 165, 250, ${Math.abs(v)})`:`rgba(248, 113, 113, ${Math.abs(v)})`}},M)})},`embed-${_}`)),e.length>0&&w.jsx("div",{className:"text-xs text-gray-500 mt-2",children:"768-dim"})]}),w.jsxs("div",{className:"flex-1 flex flex-col items-center justify-center p-2",children:[e.length>0&&n&&w.jsxs("div",{className:"bg-gray-800/30 rounded-xl p-3 border border-gray-700/50",children:[w.jsx("div",{className:"grid gap-0.5",style:{gridTemplateColumns:`repeat(${Math.min(e.length,6)}, 1fr)`},children:n.weights.slice(0,6).map((f,_)=>f.slice(0,6).map((T,M)=>w.jsx(lt.div,{initial:{opacity:0},animate:{opacity:1},transition:{delay:(_*6+M)*.01},className:`w-4 h-4 rounded-sm ${p===_?"ring-1 ring-white":""}`,style:{backgroundColor:M<=_?`rgba(139, 92, 246, ${T})`:"transparent"}},`${_}-${M}`)))}),w.jsxs("div",{className:"mt-3 flex gap-2 text-xs",children:[w.jsx("select",{value:s,onChange:f=>o(Number(f.target.value)),className:"bg-gray-700 text-gray-300 rounded px-2 py-1",children:Array.from({length:12},(f,_)=>w.jsxs("option",{value:_,children:["L",_]},_))}),w.jsx("select",{value:i,onChange:f=>a(Number(f.target.value)),className:"bg-gray-700 text-gray-300 rounded px-2 py-1",children:Array.from({length:12},(f,_)=>w.jsxs("option",{value:_,children:["H",_]},_))})]})]}),e.length===0&&w.jsx("div",{className:"text-gray-600 text-sm",children:"..."})]}),w.jsx("div",{className:"flex-1 flex flex-col items-center justify-center p-2",children:e.length>0&&w.jsxs("div",{className:"bg-gray-800/30 rounded-xl p-3 border border-gray-700/50",children:[w.jsxs("div",{className:"flex flex-col gap-2",children:[w.jsx("div",{className:"text-xs text-gray-500 text-center",children:"FFN"}),w.jsx("div",{className:"flex gap-1 justify-center",children:Array.from({length:4}).map((f,_)=>w.jsx(lt.div,{initial:{scale:0},animate:{scale:1},transition:{delay:_*.05},className:"w-3 h-3 rounded-full bg-blue-500/50"},`mlp-in-${_}`))}),w.jsx("div",{className:"text-xs text-gray-600 text-center",children:"↓"}),w.jsx("div",{className:"flex gap-1 justify-center flex-wrap max-w-[80px]",children:Array.from({length:8}).map((f,_)=>w.jsx(lt.div,{initial:{scale:0},animate:{scale:1},transition:{delay:.2+_*.03},className:"w-2 h-2 rounded-full",style:{backgroundColor:`rgba(250, 204, 21, ${.3+Math.random()*.7})`}},`mlp-hidden-${_}`))}),w.jsx("div",{className:"text-xs text-gray-600 text-center",children:"↓"}),w.jsx("div",{className:"flex gap-1 justify-center",children:Array.from({length:4}).map((f,_)=>w.jsx(lt.div,{initial:{scale:0},animate:{scale:1},transition:{delay:.4+_*.05},className:"w-3 h-3 rounded-full bg-green-500/50"},`mlp-out-${_}`))})]}),w.jsx("div",{className:"text-xs text-gray-500 mt-2 text-center",children:"768→3072→768"})]})}),w.jsxs("div",{className:"flex-1 flex flex-col items-center justify-center p-2",children:[t.length>0&&w.jsxs("div",{className:"bg-gray-800/30 rounded-xl p-3 border border-gray-700/50 w-full max-w-[140px]",children:[w.jsx("div",{className:"text-xs text-gray-500 mb-2 text-center",children:c.nextTokenPrediction}),t.slice(0,5).map((f,_)=>w.jsxs(lt.div,{initial:{opacity:0,x:20},animate:{opacity:1,x:0},transition:{delay:_*.1},className:"flex items-center gap-2 mb-1",children:[w.jsx("div",{className:"flex-1 h-4 bg-gray-700 rounded overflow-hidden",children:w.jsx(lt.div,{initial:{width:0},animate:{width:`${f.probability*100}%`},transition:{duration:.5,delay:_*.1},className:`h-full ${_===0?"bg-green-500":"bg-blue-500/50"}`})}),w.jsx("span",{className:`text-xs font-mono truncate w-12 ${_===0?"text-green-400":"text-gray-400"}`,children:f.token.slice(0,4)})]},`pred-${_}`))]}),t.length===0&&w.jsx("div",{className:"text-gray-600 text-sm",children:"..."})]})]})]}),w.jsxs("div",{className:"mt-4 pt-4 border-t border-gray-700/50 flex justify-between items-center text-xs text-gray-500",children:[w.jsxs("div",{children:[c.tipClick,": Token を選択して Attention を確認"]}),w.jsxs("div",{children:["GPT-2 Small: 12 ",c.layers," × 12 ",c.heads]})]})]})},N8=({language:e})=>{const[t,n]=Me.useState(!0),[r,s]=Me.useState(!1),o={zh:{title:"Transformer 可视化演示",subtitle:"来自 Georgia Tech & IBM Research 的交互式 GPT-2 可视化工具",loading:"加载中...",error:"加载失败，请点击下方链接直接访问",openInNewTab:"在新标签页打开",note:"注意：首次加载需要下载约 150MB 的 GPT-2 模型，请耐心等待",features:["✓ 真实运行 GPT-2 模型","✓ 可视化 Attention 权重","✓ 交互式探索每一层","✓ 实时生成下一个词"]},ja:{title:"Transformer 可視化デモ",subtitle:"Georgia Tech & IBM Research によるインタラクティブな GPT-2 可視化ツール",loading:"読み込み中...",error:"読み込み失敗。下のリンクから直接アクセスしてください",openInNewTab:"新しいタブで開く",note:"注意：初回読み込み時に約150MBのGPT-2モデルをダウンロードします",features:["✓ 実際のGPT-2モデルを実行","✓ Attentionウェイトを可視化","✓ 各層をインタラクティブに探索","✓ リアルタイムで次のトークンを生成"]}}[e];return w.jsxs("div",{className:"bg-white rounded-2xl border border-gray-200 shadow-xl overflow-hidden",children:[w.jsxs("div",{className:"px-6 py-4 bg-gradient-to-r from-purple-50 via-white to-blue-50 border-b border-gray-100",children:[w.jsxs("div",{className:"flex items-center justify-between",children:[w.jsxs("div",{children:[w.jsx("h2",{className:"text-xl font-bold text-gray-800",children:o.title}),w.jsx("p",{className:"text-sm text-gray-500",children:o.subtitle})]}),w.jsxs("a",{href:"https://poloclub.github.io/transformer-explainer/",target:"_blank",rel:"noopener noreferrer",className:"px-4 py-2 bg-purple-600 text-white text-sm rounded-lg hover:bg-purple-700 transition-colors flex items-center gap-2",children:[o.openInNewTab,w.jsx("svg",{className:"w-4 h-4",fill:"none",viewBox:"0 0 24 24",stroke:"currentColor",children:w.jsx("path",{strokeLinecap:"round",strokeLinejoin:"round",strokeWidth:2,d:"M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14"})})]})]}),w.jsx("div",{className:"flex flex-wrap gap-4 mt-3",children:o.features.map((a,l)=>w.jsx("span",{className:"text-xs text-purple-600 bg-purple-50 px-2 py-1 rounded",children:a},l))})]}),w.jsxs("div",{className:"px-6 py-2 bg-amber-50 border-b border-amber-100 text-sm text-amber-700",children:["⚠️ ",o.note]}),w.jsxs("div",{className:"relative",style:{height:"700px"},children:[t&&!r&&w.jsx("div",{className:"absolute inset-0 flex items-center justify-center bg-gray-50 z-10",children:w.jsxs("div",{className:"text-center",children:[w.jsx("div",{className:"w-12 h-12 border-4 border-purple-200 border-t-purple-600 rounded-full animate-spin mx-auto mb-4"}),w.jsx("p",{className:"text-gray-600",children:o.loading})]})}),r&&w.jsx("div",{className:"absolute inset-0 flex items-center justify-center bg-gray-50 z-10",children:w.jsxs("div",{className:"text-center p-6",children:[w.jsx("div",{className:"text-4xl mb-4",children:"😕"}),w.jsx("p",{className:"text-gray-600 mb-4",children:o.error}),w.jsxs("a",{href:"https://poloclub.github.io/transformer-explainer/",target:"_blank",rel:"noopener noreferrer",className:"inline-flex items-center gap-2 px-4 py-2 bg-purple-600 text-white rounded-lg hover:bg-purple-700",children:[o.openInNewTab,w.jsx("svg",{className:"w-4 h-4",fill:"none",viewBox:"0 0 24 24",stroke:"currentColor",children:w.jsx("path",{strokeLinecap:"round",strokeLinejoin:"round",strokeWidth:2,d:"M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14"})})]})]})}),w.jsx("iframe",{src:"https://poloclub.github.io/transformer-explainer/",className:"w-full h-full border-0",title:"Transformer Explainer",onLoad:()=>n(!1),onError:()=>{n(!1),s(!0)},allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope",sandbox:"allow-scripts allow-same-origin allow-popups allow-forms"})]}),w.jsxs("div",{className:"px-6 py-3 bg-gray-50 border-t border-gray-100 flex justify-between items-center text-xs text-gray-500",children:[w.jsxs("span",{children:["© ",w.jsx("a",{href:"https://poloclub.github.io/",target:"_blank",rel:"noopener noreferrer",className:"text-purple-600 hover:underline",children:"Polo Club of Data Science"})," @ Georgia Tech"]}),w.jsxs("a",{href:"https://github.com/poloclub/transformer-explainer",target:"_blank",rel:"noopener noreferrer",className:"flex items-center gap-1 text-gray-500 hover:text-gray-700",children:[w.jsx("svg",{className:"w-4 h-4",fill:"currentColor",viewBox:"0 0 24 24",children:w.jsx("path",{fillRule:"evenodd",d:"M12 2C6.477 2 2 6.484 2 12.017c0 4.425 2.865 8.18 6.839 9.504.5.092.682-.217.682-.483 0-.237-.008-.868-.013-1.703-2.782.605-3.369-1.343-3.369-1.343-.454-1.158-1.11-1.466-1.11-1.466-.908-.62.069-.608.069-.608 1.003.07 1.531 1.032 1.531 1.032.892 1.53 2.341 1.088 2.91.832.092-.647.35-1.088.636-1.338-2.22-.253-4.555-1.113-4.555-4.951 0-1.093.39-1.988 1.029-2.688-.103-.253-.446-1.272.098-2.65 0 0 .84-.27 2.75 1.026A9.564 9.564 0 0112 6.844c.85.004 1.705.115 2.504.337 1.909-1.296 2.747-1.027 2.747-1.027.546 1.379.202 2.398.1 2.651.64.7 1.028 1.595 1.028 2.688 0 3.848-2.339 4.695-4.566 4.943.359.309.678.92.678 1.855 0 1.338-.012 2.419-.012 2.747 0 .268.18.58.688.482A10.019 10.019 0 0022 12.017C22 6.484 17.522 2 12 2z",clipRule:"evenodd"})}),"GitHub"]})]})]})},Nr={query:"#8b5cf6",embedding:"#3b82f6",retrieval:"#f59e0b",context:"#10b981",llm:"#ef4444",output:"#6366f1"},xP=e=>Array.from({length:24},(t,n)=>Math.abs(Math.sin(e*.1+n*.3))*.8+.2),za=[{id:1,title:"Transformer原理",score:.92,snippet:"Self-attention机制是Transformer的核心..."},{id:2,title:"GPT架构详解",score:.85,snippet:"GPT使用仅解码器的Transformer架构..."},{id:3,title:"注意力计算",score:.78,snippet:"Query、Key、Value三个矩阵的计算..."}],lm=({values:e,color:t,height:n=40,animated:r=!0})=>{const o=e.length*5;return w.jsx("svg",{width:o,height:n,className:"overflow-visible",children:e.map((a,l)=>w.jsx(lt.rect,{x:l*5,y:n*(1-a),width:4,height:n*a,fill:t,opacity:.4+a*.6,rx:1,initial:r?{scaleY:0}:!1,animate:{scaleY:1},transition:{delay:l*.02,duration:.3},style:{transformOrigin:"bottom"}},l))})},Ao=({title:e,subtitle:t,color:n,children:r,isActive:s,onClick:i,expandable:o})=>w.jsxs(lt.div,{className:`
      rounded-xl border-2 p-4 transition-all duration-300 cursor-pointer
      ${s?"shadow-lg scale-105":"shadow-sm hover:shadow-md"}
    `,style:{borderColor:s?n:"#e5e7eb",backgroundColor:s?`${n}08`:"white"},onClick:i,whileHover:{scale:o?1.02:1},children:[w.jsxs("div",{className:"flex items-center gap-2 mb-2",children:[w.jsx("div",{className:"w-3 h-3 rounded-full",style:{backgroundColor:n}}),w.jsx("span",{className:"font-semibold text-gray-800 text-sm",children:e})]}),t&&w.jsx("div",{className:"text-xs text-gray-500 mb-2",children:t}),w.jsx("div",{children:r})]}),z8=({language:e})=>{const[t,n]=Me.useState(null),[r,s]=Me.useState(null),[i,o]=Me.useState(e==="zh"?"Transformer是如何工作的？":"Transformerはどのように動作しますか？"),[a,l]=Me.useState(!1),[c,d]=Me.useState(!1),p={zh:{title:"RAG 检索增强生成 可视化",subtitle:"交互式探索 RAG 的工作流程",stages:{query:"用户查询",embedding:"查询嵌入",retrieval:"向量检索",context:"上下文组装",llm:"LLM 生成",output:"最终回答"},queryPlaceholder:"输入你的问题...",run:"运行 RAG",running:"处理中...",reset:"重置",docTitle:"相关文档",similarity:"相似度",result:"Transformer是一种基于自注意力机制的神经网络架构，它通过并行处理序列数据，能够捕捉长距离依赖关系...",tip:"点击各阶段查看详细说明，悬停文档查看相关性"},ja:{title:"RAG 検索拡張生成 可視化",subtitle:"RAGのワークフローをインタラクティブに探索",stages:{query:"ユーザークエリ",embedding:"クエリ埋め込み",retrieval:"ベクトル検索",context:"コンテキスト組立",llm:"LLM 生成",output:"最終回答"},queryPlaceholder:"質問を入力...",run:"RAG実行",running:"処理中...",reset:"リセット",docTitle:"関連ドキュメント",similarity:"類似度",result:"Transformerは自己注意力機構に基づくニューラルネットワークアーキテクチャで、シーケンスデータを並列処理し、長距離依存関係を捉えることができます...",tip:"各ステージをクリックして詳細を確認、ドキュメントにホバーで関連性を表示"}}[e],h=Me.useMemo(()=>xP(i.length),[i]),m=Me.useMemo(()=>za.map((f,_)=>xP(_*10)),[]),g=Me.useCallback(()=>{l(!0),d(!1),n("query"),["query","embedding","retrieval","context","llm","output"].forEach((_,T)=>{setTimeout(()=>{n(_),_==="output"&&(l(!1),d(!0))},T*800)})},[]),I=Me.useCallback(()=>{n(null),d(!1),l(!1)},[]);return w.jsxs("div",{className:"bg-gradient-to-br from-purple-50 via-white to-blue-50 rounded-2xl border border-gray-200 shadow-xl overflow-hidden",children:[w.jsxs("div",{className:"px-6 py-4 bg-white/80 border-b border-gray-100",children:[w.jsx("h2",{className:"text-xl font-bold text-gray-800",children:p.title}),w.jsx("p",{className:"text-sm text-gray-500",children:p.subtitle})]}),w.jsxs("div",{className:"p-6",children:[w.jsxs("div",{className:"mb-6 flex gap-4 items-center",children:[w.jsx("input",{type:"text",value:i,onChange:f=>o(f.target.value),placeholder:p.queryPlaceholder,className:"flex-1 px-4 py-2 border border-gray-300 rounded-lg focus:outline-none focus:ring-2 focus:ring-purple-500",disabled:a}),w.jsx("button",{onClick:g,disabled:a,className:"px-6 py-2 bg-purple-600 text-white rounded-lg hover:bg-purple-700 disabled:opacity-50 disabled:cursor-not-allowed transition-colors",children:a?p.running:p.run}),c&&w.jsx("button",{onClick:I,className:"px-4 py-2 border border-gray-300 rounded-lg hover:bg-gray-50 transition-colors",children:p.reset})]}),w.jsxs("div",{className:"relative",children:[w.jsx("svg",{className:"absolute inset-0 w-full h-full pointer-events-none",style:{zIndex:0},children:w.jsx("defs",{children:w.jsxs("linearGradient",{id:"flowGradient",x1:"0%",y1:"0%",x2:"100%",y2:"0%",children:[w.jsx("stop",{offset:"0%",stopColor:Nr.query}),w.jsx("stop",{offset:"50%",stopColor:Nr.retrieval}),w.jsx("stop",{offset:"100%",stopColor:Nr.output})]})})}),w.jsxs("div",{className:"grid grid-cols-6 gap-4",children:[w.jsx(Ao,{title:p.stages.query,color:Nr.query,isActive:t==="query",onClick:()=>n("query"),expandable:!0,children:w.jsx("div",{className:"bg-purple-50 rounded-lg p-3 text-sm text-gray-700 min-h-[60px]",children:i})}),w.jsx(Ao,{title:p.stages.embedding,subtitle:"768-dim",color:Nr.embedding,isActive:t==="embedding",onClick:()=>n("embedding"),expandable:!0,children:w.jsx("div",{className:"flex items-center justify-center py-2",children:w.jsx(lm,{values:h,color:Nr.embedding,animated:t==="embedding"})})}),w.jsx(Ao,{title:p.stages.retrieval,subtitle:"Top-3",color:Nr.retrieval,isActive:t==="retrieval",onClick:()=>n("retrieval"),expandable:!0,children:w.jsx("div",{className:"space-y-2",children:za.map((f,_)=>w.jsxs(lt.div,{className:`text-xs p-2 rounded cursor-pointer transition-all ${r===_?"bg-amber-100 ring-2 ring-amber-400":"bg-amber-50"}`,onMouseEnter:()=>s(_),onMouseLeave:()=>s(null),initial:{opacity:0,x:-10},animate:{opacity:1,x:0},transition:{delay:_*.1},children:[w.jsx("div",{className:"font-medium text-gray-700",children:f.title}),w.jsxs("div",{className:"text-amber-600",children:[p.similarity,": ",(f.score*100).toFixed(0),"%"]})]},f.id))})}),w.jsx(Ao,{title:p.stages.context,color:Nr.context,isActive:t==="context",onClick:()=>n("context"),expandable:!0,children:w.jsxs("div",{className:"text-xs space-y-1",children:[w.jsx("div",{className:"text-gray-500",children:"Query + Documents"}),w.jsx("div",{className:"bg-green-50 rounded p-2 text-gray-600 truncate",children:za.map(f=>f.title).join(" + ")})]})}),w.jsx(Ao,{title:p.stages.llm,subtitle:"GPT-4 / Claude",color:Nr.llm,isActive:t==="llm",onClick:()=>n("llm"),expandable:!0,children:w.jsxs("div",{className:"flex items-center justify-center py-3",children:[t==="llm"&&w.jsx(lt.div,{className:"flex space-x-1",initial:{opacity:0},animate:{opacity:1},children:[0,1,2].map(f=>w.jsx(lt.div,{className:"w-2 h-2 rounded-full bg-red-500",animate:{y:[0,-8,0]},transition:{duration:.6,repeat:1/0,delay:f*.2}},f))}),t!=="llm"&&w.jsx("div",{className:"text-2xl",children:"🤖"})]})}),w.jsx(Ao,{title:p.stages.output,color:Nr.output,isActive:t==="output",onClick:()=>n("output"),expandable:!0,children:w.jsx(vr,{children:c&&w.jsx(lt.div,{className:"text-xs text-gray-700 bg-indigo-50 rounded p-2 max-h-[80px] overflow-hidden",initial:{opacity:0,height:0},animate:{opacity:1,height:"auto"},exit:{opacity:0},children:p.result})})})]})]}),w.jsx(vr,{children:t&&w.jsxs(lt.div,{className:"mt-6 p-4 bg-white rounded-xl border border-gray-200 shadow-sm",initial:{opacity:0,y:20},animate:{opacity:1,y:0},exit:{opacity:0,y:-20},children:[t==="embedding"&&w.jsxs("div",{children:[w.jsx("h3",{className:"font-semibold text-gray-800 mb-3",children:e==="zh"?"查询向量化":"クエリのベクトル化"}),w.jsxs("div",{className:"grid grid-cols-2 gap-4",children:[w.jsxs("div",{children:[w.jsx("div",{className:"text-sm text-gray-500 mb-2",children:e==="zh"?"原始文本":"元テキスト"}),w.jsx("div",{className:"bg-gray-50 p-3 rounded-lg text-sm",children:i})]}),w.jsxs("div",{children:[w.jsx("div",{className:"text-sm text-gray-500 mb-2",children:e==="zh"?"嵌入向量 (768维，展示24维)":"埋め込みベクトル (768次元、24次元表示)"}),w.jsx("div",{className:"bg-blue-50 p-3 rounded-lg",children:w.jsx(lm,{values:h,color:Nr.embedding,height:50})})]})]})]}),t==="retrieval"&&w.jsxs("div",{children:[w.jsx("h3",{className:"font-semibold text-gray-800 mb-3",children:e==="zh"?"向量相似度检索":"ベクトル類似度検索"}),w.jsx("div",{className:"space-y-3",children:za.map((f,_)=>w.jsxs("div",{className:`p-3 rounded-lg border transition-all ${r===_?"border-amber-400 bg-amber-50":"border-gray-200 bg-gray-50"}`,onMouseEnter:()=>s(_),onMouseLeave:()=>s(null),children:[w.jsxs("div",{className:"flex items-center justify-between mb-2",children:[w.jsx("span",{className:"font-medium text-gray-800",children:f.title}),w.jsxs("div",{className:"flex items-center gap-2",children:[w.jsx("div",{className:"h-2 w-24 bg-gray-200 rounded-full overflow-hidden",children:w.jsx("div",{className:"h-full bg-amber-500 rounded-full",style:{width:`${f.score*100}%`}})}),w.jsxs("span",{className:"text-sm text-amber-600 font-medium",children:[(f.score*100).toFixed(0),"%"]})]})]}),w.jsxs("div",{className:"flex items-center gap-4",children:[w.jsx("div",{className:"text-xs text-gray-500",children:e==="zh"?"文档向量:":"ドキュメントベクトル:"}),w.jsx(lm,{values:m[_],color:Nr.retrieval,height:20,animated:!1})]}),w.jsx("div",{className:"text-sm text-gray-600 mt-2",children:f.snippet})]},f.id))})]}),t==="context"&&w.jsxs("div",{children:[w.jsx("h3",{className:"font-semibold text-gray-800 mb-3",children:e==="zh"?"Prompt 组装":"Prompt組立"}),w.jsxs("div",{className:"bg-gray-900 text-green-400 p-4 rounded-lg font-mono text-sm overflow-x-auto",children:[w.jsx("div",{className:"text-gray-500",children:"# System Prompt"}),w.jsx("div",{children:e==="zh"?"你是一个AI助手，根据以下文档回答问题。":"あなたはAIアシスタントです。以下のドキュメントに基づいて質問に答えてください。"}),w.jsx("br",{}),w.jsx("div",{className:"text-gray-500",children:"# Retrieved Documents"}),za.map((f,_)=>w.jsxs("div",{className:"text-yellow-400",children:["[",_+1,"] ",f.title,": ",f.snippet]},_)),w.jsx("br",{}),w.jsx("div",{className:"text-gray-500",children:"# User Query"}),w.jsx("div",{className:"text-cyan-400",children:i})]})]})]})}),w.jsxs("div",{className:"mt-4 text-center text-sm text-gray-400",children:["💡 ",p.tip]})]})]})},Vi={think:"#8b5cf6",act:"#3b82f6",observe:"#10b981"},B8={search:"🔍",code:"💻",file:"📁",browser:"🌐",terminal:"⌨️",database:"🗃️"},Ba=[{id:1,type:"think",content:{zh:`用户想要了解如何在 Python 中读取 JSON 文件。我需要：
1. 搜索相关文档
2. 编写示例代码
3. 解释代码`,ja:`ユーザーはPythonでJSONファイルを読む方法を知りたがっています。必要なこと：
1. 関連ドキュメントを検索
2. サンプルコードを作成
3. コードを説明`}},{id:2,type:"act",tool:"search",toolInput:"Python json module documentation",content:{zh:"调用搜索工具查找 Python JSON 模块文档",ja:"検索ツールを呼び出してPython JSONモジュールのドキュメントを検索"},status:"success"},{id:3,type:"observe",content:{zh:"找到了 Python 官方文档中关于 json 模块的说明。json.load() 用于从文件读取，json.loads() 用于从字符串解析。",ja:"Python公式ドキュメントでjsonモジュールの説明を見つけました。json.load()はファイルから読み取り、json.loads()は文字列から解析します。"},toolOutput:{zh:`文档: json — JSON 编码和解码器
- json.load(fp): 从文件对象读取
- json.loads(s): 从字符串解析
- json.dump(): 写入文件
- json.dumps(): 转为字符串`,ja:`ドキュメント: json — JSONエンコーダー/デコーダー
- json.load(fp): ファイルオブジェクトから読み取り
- json.loads(s): 文字列から解析
- json.dump(): ファイルに書き込み
- json.dumps(): 文字列に変換`}},{id:4,type:"think",content:{zh:"已获取文档信息。现在需要编写一个完整的示例代码来演示如何读取 JSON 文件。",ja:"ドキュメント情報を取得しました。次にJSONファイルの読み取り方法を示す完全なサンプルコードを作成します。"}},{id:5,type:"act",tool:"code",toolInput:"write_python_json_example",content:{zh:"生成 Python 示例代码",ja:"Pythonサンプルコードを生成"},status:"success"},{id:6,type:"observe",content:{zh:"代码已生成。包含了文件读取、错误处理和数据访问的完整示例。",ja:"コードが生成されました。ファイル読み取り、エラー処理、データアクセスの完全な例を含んでいます。"},toolOutput:{zh:`import json

# 读取 JSON 文件
with open('data.json', 'r', encoding='utf-8') as f:
    data = json.load(f)

# 访问数据
print(data['name'])
print(data['items'])`,ja:`import json

# JSONファイルを読み取る
with open('data.json', 'r', encoding='utf-8') as f:
    data = json.load(f)

# データにアクセス
print(data['name'])
print(data['items'])`}},{id:7,type:"think",content:{zh:"代码示例已准备好。任务完成，可以向用户提供完整的回答了。",ja:"コード例の準備ができました。タスク完了、ユーザーに完全な回答を提供できます。"}}],cm=({label:e,color:t,isActive:n,icon:r,onClick:s})=>w.jsxs(lt.div,{className:`
      flex flex-col items-center justify-center w-24 h-24 rounded-full cursor-pointer
      transition-all duration-300 border-4
      ${n?"shadow-lg scale-110":"shadow-sm hover:shadow-md"}
    `,style:{borderColor:n?t:"#e5e7eb",backgroundColor:n?`${t}20`:"white"},onClick:s,whileHover:{scale:1.05},whileTap:{scale:.95},children:[w.jsx("div",{className:"text-3xl mb-1",children:r}),w.jsx("div",{className:"text-sm font-semibold",style:{color:n?t:"#6b7280"},children:e})]}),Ga=({direction:e,isActive:t})=>{const n={right:0,down:90,left:180,up:-90};return w.jsx(lt.div,{className:"flex items-center justify-center",style:{transform:`rotate(${n[e]}deg)`},children:w.jsx("svg",{width:"40",height:"20",viewBox:"0 0 40 20",children:w.jsx(lt.path,{d:"M 0 10 L 30 10 M 25 5 L 30 10 L 25 15",stroke:t?Vi.think:"#d1d5db",strokeWidth:t?3:2,fill:"none",strokeLinecap:"round",strokeLinejoin:"round",initial:{pathLength:0},animate:{pathLength:1},transition:{duration:.5}})})})},G8=({step:e,language:t,isActive:n,isExpanded:r,onToggle:s})=>{const i=()=>{switch(e.type){case"think":return Vi.think;case"act":return Vi.act;case"observe":return Vi.observe}},o=()=>{switch(e.type){case"think":return"🤔";case"act":return e.tool?B8[e.tool]||"🔧":"⚡";case"observe":return"👁️"}},a=()=>({think:{zh:"思考",ja:"思考"},act:{zh:"行动",ja:"アクション"},observe:{zh:"观察",ja:"観察"}})[e.type][t];return w.jsxs(lt.div,{className:`
        border-l-4 rounded-r-lg overflow-hidden mb-3
        ${n?"shadow-md":"shadow-sm"}
      `,style:{borderLeftColor:i()},initial:{opacity:0,x:-20},animate:{opacity:1,x:0},transition:{duration:.3},children:[w.jsxs("div",{className:`
          px-4 py-3 cursor-pointer transition-colors
          ${n?"bg-white":"bg-gray-50 hover:bg-gray-100"}
        `,onClick:s,children:[w.jsxs("div",{className:"flex items-center justify-between",children:[w.jsxs("div",{className:"flex items-center gap-3",children:[w.jsx("span",{className:"text-xl",children:o()}),w.jsx("span",{className:"text-sm font-semibold px-2 py-0.5 rounded",style:{backgroundColor:`${i()}20`,color:i()},children:a()}),e.tool&&w.jsx("span",{className:"text-xs text-gray-500 bg-gray-100 px-2 py-0.5 rounded",children:e.tool}),e.status&&w.jsx("span",{className:`text-xs px-2 py-0.5 rounded ${e.status==="success"?"bg-green-100 text-green-600":e.status==="error"?"bg-red-100 text-red-600":"bg-blue-100 text-blue-600"}`,children:e.status})]}),w.jsx(lt.span,{animate:{rotate:r?180:0},className:"text-gray-400",children:"▼"})]}),w.jsx("div",{className:"text-sm text-gray-600 mt-2",children:e.content[t].split(`
`)[0]})]}),w.jsx(vr,{children:r&&w.jsx(lt.div,{initial:{height:0,opacity:0},animate:{height:"auto",opacity:1},exit:{height:0,opacity:0},className:"bg-gray-50 border-t border-gray-200",children:w.jsxs("div",{className:"px-4 py-3",children:[e.type==="think"&&w.jsx("pre",{className:"text-sm text-gray-700 whitespace-pre-wrap font-sans",children:e.content[t]}),e.type==="act"&&e.toolInput&&w.jsxs("div",{className:"space-y-2",children:[w.jsx("div",{className:"text-xs text-gray-500",children:t==="zh"?"工具输入:":"ツール入力:"}),w.jsx("code",{className:"block bg-gray-900 text-green-400 p-3 rounded text-sm",children:e.toolInput})]}),e.type==="observe"&&e.toolOutput&&w.jsxs("div",{className:"space-y-2",children:[w.jsx("div",{className:"text-xs text-gray-500",children:t==="zh"?"工具输出:":"ツール出力:"}),w.jsx("pre",{className:"bg-gray-900 text-gray-300 p-3 rounded text-sm overflow-x-auto",children:e.toolOutput[t]})]})]})})})]})},V8=({language:e})=>{const[t,n]=Me.useState("think"),[r,s]=Me.useState(0),[i,o]=Me.useState(null),[a,l]=Me.useState(!1),[c,d]=Me.useState([]),u=Me.useRef(null),h={zh:{title:"AI Agent 工作循环",subtitle:"探索 ReAct (Reasoning + Acting) 模式",think:"思考",act:"行动",observe:"观察",autoPlay:"自动播放",pause:"暂停",reset:"重置",step:"步骤",tip:"点击循环节点或步骤卡片查看详情，使用自动播放观察完整流程",task:"任务：帮用户了解如何在 Python 中读取 JSON 文件",complete:"任务完成!"},ja:{title:"AI Agent 動作ループ",subtitle:"ReAct (Reasoning + Acting) パターンを探索",think:"思考",act:"アクション",observe:"観察",autoPlay:"自動再生",pause:"一時停止",reset:"リセット",step:"ステップ",tip:"ループノードまたはステップカードをクリックで詳細表示、自動再生で完全なフローを確認",task:"タスク：PythonでJSONファイルを読む方法をユーザーに説明",complete:"タスク完了!"}}[e];Me.useEffect(()=>(a&&(u.current=setInterval(()=>{s(f=>{if(f<Ba.length-1){const _=f+1;return d(T=>[...T,f]),n(Ba[_].type),_}else return l(!1),d(_=>[..._,f]),f})},2e3)),()=>{u.current&&clearInterval(u.current)}),[a]);const m=Me.useCallback(()=>{l(f=>!f)},[]),g=Me.useCallback(()=>{l(!1),s(0),d([]),n("think"),o(null)},[]),I=c.length===Ba.length;return w.jsxs("div",{className:"bg-gradient-to-br from-purple-50 via-white to-green-50 rounded-2xl border border-gray-200 shadow-xl overflow-hidden",children:[w.jsxs("div",{className:"px-6 py-4 bg-white/80 border-b border-gray-100",children:[w.jsx("h2",{className:"text-xl font-bold text-gray-800",children:h.title}),w.jsx("p",{className:"text-sm text-gray-500",children:h.subtitle})]}),w.jsxs("div",{className:"p-6",children:[w.jsx("div",{className:"mb-6 p-4 bg-blue-50 rounded-lg border border-blue-200",children:w.jsx("div",{className:"text-sm text-blue-800 font-medium",children:h.task})}),w.jsx("div",{className:"flex justify-center mb-8",children:w.jsxs("div",{className:"relative",children:[w.jsxs("div",{className:"flex flex-col items-center",children:[w.jsx(cm,{label:h.think,color:Vi.think,isActive:t==="think",icon:"🤔",onClick:()=>n("think")}),w.jsxs("div",{className:"flex items-center gap-8 mt-4",children:[w.jsxs("div",{className:"relative",children:[w.jsx(Ga,{direction:"down",isActive:t==="think"}),w.jsx("div",{className:"absolute -right-2 top-1/2 transform -translate-y-1/2 rotate-45",children:w.jsx(Ga,{direction:"right",isActive:t==="think"})})]}),w.jsxs("div",{className:"relative",children:[w.jsx(Ga,{direction:"up",isActive:t==="observe"}),w.jsx("div",{className:"absolute -left-2 top-1/2 transform -translate-y-1/2 -rotate-45",children:w.jsx(Ga,{direction:"left",isActive:t==="observe"})})]})]}),w.jsxs("div",{className:"flex items-center gap-12 mt-4",children:[w.jsx(cm,{label:h.act,color:Vi.act,isActive:t==="act",icon:"⚡",onClick:()=>n("act")}),w.jsx("div",{className:"flex items-center",children:w.jsx(Ga,{direction:"right",isActive:t==="act"})}),w.jsx(cm,{label:h.observe,color:Vi.observe,isActive:t==="observe",icon:"👁️",onClick:()=>n("observe")})]})]}),w.jsx(vr,{children:I&&w.jsxs(lt.div,{className:"absolute -top-4 -right-4 bg-green-500 text-white px-3 py-1 rounded-full text-sm font-semibold shadow-lg",initial:{scale:0},animate:{scale:1},exit:{scale:0},children:["✓ ",h.complete]})})]})}),w.jsxs("div",{className:"flex justify-center gap-4 mb-6",children:[w.jsx("button",{onClick:m,disabled:I,className:`
              px-4 py-2 rounded-lg font-medium transition-colors
              ${a?"bg-amber-500 text-white hover:bg-amber-600":"bg-purple-600 text-white hover:bg-purple-700"}
              disabled:opacity-50 disabled:cursor-not-allowed
            `,children:a?`⏸ ${h.pause}`:`▶ ${h.autoPlay}`}),w.jsxs("button",{onClick:g,className:"px-4 py-2 border border-gray-300 rounded-lg hover:bg-gray-50 transition-colors",children:["🔄 ",h.reset]})]}),w.jsx("div",{className:"flex justify-center mb-6",children:w.jsx("div",{className:"flex items-center gap-2",children:Ba.map((f,_)=>w.jsx("div",{className:`
                  w-3 h-3 rounded-full transition-all duration-300
                  ${c.includes(_)?"bg-green-500":_===r?"bg-purple-500 scale-125":"bg-gray-300"}
                `},_))})}),w.jsx("div",{className:"max-h-96 overflow-y-auto pr-2",children:Ba.map((f,_)=>w.jsx(G8,{step:f,language:e,isActive:_===r,isExpanded:i===f.id,onToggle:()=>o(i===f.id?null:f.id)},f.id))}),w.jsxs("div",{className:"mt-4 text-center text-sm text-gray-400",children:["💡 ",h.tip]})]})]})},Kc={query:"#3b82f6",key:"#ef4444",value:"#22c55e",attention:"#8b5cf6"},U8={zh:["我","喜欢","学习","人工","智能"],ja:["私","は","AI","を","学ぶ"]},bP=[[.65,.15,.1,.05,.05],[.3,.4,.15,.1,.05],[.15,.25,.35,.15,.1],[.1,.15,.2,.4,.15],[.08,.12,.2,.25,.35]],um=(e,t=8)=>Array.from({length:t},(n,r)=>Math.abs(Math.sin(e*.5+r*.7))*.8+.2),dm=({values:e,color:t,label:n,highlighted:r})=>w.jsxs("div",{className:`flex items-center gap-2 p-2 rounded transition-all ${r?"bg-gray-100 ring-2":""}`,style:{"--tw-ring-color":r?t:"transparent"},children:[w.jsx("span",{className:"text-xs font-mono w-8 text-gray-500",children:n}),w.jsx("div",{className:"flex gap-0.5",children:e.map((s,i)=>w.jsx("div",{className:"w-2 rounded-sm transition-all",style:{height:`${s*24}px`,backgroundColor:t,opacity:.4+s*.6}},i))})]}),W8=({value:e,isHighlighted:t,rowToken:n,colToken:r,onHover:s,onLeave:i})=>{const o=`rgba(139, 92, 246, ${e*.9})`;return w.jsx(lt.div,{className:`
        w-10 h-10 flex items-center justify-center text-xs cursor-pointer
        transition-all duration-200 rounded
        ${t?"ring-2 ring-purple-500 scale-110 z-10":""}
      `,style:{backgroundColor:o},onMouseEnter:s,onMouseLeave:i,whileHover:{scale:1.1},title:`${n} → ${r}: ${(e*100).toFixed(1)}%`,children:w.jsx("span",{className:e>.4?"text-white":"text-gray-700",children:(e*100).toFixed(0)})})},H8=({step:e,language:t})=>{const n=[{formula:"Q = X · W_Q, K = X · W_K, V = X · W_V",zh:"计算 Query、Key、Value 向量",ja:"Query、Key、Valueベクトルを計算"},{formula:"Score = Q · K^T",zh:"计算注意力分数（点积）",ja:"アテンションスコアを計算（ドット積）"},{formula:"Score = Score / √d_k",zh:"缩放（防止梯度消失）",ja:"スケーリング（勾配消失防止）"},{formula:"Attention = Softmax(Score)",zh:"Softmax 归一化",ja:"Softmax正規化"},{formula:"Output = Attention · V",zh:"加权求和得到输出",ja:"重み付き和で出力を取得"}],r=n[e]||n[0];return w.jsxs(lt.div,{className:"bg-gray-900 rounded-lg p-4 text-center",initial:{opacity:0},animate:{opacity:1},children:[w.jsx("div",{className:"font-mono text-lg text-purple-400 mb-2",children:r.formula}),w.jsx("div",{className:"text-sm text-gray-400",children:r[t]})]},e)},K8=({language:e})=>{const[t,n]=Me.useState(null),[r,s]=Me.useState(null),[i,o]=Me.useState(0),[a,l]=Me.useState(!1),c=U8[e],u={zh:{title:"自注意力机制可视化",subtitle:"探索 Transformer 的核心组件",heatmapTitle:"注意力权重热力图",qkvTitle:"Q、K、V 向量",steps:["生成 QKV","计算分数","缩放","Softmax","加权求和"],tip:"悬停热力图单元格查看注意力权重，点击 token 查看其注意力模式",showQKV:"显示 QKV 向量",hideQKV:"隐藏 QKV 向量",rowLabel:"查询 (Query)",colLabel:"键 (Key)",explanation:"每行表示一个 token 对其他所有 token 的注意力分布。数值越高，表示该 token 越关注对应位置。"},ja:{title:"セルフアテンション機構の可視化",subtitle:"Transformerのコアコンポーネントを探索",heatmapTitle:"アテンション重みヒートマップ",qkvTitle:"Q、K、V ベクトル",steps:["QKV生成","スコア計算","スケーリング","Softmax","重み付き和"],tip:"ヒートマップのセルにホバーでアテンション重みを確認、トークンをクリックでパターンを表示",showQKV:"QKVベクトルを表示",hideQKV:"QKVベクトルを非表示",rowLabel:"クエリ (Query)",colLabel:"キー (Key)",explanation:"各行は1つのトークンが他のすべてのトークンに対するアテンション分布を表します。値が高いほど、そのトークンが対応する位置に注目しています。"}}[e],p=Me.useMemo(()=>c.map((m,g)=>({q:um(g*3),k:um(g*3+1),v:um(g*3+2)})),[c]),h=Me.useCallback(m=>{s(r===m?null:m)},[r]);return w.jsxs("div",{className:"bg-gradient-to-br from-purple-50 via-white to-blue-50 rounded-2xl border border-gray-200 shadow-xl overflow-hidden",children:[w.jsxs("div",{className:"px-6 py-4 bg-white/80 border-b border-gray-100",children:[w.jsx("h2",{className:"text-xl font-bold text-gray-800",children:u.title}),w.jsx("p",{className:"text-sm text-gray-500",children:u.subtitle})]}),w.jsxs("div",{className:"p-6",children:[w.jsx("div",{className:"flex items-center justify-center gap-2 mb-6",children:u.steps.map((m,g)=>w.jsx("button",{onClick:()=>o(g),className:`
                px-3 py-1 text-sm rounded-full transition-all
                ${i===g?"bg-purple-600 text-white":"bg-gray-200 text-gray-600 hover:bg-gray-300"}
              `,children:m},g))}),w.jsx(H8,{step:i,language:e}),w.jsxs("div",{className:"grid grid-cols-1 lg:grid-cols-2 gap-6 mt-6",children:[w.jsxs("div",{children:[w.jsx("h3",{className:"font-semibold text-gray-700 mb-3",children:u.heatmapTitle}),w.jsxs("div",{className:"bg-white rounded-xl p-4 shadow-sm",children:[w.jsx("div",{className:"flex items-center mb-2 pl-12",children:w.jsx("div",{className:"text-xs text-red-500 font-medium mb-1 w-full text-center",children:u.colLabel})}),w.jsx("div",{className:"flex items-center mb-2 pl-12",children:c.map((m,g)=>w.jsx("div",{className:`
                      w-10 text-center text-sm cursor-pointer transition-all
                      ${r===g?"text-purple-600 font-bold":"text-gray-600"}
                      ${t?.col===g?"text-red-500 font-semibold":""}
                    `,onClick:()=>h(g),children:m},g))}),w.jsxs("div",{className:"flex",children:[w.jsxs("div",{className:"flex flex-col mr-2",children:[w.jsx("div",{className:"text-xs text-blue-500 font-medium mb-1 transform -rotate-90 origin-center whitespace-nowrap h-12 flex items-center",children:u.rowLabel}),c.map((m,g)=>w.jsx("div",{className:`
                        h-10 flex items-center justify-end pr-2 text-sm cursor-pointer transition-all
                        ${r===g?"text-purple-600 font-bold":"text-gray-600"}
                        ${t?.row===g?"text-blue-500 font-semibold":""}
                      `,onClick:()=>h(g),children:m},g))]}),w.jsx("div",{className:"flex flex-col gap-1",children:bP.map((m,g)=>w.jsx("div",{className:"flex gap-1",children:m.map((I,f)=>w.jsx(W8,{value:I,isHighlighted:t?.row===g&&t?.col===f||r===g,rowToken:c[g],colToken:c[f],onHover:()=>n({row:g,col:f}),onLeave:()=>n(null)},f))},g))})]}),w.jsxs("div",{className:"flex items-center justify-center mt-4 gap-4",children:[w.jsx("span",{className:"text-xs text-gray-500",children:"0%"}),w.jsx("div",{className:"w-32 h-3 rounded-full",style:{background:"linear-gradient(to right, rgba(139, 92, 246, 0.1), rgba(139, 92, 246, 0.9))"}}),w.jsx("span",{className:"text-xs text-gray-500",children:"100%"})]})]}),w.jsx("p",{className:"text-sm text-gray-500 mt-3",children:u.explanation})]}),w.jsxs("div",{children:[w.jsxs("div",{className:"flex items-center justify-between mb-3",children:[w.jsx("h3",{className:"font-semibold text-gray-700",children:u.qkvTitle}),w.jsx("button",{onClick:()=>l(!a),className:"text-sm text-purple-600 hover:text-purple-700",children:a?u.hideQKV:u.showQKV})]}),w.jsx(vr,{children:a&&w.jsx(lt.div,{initial:{opacity:0,height:0},animate:{opacity:1,height:"auto"},exit:{opacity:0,height:0},className:"bg-white rounded-xl p-4 shadow-sm space-y-4",children:c.map((m,g)=>w.jsxs("div",{className:`p-3 rounded-lg transition-all ${r===g?"bg-purple-50 ring-2 ring-purple-300":"bg-gray-50"}`,onClick:()=>h(g),children:[w.jsxs("div",{className:"font-medium text-gray-700 mb-2 cursor-pointer",children:['Token: "',m,'"']}),w.jsxs("div",{className:"space-y-1",children:[w.jsx(dm,{values:p[g].q,color:Kc.query,label:"Q",highlighted:r===g||t?.row===g}),w.jsx(dm,{values:p[g].k,color:Kc.key,label:"K",highlighted:r===g||t?.col===g}),w.jsx(dm,{values:p[g].v,color:Kc.value,label:"V",highlighted:r===g})]})]},g))})}),r!==null&&w.jsxs(lt.div,{className:"mt-4 bg-white rounded-xl p-4 shadow-sm",initial:{opacity:0,y:10},animate:{opacity:1,y:0},children:[w.jsxs("h4",{className:"font-medium text-gray-700 mb-3",children:['"',c[r],'" ',e==="zh"?"的注意力分布":"のアテンション分布"]}),w.jsx("div",{className:"space-y-2",children:bP[r].map((m,g)=>w.jsxs("div",{className:"flex items-center gap-3",children:[w.jsx("span",{className:"text-sm text-gray-600 w-12",children:c[g]}),w.jsx("div",{className:"flex-1 h-6 bg-gray-100 rounded-full overflow-hidden",children:w.jsx(lt.div,{className:"h-full rounded-full",style:{backgroundColor:Kc.attention},initial:{width:0},animate:{width:`${m*100}%`},transition:{duration:.5,delay:g*.1}})}),w.jsxs("span",{className:"text-sm text-gray-500 w-12 text-right",children:[(m*100).toFixed(1),"%"]})]},g))})]})]})]}),w.jsxs("div",{className:"mt-6 text-center text-sm text-gray-400",children:["💡 ",u.tip]})]})]})},Ur={embedding:"#9ca3af",query:"#3b82f6",key:"#ef4444",value:"#22c55e",mlp:"#f59e0b"},ju={numLayers:12,numHeads:12},To=(e,t=0)=>Array.from({length:e},(n,r)=>Math.abs(Math.sin(t*.1+r*.3+Math.random()*.1))*.8+.2),q8=(e,t)=>Array.from({length:t},(n,r)=>{const s=e/Math.pow(1e4,2*Math.floor(r/2)/t);return r%2===0?Math.sin(s):Math.cos(s)}).map(n=>(n+1)/2),Ru=["Data visualization empowers users to","Artificial Intelligence is transforming the","The weather today is very","Machine learning models can","Deep neural networks are"],Q8=e=>{const t=e.trim().split(/\s+/),n=[];for(const r of t)r.length>8?(n.push(r.slice(0,4)),n.push(r.slice(4))):n.push(r);return n},X8=(e,t)=>{const n=[];for(let r=0;r<e;r++){const s=[];let i=0;for(let o=0;o<e;o++)if(o<=r){let a=Math.random()*.5;r===o&&(a+=.3),r-o<=2&&(a+=.2),t%3===0&&o===0&&(a+=.4),s.push(a),i+=a}else s.push(0);n.push(s.map(o=>o/i))}return n},Y8=()=>{const e=[{token:"the",base:.15},{token:"a",base:.12},{token:"their",base:.08},{token:"new",base:.07},{token:"world",base:.06},{token:"better",base:.05},{token:"more",base:.05},{token:"future",base:.04},{token:"understand",base:.04},{token:"explore",base:.03},{token:"create",base:.03},{token:"discover",base:.02},{token:"make",base:.02},{token:"see",base:.02},{token:"find",base:.02}];let t=0;return e.map(r=>{const s=Math.log(r.base)+(Math.random()-.5)*2,i=Math.exp(s);return t+=i,{...r,logit:s,prob:i}}).map((r,s)=>({token:r.token,tokenId:1e3+s,logit:r.logit,scaledLogit:r.logit/.8,probability:r.prob/t,rank:s})).sort((r,s)=>s.probability-r.probability).map((r,s)=>({...r,rank:s}))},yf=e=>{const t=Q8(e),n=t.length,r=t.map((c,d)=>({text:c,id:1e3+d*100+Math.floor(Math.random()*100),embedding:To(24,d),positionEncoding:q8(d,24)})),s=r.map((c,d)=>({query:To(24,d*3),key:To(24,d*3+1),value:To(24,d*3+2)})),i=Math.min(6,ju.numLayers),o=[];for(let c=0;c<i;c++)for(let d=0;d<ju.numHeads;d++)o.push({layerIndex:c,headIndex:d,weights:X8(n,d+c*10),output:To(24,d*100+c*1e3)});const a=r.map((c,d)=>To(24,d*50)),l=Y8();return{tokens:r,qkv:s,attentionHeads:o,mlpOutput:a,probabilities:l,predictedToken:l[0].token}},J8=yf(Ru[0]),Z8=()=>{const[e,t]=Me.useState(Ru[0]),[n,r]=Me.useState(0),[s,i]=Me.useState(J8),[o,a]=Me.useState(null),[l,c]=Me.useState(0),[d,u]=Me.useState(0),[p,h]=Me.useState(null),[m,g]=Me.useState(null),[I,f]=Me.useState(.8),[_,T]=Me.useState(5),M=Me.useCallback(A=>{a(k=>k===A?null:A)},[]),v=Me.useCallback(()=>{const A=yf(e);i(A)},[e]),b=Me.useCallback(A=>{r(A);const k=Ru[A];t(k);const F=yf(k);i(F)},[]);return{inputText:e,setInputText:t,exampleIndex:n,setExampleIndex:r,modelOutput:s,expandedBlock:o,setExpandedBlock:a,toggleBlock:M,selectedHead:l,setSelectedHead:c,selectedLayer:d,setSelectedLayer:u,hoveredTokenIndex:p,setHoveredTokenIndex:h,hoveredMatrixCell:m,setHoveredMatrixCell:g,temperature:I,setTemperature:f,topK:_,setTopK:T,runModel:v,selectExample:b}},wP={gray:{base:"#9ca3af",light:"#d1d5db",dark:"#4b5563"},blue:{base:"#3b82f6",light:"#93c5fd",dark:"#1d4ed8"},red:{base:"#ef4444",light:"#fca5a5",dark:"#b91c1c"},green:{base:"#22c55e",light:"#86efac",dark:"#15803d"},purple:{base:"#8b5cf6",light:"#c4b5fd",dark:"#6d28d9"}},vP=({values:e,color:t,colorScale:n="gray",width:r,height:s=24,barWidth:i=3,gap:o=1,active:a=!0,vertical:l=!1,className:c=""})=>{const d=wP[n]||wP.gray,u=r||e.length*(i+o),p=h=>{if(n==="diverging"){const m=h;return m<.5?`rgba(239, 68, 68, ${.3+(.5-m)*1.4})`:`rgba(59, 130, 246, ${.3+(m-.5)*1.4})`}return t||d.base};return l?w.jsx("svg",{width:s,height:u,className:c,children:e.map((h,m)=>w.jsx(lt.rect,{x:0,y:m*(i+o),width:s*h,height:i,fill:p(h),opacity:a?.3+h*.7:.2,rx:1,initial:{scaleX:0},animate:{scaleX:1},transition:{delay:m*.01,duration:.2},style:{transformOrigin:"left"}},m))}):w.jsx("svg",{width:u,height:s,className:c,style:{overflow:"visible"},children:e.map((h,m)=>w.jsx(lt.rect,{x:m*(i+o),y:s*(1-h),width:i,height:s*h,fill:p(h),opacity:a?.3+h*.7:.2,rx:1,initial:{scaleY:0},animate:{scaleY:1},transition:{delay:m*.01,duration:.2},style:{transformOrigin:"bottom"}},m))})},jo=({color:e,height:t=24,width:n=12,active:r=!1,className:s=""})=>w.jsx("div",{className:`rounded-sm transition-all duration-200 ${s}`,style:{width:n,height:t,backgroundColor:e,opacity:r?1:.5}}),eH=({tokens:e,expanded:t,onToggle:n,hoveredIndex:r,onHover:s,language:i})=>{const a={zh:{title:"Embedding",tokenization:"分词",tokenEmbedding:"Token 嵌入",positionEncoding:"位置编码",combined:"组合嵌入",id:"ID",position:"位置",clickToExpand:"点击展开详情"},ja:{title:"Embedding",tokenization:"トークン化",tokenEmbedding:"トークン埋め込み",positionEncoding:"位置エンコーディング",combined:"組合せ埋め込み",id:"ID",position:"位置",clickToExpand:"クリックで詳細表示"}}[i];return w.jsxs("div",{className:`embedding-stage relative transition-all duration-300 ${t?"z-20":"z-10"}`,onClick:n,children:[w.jsxs("div",{className:`
          title text-sm font-medium mb-4 cursor-pointer
          flex items-center gap-2 transition-colors
          ${t?"text-gray-800":"text-gray-400 hover:text-gray-600"}
        `,children:[w.jsx("span",{children:a.title}),w.jsx(lt.svg,{className:"w-4 h-4",viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",animate:{rotate:t?90:0},children:w.jsx("path",{strokeLinecap:"round",strokeLinejoin:"round",strokeWidth:2,d:"M9 5l7 7-7 7"})})]}),w.jsxs("div",{className:"content flex gap-4",children:[w.jsxs("div",{className:"token-column flex flex-col gap-2",children:[w.jsx(vr,{children:t&&w.jsx(lt.div,{className:"text-xs text-gray-400 mb-1 text-right pr-2",initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},children:a.tokenization})}),e.map((l,c)=>w.jsx(lt.div,{className:`
                token-cell h-6 flex items-center justify-end pr-2
                cursor-pointer transition-colors rounded
                ${r===c?"bg-purple-50":"hover:bg-gray-50"}
              `,onMouseEnter:()=>s(c),onMouseLeave:()=>s(null),whileHover:{scale:1.02},children:w.jsx("span",{className:"text-sm text-gray-700 font-mono truncate max-w-20",children:l.text})},c))]}),w.jsx(vr,{children:t&&w.jsxs(w.Fragment,{children:[w.jsxs(lt.div,{className:"token-embedding-column flex flex-col gap-2",initial:{opacity:0,x:-20},animate:{opacity:1,x:0},exit:{opacity:0,x:-20},transition:{duration:.3},children:[w.jsx("div",{className:"text-xs text-gray-400 mb-1 text-center",children:a.tokenEmbedding}),e.map((l,c)=>w.jsxs("div",{className:`
                      h-6 flex items-center gap-2
                      ${r===c?"ring-2 ring-purple-300 rounded":""}
                    `,onMouseEnter:()=>s(c),onMouseLeave:()=>s(null),children:[w.jsxs("span",{className:"text-xs text-gray-400 w-8",children:[c===0&&w.jsx("span",{className:"text-gray-300",children:a.id}),c===0&&w.jsx("br",{}),l.id]}),w.jsx("div",{className:"flex items-center",children:w.jsx("svg",{className:"w-6 h-3 text-gray-300",viewBox:"0 0 24 12",children:w.jsx("path",{fill:"currentColor",d:"M0 6h20l-4-4v3H0v2h16v3l4-4H0z"})})}),w.jsx(vP,{values:l.embedding,colorScale:"gray",height:20,barWidth:2,gap:1,active:r===c})]},c))]}),w.jsxs(lt.div,{className:"plus-column flex flex-col gap-2 items-center justify-center",initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{delay:.1},children:[w.jsx("div",{className:"text-xs text-transparent mb-1",children:"+"}),e.map((l,c)=>w.jsx("div",{className:"h-6 flex items-center text-gray-400",children:"+"},c))]}),w.jsxs(lt.div,{className:"position-encoding-column flex flex-col gap-2",initial:{opacity:0,x:-20},animate:{opacity:1,x:0},exit:{opacity:0,x:-20},transition:{duration:.3,delay:.1},children:[w.jsx("div",{className:"text-xs text-gray-400 mb-1 text-center",children:a.positionEncoding}),e.map((l,c)=>w.jsxs("div",{className:`
                      h-6 flex items-center gap-2
                      ${r===c?"ring-2 ring-purple-300 rounded":""}
                    `,onMouseEnter:()=>s(c),onMouseLeave:()=>s(null),children:[w.jsx(vP,{values:l.positionEncoding,colorScale:"diverging",height:20,barWidth:2,gap:1,active:r===c}),w.jsxs("span",{className:"text-xs text-gray-400 w-8",children:[c===0&&w.jsx("span",{className:"text-gray-300",children:a.position}),c===0&&w.jsx("br",{}),c]})]},c))]}),w.jsxs(lt.div,{className:"equals-column flex flex-col gap-2 items-center justify-center",initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{delay:.2},children:[w.jsx("div",{className:"text-xs text-transparent mb-1",children:"="}),e.map((l,c)=>w.jsx("div",{className:"h-6 flex items-center text-gray-400",children:"="},c))]})]})}),w.jsxs("div",{className:"output-column flex flex-col gap-2",children:[w.jsx(vr,{children:t&&w.jsx(lt.div,{className:"text-xs text-gray-400 mb-1 text-center",initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},children:a.combined})}),e.map((l,c)=>w.jsx("div",{className:`
                h-6 flex items-center
                ${r===c?"ring-2 ring-purple-300 rounded":""}
              `,onMouseEnter:()=>s(c),onMouseLeave:()=>s(null),children:w.jsx(jo,{color:Ur.embedding,height:20,width:12,active:r===c||t})},c))]})]}),!t&&w.jsx("div",{className:"absolute inset-0 flex items-center justify-center opacity-0 hover:opacity-100 transition-opacity pointer-events-none",children:w.jsx("div",{className:"bg-black/70 text-white text-xs px-2 py-1 rounded",children:a.clickToExpand})})]})},tH=({tokens:e,qkv:t,hoveredIndex:n,onHover:r,language:s})=>w.jsx("div",{className:"qkv-stage relative",children:w.jsxs("div",{className:"flex gap-4",children:[w.jsx("div",{className:"input-column flex flex-col gap-2",children:e.map((i,o)=>w.jsx("div",{className:`
                h-6 flex items-center
                ${n===o?"ring-2 ring-purple-300 rounded":""}
              `,onMouseEnter:()=>r(o),onMouseLeave:()=>r(null),children:w.jsx(jo,{color:Ur.embedding,height:20,width:12,active:n===o})},o))}),w.jsx("div",{className:"qkv-output flex flex-col gap-2",children:t.map((i,o)=>w.jsxs(lt.div,{className:`
                h-6 flex items-center gap-0.5
                ${n===o?"ring-2 ring-purple-300 rounded px-1":""}
              `,onMouseEnter:()=>r(o),onMouseLeave:()=>r(null),whileHover:{scale:1.02},children:[w.jsxs("div",{className:"relative group",children:[w.jsx(jo,{color:Ur.query,height:20,width:12,active:n===o}),n===o&&w.jsx(lt.span,{className:"absolute -top-4 left-1/2 -translate-x-1/2 text-xs text-blue-500 font-medium whitespace-nowrap",initial:{opacity:0,y:5},animate:{opacity:1,y:0},children:"Q"})]}),w.jsxs("div",{className:"relative group",children:[w.jsx(jo,{color:Ur.key,height:20,width:12,active:n===o}),n===o&&w.jsx(lt.span,{className:"absolute -top-4 left-1/2 -translate-x-1/2 text-xs text-red-500 font-medium whitespace-nowrap",initial:{opacity:0,y:5},animate:{opacity:1,y:0},children:"K"})]}),w.jsxs("div",{className:"relative group",children:[w.jsx(jo,{color:Ur.value,height:20,width:12,active:n===o}),n===o&&w.jsx(lt.span,{className:"absolute -top-4 left-1/2 -translate-x-1/2 text-xs text-green-500 font-medium whitespace-nowrap",initial:{opacity:0,y:5},animate:{opacity:1,y:0},children:"V"})]})]},o))})]})}),nH=({matrix:e,tokens:t,hoveredCell:n,onCellHover:r,hoveredIndex:s,onHover:i,cellSize:o=24})=>{const a=t.length,l=c=>`rgba(139, 92, 246, ${.1+Math.min(c,1)*.9})`;return w.jsxs("div",{className:"attention-matrix",children:[w.jsx("div",{className:"flex ml-16 mb-1",children:t.map((c,d)=>w.jsx("div",{className:`
              text-xs text-center truncate cursor-pointer transition-colors
              ${n?.col===d||s===d?"text-purple-600 font-medium":"text-gray-400"}
            `,style:{width:o},onMouseEnter:()=>i(d),onMouseLeave:()=>i(null),children:c.text.slice(0,3)},d))}),w.jsxs("div",{className:"flex",children:[w.jsx("div",{className:"flex flex-col mr-1",children:t.map((c,d)=>w.jsx("div",{className:`
                w-14 text-xs text-right pr-1 truncate cursor-pointer transition-colors flex items-center justify-end
                ${n?.row===d||s===d?"text-purple-600 font-medium":"text-gray-400"}
              `,style:{height:o},onMouseEnter:()=>i(d),onMouseLeave:()=>i(null),children:c.text.slice(0,4)},d))}),w.jsx("div",{className:"grid gap-px bg-gray-100 p-px rounded",style:{gridTemplateColumns:`repeat(${a}, ${o}px)`,gridTemplateRows:`repeat(${a}, ${o}px)`},children:e.map((c,d)=>c.map((u,p)=>{const h=n?.row===d&&n?.col===p,m=n?.row===d||s===d,g=n?.col===p||s===p;return w.jsx(lt.div,{className:`
                    relative cursor-pointer transition-all
                    ${h?"ring-2 ring-purple-500 z-10":""}
                    ${m||g?"brightness-110":""}
                  `,style:{backgroundColor:l(u),width:o,height:o},onMouseEnter:()=>r({row:d,col:p}),onMouseLeave:()=>r(null),whileHover:{scale:1.1},children:h&&w.jsxs(lt.div,{className:"absolute -top-8 left-1/2 -translate-x-1/2 bg-gray-800 text-white text-xs px-2 py-1 rounded whitespace-nowrap z-20",initial:{opacity:0,y:5},animate:{opacity:1,y:0},children:[(u*100).toFixed(1),"%"]})},`${d}-${p}`)}))})]})]})},rH=({numHeads:e,numLayers:t,selectedHead:n,selectedLayer:r,onHeadChange:s,onLayerChange:i,language:o})=>{const l={zh:{layer:"层",head:"头"},ja:{layer:"レイヤー",head:"ヘッド"}}[o];return w.jsxs("div",{className:"head-selector flex flex-col gap-3 p-3 bg-gray-50 rounded-lg",children:[w.jsxs("div",{className:"flex items-center gap-2",children:[w.jsx("span",{className:"text-xs text-gray-500 w-16",children:l.layer}),w.jsxs("div",{className:"flex flex-wrap gap-1",children:[Array.from({length:Math.min(t,6)},(c,d)=>w.jsx("button",{className:`
                w-6 h-6 text-xs rounded transition-all
                ${r===d?"bg-purple-500 text-white":"bg-white text-gray-600 hover:bg-purple-100"}
              `,onClick:()=>i(d),children:d+1},d)),t>6&&w.jsx("span",{className:"text-xs text-gray-400 flex items-center",children:"..."})]})]}),w.jsxs("div",{className:"flex items-center gap-2",children:[w.jsx("span",{className:"text-xs text-gray-500 w-16",children:l.head}),w.jsxs("div",{className:"flex flex-wrap gap-1",children:[Array.from({length:Math.min(e,6)},(c,d)=>w.jsx("button",{className:`
                w-6 h-6 text-xs rounded transition-all
                ${n===d?"bg-blue-500 text-white":"bg-white text-gray-600 hover:bg-blue-100"}
              `,onClick:()=>s(d),children:d+1},d)),e>6&&w.jsx("span",{className:"text-xs text-gray-400 flex items-center",children:"..."})]})]})]})},sH=({selectedHead:e,selectedLayer:t,language:n})=>{const r={zh:["关注前一个词","关注句子开头","关注标点符号","关注相似词性","关注语义相关","全局注意力"],ja:["前の単語に注目","文の先頭に注目","句読点に注目","品詞が類似する語に注目","意味的に関連する語に注目","グローバルな注意"]},s=(e+t)%r[n].length;return w.jsx(lt.div,{className:"text-xs text-gray-500 mt-2",initial:{opacity:0},animate:{opacity:1},children:r[n][s]},`${e}-${t}`)},iH=({tokens:e,attentionHeads:t,selectedHead:n,selectedLayer:r,onHeadChange:s,onLayerChange:i,hoveredIndex:o,onHover:a,hoveredCell:l,onCellHover:c,expanded:d,onToggle:u,language:p})=>{const m={zh:{title:"Self-Attention",subtitle:"自注意力机制",clickToExpand:"点击展开详情",attentionWeights:"注意力权重"},ja:{title:"Self-Attention",subtitle:"自己注意機構",clickToExpand:"クリックで詳細表示",attentionWeights:"注意重み"}}[p],g=Me.useMemo(()=>t.find(f=>f.headIndex===n&&f.layerIndex===r)?.weights||[],[t,n,r]);return w.jsxs("div",{className:`attention-stage relative transition-all duration-300 ${d?"z-20":"z-10"}`,children:[w.jsxs("div",{className:`
          title text-sm font-medium mb-4 cursor-pointer
          flex items-center gap-2 transition-colors
          ${d?"text-gray-800":"text-gray-400 hover:text-gray-600"}
        `,onClick:u,children:[w.jsx("span",{children:m.title}),w.jsx(lt.svg,{className:"w-4 h-4",viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",animate:{rotate:d?90:0},children:w.jsx("path",{strokeLinecap:"round",strokeLinejoin:"round",strokeWidth:2,d:"M9 5l7 7-7 7"})})]}),w.jsx("div",{className:"content",children:w.jsx(vr,{children:d?w.jsxs(lt.div,{className:"expanded-content flex flex-col gap-4",initial:{opacity:0,height:0},animate:{opacity:1,height:"auto"},exit:{opacity:0,height:0},transition:{duration:.3},children:[w.jsx(rH,{numHeads:ju.numHeads,numLayers:ju.numLayers,selectedHead:n,selectedLayer:r,onHeadChange:s,onLayerChange:i,language:p}),w.jsxs("div",{className:"matrix-container",children:[w.jsx("div",{className:"text-xs text-gray-400 mb-2",children:m.attentionWeights}),g.length>0&&w.jsx(nH,{matrix:g,tokens:e,hoveredCell:l,onCellHover:c,hoveredIndex:o,onHover:a,cellSize:Math.min(24,200/e.length)})]}),w.jsx(sH,{selectedHead:n,selectedLayer:r,language:p})]}):w.jsx(lt.div,{className:"collapsed-content",initial:{opacity:0},animate:{opacity:1},children:w.jsx("div",{className:"mini-heatmap grid gap-px bg-gray-100 p-1 rounded cursor-pointer",style:{gridTemplateColumns:`repeat(${Math.min(e.length,8)}, 8px)`,gridTemplateRows:`repeat(${Math.min(e.length,8)}, 8px)`},onClick:u,children:g.slice(0,8).map((I,f)=>I.slice(0,8).map((_,T)=>w.jsx("div",{className:"rounded-sm",style:{backgroundColor:`rgba(139, 92, 246, ${.1+_*.9})`,width:8,height:8}},`mini-${f}-${T}`)))})})})}),!d&&w.jsx("div",{className:"absolute inset-0 flex items-center justify-center opacity-0 hover:opacity-100 transition-opacity pointer-events-none",children:w.jsx("div",{className:"bg-black/70 text-white text-xs px-2 py-1 rounded",children:m.clickToExpand})})]})},oH=({width:e=80,height:t=40})=>{const n=[];for(let r=0;r<=20;r++){const s=r/20*e,i=r/20*6-3,o=.5*i*(1+Math.tanh(Math.sqrt(2/Math.PI)*(i+.044715*Math.pow(i,3)))),a=t-(o+1.5)/5*t;n.push(`${s},${a}`)}return w.jsxs("svg",{width:e,height:t,className:"overflow-visible",children:[w.jsx("line",{x1:0,y1:t/2,x2:e,y2:t/2,stroke:"#e5e7eb",strokeWidth:1}),w.jsx("line",{x1:e/2,y1:0,x2:e/2,y2:t,stroke:"#e5e7eb",strokeWidth:1}),w.jsx(lt.polyline,{points:n.join(" "),fill:"none",stroke:Ur.mlp,strokeWidth:2,initial:{pathLength:0},animate:{pathLength:1},transition:{duration:.8}})]})},pm=({count:e,maxDisplay:t=8,color:n,label:r,highlighted:s=!1})=>{const i=Math.min(e,t),o=e>t;return w.jsxs("div",{className:"flex flex-col items-center gap-1",children:[r&&w.jsx("div",{className:"text-xs text-gray-400 mb-1",children:r}),w.jsxs("div",{className:"flex flex-col gap-0.5",children:[Array.from({length:i},(a,l)=>w.jsx(lt.div,{className:"rounded-full transition-all",style:{width:6,height:6,backgroundColor:n,opacity:s?1:.5},initial:{scale:0},animate:{scale:1},transition:{delay:l*.03}},l)),o&&w.jsx("div",{className:"text-gray-300 text-xs leading-none",children:"⋮"})]}),w.jsx("div",{className:"text-xs text-gray-400 mt-1",children:e})]})},MP=({leftCount:e,rightCount:t,highlighted:n=!1})=>{const r=Math.min(e,8),s=Math.min(t,8),i=r*6.5;return w.jsx("svg",{width:30,height:i,className:"overflow-visible",children:Array.from({length:r},(o,a)=>Array.from({length:Math.min(3,s)},(l,c)=>{const d=a*6.5+3,u=c*(i/Math.min(3,s))+3;return w.jsx(lt.line,{x1:0,y1:d,x2:30,y2:u,stroke:Ur.mlp,strokeWidth:.5,opacity:n?.4:.15,initial:{pathLength:0},animate:{pathLength:1},transition:{delay:(a+c)*.01,duration:.3}},`${a}-${c}`)}))})},aH=({tokens:e,mlpOutput:t,hoveredIndex:n,onHover:r,expanded:s,onToggle:i,language:o})=>{const l={zh:{title:"MLP",subtitle:"前馈网络",input:"输入",hidden:"隐藏层",output:"输出",gelu:"GELU",expansion:"4x 扩展",clickToExpand:"点击展开详情"},ja:{title:"MLP",subtitle:"フィードフォワード",input:"入力",hidden:"隠れ層",output:"出力",gelu:"GELU",expansion:"4x 拡張",clickToExpand:"クリックで詳細表示"}}[o];return w.jsxs("div",{className:`mlp-stage relative transition-all duration-300 ${s?"z-20":"z-10"}`,children:[w.jsxs("div",{className:`
          title text-sm font-medium mb-4 cursor-pointer
          flex items-center gap-2 transition-colors
          ${s?"text-gray-800":"text-gray-400 hover:text-gray-600"}
        `,onClick:i,children:[w.jsx("span",{children:l.title}),w.jsx(lt.svg,{className:"w-4 h-4",viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",animate:{rotate:s?90:0},children:w.jsx("path",{strokeLinecap:"round",strokeLinejoin:"round",strokeWidth:2,d:"M9 5l7 7-7 7"})})]}),w.jsx("div",{className:"content",children:w.jsx(vr,{children:s?w.jsxs(lt.div,{className:"expanded-content",initial:{opacity:0,height:0},animate:{opacity:1,height:"auto"},exit:{opacity:0,height:0},transition:{duration:.3},children:[w.jsxs("div",{className:"network-structure bg-gray-50 rounded-lg p-4 mb-4",children:[w.jsx("div",{className:"text-xs text-gray-500 mb-3",children:l.subtitle}),w.jsxs("div",{className:"flex items-center justify-center gap-2",children:[w.jsx(pm,{count:768,color:Ur.embedding,label:l.input,highlighted:n!==null}),w.jsx(MP,{leftCount:768,rightCount:3072,highlighted:n!==null}),w.jsxs("div",{className:"flex flex-col items-center",children:[w.jsx(pm,{count:3072,color:Ur.mlp,label:l.hidden,highlighted:n!==null}),w.jsxs("div",{className:"mt-2 flex flex-col items-center",children:[w.jsx("div",{className:"text-xs text-orange-500 mb-1",children:l.gelu}),w.jsx(oH,{})]}),w.jsx("div",{className:"text-xs text-gray-400 mt-1",children:l.expansion})]}),w.jsx(MP,{leftCount:3072,rightCount:768,highlighted:n!==null}),w.jsx(pm,{count:768,color:Ur.embedding,label:l.output,highlighted:n!==null})]})]}),w.jsx("div",{className:"token-outputs flex flex-col gap-2",children:e.map((c,d)=>w.jsxs("div",{className:`
                      flex items-center gap-2 p-1 rounded transition-all cursor-pointer
                      ${n===d?"bg-orange-50 ring-1 ring-orange-200":"hover:bg-gray-50"}
                    `,onMouseEnter:()=>r(d),onMouseLeave:()=>r(null),children:[w.jsx("span",{className:"text-xs text-gray-500 w-12 truncate",children:c.text}),w.jsx("div",{className:"flex-1 h-4 bg-gray-100 rounded overflow-hidden",children:w.jsx(lt.div,{className:"h-full rounded",style:{backgroundColor:Ur.mlp},initial:{width:0},animate:{width:`${(t[d]?.[0]||.5)*100}%`},transition:{delay:d*.05}})})]},d))})]}):w.jsx(lt.div,{className:"collapsed-content flex gap-2",initial:{opacity:0},animate:{opacity:1},children:e.map((c,d)=>w.jsx("div",{className:`
                    h-6 flex items-center
                    ${n===d?"ring-2 ring-orange-300 rounded":""}
                  `,onMouseEnter:()=>r(d),onMouseLeave:()=>r(null),children:w.jsx(jo,{color:Ur.mlp,height:20,width:12,active:n===d})},d))})})}),!s&&w.jsx("div",{className:"absolute inset-0 flex items-center justify-center opacity-0 hover:opacity-100 transition-opacity pointer-events-none",children:w.jsx("div",{className:"bg-black/70 text-white text-xs px-2 py-1 rounded",children:l.clickToExpand})})]})},lH=({item:e,maxProb:t,rank:n,isTopK:r,onHover:s})=>{const i=e.probability/t*100;return w.jsxs(lt.div,{className:`
        flex items-center gap-2 p-1 rounded cursor-pointer transition-all
        ${r?"bg-green-50":"bg-gray-50"}
      `,onMouseEnter:()=>s(e),onMouseLeave:()=>s(null),initial:{opacity:0,x:-20},animate:{opacity:1,x:0},transition:{delay:n*.05},whileHover:{scale:1.02},children:[w.jsx("span",{className:"text-xs text-gray-400 w-4",children:n+1}),w.jsx("span",{className:`
          text-sm font-mono w-20 truncate
          ${r?"text-green-700 font-medium":"text-gray-600"}
        `,children:e.token}),w.jsxs("div",{className:"flex-1 h-5 bg-gray-200 rounded overflow-hidden relative",children:[w.jsx(lt.div,{className:`h-full rounded ${r?"bg-green-500":"bg-gray-400"}`,initial:{width:0},animate:{width:`${i}%`},transition:{duration:.3,delay:n*.03}}),w.jsxs("span",{className:`
            absolute right-1 top-1/2 -translate-y-1/2 text-xs
            ${i>50?"text-white":"text-gray-600"}
          `,children:[(e.probability*100).toFixed(1),"%"]})]})]})},cH=({value:e,onChange:t,language:n})=>{const s={zh:{temperature:"温度",low:"确定性",high:"随机性"},ja:{temperature:"温度",low:"確定的",high:"ランダム"}}[n];return w.jsxs("div",{className:"temperature-control",children:[w.jsxs("div",{className:"flex items-center justify-between mb-1",children:[w.jsx("span",{className:"text-xs text-gray-500",children:s.temperature}),w.jsx("span",{className:"text-xs text-gray-700 font-medium",children:e.toFixed(2)})]}),w.jsxs("div",{className:"flex items-center gap-2",children:[w.jsx("span",{className:"text-xs text-blue-500",children:s.low}),w.jsx("input",{type:"range",min:"0.1",max:"2.0",step:"0.1",value:e,onChange:i=>t(parseFloat(i.target.value)),className:"flex-1 h-1 bg-gray-200 rounded-lg appearance-none cursor-pointer accent-purple-500"}),w.jsx("span",{className:"text-xs text-red-500",children:s.high})]})]})},uH=({value:e,onChange:t,language:n})=>{const s={zh:{topK:"Top-K",all:"全部"},ja:{topK:"Top-K",all:"すべて"}}[n],i=[1,3,5,10,20];return w.jsxs("div",{className:"topk-control",children:[w.jsx("div",{className:"text-xs text-gray-500 mb-1",children:s.topK}),w.jsx("div",{className:"flex gap-1",children:i.map(o=>w.jsx("button",{className:`
              px-2 py-1 text-xs rounded transition-all
              ${e===o?"bg-green-500 text-white":"bg-gray-100 text-gray-600 hover:bg-green-100"}
            `,onClick:()=>t(o),children:o},o))})]})},dH=({language:e})=>{const t={zh:{formula:"Softmax 公式"},ja:{formula:"Softmax 式"}};return w.jsxs("div",{className:"softmax-formula bg-gray-50 rounded p-2 text-center",children:[w.jsx("div",{className:"text-xs text-gray-400 mb-1",children:t[e].formula}),w.jsxs("div",{className:"font-mono text-sm text-gray-700",children:["P(i) = exp(z",w.jsx("sub",{children:"i"}),"/T) / Σ exp(z",w.jsx("sub",{children:"j"}),"/T)"]})]})},pH=({temperature:e})=>{const t=Me.useMemo(()=>{const r=[.6,.25,.1,.03,.02].map(i=>Math.pow(i,1/e)),s=r.reduce((i,o)=>i+o,0);return r.map(i=>i/s*100)},[e]);return w.jsx("div",{className:"flex items-end gap-1 h-12",children:t.map((n,r)=>w.jsx(lt.div,{className:"w-3 bg-purple-400 rounded-t",initial:{height:0},animate:{height:`${n}%`},transition:{duration:.2}},r))})},hH=({probabilities:e,temperature:t,topK:n,onTemperatureChange:r,onTopKChange:s,expanded:i,onToggle:o,language:a})=>{const c={zh:{title:"Output",subtitle:"输出概率",nextToken:"下一个词预测",clickToExpand:"点击展开详情",sampledToken:"采样结果"},ja:{title:"Output",subtitle:"出力確率",nextToken:"次の単語予測",clickToExpand:"クリックで詳細表示",sampledToken:"サンプル結果"}}[a],d=Me.useMemo(()=>{const g=e.map(f=>Math.log(f.probability+1e-10)).map(f=>Math.exp(f/t)),I=g.reduce((f,_)=>f+_,0);return e.map((f,_)=>({...f,probability:g[_]/I}))},[e,t]),u=Me.useMemo(()=>[...d].sort((m,g)=>g.probability-m.probability),[d]),p=u[0]?.probability||1,[,h]=TP.useState(null);return w.jsxs("div",{className:`output-stage relative transition-all duration-300 ${i?"z-20":"z-10"}`,children:[w.jsxs("div",{className:`
          title text-sm font-medium mb-4 cursor-pointer
          flex items-center gap-2 transition-colors
          ${i?"text-gray-800":"text-gray-400 hover:text-gray-600"}
        `,onClick:o,children:[w.jsx("span",{children:c.title}),w.jsx(lt.svg,{className:"w-4 h-4",viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",animate:{rotate:i?90:0},children:w.jsx("path",{strokeLinecap:"round",strokeLinejoin:"round",strokeWidth:2,d:"M9 5l7 7-7 7"})})]}),w.jsx("div",{className:"content",children:w.jsx(vr,{children:i?w.jsxs(lt.div,{className:"expanded-content flex flex-col gap-4",initial:{opacity:0,height:0},animate:{opacity:1,height:"auto"},exit:{opacity:0,height:0},transition:{duration:.3},children:[w.jsxs("div",{className:"controls grid grid-cols-2 gap-4 p-3 bg-gray-50 rounded-lg",children:[w.jsxs("div",{children:[w.jsx(cH,{value:t,onChange:r,language:a}),w.jsx("div",{className:"mt-2",children:w.jsx(pH,{temperature:t})})]}),w.jsxs("div",{children:[w.jsx(uH,{value:n,onChange:s,language:a}),w.jsx("div",{className:"mt-3",children:w.jsx(dH,{language:a})})]})]}),w.jsxs("div",{className:"probability-distribution",children:[w.jsx("div",{className:"text-xs text-gray-500 mb-2",children:c.nextToken}),w.jsx("div",{className:"flex flex-col gap-1 max-h-60 overflow-y-auto",children:u.slice(0,20).map((m,g)=>w.jsx(lH,{item:m,maxProb:p,rank:g,isTopK:g<n,onHover:h},m.token))})]}),u.length>0&&w.jsxs("div",{className:"sampled-result bg-green-50 rounded-lg p-3",children:[w.jsx("div",{className:"text-xs text-gray-500 mb-1",children:c.sampledToken}),w.jsxs(lt.div,{className:"text-lg font-mono text-green-700 font-bold",initial:{scale:.8,opacity:0},animate:{scale:1,opacity:1},children:['"',u[0].token,'"']},u[0].token),w.jsxs("div",{className:"text-xs text-gray-500 mt-1",children:[(u[0].probability*100).toFixed(1),"%"]})]})]}):w.jsxs(lt.div,{className:"collapsed-content",initial:{opacity:0},animate:{opacity:1},children:[w.jsx("div",{className:"flex items-end gap-0.5 h-8",children:u.slice(0,10).map((m,g)=>w.jsx(lt.div,{className:`w-2 rounded-t ${g<n?"bg-green-500":"bg-gray-300"}`,style:{height:`${m.probability*100}%`},initial:{height:0},animate:{height:`${m.probability*100}%`},transition:{delay:g*.03}},m.token))}),w.jsxs("div",{className:"text-xs text-green-600 font-mono mt-1 truncate",children:["→ ",u[0]?.token]})]})})}),!i&&w.jsx("div",{className:"absolute inset-0 flex items-center justify-center opacity-0 hover:opacity-100 transition-opacity pointer-events-none",children:w.jsx("div",{className:"bg-black/70 text-white text-xs px-2 py-1 rounded",children:c.clickToExpand})})]})},mH=({exampleIndex:e,onExampleSelect:t,onRun:n})=>w.jsxs("div",{className:"flex items-center gap-3 mb-4 flex-wrap",children:[w.jsx("span",{className:"text-sm text-gray-500",children:"输入:"}),w.jsx("div",{className:"flex gap-2 flex-wrap",children:Ru.map((r,s)=>w.jsxs("button",{onClick:()=>t(s),className:`
              px-3 py-1.5 text-sm rounded-lg transition-all
              ${e===s?"bg-purple-600 text-white":"bg-gray-100 text-gray-600 hover:bg-purple-100"}
            `,children:[r.slice(0,15),"..."]},s))}),w.jsx("button",{onClick:n,className:"px-4 py-1.5 bg-purple-600 text-white text-sm rounded-lg hover:bg-purple-700 transition-colors ml-auto",children:"刷新"})]}),Va=({children:e,expanded:t=!1})=>w.jsx(lt.div,{className:`
        stage-card bg-white rounded-lg shadow-sm p-3 min-w-[160px]
        transition-all duration-300
        ${t?"ring-2 ring-purple-300":""}
      `,layout:!0,children:e}),qc=()=>w.jsx("div",{className:"flex items-center px-1",children:w.jsx("svg",{className:"w-6 h-6 text-gray-300",viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",children:w.jsx("path",{strokeLinecap:"round",strokeLinejoin:"round",strokeWidth:2,d:"M9 5l7 7-7 7"})})}),fH=({language:e="zh",showInput:t=!0,compact:n=!1})=>{const r=Z8();return Me.useEffect(()=>{r.runModel()},[]),w.jsxs("div",{className:`transformer-explainer-v2 ${n?"p-2":"p-4"}`,children:[t&&w.jsx(mH,{exampleIndex:r.exampleIndex,onExampleSelect:r.selectExample,onRun:r.runModel}),w.jsx("div",{className:`pipeline bg-gray-50 rounded-xl ${n?"p-3":"p-4"}`,children:w.jsxs("div",{className:"flex items-start gap-2 overflow-x-auto pb-2",children:[w.jsx(Va,{expanded:r.expandedBlock==="embedding",children:w.jsx(eH,{tokens:r.modelOutput.tokens,expanded:r.expandedBlock==="embedding",onToggle:()=>r.toggleBlock("embedding"),hoveredIndex:r.hoveredTokenIndex,onHover:r.setHoveredTokenIndex,language:e})}),w.jsx(qc,{}),w.jsx(Va,{expanded:r.expandedBlock==="qkv",children:w.jsx(tH,{tokens:r.modelOutput.tokens,qkv:r.modelOutput.qkv,hoveredIndex:r.hoveredTokenIndex,onHover:r.setHoveredTokenIndex,language:e})}),w.jsx(qc,{}),w.jsx(Va,{expanded:r.expandedBlock==="attention",children:w.jsx(iH,{tokens:r.modelOutput.tokens,attentionHeads:r.modelOutput.attentionHeads,selectedHead:r.selectedHead,selectedLayer:r.selectedLayer,onHeadChange:r.setSelectedHead,onLayerChange:r.setSelectedLayer,hoveredIndex:r.hoveredTokenIndex,onHover:r.setHoveredTokenIndex,hoveredCell:r.hoveredMatrixCell,onCellHover:r.setHoveredMatrixCell,expanded:r.expandedBlock==="attention",onToggle:()=>r.toggleBlock("attention"),language:e})}),w.jsx(qc,{}),w.jsx(Va,{expanded:r.expandedBlock==="mlp",children:w.jsx(aH,{tokens:r.modelOutput.tokens,mlpOutput:r.modelOutput.mlpOutput,hoveredIndex:r.hoveredTokenIndex,onHover:r.setHoveredTokenIndex,expanded:r.expandedBlock==="mlp",onToggle:()=>r.toggleBlock("mlp"),language:e})}),w.jsx(qc,{}),w.jsx(Va,{expanded:r.expandedBlock==="output",children:w.jsx(hH,{probabilities:r.modelOutput.probabilities,temperature:r.temperature,topK:r.topK,onTemperatureChange:r.setTemperature,onTopKChange:r.setTopK,expanded:r.expandedBlock==="output",onToggle:()=>r.toggleBlock("output"),language:e})})]})})]})},gH={zh:{title:"GPT-2 实时推理可视化",subtitle:"在浏览器中运行真实的 GPT-2 模型",loadModel:"加载模型",loadingModel:"正在加载模型...",modelReady:"模型已就绪",modelError:"模型加载失败",downloadProgress:"下载进度",initializingModel:"初始化模型...",loadingTokenizer:"加载分词器...",loadingWeights:"加载模型权重...",inputPlaceholder:"输入文本（如：The quick brown fox）",generate:"生成",generating:"生成中...",reset:"重置",temperature:"温度",temperatureDesc:"控制随机性（低=确定，高=多样）",topK:"Top-K",topKDesc:"从概率最高的 K 个词中选择",topP:"Top-P",topPDesc:"累积概率阈值",stage1:"步骤 1：分词",stage1Desc:"将文本转换为 Token ID",stage2:"步骤 2：嵌入",stage2Desc:"Token 嵌入 + 位置编码",stage3:"步骤 3：自注意力",stage3Desc:"计算 Token 之间的关系",stage4:"步骤 4：前馈网络",stage4Desc:"MLP 非线性变换",stage5:"步骤 5：输出预测",stage5Desc:"生成下一个 Token 的概率",layer:"层",head:"头",selectLayer:"选择层",selectHead:"选择注意力头",attentionPattern:"注意力模式",showAllHeads:"显示所有头",tokenEmbedding:"Token 嵌入",positionEncoding:"位置编码",combinedEmbedding:"合并嵌入",dimension:"维度",nextTokenPrediction:"下一个 Token 预测",probability:"概率",logit:"Logit",rank:"排名",modelInfo:"模型信息",parameters:"参数量",layers:"层数",heads:"注意力头",embeddingDim:"嵌入维度",vocabSize:"词表大小",ready:"就绪",processing:"处理中",complete:"完成",error:"错误",tipHover:"悬停查看详情",tipClick:"点击选中",tipScroll:"滚动缩放"},ja:{title:"GPT-2 リアルタイム推論可視化",subtitle:"ブラウザで実際のGPT-2モデルを実行",loadModel:"モデルを読み込む",loadingModel:"モデルを読み込み中...",modelReady:"モデル準備完了",modelError:"モデルの読み込みに失敗",downloadProgress:"ダウンロード進捗",initializingModel:"モデルを初期化中...",loadingTokenizer:"トークナイザーを読み込み中...",loadingWeights:"モデル重みを読み込み中...",inputPlaceholder:"テキストを入力（例：The quick brown fox）",generate:"生成",generating:"生成中...",reset:"リセット",temperature:"温度",temperatureDesc:"ランダム性を制御（低=確定的、高=多様）",topK:"Top-K",topKDesc:"確率上位K個から選択",topP:"Top-P",topPDesc:"累積確率閾値",stage1:"ステップ1：トークン化",stage1Desc:"テキストをToken IDに変換",stage2:"ステップ2：埋め込み",stage2Desc:"Token埋め込み + 位置エンコーディング",stage3:"ステップ3：自己注意",stage3Desc:"Token間の関係を計算",stage4:"ステップ4：フィードフォワード",stage4Desc:"MLP非線形変換",stage5:"ステップ5：出力予測",stage5Desc:"次のTokenの確率を生成",layer:"層",head:"ヘッド",selectLayer:"層を選択",selectHead:"アテンションヘッドを選択",attentionPattern:"アテンションパターン",showAllHeads:"全ヘッドを表示",tokenEmbedding:"Token埋め込み",positionEncoding:"位置エンコーディング",combinedEmbedding:"結合埋め込み",dimension:"次元",nextTokenPrediction:"次のToken予測",probability:"確率",logit:"Logit",rank:"ランク",modelInfo:"モデル情報",parameters:"パラメータ数",layers:"層数",heads:"アテンションヘッド",embeddingDim:"埋め込み次元",vocabSize:"語彙サイズ",ready:"準備完了",processing:"処理中",complete:"完了",error:"エラー",tipHover:"ホバーで詳細表示",tipClick:"クリックで選択",tipScroll:"スクロールでズーム"}},_H=({language:e="zh"})=>{const t=gH[e],{loadingState:n,isLoaded:r,isLoading:s,loadModel:i,isInferring:o,currentStage:a,result:l,error:c,runInference:d,settings:u,updateSettings:p}=$V(),[h,m]=Me.useState("The quick brown fox"),[g,I]=Me.useState(""),f=Me.useCallback(async()=>{if(!h.trim())return;const v=await d(h);v&&I(h+v.generatedToken)},[h,d]),_=Me.useCallback(async()=>{await i()},[i]),T=Me.useCallback(()=>{m("The quick brown fox"),I("")},[]),M=l?.attention?.layers?.[u.selectedLayer]?.[u.selectedHead]??null;return w.jsxs("div",{className:"bg-gray-900 rounded-2xl overflow-hidden",children:[w.jsxs("div",{className:"bg-gradient-to-r from-blue-600/20 via-purple-600/20 to-green-600/20 border-b border-gray-800 p-6",children:[w.jsx("h2",{className:"text-2xl font-bold text-white mb-2",children:t.title}),w.jsx("p",{className:"text-gray-400",children:t.subtitle})]}),!r&&w.jsxs("div",{className:"p-6 border-b border-gray-800",children:[w.jsxs("div",{className:"flex items-center gap-4",children:[w.jsx("button",{onClick:_,disabled:s,className:`
                px-6 py-3 rounded-lg font-medium transition-all
                ${s?"bg-gray-700 text-gray-400 cursor-not-allowed":"bg-blue-600 hover:bg-blue-500 text-white"}
              `,children:s?t.loadingModel:t.loadModel}),s&&w.jsxs("div",{className:"flex-1",children:[w.jsxs("div",{className:"flex items-center gap-2 mb-1",children:[w.jsx("div",{className:"text-sm text-gray-400",children:n.statusText}),w.jsxs("div",{className:"text-sm text-blue-400",children:[Math.round(n.progress),"%"]})]}),w.jsx("div",{className:"h-2 bg-gray-800 rounded-full overflow-hidden",children:w.jsx(lt.div,{className:"h-full bg-gradient-to-r from-blue-500 to-purple-500",initial:{width:0},animate:{width:`${n.progress}%`}})})]})]}),n.error&&w.jsxs("div",{className:"mt-4 p-3 bg-red-500/10 border border-red-500/30 rounded-lg text-red-400 text-sm",children:[t.modelError,": ",n.error]})]}),r&&w.jsxs("div",{className:"px-6 py-3 border-b border-gray-800 flex items-center gap-4 text-sm",children:[w.jsxs("span",{className:"flex items-center gap-2 text-green-400",children:[w.jsx("span",{className:"w-2 h-2 rounded-full bg-green-500 animate-pulse"}),t.modelReady]}),w.jsxs("span",{className:"text-gray-500",children:["GPT-2 Small • ",Ni.nLayer," ",t.layers," • ",Ni.nHead," ",t.heads]})]}),w.jsxs("div",{className:"p-6",children:[w.jsxs("div",{className:"mb-6",children:[w.jsxs("div",{className:"flex gap-4 mb-4",children:[w.jsx("input",{type:"text",value:h,onChange:v=>m(v.target.value),placeholder:t.inputPlaceholder,disabled:!r||o,className:`flex-1 px-4 py-3 bg-gray-800 border border-gray-700 rounded-lg
                         text-white placeholder-gray-500 focus:border-blue-500 focus:outline-none
                         disabled:opacity-50 disabled:cursor-not-allowed`}),w.jsx("button",{onClick:f,disabled:!r||o||!h.trim(),className:`
                px-6 py-3 rounded-lg font-medium transition-all
                ${!r||o||!h.trim()?"bg-gray-700 text-gray-400 cursor-not-allowed":"bg-green-600 hover:bg-green-500 text-white"}
              `,children:o?t.generating:t.generate}),w.jsx("button",{onClick:T,className:"px-4 py-3 bg-gray-700 hover:bg-gray-600 text-gray-300 rounded-lg transition-all",children:t.reset})]}),w.jsxs("div",{className:"flex flex-wrap gap-6",children:[w.jsxs("div",{className:"flex items-center gap-2",children:[w.jsxs("label",{className:"text-sm text-gray-400",children:[t.temperature,":"]}),w.jsx("input",{type:"range",min:"0.1",max:"2",step:"0.1",value:u.temperature,onChange:v=>p({temperature:parseFloat(v.target.value)}),className:"w-24 accent-blue-500"}),w.jsx("span",{className:"text-sm text-white w-8",children:u.temperature.toFixed(1)})]}),w.jsxs("div",{className:"flex items-center gap-2",children:[w.jsxs("label",{className:"text-sm text-gray-400",children:[t.topK,":"]}),w.jsx("input",{type:"range",min:"1",max:"100",step:"1",value:u.topK,onChange:v=>p({topK:parseInt(v.target.value)}),className:"w-24 accent-blue-500"}),w.jsx("span",{className:"text-sm text-white w-8",children:u.topK})]}),w.jsxs("div",{className:"flex items-center gap-2",children:[w.jsxs("label",{className:"text-sm text-gray-400",children:[t.topP,":"]}),w.jsx("input",{type:"range",min:"0.1",max:"1",step:"0.05",value:u.topP,onChange:v=>p({topP:parseFloat(v.target.value)}),className:"w-24 accent-blue-500"}),w.jsx("span",{className:"text-sm text-white w-8",children:u.topP.toFixed(2)})]})]})]}),g&&w.jsxs("div",{className:"mb-6 p-4 bg-gray-800/50 border border-gray-700 rounded-lg",children:[w.jsx("div",{className:"text-xs text-gray-500 mb-2",children:"Generated:"}),w.jsxs("div",{className:"text-lg text-white font-mono",children:[g,w.jsx("span",{className:"animate-pulse",children:"|"})]})]}),c&&w.jsx("div",{className:"mb-6 p-4 bg-red-500/10 border border-red-500/30 rounded-lg",children:w.jsxs("div",{className:"text-red-400",children:[t.error,": ",c]})}),w.jsx(vr,{children:r&&w.jsx(lt.div,{initial:{opacity:0,y:20},animate:{opacity:1,y:0},className:"mb-6",children:w.jsx(R8,{tokens:l?.tokens??[],predictions:l?.predictions??[],attentionHead:M,currentStage:a,selectedLayer:u.selectedLayer,selectedHead:u.selectedHead,onLayerChange:v=>p({selectedLayer:v}),onHeadChange:v=>p({selectedHead:v}),isActive:a!=="idle",i18n:t})})}),l&&w.jsxs("details",{className:"group",children:[w.jsxs("summary",{className:"cursor-pointer text-gray-400 hover:text-white transition-colors mb-4 flex items-center gap-2",children:[w.jsx("span",{className:"text-sm",children:"详细视图 / Detailed Views"}),w.jsx("svg",{className:"w-4 h-4 transition-transform group-open:rotate-180",fill:"none",viewBox:"0 0 24 24",stroke:"currentColor",children:w.jsx("path",{strokeLinecap:"round",strokeLinejoin:"round",strokeWidth:2,d:"M19 9l-7 7-7-7"})})]}),w.jsxs(lt.div,{initial:{opacity:0,height:0},animate:{opacity:1,height:"auto"},className:"grid grid-cols-1 lg:grid-cols-2 gap-6",children:[w.jsx(jV,{tokens:l?.tokens??[],inputText:h,isActive:a==="tokenizing"||a==="complete",i18n:t}),w.jsx(O8,{attentionHead:M,tokens:l?.tokens??[],selectedLayer:u.selectedLayer,selectedHead:u.selectedHead,onLayerChange:v=>p({selectedLayer:v}),onHeadChange:v=>p({selectedHead:v}),isActive:a==="attention"||a==="complete",i18n:t}),w.jsx("div",{className:"lg:col-span-2",children:w.jsx(RV,{predictions:l?.predictions??[],generatedToken:l?.generatedToken??"",isActive:a==="output"||a==="complete",i18n:t})})]})]}),o&&w.jsxs("div",{className:"mt-6 flex items-center justify-center gap-2",children:[w.jsx("div",{className:"animate-spin w-5 h-5 border-2 border-blue-500 border-t-transparent rounded-full"}),w.jsxs("span",{className:"text-gray-400",children:[a==="tokenizing"&&t.stage1,a==="embedding"&&t.stage2,a==="attention"&&t.stage3,a==="mlp"&&t.stage4,a==="output"&&t.stage5]})]})]}),w.jsx("div",{className:"px-6 py-4 border-t border-gray-800 bg-gray-900/50",children:w.jsxs("div",{className:"flex justify-between text-xs text-gray-500",children:[w.jsxs("div",{children:[t.modelInfo,": GPT-2 Small (",t.parameters,": 124M)"]}),w.jsxs("div",{children:[t.embeddingDim,": ",Ni.nEmbd," • ",t.vocabSize,": ",Ni.vocabSize.toLocaleString()]})]})})]})},yH=({content:e,language:t,markdownComponents:n})=>{const r=/(::transformer-viz::|::transformer-v2-viz::|::gpt2-viz::|::rag-viz::|::agent-viz::|::attention-viz::)/,s=e.split(r);return s.length===1?w.jsx(Kw,{remarkPlugins:[t1],components:n,children:e}):w.jsx(w.Fragment,{children:s.map((i,o)=>i==="::transformer-viz::"?w.jsx("div",{className:"my-8",children:w.jsx(N8,{language:t})},`marker-${o}`):i==="::transformer-v2-viz::"?w.jsx("div",{className:"my-8",children:w.jsx(fH,{language:t})},`marker-${o}`):i==="::gpt2-viz::"?w.jsx("div",{className:"my-8",children:w.jsx(_H,{language:t})},`marker-${o}`):i==="::rag-viz::"?w.jsx("div",{className:"my-8",children:w.jsx(z8,{language:t})},`marker-${o}`):i==="::agent-viz::"?w.jsx("div",{className:"my-8",children:w.jsx(V8,{language:t})},`marker-${o}`):i==="::attention-viz::"?w.jsx("div",{className:"my-8",children:w.jsx(K8,{language:t})},`marker-${o}`):i&&i.trim()?w.jsx(Kw,{remarkPlugins:[t1],components:n,children:i},`content-${o}`):null)})},xH={beginner:PP,advanced:RL},CH=()=>{const e=IL(),{bookId:t}=kL(),{language:n}=EL(),r=n,s=Me.useMemo(()=>xH[t||"beginner"]||PP,[t]),i=Me.useMemo(()=>OL(s),[s]),o=Me.useMemo(()=>jL(s),[s]),[a,l]=Me.useState(0),[c,d]=Me.useState(new Set),[u,p]=Me.useState(!0),[h,m]=Me.useState(new Set(["chapter-1"])),g=`ai-book-progress-${s.id}`;Me.useEffect(()=>{const b=localStorage.getItem(g);if(b){const A=JSON.parse(b);l(A.currentIndex||0),d(new Set(A.readSections||[]))}else l(0),d(new Set)},[g]),Me.useEffect(()=>{localStorage.setItem(g,JSON.stringify({currentIndex:a,readSections:Array.from(c)}))},[a,c,g]),Me.useEffect(()=>{const b=i[a];b&&(d(A=>new Set([...A,b.section.id])),m(A=>new Set([...A,b.chapter.id])))},[a,i]);const I=i[a],f=Math.round(c.size/o*100),_=b=>{l(b),window.scrollTo({top:0,behavior:"smooth"})},T=()=>{a<i.length-1&&_(a+1)},M=()=>{a>0&&_(a-1)},v=b=>{m(A=>{const k=new Set(A);return k.has(b)?k.delete(b):k.add(b),k})};return Me.useEffect(()=>{const b=A=>{A.key==="ArrowRight"||A.key==="ArrowDown"?a<i.length-1&&(l(k=>k+1),window.scrollTo({top:0,behavior:"smooth"})):(A.key==="ArrowLeft"||A.key==="ArrowUp")&&a>0&&(l(k=>k-1),window.scrollTo({top:0,behavior:"smooth"}))};return window.addEventListener("keydown",b),()=>window.removeEventListener("keydown",b)},[a,i.length]),w.jsxs("div",{className:"min-h-screen bg-slate-50",children:[w.jsx("header",{className:"fixed top-0 left-0 right-0 z-50 bg-white border-b border-slate-200 shadow-sm",children:w.jsxs("div",{className:"flex items-center justify-between h-14 px-4",children:[w.jsxs("div",{className:"flex items-center gap-4",children:[w.jsx("button",{onClick:()=>p(!u),className:"p-2 hover:bg-slate-100 rounded-lg transition-colors lg:hidden",children:u?w.jsx(LL,{size:20}):w.jsx(DL,{size:20})}),w.jsxs("button",{onClick:()=>e("/"),className:"flex items-center gap-2 text-slate-600 hover:text-slate-900 transition-colors",children:[w.jsx($L,{size:20}),w.jsx("span",{className:"text-base font-medium hidden sm:inline",children:r==="ja"?"ホーム":"首页"})]})]}),w.jsxs("div",{className:"flex items-center gap-2",children:[w.jsx(lw,{size:20,className:"text-amber-600"}),w.jsx("span",{className:"font-serif text-lg text-slate-800 hidden sm:inline",children:s.title[r]})]}),w.jsx("div",{className:"flex items-center gap-4",children:w.jsxs("div",{className:"flex items-center gap-2",children:[w.jsx("div",{className:"w-24 h-2 bg-slate-200 rounded-full overflow-hidden",children:w.jsx("div",{className:"h-full bg-amber-500 transition-all duration-500",style:{width:`${f}%`}})}),w.jsxs("span",{className:"text-sm text-slate-500",children:[f,"%"]})]})})]})}),w.jsxs("div",{className:"flex pt-14",children:[w.jsx("aside",{className:`fixed lg:sticky top-14 left-0 h-[calc(100vh-56px)] bg-white border-r border-slate-200 transition-all duration-300 z-40 overflow-hidden ${u?"w-80":"w-0 lg:w-80"}`,children:w.jsxs("div",{className:"w-80 h-full overflow-y-auto",children:[w.jsxs("div",{className:"p-4 border-b border-slate-100",children:[w.jsx("h2",{className:"font-serif text-2xl font-semibold text-slate-800",children:s.title[r]}),w.jsx("p",{className:"text-base text-slate-500 mt-1",children:s.subtitle[r]})]}),w.jsx("nav",{className:"p-2",children:s.chapters.map(b=>{const A=h.has(b.id),F=i.filter(G=>G.chapter.id===b.id).every(G=>c.has(G.section.id)),L=i.findIndex(G=>G.chapter.id===b.id);return w.jsxs("div",{className:"mb-1",children:[w.jsxs("button",{onClick:()=>v(b.id),className:"w-full flex items-center gap-2 p-3 rounded-lg hover:bg-slate-50 transition-colors text-left",children:[w.jsx("span",{className:`transition-transform ${A?"rotate-90":""}`,children:w.jsx(cw,{size:16,className:"text-slate-400"})}),w.jsxs("div",{className:"flex-1 min-w-0",children:[w.jsxs("div",{className:"flex items-center gap-2",children:[w.jsx("span",{className:"text-amber-600 font-medium text-base",children:r==="ja"?`第${b.number}章`:`第${b.number}章`}),F&&w.jsx(uw,{size:14,className:"text-green-500"})]}),w.jsx("div",{className:"text-slate-800 font-medium text-lg truncate",children:b.title[r]})]})]}),A&&w.jsx("div",{className:"ml-4 border-l-2 border-slate-100 pl-2",children:b.sections.map((G,j)=>{const R=L+j,K=R===a,U=c.has(G.id);return w.jsx("button",{onClick:()=>{_(R),window.innerWidth<1024&&p(!1)},className:`w-full text-left p-2 rounded-lg transition-colors text-base ${K?"bg-amber-50 text-amber-800 font-medium":"text-slate-600 hover:bg-slate-50"}`,children:w.jsxs("div",{className:"flex items-center gap-2",children:[U&&!K?w.jsx(uw,{size:12,className:"text-green-500 flex-shrink-0"}):w.jsx("span",{className:`w-3 h-3 rounded-full flex-shrink-0 ${K?"bg-amber-500":"bg-slate-200"}`}),w.jsx("span",{className:"truncate",children:G.title[r]})]})},G.id)})})]},b.id)})})]})}),u&&w.jsx("div",{className:"fixed inset-0 bg-black/20 z-30 lg:hidden",onClick:()=>p(!1)}),w.jsx("main",{className:"flex-1 min-w-0",children:w.jsx("div",{className:"w-full px-6 sm:px-10 lg:px-16 py-8",children:I&&w.jsxs(w.Fragment,{children:[w.jsxs("div",{className:"mb-8",children:[w.jsxs("div",{className:"flex items-center gap-2 text-amber-600 text-sm font-medium mb-2",children:[w.jsx("span",{children:r==="ja"?`第${I.chapter.number}章`:`第${I.chapter.number}章`}),w.jsx(cw,{size:14}),w.jsx("span",{children:I.chapter.title[r]})]}),w.jsx("h1",{className:"font-serif text-4xl sm:text-5xl font-bold text-slate-900 leading-tight",children:I.section.title[r]})]}),w.jsx("article",{className:"prose prose-slate prose-xl max-w-none",children:w.jsx(yH,{content:I.section.content[r],language:r,markdownComponents:{h1:({children:b})=>w.jsx("h1",{className:"font-serif text-4xl font-bold text-slate-900 mt-12 mb-6",children:b}),h2:({children:b})=>w.jsx("h2",{className:"font-serif text-3xl font-bold text-slate-800 mt-10 mb-4 pb-2 border-b border-slate-200",children:b}),h3:({children:b})=>w.jsx("h3",{className:"font-serif text-2xl font-semibold text-slate-800 mt-8 mb-3",children:b}),h4:({children:b})=>w.jsx("h4",{className:"font-serif text-xl font-semibold text-slate-700 mt-6 mb-2",children:b}),p:({children:b})=>w.jsx("p",{className:"text-slate-700 leading-relaxed mb-5 text-lg sm:text-xl",children:b}),ul:({children:b})=>w.jsx("ul",{className:"space-y-3 my-5 text-slate-700 text-lg sm:text-xl",children:b}),ol:({children:b})=>w.jsx("ol",{className:"space-y-3 my-5 text-slate-700 text-lg sm:text-xl list-decimal list-inside",children:b}),li:({children:b})=>w.jsx("li",{className:"text-slate-700 leading-relaxed text-lg sm:text-xl",children:b}),strong:({children:b})=>w.jsx("strong",{className:"font-semibold text-slate-900",children:b}),em:({children:b})=>w.jsx("em",{className:"italic text-slate-600",children:b}),blockquote:({children:b})=>w.jsx("blockquote",{className:"border-l-4 border-amber-400 pl-6 py-3 my-6 bg-amber-50 rounded-r-lg text-slate-700 italic text-lg sm:text-xl",children:b}),hr:()=>w.jsx("hr",{className:"my-8 border-slate-200"}),table:({children:b})=>w.jsx("div",{className:"overflow-x-auto my-6 rounded-lg border border-slate-200",children:w.jsx("table",{className:"w-full border-collapse",children:b})}),thead:({children:b})=>w.jsx("thead",{className:"bg-slate-100",children:b}),th:({children:b})=>w.jsx("th",{className:"px-5 py-4 text-left text-slate-800 font-semibold border-b border-slate-200 text-lg",children:b}),td:({children:b})=>w.jsx("td",{className:"px-5 py-4 text-slate-700 border-b border-slate-100 text-lg",children:b}),code:({className:b,children:A})=>b?w.jsx("pre",{className:"bg-slate-900 text-slate-100 rounded-lg p-4 overflow-x-auto my-4",children:w.jsx("code",{className:"text-sm font-mono",children:A})}):w.jsx("code",{className:"bg-slate-100 text-amber-700 px-1.5 py-0.5 rounded text-sm font-mono",children:A}),img:({alt:b})=>w.jsxs("figure",{className:"my-8",children:[w.jsx("div",{className:"bg-slate-100 rounded-lg p-4 flex items-center justify-center min-h-[200px]",children:w.jsxs("div",{className:"text-center text-slate-500",children:[w.jsx(lw,{size:48,className:"mx-auto mb-2 text-slate-300"}),w.jsx("p",{className:"text-sm",children:b||"图片"})]})}),b&&w.jsx("figcaption",{className:"text-center text-sm text-slate-500 mt-2",children:b})]}),a:({href:b,children:A})=>w.jsx("a",{href:b,className:"text-amber-600 hover:text-amber-700 underline decoration-amber-300 hover:decoration-amber-500 transition-colors",target:"_blank",rel:"noopener noreferrer",children:A})}})}),w.jsx("div",{className:"mt-16 pt-8 border-t border-slate-200",children:w.jsxs("div",{className:"flex items-center justify-between gap-4",children:[w.jsxs("button",{onClick:M,disabled:a===0,className:`flex-1 max-w-xs text-left p-4 rounded-xl border transition-colors ${a===0?"border-slate-100 text-slate-300 cursor-not-allowed":"border-slate-200 hover:border-amber-300 hover:bg-amber-50 text-slate-600"}`,children:[w.jsxs("div",{className:"flex items-center gap-2 text-sm mb-1",children:[w.jsx(SL,{size:14}),w.jsx("span",{children:r==="ja"?"前へ":"上一节"})]}),a>0&&w.jsx("div",{className:"text-slate-800 font-medium truncate",children:i[a-1].section.title[r]})]}),w.jsxs("div",{className:"text-slate-400 text-sm hidden sm:block",children:[a+1," / ",i.length]}),w.jsxs("button",{onClick:T,disabled:a===i.length-1,className:`flex-1 max-w-xs text-right p-4 rounded-xl border transition-colors ${a===i.length-1?"border-slate-100 text-slate-300 cursor-not-allowed":"border-slate-200 hover:border-amber-300 hover:bg-amber-50 text-slate-600"}`,children:[w.jsxs("div",{className:"flex items-center justify-end gap-2 text-sm mb-1",children:[w.jsx("span",{children:r==="ja"?"次へ":"下一节"}),w.jsx(FL,{size:14})]}),a<i.length-1&&w.jsx("div",{className:"text-slate-800 font-medium truncate",children:i[a+1].section.title[r]})]})]})}),w.jsx("div",{className:"mt-8 text-center text-slate-400 text-sm",children:r==="ja"?"← → キーでページ移動":"使用 ← → 方向键翻页"})]})})})]})]})};export{CH as default};
